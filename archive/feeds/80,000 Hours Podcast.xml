<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="stylesheet.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:podcast="https://podcastindex.org/namespace/1.0">
  <channel>
    <atom:link rel="self" type="application/atom+xml" href="https://feeds.transistor.fm/80000-hours-podcast" title="MP3 Audio"/>
    <atom:link rel="hub" href="https://pubsubhubbub.appspot.com/"/>
    <podcast:podping usesPodping="true"/>
    <title>80,000 Hours Podcast</title>
    <generator>Transistor (https://transistor.fm)</generator>
    <itunes:new-feed-url>https://feeds.transistor.fm/80000-hours-podcast</itunes:new-feed-url>
    <description>Unusually in-depth conversations about the world's most pressing problems and what you can do to solve them. 

Subscribe by searching for '80000 Hours' wherever you get podcasts.

Hosted by Rob Wiblin and Luisa Rodriguez.</description>
    <copyright>All rights reserved</copyright>
    <podcast:guid>76e4b96a-5629-54ea-a7c8-672a9e167714</podcast:guid>
    <podcast:locked owner="podcast@80000hours.org">yes</podcast:locked>
    <podcast:trailer pubdate="Mon, 01 May 2017 21:39:00 +0000" url="https://media.transistor.fm/db72a630/7f68fba4.mp3" length="1945265" type="audio/mpeg">#0 – Introducing the 80,000 Hours Podcast</podcast:trailer>
    <language>en</language>
    <pubDate>Tue, 10 Jun 2025 23:46:42 +0000</pubDate>
    <lastBuildDate>Wed, 11 Jun 2025 23:08:17 +0000</lastBuildDate>
    <link>https://80000hours.org/podcast/</link>
    <image>
      <url>https://img.transistor.fm/mZHicC8ElmGVefbRETEcnad2anuxGP3MIEK85yIIi6U/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9zaG93/LzQxNDAyLzE2ODM1/NDQ1NDAtYXJ0d29y/ay5qcGc.jpg</url>
      <title>80,000 Hours Podcast</title>
      <link>https://80000hours.org/podcast/</link>
    </image>
    <itunes:category text="Education"/>
    <itunes:category text="Technology"/>
    <itunes:type>episodic</itunes:type>
    <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
    <itunes:image href="https://img.transistor.fm/mZHicC8ElmGVefbRETEcnad2anuxGP3MIEK85yIIi6U/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9zaG93/LzQxNDAyLzE2ODM1/NDQ1NDAtYXJ0d29y/ay5qcGc.jpg"/>
    <itunes:summary>Unusually in-depth conversations about the world's most pressing problems and what you can do to solve them. 

Subscribe by searching for '80000 Hours' wherever you get podcasts.

Hosted by Rob Wiblin and Luisa Rodriguez.</itunes:summary>
    <itunes:subtitle>Unusually in-depth conversations about the world's most pressing problems and what you can do to solve them.</itunes:subtitle>
    <itunes:keywords></itunes:keywords>
    <itunes:owner>
      <itunes:name>The 80,000 Hours Podcast</itunes:name>
    </itunes:owner>
    <itunes:complete>No</itunes:complete>
    <itunes:explicit>No</itunes:explicit>
    <item>
      <title>#217 – Beth Barnes on the most important graph in AI right now — and the 7-month rule that governs its progress</title>
      <itunes:title>#217 – Beth Barnes on the most important graph in AI right now — and the 7-month rule that governs its progress</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">9688e100-3f5a-4fbd-9c24-6cf105c61723</guid>
      <link>https://80000hours.org/podcast/episodes/beth-barnes-ai-safety-evals/?utm_campaign=podcast__beth-barnes&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>AI models today have a 50% chance of successfully completing a task that would take an expert human one hour. Seven months ago, that number was roughly 30 minutes — and seven months before that, 15 minutes. (See <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/"><strong>graph</strong></a>.)</p><p>These are substantial, multi-step tasks requiring sustained focus: building web applications, conducting machine learning research, or solving complex programming challenges.</p><p>Today’s guest, Beth Barnes, is CEO of <a href="https://metr.org/">METR</a> (Model Evaluation &amp; Threat Research) — the leading organisation measuring these capabilities.</p><p><a href="https://80k.info/bb"><strong>Links to learn more, video, highlights, and full transcript</strong></a>: <a href="https://80k.info/bb">https://80k.info/bb</a></p><p>Beth's team has been timing how long it takes skilled humans to complete projects of varying length, then seeing how AI models perform on the same work. The resulting paper “<a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Measuring AI ability to complete long tasks</a>” made waves by revealing that the planning horizon of AI models was doubling roughly every seven months. It's regarded by many as the most useful AI forecasting work in years.</p><p>Beth has found models can already do “meaningful work” improving themselves, and she wouldn’t be surprised if AI models were able to autonomously self-improve as little as two years from now — in fact, “It seems hard to rule out even shorter [timelines]. Is there 1% chance of this happening in six, nine months? Yeah, that seems pretty plausible.”</p><p>Beth adds:</p>The sense I really want to dispel is, “But the experts must be on top of this. The experts would be telling us if it really was time to freak out.” The experts are not on top of this. Inasmuch as there are experts, they are saying that this is a concerning risk. … And to the extent that I am an expert, I am an expert telling you you should freak out.<p><br><a href="https://forms.gle/sFuDkoznxBcHPVmX6"><strong>What did you think of this episode?</strong></a> <a href="https://forms.gle/sFuDkoznxBcHPVmX6">https://forms.gle/sFuDkoznxBcHPVmX6</a></p><p><br><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who is Beth Barnes? (00:01:19)</li><li>Can we see AI scheming in the chain of thought? (00:01:52)</li><li>The chain of thought is essential for safety checking (00:08:58)</li><li>Alignment faking in large language models (00:12:24)</li><li>We have to test model honesty even before they're used inside AI companies (00:16:48)</li><li>We have to test models when unruly and unconstrained (00:25:57)</li><li>Each 7 months models can do tasks twice as long (00:30:40)</li><li>METR's research finds AIs are solid at AI research already (00:49:33)</li><li>AI may turn out to be strong at novel and creative research (00:55:53)</li><li>When can we expect an algorithmic 'intelligence explosion'? (00:59:11)</li><li>Recursively self-improving AI might even be here in two years — which is alarming (01:05:02)</li><li>Could evaluations backfire by increasing AI hype and racing? (01:11:36)</li><li>Governments first ignore new risks, but can overreact once they arrive (01:26:38)</li><li>Do we need external auditors doing AI safety tests, not just the companies themselves? (01:35:10)</li><li>A case against safety-focused people working at frontier AI companies (01:48:44)</li><li>The new, more dire situation has forced changes to METR's strategy (02:02:29)</li><li>AI companies are being locally reasonable, but globally reckless (02:10:31)</li><li>Overrated: Interpretability research (02:15:11)</li><li>Underrated: Developing more narrow AIs (02:17:01)</li><li>Underrated: Helping humans judge confusing model outputs (02:23:36)</li><li>Overrated: Major AI companies' contributions to safety research (02:25:52)</li><li>Could we have a science of translating AI models' nonhuman language or neuralese? (02:29:24)</li><li>Could we ban using AI to enhance AI, or is that just naive? (02:31:47)</li><li>Open-weighting models is often good, and Beth has changed her attitude to it (02:37:52)</li><li>What we can learn about AGI from the nuclear arms race (02:42:25)</li><li>Infosec is so bad that no models are truly closed-weight models (02:57:24)</li><li>AI is more like bioweapons because it undermines the leading power (03:02:02)</li><li>What METR can do best that others can't (03:12:09)</li><li>What METR isn't doing that other people have to step up and do (03:27:07)</li><li>What research METR plans to do next (03:32:09)</li></ul><p><em>This episode was originally recorded on February 17, 2025.<br></em><br></p><p><em>Video editing: Luke Monsour and Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>AI models today have a 50% chance of successfully completing a task that would take an expert human one hour. Seven months ago, that number was roughly 30 minutes — and seven months before that, 15 minutes. (See <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/"><strong>graph</strong></a>.)</p><p>These are substantial, multi-step tasks requiring sustained focus: building web applications, conducting machine learning research, or solving complex programming challenges.</p><p>Today’s guest, Beth Barnes, is CEO of <a href="https://metr.org/">METR</a> (Model Evaluation &amp; Threat Research) — the leading organisation measuring these capabilities.</p><p><a href="https://80k.info/bb"><strong>Links to learn more, video, highlights, and full transcript</strong></a>: <a href="https://80k.info/bb">https://80k.info/bb</a></p><p>Beth's team has been timing how long it takes skilled humans to complete projects of varying length, then seeing how AI models perform on the same work. The resulting paper “<a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Measuring AI ability to complete long tasks</a>” made waves by revealing that the planning horizon of AI models was doubling roughly every seven months. It's regarded by many as the most useful AI forecasting work in years.</p><p>Beth has found models can already do “meaningful work” improving themselves, and she wouldn’t be surprised if AI models were able to autonomously self-improve as little as two years from now — in fact, “It seems hard to rule out even shorter [timelines]. Is there 1% chance of this happening in six, nine months? Yeah, that seems pretty plausible.”</p><p>Beth adds:</p>The sense I really want to dispel is, “But the experts must be on top of this. The experts would be telling us if it really was time to freak out.” The experts are not on top of this. Inasmuch as there are experts, they are saying that this is a concerning risk. … And to the extent that I am an expert, I am an expert telling you you should freak out.<p><br><a href="https://forms.gle/sFuDkoznxBcHPVmX6"><strong>What did you think of this episode?</strong></a> <a href="https://forms.gle/sFuDkoznxBcHPVmX6">https://forms.gle/sFuDkoznxBcHPVmX6</a></p><p><br><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who is Beth Barnes? (00:01:19)</li><li>Can we see AI scheming in the chain of thought? (00:01:52)</li><li>The chain of thought is essential for safety checking (00:08:58)</li><li>Alignment faking in large language models (00:12:24)</li><li>We have to test model honesty even before they're used inside AI companies (00:16:48)</li><li>We have to test models when unruly and unconstrained (00:25:57)</li><li>Each 7 months models can do tasks twice as long (00:30:40)</li><li>METR's research finds AIs are solid at AI research already (00:49:33)</li><li>AI may turn out to be strong at novel and creative research (00:55:53)</li><li>When can we expect an algorithmic 'intelligence explosion'? (00:59:11)</li><li>Recursively self-improving AI might even be here in two years — which is alarming (01:05:02)</li><li>Could evaluations backfire by increasing AI hype and racing? (01:11:36)</li><li>Governments first ignore new risks, but can overreact once they arrive (01:26:38)</li><li>Do we need external auditors doing AI safety tests, not just the companies themselves? (01:35:10)</li><li>A case against safety-focused people working at frontier AI companies (01:48:44)</li><li>The new, more dire situation has forced changes to METR's strategy (02:02:29)</li><li>AI companies are being locally reasonable, but globally reckless (02:10:31)</li><li>Overrated: Interpretability research (02:15:11)</li><li>Underrated: Developing more narrow AIs (02:17:01)</li><li>Underrated: Helping humans judge confusing model outputs (02:23:36)</li><li>Overrated: Major AI companies' contributions to safety research (02:25:52)</li><li>Could we have a science of translating AI models' nonhuman language or neuralese? (02:29:24)</li><li>Could we ban using AI to enhance AI, or is that just naive? (02:31:47)</li><li>Open-weighting models is often good, and Beth has changed her attitude to it (02:37:52)</li><li>What we can learn about AGI from the nuclear arms race (02:42:25)</li><li>Infosec is so bad that no models are truly closed-weight models (02:57:24)</li><li>AI is more like bioweapons because it undermines the leading power (03:02:02)</li><li>What METR can do best that others can't (03:12:09)</li><li>What METR isn't doing that other people have to step up and do (03:27:07)</li><li>What research METR plans to do next (03:32:09)</li></ul><p><em>This episode was originally recorded on February 17, 2025.<br></em><br></p><p><em>Video editing: Luke Monsour and Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 02 Jun 2025 16:03:24 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/be4d8ef6/082fe5d7.mp3" length="109084288" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/gQZ4zf59ffPRoBeCDpfuNZm3giIp42o1CpU0Eu6aNPk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lMzY5/Zjk2YmIxMzFjNzhl/NGNkOTAwYjY0YWZl/NmYxNi5qcGc.jpg"/>
      <itunes:duration>13629</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>AI models today have a 50% chance of successfully completing a task that would take an expert human one hour. Seven months ago, that number was roughly 30 minutes — and seven months before that, 15 minutes. (See <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/"><strong>graph</strong></a>.)</p><p>These are substantial, multi-step tasks requiring sustained focus: building web applications, conducting machine learning research, or solving complex programming challenges.</p><p>Today’s guest, Beth Barnes, is CEO of <a href="https://metr.org/">METR</a> (Model Evaluation &amp; Threat Research) — the leading organisation measuring these capabilities.</p><p><a href="https://80k.info/bb"><strong>Links to learn more, video, highlights, and full transcript</strong></a>: <a href="https://80k.info/bb">https://80k.info/bb</a></p><p>Beth's team has been timing how long it takes skilled humans to complete projects of varying length, then seeing how AI models perform on the same work. The resulting paper “<a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Measuring AI ability to complete long tasks</a>” made waves by revealing that the planning horizon of AI models was doubling roughly every seven months. It's regarded by many as the most useful AI forecasting work in years.</p><p>Beth has found models can already do “meaningful work” improving themselves, and she wouldn’t be surprised if AI models were able to autonomously self-improve as little as two years from now — in fact, “It seems hard to rule out even shorter [timelines]. Is there 1% chance of this happening in six, nine months? Yeah, that seems pretty plausible.”</p><p>Beth adds:</p>The sense I really want to dispel is, “But the experts must be on top of this. The experts would be telling us if it really was time to freak out.” The experts are not on top of this. Inasmuch as there are experts, they are saying that this is a concerning risk. … And to the extent that I am an expert, I am an expert telling you you should freak out.<p><br><a href="https://forms.gle/sFuDkoznxBcHPVmX6"><strong>What did you think of this episode?</strong></a> <a href="https://forms.gle/sFuDkoznxBcHPVmX6">https://forms.gle/sFuDkoznxBcHPVmX6</a></p><p><br><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who is Beth Barnes? (00:01:19)</li><li>Can we see AI scheming in the chain of thought? (00:01:52)</li><li>The chain of thought is essential for safety checking (00:08:58)</li><li>Alignment faking in large language models (00:12:24)</li><li>We have to test model honesty even before they're used inside AI companies (00:16:48)</li><li>We have to test models when unruly and unconstrained (00:25:57)</li><li>Each 7 months models can do tasks twice as long (00:30:40)</li><li>METR's research finds AIs are solid at AI research already (00:49:33)</li><li>AI may turn out to be strong at novel and creative research (00:55:53)</li><li>When can we expect an algorithmic 'intelligence explosion'? (00:59:11)</li><li>Recursively self-improving AI might even be here in two years — which is alarming (01:05:02)</li><li>Could evaluations backfire by increasing AI hype and racing? (01:11:36)</li><li>Governments first ignore new risks, but can overreact once they arrive (01:26:38)</li><li>Do we need external auditors doing AI safety tests, not just the companies themselves? (01:35:10)</li><li>A case against safety-focused people working at frontier AI companies (01:48:44)</li><li>The new, more dire situation has forced changes to METR's strategy (02:02:29)</li><li>AI companies are being locally reasonable, but globally reckless (02:10:31)</li><li>Overrated: Interpretability research (02:15:11)</li><li>Underrated: Developing more narrow AIs (02:17:01)</li><li>Underrated: Helping humans judge confusing model outputs (02:23:36)</li><li>Overrated: Major AI companies' contributions to safety research (02:25:52)</li><li>Could we have a science of translating AI models' nonhuman language or neuralese? (02:29:24)</li><li>Could we ban using AI to enhance AI, or is that just naive? (02:31:47)</li><li>Open-weighting models is often good, and Beth has changed her attitude to it (02:37:52)</li><li>What we can learn about AGI from the nuclear arms race (02:42:25)</li><li>Infosec is so bad that no models are truly closed-weight models (02:57:24)</li><li>AI is more like bioweapons because it undermines the leading power (03:02:02)</li><li>What METR can do best that others can't (03:12:09)</li><li>What METR isn't doing that other people have to step up and do (03:27:07)</li><li>What research METR plans to do next (03:32:09)</li></ul><p><em>This episode was originally recorded on February 17, 2025.<br></em><br></p><p><em>Video editing: Luke Monsour and Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/be4d8ef6/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/be4d8ef6/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Beyond human minds: The bewildering frontier of consciousness in insects, AI, and more</title>
      <itunes:title>Beyond human minds: The bewildering frontier of consciousness in insects, AI, and more</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d30cafed-3063-4039-939a-cb5d8eda73b8</guid>
      <link>https://80000hours.org/podcast/episodes/nonhuman-sentience-consciousness-compilation/?utm_campaign=podcast__nonhuman-sentience&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>What if there’s something it’s like to be a shrimp — or a chatbot?</p><p>For centuries, humans have debated the nature of consciousness, often placing ourselves at the very top. But what about the minds of others — both the animals we share this planet with and the artificial intelligences we’re creating?</p><p>We’ve pulled together clips from past conversations with researchers and philosophers who’ve spent years trying to make sense of animal consciousness, artificial sentience, and moral consideration under deep uncertainty.</p><p><a href="https://80k.info/nhs"><strong>Links to learn more and full transcript</strong></a>: <a href="https://80k.info/nhs">https://80k.info/nhs</a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>Robert Long on what we should picture when we think about artificial sentience (00:02:49)</li><li>Jeff Sebo on what the threshold is for AI systems meriting moral consideration (00:07:22)</li><li>Meghan Barrett on the evolutionary argument for insect sentience (00:11:24)</li><li>Andrés Jiménez Zorrilla on whether there’s something it’s like to be a shrimp (00:15:09)</li><li>Jonathan Birch on the cautionary tale of newborn pain (00:21:53)</li><li>David Chalmers on why artificial consciousness is possible (00:26:12)</li><li>Holden Karnofsky on how we’ll see digital people as... people (00:32:18)</li><li>Jeff Sebo on grappling with our biases and ignorance when thinking about sentience (00:38:59)</li><li>Bob Fischer on how to think about the moral weight of a chicken (00:49:37)</li><li>Cameron Meyer Shorb on the range of suffering in wild animals (01:01:41)</li><li>Sébastien Moro on whether fish are conscious or sentient (01:11:17)</li><li>David Chalmers on when to start worrying about artificial consciousness (01:16:36)</li><li>Robert Long on how we might stumble into causing AI systems enormous suffering (01:21:04)</li><li>Jonathan Birch on how we might accidentally create artificial sentience (01:26:13)</li><li>Anil Seth on which parts of the brain are required for consciousness (01:32:33)</li><li>Peter Godfrey-Smith on uploads of ourselves (01:44:47)</li><li>Jonathan Birch on treading lightly around the “edge cases” of sentience (02:00:12)</li><li>Meghan Barrett on whether brain size and sentience are related (02:05:25)</li><li>Lewis Bollard on how animal advocacy has changed in response to sentience studies (02:12:01)</li><li>Bob Fischer on using proxies to determine sentience (02:22:27)</li><li>Cameron Meyer Shorb on how we can practically study wild animals’ subjective experiences (02:26:28)</li><li>Jeff Sebo on the problem of false positives in assessing artificial sentience (02:33:16)</li><li>Stuart Russell on the moral rights of AIs (02:38:31)</li><li>Buck Shlegeris on whether AI control strategies make humans the bad guys (02:41:50)</li><li>Meghan Barrett on why she can’t be totally confident about insect sentience (02:47:12)</li><li>Bob Fischer on what surprised him most about the findings of the Moral Weight Project (02:58:30)</li><li>Jeff Sebo on why we’re likely to sleepwalk into causing massive amounts of suffering in AI systems (03:02:46)</li><li>Will MacAskill on the rights of future digital beings (03:05:29)</li><li>Carl Shulman on sharing the world with digital minds (03:19:25)</li><li>Luisa's outro (03:33:43)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Additional content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>What if there’s something it’s like to be a shrimp — or a chatbot?</p><p>For centuries, humans have debated the nature of consciousness, often placing ourselves at the very top. But what about the minds of others — both the animals we share this planet with and the artificial intelligences we’re creating?</p><p>We’ve pulled together clips from past conversations with researchers and philosophers who’ve spent years trying to make sense of animal consciousness, artificial sentience, and moral consideration under deep uncertainty.</p><p><a href="https://80k.info/nhs"><strong>Links to learn more and full transcript</strong></a>: <a href="https://80k.info/nhs">https://80k.info/nhs</a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>Robert Long on what we should picture when we think about artificial sentience (00:02:49)</li><li>Jeff Sebo on what the threshold is for AI systems meriting moral consideration (00:07:22)</li><li>Meghan Barrett on the evolutionary argument for insect sentience (00:11:24)</li><li>Andrés Jiménez Zorrilla on whether there’s something it’s like to be a shrimp (00:15:09)</li><li>Jonathan Birch on the cautionary tale of newborn pain (00:21:53)</li><li>David Chalmers on why artificial consciousness is possible (00:26:12)</li><li>Holden Karnofsky on how we’ll see digital people as... people (00:32:18)</li><li>Jeff Sebo on grappling with our biases and ignorance when thinking about sentience (00:38:59)</li><li>Bob Fischer on how to think about the moral weight of a chicken (00:49:37)</li><li>Cameron Meyer Shorb on the range of suffering in wild animals (01:01:41)</li><li>Sébastien Moro on whether fish are conscious or sentient (01:11:17)</li><li>David Chalmers on when to start worrying about artificial consciousness (01:16:36)</li><li>Robert Long on how we might stumble into causing AI systems enormous suffering (01:21:04)</li><li>Jonathan Birch on how we might accidentally create artificial sentience (01:26:13)</li><li>Anil Seth on which parts of the brain are required for consciousness (01:32:33)</li><li>Peter Godfrey-Smith on uploads of ourselves (01:44:47)</li><li>Jonathan Birch on treading lightly around the “edge cases” of sentience (02:00:12)</li><li>Meghan Barrett on whether brain size and sentience are related (02:05:25)</li><li>Lewis Bollard on how animal advocacy has changed in response to sentience studies (02:12:01)</li><li>Bob Fischer on using proxies to determine sentience (02:22:27)</li><li>Cameron Meyer Shorb on how we can practically study wild animals’ subjective experiences (02:26:28)</li><li>Jeff Sebo on the problem of false positives in assessing artificial sentience (02:33:16)</li><li>Stuart Russell on the moral rights of AIs (02:38:31)</li><li>Buck Shlegeris on whether AI control strategies make humans the bad guys (02:41:50)</li><li>Meghan Barrett on why she can’t be totally confident about insect sentience (02:47:12)</li><li>Bob Fischer on what surprised him most about the findings of the Moral Weight Project (02:58:30)</li><li>Jeff Sebo on why we’re likely to sleepwalk into causing massive amounts of suffering in AI systems (03:02:46)</li><li>Will MacAskill on the rights of future digital beings (03:05:29)</li><li>Carl Shulman on sharing the world with digital minds (03:19:25)</li><li>Luisa's outro (03:33:43)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Additional content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 23 May 2025 14:33:05 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3bbeb269/f47b4150.mp3" length="206189932" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/WvubhnJalnrJRCKLVgyLM4lzbHEhYthxQK0tciHSsLY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS81NTQ0/YTQwMTQyNjFjMjBm/OTMxNDQzMWVhYzZh/YzQzNC5qcGc.jpg"/>
      <itunes:duration>12880</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>What if there’s something it’s like to be a shrimp — or a chatbot?</p><p>For centuries, humans have debated the nature of consciousness, often placing ourselves at the very top. But what about the minds of others — both the animals we share this planet with and the artificial intelligences we’re creating?</p><p>We’ve pulled together clips from past conversations with researchers and philosophers who’ve spent years trying to make sense of animal consciousness, artificial sentience, and moral consideration under deep uncertainty.</p><p><a href="https://80k.info/nhs"><strong>Links to learn more and full transcript</strong></a>: <a href="https://80k.info/nhs">https://80k.info/nhs</a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>Robert Long on what we should picture when we think about artificial sentience (00:02:49)</li><li>Jeff Sebo on what the threshold is for AI systems meriting moral consideration (00:07:22)</li><li>Meghan Barrett on the evolutionary argument for insect sentience (00:11:24)</li><li>Andrés Jiménez Zorrilla on whether there’s something it’s like to be a shrimp (00:15:09)</li><li>Jonathan Birch on the cautionary tale of newborn pain (00:21:53)</li><li>David Chalmers on why artificial consciousness is possible (00:26:12)</li><li>Holden Karnofsky on how we’ll see digital people as... people (00:32:18)</li><li>Jeff Sebo on grappling with our biases and ignorance when thinking about sentience (00:38:59)</li><li>Bob Fischer on how to think about the moral weight of a chicken (00:49:37)</li><li>Cameron Meyer Shorb on the range of suffering in wild animals (01:01:41)</li><li>Sébastien Moro on whether fish are conscious or sentient (01:11:17)</li><li>David Chalmers on when to start worrying about artificial consciousness (01:16:36)</li><li>Robert Long on how we might stumble into causing AI systems enormous suffering (01:21:04)</li><li>Jonathan Birch on how we might accidentally create artificial sentience (01:26:13)</li><li>Anil Seth on which parts of the brain are required for consciousness (01:32:33)</li><li>Peter Godfrey-Smith on uploads of ourselves (01:44:47)</li><li>Jonathan Birch on treading lightly around the “edge cases” of sentience (02:00:12)</li><li>Meghan Barrett on whether brain size and sentience are related (02:05:25)</li><li>Lewis Bollard on how animal advocacy has changed in response to sentience studies (02:12:01)</li><li>Bob Fischer on using proxies to determine sentience (02:22:27)</li><li>Cameron Meyer Shorb on how we can practically study wild animals’ subjective experiences (02:26:28)</li><li>Jeff Sebo on the problem of false positives in assessing artificial sentience (02:33:16)</li><li>Stuart Russell on the moral rights of AIs (02:38:31)</li><li>Buck Shlegeris on whether AI control strategies make humans the bad guys (02:41:50)</li><li>Meghan Barrett on why she can’t be totally confident about insect sentience (02:47:12)</li><li>Bob Fischer on what surprised him most about the findings of the Moral Weight Project (02:58:30)</li><li>Jeff Sebo on why we’re likely to sleepwalk into causing massive amounts of suffering in AI systems (03:02:46)</li><li>Will MacAskill on the rights of future digital beings (03:05:29)</li><li>Carl Shulman on sharing the world with digital minds (03:19:25)</li><li>Luisa's outro (03:33:43)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Additional content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/3bbeb269/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/3bbeb269/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Don’t believe OpenAI’s “nonprofit” spin (emergency pod with Tyler Whitmer)</title>
      <itunes:title>Don’t believe OpenAI’s “nonprofit” spin (emergency pod with Tyler Whitmer)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b5cbc3f9-bed0-465b-ab3d-90c61e36ffb9</guid>
      <link>https://80000hours.org/podcast/episodes/tyler-whitmer-openai-nonprofit-restructure-control/?utm_campaign=podcast__tyler-whitmer&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>OpenAI’s <a href="https://openai.com/index/evolving-our-structure/">recent announcement</a> that its nonprofit would “retain control” of its for-profit business sounds reassuring. But this seemingly major concession, celebrated by so many, is in itself largely meaningless.</p><p>Litigator Tyler Whitmer is a coauthor of a <a href="https://notforprivategain.org/follow-up"><strong>newly published letter</strong></a> that describes this attempted sleight of hand and directs regulators on how to stop it.</p><p>As Tyler explains, the plan both before and after this announcement has been to convert OpenAI into a Delaware public benefit corporation (PBC) — and this alone will dramatically weaken the nonprofit’s ability to direct the business in pursuit of its charitable purpose: ensuring AGI is safe and “benefits all of humanity.”</p><p>Right now, the nonprofit directly controls the business. But were OpenAI to become a PBC, the nonprofit, rather than having its “hand on the lever,” would merely contribute to the decision of who does.</p><p>Why does this matter? Today, if OpenAI’s commercial arm were about to release an unhinged AI model that might make money but be bad for humanity, the nonprofit could directly intervene to stop it. In the proposed new structure, it likely couldn’t do much at all.</p><p>But it’s even worse than that: even if the nonprofit could select the PBC’s directors, those directors would have fundamentally different legal obligations from those of the nonprofit. A PBC director must <em>balance</em> public benefit with the interests of profit-driven shareholders — by default, they cannot legally prioritise public interest over profits, even if they and the controlling shareholder that appointed them want to do so.</p><p>As Tyler points out, there isn’t a single reported case of a shareholder successfully suing to enforce a PBC’s public benefit mission in the 10+ years since the Delaware PBC statute was enacted.</p><p>This extra step from the nonprofit to the PBC would also mean that the attorneys general of California and Delaware — who today are empowered to ensure the nonprofit pursues its mission — would find themselves powerless to act. These are probably not side effects but rather a Trojan horse for-profit investors are trying to slip past regulators.</p><p>Fortunately this can all be addressed — but it requires either the nonprofit board or the attorneys general of California and Delaware to promptly put their foot down and insist on watertight legal agreements that preserve OpenAI’s current governance safeguards and enforcement mechanisms.</p><p>As Tyler explains, the same arrangements that currently bind the OpenAI business have to be written into a new PBC’s certificate of incorporation — something that won’t happen by default and that powerful investors have every incentive to resist.</p><p><a href="https://80k.info/tw"><strong>Full transcript and links to learn more</strong></a>: <a href="https://80k.info/tw">https://80k.info/tw</a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who’s Tyler Whitmer? (00:01:35)</li><li>The new plan may be no improvement (00:02:04)</li><li>The public hasn't even been allowed to know what they are owed (00:06:55)</li><li>Issues beyond control (00:11:02)</li><li>The new directors wouldn’t have to pursue the current purpose (00:12:06)</li><li>The nonprofit might not even retain voting control (00:16:58)</li><li>The attorneys general could lose their enforcement oversight (00:22:11)</li><li>By default things go badly (00:29:09)</li><li>How to keep the mission in the restructure (00:32:25)</li><li>What will become of OpenAI’s Charter? (00:37:11)</li><li>Ways to make things better, and not just avoid them getting worse (00:42:38)</li><li>How the AGs can avoid being disempowered (00:48:35)</li><li>Retaining the power to fire the CEO (00:54:49)</li><li>Will the current board get a financial stake in OpenAI? (00:57:40)</li><li>Could the AGs insist the current nonprofit agreement be made public? (00:59:15)</li><li>How OpenAI is valued should be transparent and scrutinised (01:01:00)</li><li>Investors aren't bad people, but they can't be trusted either (01:06:05)</li></ul><p><em>This episode was originally recorded on May 13, 2025.<br></em><br></p><p><em>Video editing: Simon Monsour and Luke Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>OpenAI’s <a href="https://openai.com/index/evolving-our-structure/">recent announcement</a> that its nonprofit would “retain control” of its for-profit business sounds reassuring. But this seemingly major concession, celebrated by so many, is in itself largely meaningless.</p><p>Litigator Tyler Whitmer is a coauthor of a <a href="https://notforprivategain.org/follow-up"><strong>newly published letter</strong></a> that describes this attempted sleight of hand and directs regulators on how to stop it.</p><p>As Tyler explains, the plan both before and after this announcement has been to convert OpenAI into a Delaware public benefit corporation (PBC) — and this alone will dramatically weaken the nonprofit’s ability to direct the business in pursuit of its charitable purpose: ensuring AGI is safe and “benefits all of humanity.”</p><p>Right now, the nonprofit directly controls the business. But were OpenAI to become a PBC, the nonprofit, rather than having its “hand on the lever,” would merely contribute to the decision of who does.</p><p>Why does this matter? Today, if OpenAI’s commercial arm were about to release an unhinged AI model that might make money but be bad for humanity, the nonprofit could directly intervene to stop it. In the proposed new structure, it likely couldn’t do much at all.</p><p>But it’s even worse than that: even if the nonprofit could select the PBC’s directors, those directors would have fundamentally different legal obligations from those of the nonprofit. A PBC director must <em>balance</em> public benefit with the interests of profit-driven shareholders — by default, they cannot legally prioritise public interest over profits, even if they and the controlling shareholder that appointed them want to do so.</p><p>As Tyler points out, there isn’t a single reported case of a shareholder successfully suing to enforce a PBC’s public benefit mission in the 10+ years since the Delaware PBC statute was enacted.</p><p>This extra step from the nonprofit to the PBC would also mean that the attorneys general of California and Delaware — who today are empowered to ensure the nonprofit pursues its mission — would find themselves powerless to act. These are probably not side effects but rather a Trojan horse for-profit investors are trying to slip past regulators.</p><p>Fortunately this can all be addressed — but it requires either the nonprofit board or the attorneys general of California and Delaware to promptly put their foot down and insist on watertight legal agreements that preserve OpenAI’s current governance safeguards and enforcement mechanisms.</p><p>As Tyler explains, the same arrangements that currently bind the OpenAI business have to be written into a new PBC’s certificate of incorporation — something that won’t happen by default and that powerful investors have every incentive to resist.</p><p><a href="https://80k.info/tw"><strong>Full transcript and links to learn more</strong></a>: <a href="https://80k.info/tw">https://80k.info/tw</a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who’s Tyler Whitmer? (00:01:35)</li><li>The new plan may be no improvement (00:02:04)</li><li>The public hasn't even been allowed to know what they are owed (00:06:55)</li><li>Issues beyond control (00:11:02)</li><li>The new directors wouldn’t have to pursue the current purpose (00:12:06)</li><li>The nonprofit might not even retain voting control (00:16:58)</li><li>The attorneys general could lose their enforcement oversight (00:22:11)</li><li>By default things go badly (00:29:09)</li><li>How to keep the mission in the restructure (00:32:25)</li><li>What will become of OpenAI’s Charter? (00:37:11)</li><li>Ways to make things better, and not just avoid them getting worse (00:42:38)</li><li>How the AGs can avoid being disempowered (00:48:35)</li><li>Retaining the power to fire the CEO (00:54:49)</li><li>Will the current board get a financial stake in OpenAI? (00:57:40)</li><li>Could the AGs insist the current nonprofit agreement be made public? (00:59:15)</li><li>How OpenAI is valued should be transparent and scrutinised (01:01:00)</li><li>Investors aren't bad people, but they can't be trusted either (01:06:05)</li></ul><p><em>This episode was originally recorded on May 13, 2025.<br></em><br></p><p><em>Video editing: Simon Monsour and Luke Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 15 May 2025 15:51:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/bba94b22/761aa11b.mp3" length="69217508" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/FVbijiEF9neUX_Vy16HXxr0sJBqXkHwCyKy5RwlhprU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mM2Q2/NTc3NmVjNGFhMDU3/MmFkZWE3NGI1YzJm/YjVmZC5qcGVn.jpg"/>
      <itunes:duration>4324</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>OpenAI’s <a href="https://openai.com/index/evolving-our-structure/">recent announcement</a> that its nonprofit would “retain control” of its for-profit business sounds reassuring. But this seemingly major concession, celebrated by so many, is in itself largely meaningless.</p><p>Litigator Tyler Whitmer is a coauthor of a <a href="https://notforprivategain.org/follow-up"><strong>newly published letter</strong></a> that describes this attempted sleight of hand and directs regulators on how to stop it.</p><p>As Tyler explains, the plan both before and after this announcement has been to convert OpenAI into a Delaware public benefit corporation (PBC) — and this alone will dramatically weaken the nonprofit’s ability to direct the business in pursuit of its charitable purpose: ensuring AGI is safe and “benefits all of humanity.”</p><p>Right now, the nonprofit directly controls the business. But were OpenAI to become a PBC, the nonprofit, rather than having its “hand on the lever,” would merely contribute to the decision of who does.</p><p>Why does this matter? Today, if OpenAI’s commercial arm were about to release an unhinged AI model that might make money but be bad for humanity, the nonprofit could directly intervene to stop it. In the proposed new structure, it likely couldn’t do much at all.</p><p>But it’s even worse than that: even if the nonprofit could select the PBC’s directors, those directors would have fundamentally different legal obligations from those of the nonprofit. A PBC director must <em>balance</em> public benefit with the interests of profit-driven shareholders — by default, they cannot legally prioritise public interest over profits, even if they and the controlling shareholder that appointed them want to do so.</p><p>As Tyler points out, there isn’t a single reported case of a shareholder successfully suing to enforce a PBC’s public benefit mission in the 10+ years since the Delaware PBC statute was enacted.</p><p>This extra step from the nonprofit to the PBC would also mean that the attorneys general of California and Delaware — who today are empowered to ensure the nonprofit pursues its mission — would find themselves powerless to act. These are probably not side effects but rather a Trojan horse for-profit investors are trying to slip past regulators.</p><p>Fortunately this can all be addressed — but it requires either the nonprofit board or the attorneys general of California and Delaware to promptly put their foot down and insist on watertight legal agreements that preserve OpenAI’s current governance safeguards and enforcement mechanisms.</p><p>As Tyler explains, the same arrangements that currently bind the OpenAI business have to be written into a new PBC’s certificate of incorporation — something that won’t happen by default and that powerful investors have every incentive to resist.</p><p><a href="https://80k.info/tw"><strong>Full transcript and links to learn more</strong></a>: <a href="https://80k.info/tw">https://80k.info/tw</a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who’s Tyler Whitmer? (00:01:35)</li><li>The new plan may be no improvement (00:02:04)</li><li>The public hasn't even been allowed to know what they are owed (00:06:55)</li><li>Issues beyond control (00:11:02)</li><li>The new directors wouldn’t have to pursue the current purpose (00:12:06)</li><li>The nonprofit might not even retain voting control (00:16:58)</li><li>The attorneys general could lose their enforcement oversight (00:22:11)</li><li>By default things go badly (00:29:09)</li><li>How to keep the mission in the restructure (00:32:25)</li><li>What will become of OpenAI’s Charter? (00:37:11)</li><li>Ways to make things better, and not just avoid them getting worse (00:42:38)</li><li>How the AGs can avoid being disempowered (00:48:35)</li><li>Retaining the power to fire the CEO (00:54:49)</li><li>Will the current board get a financial stake in OpenAI? (00:57:40)</li><li>Could the AGs insist the current nonprofit agreement be made public? (00:59:15)</li><li>How OpenAI is valued should be transparent and scrutinised (01:01:00)</li><li>Investors aren't bad people, but they can't be trusted either (01:06:05)</li></ul><p><em>This episode was originally recorded on May 13, 2025.<br></em><br></p><p><em>Video editing: Simon Monsour and Luke Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/bba94b22/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/bba94b22/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>The case for and against AGI by 2030 (article by Benjamin Todd)</title>
      <itunes:title>The case for and against AGI by 2030 (article by Benjamin Todd)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">fd116269-8a6c-497e-b40d-076f69cd4d78</guid>
      <link>https://80000hours.org/agi/guide/when-will-agi-arrive/</link>
      <description>
        <![CDATA[<p>More and more people have been saying that we might have AGI (artificial general intelligence) before 2030. Is that really plausible? </p><p>This article by Benjamin Todd looks into the cases for and against, and summarises the key things you need to know to understand the debate. You can see all the images and many footnotes in the <a href="https://80000hours.org/agi/guide/when-will-agi-arrive/">original article</a> on the 80,000 Hours website.</p><p>In a nutshell:</p><ul><li>Four key factors are driving AI progress: larger base models, teaching models to reason, increasing models’ thinking time, and building agent scaffolding for multi-step tasks. These are underpinned by increasing computational power to run and train AI systems, as well as increasing human capital going into algorithmic research.</li><li>All of these drivers are set to continue until 2028 and perhaps until 2032.</li><li>This means we should expect major further gains in AI performance. We don’t know how large they’ll be, but extrapolating recent trends on benchmarks suggests we’ll reach systems with beyond-human performance in coding and scientific reasoning, and that can autonomously complete multi-week projects.</li><li>Whether we call these systems ’AGI’ or not, they could be sufficient to enable AI research itself, robotics, the technology industry, and scientific research to accelerate — leading to transformative impacts.</li><li>Alternatively, AI might fail to overcome issues with ill-defined, high-context work over long time horizons and remain a tool (even if much improved compared to today).</li><li>Increasing AI performance requires exponential growth in investment and the research workforce. At current rates, we will likely start to reach bottlenecks around 2030. Simplifying a bit, that means we’ll likely either reach AGI by around 2030 or see progress slow significantly. Hybrid scenarios are also possible, but the next five years seem especially crucial.</li></ul><p><strong>Chapters:</strong></p><ul><li>Introduction (00:00:00)</li><li>The case for AGI by 2030 (00:00:33)</li><li>The article in a nutshell (00:04:04)</li><li>Section 1: What's driven recent AI progress? (00:05:46)</li><li>How we got here: the deep learning era (00:05:52)</li><li>Where are we now: the four key drivers (00:07:45)</li><li>Driver 1: Scaling pretraining (00:08:57)</li><li>Algorithmic efficiency (00:12:14)</li><li>How much further can pretraining scale? (00:14:22)</li><li>Driver 2: Training the models to reason (00:16:15)</li><li>How far can scaling reasoning continue? (00:22:06)</li><li>Driver 3: Increasing how long models think (00:25:01)</li><li>Driver 4: Building better agents (00:28:00)</li><li>How far can agent improvements continue? (00:33:40)</li><li>Section 2: How good will AI become by 2030? (00:35:59)</li><li>Trend extrapolation of AI capabilities (00:37:42)</li><li>What jobs would these systems help with? (00:39:59)</li><li>Software engineering (00:40:50)</li><li>Scientific research (00:42:13)</li><li>AI research (00:43:21)</li><li>What's the case against this? (00:44:30)</li><li>Additional resources on the sceptical view (00:49:18)</li><li>When do the 'experts' expect AGI? (00:49:50)</li><li>Section 3: Why the next 5 years are crucial (00:51:06)</li><li>Bottlenecks around 2030 (00:52:10)</li><li>Two potential futures for AI (00:56:02)</li><li>Conclusion (00:58:05)</li><li>Thanks for listening (00:59:27)</li></ul><p><em>Audio engineering: Dominic Armstrong<br>Music: Ben Cordell</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>More and more people have been saying that we might have AGI (artificial general intelligence) before 2030. Is that really plausible? </p><p>This article by Benjamin Todd looks into the cases for and against, and summarises the key things you need to know to understand the debate. You can see all the images and many footnotes in the <a href="https://80000hours.org/agi/guide/when-will-agi-arrive/">original article</a> on the 80,000 Hours website.</p><p>In a nutshell:</p><ul><li>Four key factors are driving AI progress: larger base models, teaching models to reason, increasing models’ thinking time, and building agent scaffolding for multi-step tasks. These are underpinned by increasing computational power to run and train AI systems, as well as increasing human capital going into algorithmic research.</li><li>All of these drivers are set to continue until 2028 and perhaps until 2032.</li><li>This means we should expect major further gains in AI performance. We don’t know how large they’ll be, but extrapolating recent trends on benchmarks suggests we’ll reach systems with beyond-human performance in coding and scientific reasoning, and that can autonomously complete multi-week projects.</li><li>Whether we call these systems ’AGI’ or not, they could be sufficient to enable AI research itself, robotics, the technology industry, and scientific research to accelerate — leading to transformative impacts.</li><li>Alternatively, AI might fail to overcome issues with ill-defined, high-context work over long time horizons and remain a tool (even if much improved compared to today).</li><li>Increasing AI performance requires exponential growth in investment and the research workforce. At current rates, we will likely start to reach bottlenecks around 2030. Simplifying a bit, that means we’ll likely either reach AGI by around 2030 or see progress slow significantly. Hybrid scenarios are also possible, but the next five years seem especially crucial.</li></ul><p><strong>Chapters:</strong></p><ul><li>Introduction (00:00:00)</li><li>The case for AGI by 2030 (00:00:33)</li><li>The article in a nutshell (00:04:04)</li><li>Section 1: What's driven recent AI progress? (00:05:46)</li><li>How we got here: the deep learning era (00:05:52)</li><li>Where are we now: the four key drivers (00:07:45)</li><li>Driver 1: Scaling pretraining (00:08:57)</li><li>Algorithmic efficiency (00:12:14)</li><li>How much further can pretraining scale? (00:14:22)</li><li>Driver 2: Training the models to reason (00:16:15)</li><li>How far can scaling reasoning continue? (00:22:06)</li><li>Driver 3: Increasing how long models think (00:25:01)</li><li>Driver 4: Building better agents (00:28:00)</li><li>How far can agent improvements continue? (00:33:40)</li><li>Section 2: How good will AI become by 2030? (00:35:59)</li><li>Trend extrapolation of AI capabilities (00:37:42)</li><li>What jobs would these systems help with? (00:39:59)</li><li>Software engineering (00:40:50)</li><li>Scientific research (00:42:13)</li><li>AI research (00:43:21)</li><li>What's the case against this? (00:44:30)</li><li>Additional resources on the sceptical view (00:49:18)</li><li>When do the 'experts' expect AGI? (00:49:50)</li><li>Section 3: Why the next 5 years are crucial (00:51:06)</li><li>Bottlenecks around 2030 (00:52:10)</li><li>Two potential futures for AI (00:56:02)</li><li>Conclusion (00:58:05)</li><li>Thanks for listening (00:59:27)</li></ul><p><em>Audio engineering: Dominic Armstrong<br>Music: Ben Cordell</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 12 May 2025 13:38:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/64f34231/52c01b0b.mp3" length="57749461" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/KSaqJsbWg60pK-H-UxOzMKeqLt2B6Rcz5xCd2wBYY-w/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8wNjc0/OWNmM2YyNTQ2YTY4/NGViZTIxYjI4Zjkx/MWFlMy5qcGc.jpg"/>
      <itunes:duration>3606</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>More and more people have been saying that we might have AGI (artificial general intelligence) before 2030. Is that really plausible? </p><p>This article by Benjamin Todd looks into the cases for and against, and summarises the key things you need to know to understand the debate. You can see all the images and many footnotes in the <a href="https://80000hours.org/agi/guide/when-will-agi-arrive/">original article</a> on the 80,000 Hours website.</p><p>In a nutshell:</p><ul><li>Four key factors are driving AI progress: larger base models, teaching models to reason, increasing models’ thinking time, and building agent scaffolding for multi-step tasks. These are underpinned by increasing computational power to run and train AI systems, as well as increasing human capital going into algorithmic research.</li><li>All of these drivers are set to continue until 2028 and perhaps until 2032.</li><li>This means we should expect major further gains in AI performance. We don’t know how large they’ll be, but extrapolating recent trends on benchmarks suggests we’ll reach systems with beyond-human performance in coding and scientific reasoning, and that can autonomously complete multi-week projects.</li><li>Whether we call these systems ’AGI’ or not, they could be sufficient to enable AI research itself, robotics, the technology industry, and scientific research to accelerate — leading to transformative impacts.</li><li>Alternatively, AI might fail to overcome issues with ill-defined, high-context work over long time horizons and remain a tool (even if much improved compared to today).</li><li>Increasing AI performance requires exponential growth in investment and the research workforce. At current rates, we will likely start to reach bottlenecks around 2030. Simplifying a bit, that means we’ll likely either reach AGI by around 2030 or see progress slow significantly. Hybrid scenarios are also possible, but the next five years seem especially crucial.</li></ul><p><strong>Chapters:</strong></p><ul><li>Introduction (00:00:00)</li><li>The case for AGI by 2030 (00:00:33)</li><li>The article in a nutshell (00:04:04)</li><li>Section 1: What's driven recent AI progress? (00:05:46)</li><li>How we got here: the deep learning era (00:05:52)</li><li>Where are we now: the four key drivers (00:07:45)</li><li>Driver 1: Scaling pretraining (00:08:57)</li><li>Algorithmic efficiency (00:12:14)</li><li>How much further can pretraining scale? (00:14:22)</li><li>Driver 2: Training the models to reason (00:16:15)</li><li>How far can scaling reasoning continue? (00:22:06)</li><li>Driver 3: Increasing how long models think (00:25:01)</li><li>Driver 4: Building better agents (00:28:00)</li><li>How far can agent improvements continue? (00:33:40)</li><li>Section 2: How good will AI become by 2030? (00:35:59)</li><li>Trend extrapolation of AI capabilities (00:37:42)</li><li>What jobs would these systems help with? (00:39:59)</li><li>Software engineering (00:40:50)</li><li>Scientific research (00:42:13)</li><li>AI research (00:43:21)</li><li>What's the case against this? (00:44:30)</li><li>Additional resources on the sceptical view (00:49:18)</li><li>When do the 'experts' expect AGI? (00:49:50)</li><li>Section 3: Why the next 5 years are crucial (00:51:06)</li><li>Bottlenecks around 2030 (00:52:10)</li><li>Two potential futures for AI (00:56:02)</li><li>Conclusion (00:58:05)</li><li>Thanks for listening (00:59:27)</li></ul><p><em>Audio engineering: Dominic Armstrong<br>Music: Ben Cordell</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/64f34231/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/64f34231/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Emergency pod: Did OpenAI give up, or is this just a new trap? (with Rose Chan Loui)</title>
      <itunes:title>Emergency pod: Did OpenAI give up, or is this just a new trap? (with Rose Chan Loui)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1e98482a-c1c5-4f64-b1d4-97e297d93ab1</guid>
      <link>https://80000hours.org/podcast/episodes/rose-chan-loui-openai-nonprofit-control/?utm_campaign=podcast__rose-chan-loui&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>When attorneys general intervene in corporate affairs, it usually means something has gone seriously wrong. In OpenAI’s case, it appears to have forced a dramatic reversal of the company’s plans to sideline its nonprofit foundation, announced in a <a href="https://openai.com/index/evolving-our-structure/">blog post</a> that made headlines worldwide.</p><p>The company’s sudden announcement that its nonprofit will “retain control” credits “constructive dialogue” with the attorneys general of California and Delaware — corporate-speak for what was likely a far more consequential confrontation behind closed doors. A confrontation perhaps driven by <a href="https://notforprivategain.org/">public pressure</a> from Nobel Prize winners, past OpenAI staff, and community organisations.</p><p>But whether this change will help depends entirely on the details of implementation — details that remain worryingly vague in the company’s announcement.</p><p>Return guest <a href="https://law.ucla.edu/faculty/faculty-profiles/rose-chan-loui">Rose Chan Loui</a>, nonprofit law expert at UCLA, sees potential in OpenAI’s new proposal, but emphasises that “control” must be carefully defined and enforced: “The words are great, but what’s going to back that up?” Without explicitly defining the nonprofit’s authority over safety decisions, the shift could be largely cosmetic.</p><p><a href="https://80k.info/rcl4"><strong>Links to learn more, video, and full transcript</strong></a>: <a href="https://80k.info/rcl4">https://80k.info/rcl4</a></p><p>Why have state officials taken such an interest so far? Host Rob Wiblin notes, “OpenAI was proposing that the AGs would no longer have any say over what this super momentous company might end up doing. … It was just crazy how they were suggesting that they would take all of the existing money and then pursue a completely different purpose.”</p><p>Now that they’re in the picture, the AGs have leverage to ensure the nonprofit maintains genuine control over issues of public safety as OpenAI develops increasingly powerful AI.</p><p>Rob and Rose explain three key areas where the AGs can make a huge difference to whether this plays out in the public’s best interest:</p><ol><li>Ensuring that the contractual agreements giving the nonprofit control over the new Delaware public benefit corporation are watertight, and don’t accidentally shut the AGs out of the picture.</li><li>Insisting that a majority of board members are truly independent by prohibiting indirect as well as direct financial stakes in the business.</li><li>Insisting that the board is empowered with the money, independent staffing, and access to information which they need to do their jobs.</li></ol><p><em>This episode was originally recorded on May 6, 2025.</em></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Rose is back! (00:01:06)</li><li>The nonprofit will stay 'in control' (00:01:28)</li><li>Backlash to OpenAI’s original plans (00:08:22)</li><li>The new proposal (00:16:33)</li><li>Giving up the super-profits (00:20:52)</li><li>Can the nonprofit maintain control of the company? (00:24:49)</li><li>Could for profit investors sue if profits aren't prioritised? (00:33:01)</li><li>The 6 governance safeguards at risk with the restructure (00:34:33)</li><li>Will the nonprofit’s giving just be corporate PR for the for-profit? (00:49:12)</li><li>Is this good, or not? (00:51:06)</li><li>Ways this could still go wrong – but reasons for optimism (00:54:19)</li></ul><p><em>Video editing: Simon Monsour and Luke Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>When attorneys general intervene in corporate affairs, it usually means something has gone seriously wrong. In OpenAI’s case, it appears to have forced a dramatic reversal of the company’s plans to sideline its nonprofit foundation, announced in a <a href="https://openai.com/index/evolving-our-structure/">blog post</a> that made headlines worldwide.</p><p>The company’s sudden announcement that its nonprofit will “retain control” credits “constructive dialogue” with the attorneys general of California and Delaware — corporate-speak for what was likely a far more consequential confrontation behind closed doors. A confrontation perhaps driven by <a href="https://notforprivategain.org/">public pressure</a> from Nobel Prize winners, past OpenAI staff, and community organisations.</p><p>But whether this change will help depends entirely on the details of implementation — details that remain worryingly vague in the company’s announcement.</p><p>Return guest <a href="https://law.ucla.edu/faculty/faculty-profiles/rose-chan-loui">Rose Chan Loui</a>, nonprofit law expert at UCLA, sees potential in OpenAI’s new proposal, but emphasises that “control” must be carefully defined and enforced: “The words are great, but what’s going to back that up?” Without explicitly defining the nonprofit’s authority over safety decisions, the shift could be largely cosmetic.</p><p><a href="https://80k.info/rcl4"><strong>Links to learn more, video, and full transcript</strong></a>: <a href="https://80k.info/rcl4">https://80k.info/rcl4</a></p><p>Why have state officials taken such an interest so far? Host Rob Wiblin notes, “OpenAI was proposing that the AGs would no longer have any say over what this super momentous company might end up doing. … It was just crazy how they were suggesting that they would take all of the existing money and then pursue a completely different purpose.”</p><p>Now that they’re in the picture, the AGs have leverage to ensure the nonprofit maintains genuine control over issues of public safety as OpenAI develops increasingly powerful AI.</p><p>Rob and Rose explain three key areas where the AGs can make a huge difference to whether this plays out in the public’s best interest:</p><ol><li>Ensuring that the contractual agreements giving the nonprofit control over the new Delaware public benefit corporation are watertight, and don’t accidentally shut the AGs out of the picture.</li><li>Insisting that a majority of board members are truly independent by prohibiting indirect as well as direct financial stakes in the business.</li><li>Insisting that the board is empowered with the money, independent staffing, and access to information which they need to do their jobs.</li></ol><p><em>This episode was originally recorded on May 6, 2025.</em></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Rose is back! (00:01:06)</li><li>The nonprofit will stay 'in control' (00:01:28)</li><li>Backlash to OpenAI’s original plans (00:08:22)</li><li>The new proposal (00:16:33)</li><li>Giving up the super-profits (00:20:52)</li><li>Can the nonprofit maintain control of the company? (00:24:49)</li><li>Could for profit investors sue if profits aren't prioritised? (00:33:01)</li><li>The 6 governance safeguards at risk with the restructure (00:34:33)</li><li>Will the nonprofit’s giving just be corporate PR for the for-profit? (00:49:12)</li><li>Is this good, or not? (00:51:06)</li><li>Ways this could still go wrong – but reasons for optimism (00:54:19)</li></ul><p><em>Video editing: Simon Monsour and Luke Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 08 May 2025 18:14:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6a17aea8/ad327c75.mp3" length="60367249" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/jQPXYSp8OraVulCK9BhVyjmYVpE9Z2SQXpRMDoDx6tM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8wM2Iy/YzQyNGI4YTk2MTYz/N2ZmNTAzYTc2ZGU3/ZmMyNy5qcGc.jpg"/>
      <itunes:duration>3770</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>When attorneys general intervene in corporate affairs, it usually means something has gone seriously wrong. In OpenAI’s case, it appears to have forced a dramatic reversal of the company’s plans to sideline its nonprofit foundation, announced in a <a href="https://openai.com/index/evolving-our-structure/">blog post</a> that made headlines worldwide.</p><p>The company’s sudden announcement that its nonprofit will “retain control” credits “constructive dialogue” with the attorneys general of California and Delaware — corporate-speak for what was likely a far more consequential confrontation behind closed doors. A confrontation perhaps driven by <a href="https://notforprivategain.org/">public pressure</a> from Nobel Prize winners, past OpenAI staff, and community organisations.</p><p>But whether this change will help depends entirely on the details of implementation — details that remain worryingly vague in the company’s announcement.</p><p>Return guest <a href="https://law.ucla.edu/faculty/faculty-profiles/rose-chan-loui">Rose Chan Loui</a>, nonprofit law expert at UCLA, sees potential in OpenAI’s new proposal, but emphasises that “control” must be carefully defined and enforced: “The words are great, but what’s going to back that up?” Without explicitly defining the nonprofit’s authority over safety decisions, the shift could be largely cosmetic.</p><p><a href="https://80k.info/rcl4"><strong>Links to learn more, video, and full transcript</strong></a>: <a href="https://80k.info/rcl4">https://80k.info/rcl4</a></p><p>Why have state officials taken such an interest so far? Host Rob Wiblin notes, “OpenAI was proposing that the AGs would no longer have any say over what this super momentous company might end up doing. … It was just crazy how they were suggesting that they would take all of the existing money and then pursue a completely different purpose.”</p><p>Now that they’re in the picture, the AGs have leverage to ensure the nonprofit maintains genuine control over issues of public safety as OpenAI develops increasingly powerful AI.</p><p>Rob and Rose explain three key areas where the AGs can make a huge difference to whether this plays out in the public’s best interest:</p><ol><li>Ensuring that the contractual agreements giving the nonprofit control over the new Delaware public benefit corporation are watertight, and don’t accidentally shut the AGs out of the picture.</li><li>Insisting that a majority of board members are truly independent by prohibiting indirect as well as direct financial stakes in the business.</li><li>Insisting that the board is empowered with the money, independent staffing, and access to information which they need to do their jobs.</li></ol><p><em>This episode was originally recorded on May 6, 2025.</em></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Rose is back! (00:01:06)</li><li>The nonprofit will stay 'in control' (00:01:28)</li><li>Backlash to OpenAI’s original plans (00:08:22)</li><li>The new proposal (00:16:33)</li><li>Giving up the super-profits (00:20:52)</li><li>Can the nonprofit maintain control of the company? (00:24:49)</li><li>Could for profit investors sue if profits aren't prioritised? (00:33:01)</li><li>The 6 governance safeguards at risk with the restructure (00:34:33)</li><li>Will the nonprofit’s giving just be corporate PR for the for-profit? (00:49:12)</li><li>Is this good, or not? (00:51:06)</li><li>Ways this could still go wrong – but reasons for optimism (00:54:19)</li></ul><p><em>Video editing: Simon Monsour and Luke Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Music: Ben Cordell</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/6a17aea8/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/6a17aea8/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#216 – Ian Dunt on why governments in Britain and elsewhere can't get anything done – and how to fix it</title>
      <itunes:title>#216 – Ian Dunt on why governments in Britain and elsewhere can't get anything done – and how to fix it</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">abec2a26-9a84-49fc-9ba8-6b8bf77a9148</guid>
      <link>https://80000hours.org/podcast/episodes/ian-dunt-why-governments-fail/?utm_campaign=podcast__ian-dunt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>When you have a system where ministers almost never understand their portfolios, civil servants change jobs every few months, and MPs don't grasp parliamentary procedure even after decades in office — is the problem the people, or the structure they work in?</p><p>Today's guest, political journalist <a href="https://www.iandunt.com/">Ian Dunt</a>, studies the systemic reasons governments succeed and fail.</p><p>And in his book <a href="https://www.amazon.com/How-Westminster-Works-Why-Doesnt/dp/139960273X"><em>How Westminster Works ...and Why It Doesn't</em></a>, he argues that Britain's government dysfunction and multi-decade failure to solve its key problems stems primarily from bad incentives and bad processes. </p><p>Even brilliant, well-intentioned people are set up to fail by a long list of institutional absurdities that Ian runs through — from the constant churn of ministers and civil servants that means no one understands what they’re working on, to the “pathological national sentimentality” that keeps 10 Downing Street (a 17th century townhouse) as the beating heart of British government.</p><p>While some of these are unique British failings, we see similar dynamics in other governments and large corporations around the world.</p><p>But Ian also lays out how some countries have found structural solutions that help ensure decisions are made by the right people, with the information they need, and that success is rewarded.</p><p><a href="https://80k.info/id"><strong>Links to learn more, video, highlights, and full transcript. </strong></a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>How Ian got obsessed with Britain's endless failings (00:01:05)</li><li>Should we blame individuals or incentives? (00:03:24)</li><li>The UK left its allies to be murdered in Afghanistan (to save cats and dogs) (00:09:02)</li><li>The UK is governed from a tiny cramped house (00:17:54)</li><li>“It's the stupidest conceivable system for how to run a country” (00:23:30)</li><li>The problems that never get solved in the UK (00:28:14)</li><li>Why UK ministers have no expertise in the areas they govern (00:31:32)</li><li>Why MPs are chosen to have no idea about legislation (00:44:08)</li><li>Is any country doing things better? (00:46:14)</li><li>Is rushing inevitable or artificial? (00:57:20)</li><li>How unelected septuagenarians are the heroes of UK governance (01:01:02)</li><li>How Thatcher unintentionally made one part of parliament work (01:10:48)</li><li>Maybe secrecy is the best disinfectant for incompetence (01:14:17)</li><li>The House of Commons may as well be in a coma (01:22:34)</li><li>Why it's in the PM's interest to ban electronic voting (01:33:13)</li><li>MPs are deliberately kept ignorant of parliamentary procedure (01:35:53)</li><li>“Whole areas of law have fallen almost completely into the vortex” (01:40:37)</li><li>What's the seed of all this going wrong? (01:44:00)</li><li>Why won't the Commons challenge the executive when it can? (01:53:10)</li><li>Better ways to choose MPs (01:58:33)</li><li>Citizens’ juries (02:07:16)</li><li>Do more independent-minded legislatures actually lead to better outcomes? (02:10:42)</li><li>"There’s no time for this bourgeois constitutional reform bulls***" (02:16:50)</li><li>How to keep expert civil servants (02:22:35)</li><li>Improving legislation like you’d improve Netflix dramas (02:34:34)</li><li>MPs waste much of their time helping constituents with random complaints (02:39:59)</li><li>Party culture prevents independent thinking (02:43:52)</li><li>Would a written constitution help or hurt? (02:48:37)</li><li>Can we give the PM room to appoint ministers based on expertise and competence? (02:51:51)</li><li>Would proportional representation help? (02:56:20)</li><li>Proportional representation encourages collaboration but does have weaknesses (02:58:51)</li><li>Alternative electoral systems (03:07:44)</li></ul><p><br></p><p><em>This episode was originally recorded on January 30, 2025.</em></p><p>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Music: Ben Cordell<br>Camera operator: Jeremy Chevillotte<br>Transcriptions and web: Katy Moore</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>When you have a system where ministers almost never understand their portfolios, civil servants change jobs every few months, and MPs don't grasp parliamentary procedure even after decades in office — is the problem the people, or the structure they work in?</p><p>Today's guest, political journalist <a href="https://www.iandunt.com/">Ian Dunt</a>, studies the systemic reasons governments succeed and fail.</p><p>And in his book <a href="https://www.amazon.com/How-Westminster-Works-Why-Doesnt/dp/139960273X"><em>How Westminster Works ...and Why It Doesn't</em></a>, he argues that Britain's government dysfunction and multi-decade failure to solve its key problems stems primarily from bad incentives and bad processes. </p><p>Even brilliant, well-intentioned people are set up to fail by a long list of institutional absurdities that Ian runs through — from the constant churn of ministers and civil servants that means no one understands what they’re working on, to the “pathological national sentimentality” that keeps 10 Downing Street (a 17th century townhouse) as the beating heart of British government.</p><p>While some of these are unique British failings, we see similar dynamics in other governments and large corporations around the world.</p><p>But Ian also lays out how some countries have found structural solutions that help ensure decisions are made by the right people, with the information they need, and that success is rewarded.</p><p><a href="https://80k.info/id"><strong>Links to learn more, video, highlights, and full transcript. </strong></a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>How Ian got obsessed with Britain's endless failings (00:01:05)</li><li>Should we blame individuals or incentives? (00:03:24)</li><li>The UK left its allies to be murdered in Afghanistan (to save cats and dogs) (00:09:02)</li><li>The UK is governed from a tiny cramped house (00:17:54)</li><li>“It's the stupidest conceivable system for how to run a country” (00:23:30)</li><li>The problems that never get solved in the UK (00:28:14)</li><li>Why UK ministers have no expertise in the areas they govern (00:31:32)</li><li>Why MPs are chosen to have no idea about legislation (00:44:08)</li><li>Is any country doing things better? (00:46:14)</li><li>Is rushing inevitable or artificial? (00:57:20)</li><li>How unelected septuagenarians are the heroes of UK governance (01:01:02)</li><li>How Thatcher unintentionally made one part of parliament work (01:10:48)</li><li>Maybe secrecy is the best disinfectant for incompetence (01:14:17)</li><li>The House of Commons may as well be in a coma (01:22:34)</li><li>Why it's in the PM's interest to ban electronic voting (01:33:13)</li><li>MPs are deliberately kept ignorant of parliamentary procedure (01:35:53)</li><li>“Whole areas of law have fallen almost completely into the vortex” (01:40:37)</li><li>What's the seed of all this going wrong? (01:44:00)</li><li>Why won't the Commons challenge the executive when it can? (01:53:10)</li><li>Better ways to choose MPs (01:58:33)</li><li>Citizens’ juries (02:07:16)</li><li>Do more independent-minded legislatures actually lead to better outcomes? (02:10:42)</li><li>"There’s no time for this bourgeois constitutional reform bulls***" (02:16:50)</li><li>How to keep expert civil servants (02:22:35)</li><li>Improving legislation like you’d improve Netflix dramas (02:34:34)</li><li>MPs waste much of their time helping constituents with random complaints (02:39:59)</li><li>Party culture prevents independent thinking (02:43:52)</li><li>Would a written constitution help or hurt? (02:48:37)</li><li>Can we give the PM room to appoint ministers based on expertise and competence? (02:51:51)</li><li>Would proportional representation help? (02:56:20)</li><li>Proportional representation encourages collaboration but does have weaknesses (02:58:51)</li><li>Alternative electoral systems (03:07:44)</li></ul><p><br></p><p><em>This episode was originally recorded on January 30, 2025.</em></p><p>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Music: Ben Cordell<br>Camera operator: Jeremy Chevillotte<br>Transcriptions and web: Katy Moore</p>]]>
      </content:encoded>
      <pubDate>Fri, 02 May 2025 14:52:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/edb24229/4a5c6d8f.mp3" length="187108437" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/mQxv69R36D1HfldvKxe2lHvi1o62olKeOlOHFsdCzOc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8yNDZk/NjRlODVjMTYxNjZm/YTc1ZDAzZWFlNWQ3/ZTE0Mi5qcGc.jpg"/>
      <itunes:duration>11692</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>When you have a system where ministers almost never understand their portfolios, civil servants change jobs every few months, and MPs don't grasp parliamentary procedure even after decades in office — is the problem the people, or the structure they work in?</p><p>Today's guest, political journalist <a href="https://www.iandunt.com/">Ian Dunt</a>, studies the systemic reasons governments succeed and fail.</p><p>And in his book <a href="https://www.amazon.com/How-Westminster-Works-Why-Doesnt/dp/139960273X"><em>How Westminster Works ...and Why It Doesn't</em></a>, he argues that Britain's government dysfunction and multi-decade failure to solve its key problems stems primarily from bad incentives and bad processes. </p><p>Even brilliant, well-intentioned people are set up to fail by a long list of institutional absurdities that Ian runs through — from the constant churn of ministers and civil servants that means no one understands what they’re working on, to the “pathological national sentimentality” that keeps 10 Downing Street (a 17th century townhouse) as the beating heart of British government.</p><p>While some of these are unique British failings, we see similar dynamics in other governments and large corporations around the world.</p><p>But Ian also lays out how some countries have found structural solutions that help ensure decisions are made by the right people, with the information they need, and that success is rewarded.</p><p><a href="https://80k.info/id"><strong>Links to learn more, video, highlights, and full transcript. </strong></a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>How Ian got obsessed with Britain's endless failings (00:01:05)</li><li>Should we blame individuals or incentives? (00:03:24)</li><li>The UK left its allies to be murdered in Afghanistan (to save cats and dogs) (00:09:02)</li><li>The UK is governed from a tiny cramped house (00:17:54)</li><li>“It's the stupidest conceivable system for how to run a country” (00:23:30)</li><li>The problems that never get solved in the UK (00:28:14)</li><li>Why UK ministers have no expertise in the areas they govern (00:31:32)</li><li>Why MPs are chosen to have no idea about legislation (00:44:08)</li><li>Is any country doing things better? (00:46:14)</li><li>Is rushing inevitable or artificial? (00:57:20)</li><li>How unelected septuagenarians are the heroes of UK governance (01:01:02)</li><li>How Thatcher unintentionally made one part of parliament work (01:10:48)</li><li>Maybe secrecy is the best disinfectant for incompetence (01:14:17)</li><li>The House of Commons may as well be in a coma (01:22:34)</li><li>Why it's in the PM's interest to ban electronic voting (01:33:13)</li><li>MPs are deliberately kept ignorant of parliamentary procedure (01:35:53)</li><li>“Whole areas of law have fallen almost completely into the vortex” (01:40:37)</li><li>What's the seed of all this going wrong? (01:44:00)</li><li>Why won't the Commons challenge the executive when it can? (01:53:10)</li><li>Better ways to choose MPs (01:58:33)</li><li>Citizens’ juries (02:07:16)</li><li>Do more independent-minded legislatures actually lead to better outcomes? (02:10:42)</li><li>"There’s no time for this bourgeois constitutional reform bulls***" (02:16:50)</li><li>How to keep expert civil servants (02:22:35)</li><li>Improving legislation like you’d improve Netflix dramas (02:34:34)</li><li>MPs waste much of their time helping constituents with random complaints (02:39:59)</li><li>Party culture prevents independent thinking (02:43:52)</li><li>Would a written constitution help or hurt? (02:48:37)</li><li>Can we give the PM room to appoint ministers based on expertise and competence? (02:51:51)</li><li>Would proportional representation help? (02:56:20)</li><li>Proportional representation encourages collaboration but does have weaknesses (02:58:51)</li><li>Alternative electoral systems (03:07:44)</li></ul><p><br></p><p><em>This episode was originally recorded on January 30, 2025.</em></p><p>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Music: Ben Cordell<br>Camera operator: Jeremy Chevillotte<br>Transcriptions and web: Katy Moore</p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>Yes</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/edb24229/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/edb24229/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Serendipity, weird bets, &amp; cold emails that actually work: Career advice from 16 former guests</title>
      <itunes:title>Serendipity, weird bets, &amp; cold emails that actually work: Career advice from 16 former guests</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">62e15e79-4d27-4d42-adcf-e9705cd2dd22</guid>
      <link>https://80000hours.org/podcast/episodes/concrete-unconventional-career-advice/?utm_campaign=podcast__career-advice&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>How do you navigate a career path when the future of work is uncertain? How important is mentorship versus immediate impact? Is it better to focus on your strengths or on the world’s most pressing problems? Should you specialise deeply or develop a unique combination of skills?</p><p>From embracing failure to finding unlikely allies, we bring you 16 diverse perspectives from past guests who’ve found unconventional paths to impact and helped others do the same.</p><p><a href="https://80k.info/ca"><strong>Links to learn more and full transcript.</strong></a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>Holden Karnofsky on just kicking ass at whatever (00:02:53)</li><li>Jeff Sebo on what improv comedy can teach us about doing good in the world (00:12:23)</li><li>Dean Spears on being open to randomness and serendipity (00:19:26)</li><li>Michael Webb on how to think about career planning given the rapid developments in AI (00:21:17)</li><li>Michelle Hutchinson on finding what motivates you and reaching out to people for help (00:41:10)</li><li>Benjamin Todd on figuring out if a career path is a good fit for you (00:46:03)</li><li>Chris Olah on the value of unusual combinations of skills (00:50:23)</li><li>Holden Karnofsky on deciding which weird ideas are worth betting on (00:58:03)</li><li>Karen Levy on travelling to learn about yourself (01:03:10)</li><li>Leah Garcés on finding common ground with unlikely allies (01:06:53)</li><li>Spencer Greenberg on recognising toxic people who could derail your career and life (01:13:34)</li><li>Holden Karnofsky on the many jobs that can help with AI (01:23:13)</li><li>Danny Hernandez on using world events to trigger you to work on something else (01:30:46)</li><li>Sarah Eustis-Guthrie on exploring and pivoting in careers (01:33:07)</li><li>Benjamin Todd on making tough career decisions (01:38:36)</li><li>Hannah Ritchie on being selective when following others’ advice (01:44:22)</li><li>Alex Lawsen on getting good mentorship (01:47:25)</li><li>Chris Olah on cold emailing that actually works (01:54:49)</li><li>Pardis Sabeti on prioritising physical health to do your best work (01:58:34)</li><li>Chris Olah on developing good taste and technique as a researcher (02:04:39)</li><li>Benjamin Todd on why it’s so important to apply to loads of jobs (02:09:52)</li><li>Varsha Venugopal on embracing uncomfortable situations and celebrating failures (02:14:25)</li><li>Luisa's outro (02:17:43)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>How do you navigate a career path when the future of work is uncertain? How important is mentorship versus immediate impact? Is it better to focus on your strengths or on the world’s most pressing problems? Should you specialise deeply or develop a unique combination of skills?</p><p>From embracing failure to finding unlikely allies, we bring you 16 diverse perspectives from past guests who’ve found unconventional paths to impact and helped others do the same.</p><p><a href="https://80k.info/ca"><strong>Links to learn more and full transcript.</strong></a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>Holden Karnofsky on just kicking ass at whatever (00:02:53)</li><li>Jeff Sebo on what improv comedy can teach us about doing good in the world (00:12:23)</li><li>Dean Spears on being open to randomness and serendipity (00:19:26)</li><li>Michael Webb on how to think about career planning given the rapid developments in AI (00:21:17)</li><li>Michelle Hutchinson on finding what motivates you and reaching out to people for help (00:41:10)</li><li>Benjamin Todd on figuring out if a career path is a good fit for you (00:46:03)</li><li>Chris Olah on the value of unusual combinations of skills (00:50:23)</li><li>Holden Karnofsky on deciding which weird ideas are worth betting on (00:58:03)</li><li>Karen Levy on travelling to learn about yourself (01:03:10)</li><li>Leah Garcés on finding common ground with unlikely allies (01:06:53)</li><li>Spencer Greenberg on recognising toxic people who could derail your career and life (01:13:34)</li><li>Holden Karnofsky on the many jobs that can help with AI (01:23:13)</li><li>Danny Hernandez on using world events to trigger you to work on something else (01:30:46)</li><li>Sarah Eustis-Guthrie on exploring and pivoting in careers (01:33:07)</li><li>Benjamin Todd on making tough career decisions (01:38:36)</li><li>Hannah Ritchie on being selective when following others’ advice (01:44:22)</li><li>Alex Lawsen on getting good mentorship (01:47:25)</li><li>Chris Olah on cold emailing that actually works (01:54:49)</li><li>Pardis Sabeti on prioritising physical health to do your best work (01:58:34)</li><li>Chris Olah on developing good taste and technique as a researcher (02:04:39)</li><li>Benjamin Todd on why it’s so important to apply to loads of jobs (02:09:52)</li><li>Varsha Venugopal on embracing uncomfortable situations and celebrating failures (02:14:25)</li><li>Luisa's outro (02:17:43)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 24 Apr 2025 16:39:22 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/04d482e7/2bfcccf0.mp3" length="133239660" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Cin8nxjHXoUDBpiWh5oOGR9tIg1WvjHvIH-_k_nX5iU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9jZmMw/MTljM2ZjZjI2ODgy/OGMxMjY3NjU5ODY2/YWU3MS5qcGc.jpg"/>
      <itunes:duration>8321</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>How do you navigate a career path when the future of work is uncertain? How important is mentorship versus immediate impact? Is it better to focus on your strengths or on the world’s most pressing problems? Should you specialise deeply or develop a unique combination of skills?</p><p>From embracing failure to finding unlikely allies, we bring you 16 diverse perspectives from past guests who’ve found unconventional paths to impact and helped others do the same.</p><p><a href="https://80k.info/ca"><strong>Links to learn more and full transcript.</strong></a></p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>Holden Karnofsky on just kicking ass at whatever (00:02:53)</li><li>Jeff Sebo on what improv comedy can teach us about doing good in the world (00:12:23)</li><li>Dean Spears on being open to randomness and serendipity (00:19:26)</li><li>Michael Webb on how to think about career planning given the rapid developments in AI (00:21:17)</li><li>Michelle Hutchinson on finding what motivates you and reaching out to people for help (00:41:10)</li><li>Benjamin Todd on figuring out if a career path is a good fit for you (00:46:03)</li><li>Chris Olah on the value of unusual combinations of skills (00:50:23)</li><li>Holden Karnofsky on deciding which weird ideas are worth betting on (00:58:03)</li><li>Karen Levy on travelling to learn about yourself (01:03:10)</li><li>Leah Garcés on finding common ground with unlikely allies (01:06:53)</li><li>Spencer Greenberg on recognising toxic people who could derail your career and life (01:13:34)</li><li>Holden Karnofsky on the many jobs that can help with AI (01:23:13)</li><li>Danny Hernandez on using world events to trigger you to work on something else (01:30:46)</li><li>Sarah Eustis-Guthrie on exploring and pivoting in careers (01:33:07)</li><li>Benjamin Todd on making tough career decisions (01:38:36)</li><li>Hannah Ritchie on being selective when following others’ advice (01:44:22)</li><li>Alex Lawsen on getting good mentorship (01:47:25)</li><li>Chris Olah on cold emailing that actually works (01:54:49)</li><li>Pardis Sabeti on prioritising physical health to do your best work (01:58:34)</li><li>Chris Olah on developing good taste and technique as a researcher (02:04:39)</li><li>Benjamin Todd on why it’s so important to apply to loads of jobs (02:09:52)</li><li>Varsha Venugopal on embracing uncomfortable situations and celebrating failures (02:14:25)</li><li>Luisa's outro (02:17:43)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/04d482e7/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/04d482e7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#215 – Tom Davidson on how AI-enabled coups could allow a tiny group to seize power</title>
      <itunes:title>#215 – Tom Davidson on how AI-enabled coups could allow a tiny group to seize power</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6e84cda3-e359-4b8b-987d-e842998eb0f4</guid>
      <link>https://80000hours.org/podcast/episodes/tom-davidson-ai-enabled-human-power-grabs/?utm_campaign=podcast__tom-davidson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Throughout history, technological revolutions have fundamentally shifted the balance of power in society. The Industrial Revolution created conditions where democracies could flourish for the first time — as nations needed educated, informed, and empowered citizens to deploy advanced technologies and remain competitive.</p><p>Unfortunately there’s every reason to think artificial general intelligence (AGI) will reverse that trend. </p><p>Today’s guest — <a href="https://tomdavidson-ai.github.io/">Tom Davidson</a> of the <a href="https://www.forethought.org/">Forethought Centre for AI Strategy</a> — claims in <a href="https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power">a new paper published today</a> that advanced AI enables power grabs by small groups, by removing the need for widespread human participation. </p><p><a href="https://80k.info/td"><strong>Links to learn more, video, highlights, and full transcript. </strong>https://80k.info/td</a></p><p><a href="https://80k.info/work"><strong>Also: come work with us on the 80,000 Hours podcast team!</strong></a> <a href="https://80k.info/work">https://80k.info/work</a></p><p>There are a few routes by which small groups might seize power:</p><ul><li>Military coups: Though rare in established democracies due to citizen/soldier resistance, future AI-controlled militaries may lack such constraints. </li><li>Self-built hard power: History suggests maybe only 10,000 obedient military drones could seize power.</li><li>Autocratisation: Leaders using millions of loyal AI workers, while denying others access, could remove democratic checks and balances.</li></ul><p>Tom explains several reasons why AI systems might follow a tyrant’s orders:</p><ul><li>They might be programmed to obey the top of the chain of command, with no checks on that power.</li><li>Systems could contain "secret loyalties" inserted during development.</li><li>Superior cyber capabilities could allow small groups to control AI-operated military infrastructure.</li></ul><p>Host Rob Wiblin and Tom discuss all this plus potential countermeasures.</p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>A major update on the show (00:00:55)</li><li>How AI enables tiny groups to seize power (00:06:24)</li><li>The 3 different threats (00:07:42)</li><li>Is this common sense or far-fetched? (00:08:51)</li><li>“No person rules alone.” Except now they might. (00:11:48)</li><li>Underpinning all 3 threats: Secret AI loyalties (00:17:46)</li><li>Key risk factors (00:25:38)</li><li>Preventing secret loyalties in a nutshell (00:27:12)</li><li>Are human power grabs more plausible than 'rogue AI'? (00:29:32)</li><li>If you took over the US, could you take over the whole world? (00:38:11)</li><li>Will this make it impossible to escape autocracy? (00:42:20)</li><li>Threat 1: AI-enabled military coups (00:46:19)</li><li>Will we sleepwalk into an AI military coup? (00:56:23)</li><li>Could AIs be more coup-resistant than humans? (01:02:28)</li><li>Threat 2: Autocratisation (01:05:22)</li><li>Will AGI be super-persuasive? (01:15:32)</li><li>Threat 3: Self-built hard power (01:17:56)</li><li>Can you stage a coup with 10,000 drones? (01:25:42)</li><li>That sounds a lot like sci-fi... is it credible? (01:27:49)</li><li>Will we foresee and prevent all this? (01:32:08)</li><li>Are people psychologically willing to do coups? (01:33:34)</li><li>Will a balance of power between AIs prevent this? (01:37:39)</li><li>Will whistleblowers or internal mistrust prevent coups? (01:39:55)</li><li>Would other countries step in? (01:46:03)</li><li>Will rogue AI preempt a human power grab? (01:48:30)</li><li>The best reasons not to worry (01:51:05)</li><li>How likely is this in the US? (01:53:23)</li><li>Is a small group seizing power really so bad? (02:00:47)</li><li>Countermeasure 1: Block internal misuse (02:04:19)</li><li>Countermeasure 2: Cybersecurity (02:14:02)</li><li>Countermeasure 3: Model spec transparency (02:16:11)</li><li>Countermeasure 4: Sharing AI access broadly (02:25:23)</li><li>Is it more dangerous to concentrate or share AGI? (02:30:13)</li><li>Is it important to have more than one powerful AI country? (02:32:56)</li><li>In defence of open sourcing AI models (02:35:59)</li><li>2 ways to stop secret AI loyalties (02:43:34)</li><li>Preventing AI-enabled military coups in particular (02:56:20)</li><li>How listeners can help (03:01:59)</li><li>How to help if you work at an AI company (03:05:49)</li><li>The power ML researchers still have, for now (03:09:53)</li><li>How to help if you're an elected leader (03:13:14)</li><li>Rob’s outro (03:19:05)</li></ul><p><em>This episode was originally recorded on January 20, 2025.</em></p><p>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Camera operator: Jeremy Chevillotte<br>Transcriptions and web: Katy Moore</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Throughout history, technological revolutions have fundamentally shifted the balance of power in society. The Industrial Revolution created conditions where democracies could flourish for the first time — as nations needed educated, informed, and empowered citizens to deploy advanced technologies and remain competitive.</p><p>Unfortunately there’s every reason to think artificial general intelligence (AGI) will reverse that trend. </p><p>Today’s guest — <a href="https://tomdavidson-ai.github.io/">Tom Davidson</a> of the <a href="https://www.forethought.org/">Forethought Centre for AI Strategy</a> — claims in <a href="https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power">a new paper published today</a> that advanced AI enables power grabs by small groups, by removing the need for widespread human participation. </p><p><a href="https://80k.info/td"><strong>Links to learn more, video, highlights, and full transcript. </strong>https://80k.info/td</a></p><p><a href="https://80k.info/work"><strong>Also: come work with us on the 80,000 Hours podcast team!</strong></a> <a href="https://80k.info/work">https://80k.info/work</a></p><p>There are a few routes by which small groups might seize power:</p><ul><li>Military coups: Though rare in established democracies due to citizen/soldier resistance, future AI-controlled militaries may lack such constraints. </li><li>Self-built hard power: History suggests maybe only 10,000 obedient military drones could seize power.</li><li>Autocratisation: Leaders using millions of loyal AI workers, while denying others access, could remove democratic checks and balances.</li></ul><p>Tom explains several reasons why AI systems might follow a tyrant’s orders:</p><ul><li>They might be programmed to obey the top of the chain of command, with no checks on that power.</li><li>Systems could contain "secret loyalties" inserted during development.</li><li>Superior cyber capabilities could allow small groups to control AI-operated military infrastructure.</li></ul><p>Host Rob Wiblin and Tom discuss all this plus potential countermeasures.</p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>A major update on the show (00:00:55)</li><li>How AI enables tiny groups to seize power (00:06:24)</li><li>The 3 different threats (00:07:42)</li><li>Is this common sense or far-fetched? (00:08:51)</li><li>“No person rules alone.” Except now they might. (00:11:48)</li><li>Underpinning all 3 threats: Secret AI loyalties (00:17:46)</li><li>Key risk factors (00:25:38)</li><li>Preventing secret loyalties in a nutshell (00:27:12)</li><li>Are human power grabs more plausible than 'rogue AI'? (00:29:32)</li><li>If you took over the US, could you take over the whole world? (00:38:11)</li><li>Will this make it impossible to escape autocracy? (00:42:20)</li><li>Threat 1: AI-enabled military coups (00:46:19)</li><li>Will we sleepwalk into an AI military coup? (00:56:23)</li><li>Could AIs be more coup-resistant than humans? (01:02:28)</li><li>Threat 2: Autocratisation (01:05:22)</li><li>Will AGI be super-persuasive? (01:15:32)</li><li>Threat 3: Self-built hard power (01:17:56)</li><li>Can you stage a coup with 10,000 drones? (01:25:42)</li><li>That sounds a lot like sci-fi... is it credible? (01:27:49)</li><li>Will we foresee and prevent all this? (01:32:08)</li><li>Are people psychologically willing to do coups? (01:33:34)</li><li>Will a balance of power between AIs prevent this? (01:37:39)</li><li>Will whistleblowers or internal mistrust prevent coups? (01:39:55)</li><li>Would other countries step in? (01:46:03)</li><li>Will rogue AI preempt a human power grab? (01:48:30)</li><li>The best reasons not to worry (01:51:05)</li><li>How likely is this in the US? (01:53:23)</li><li>Is a small group seizing power really so bad? (02:00:47)</li><li>Countermeasure 1: Block internal misuse (02:04:19)</li><li>Countermeasure 2: Cybersecurity (02:14:02)</li><li>Countermeasure 3: Model spec transparency (02:16:11)</li><li>Countermeasure 4: Sharing AI access broadly (02:25:23)</li><li>Is it more dangerous to concentrate or share AGI? (02:30:13)</li><li>Is it important to have more than one powerful AI country? (02:32:56)</li><li>In defence of open sourcing AI models (02:35:59)</li><li>2 ways to stop secret AI loyalties (02:43:34)</li><li>Preventing AI-enabled military coups in particular (02:56:20)</li><li>How listeners can help (03:01:59)</li><li>How to help if you work at an AI company (03:05:49)</li><li>The power ML researchers still have, for now (03:09:53)</li><li>How to help if you're an elected leader (03:13:14)</li><li>Rob’s outro (03:19:05)</li></ul><p><em>This episode was originally recorded on January 20, 2025.</em></p><p>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Camera operator: Jeremy Chevillotte<br>Transcriptions and web: Katy Moore</p>]]>
      </content:encoded>
      <pubDate>Wed, 16 Apr 2025 16:02:11 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/82672d9e/1b3d43a8.mp3" length="194677095" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lKM9rD6NOBdT_RZBKr6s7p_ngZmF1BiJZTwBDFBWXow/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xYzNm/YjllMmNiZWIyNjVh/OWRlNjczMjAyNWFh/YjdkMC5qcGc.jpg"/>
      <itunes:duration>12164</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Throughout history, technological revolutions have fundamentally shifted the balance of power in society. The Industrial Revolution created conditions where democracies could flourish for the first time — as nations needed educated, informed, and empowered citizens to deploy advanced technologies and remain competitive.</p><p>Unfortunately there’s every reason to think artificial general intelligence (AGI) will reverse that trend. </p><p>Today’s guest — <a href="https://tomdavidson-ai.github.io/">Tom Davidson</a> of the <a href="https://www.forethought.org/">Forethought Centre for AI Strategy</a> — claims in <a href="https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power">a new paper published today</a> that advanced AI enables power grabs by small groups, by removing the need for widespread human participation. </p><p><a href="https://80k.info/td"><strong>Links to learn more, video, highlights, and full transcript. </strong>https://80k.info/td</a></p><p><a href="https://80k.info/work"><strong>Also: come work with us on the 80,000 Hours podcast team!</strong></a> <a href="https://80k.info/work">https://80k.info/work</a></p><p>There are a few routes by which small groups might seize power:</p><ul><li>Military coups: Though rare in established democracies due to citizen/soldier resistance, future AI-controlled militaries may lack such constraints. </li><li>Self-built hard power: History suggests maybe only 10,000 obedient military drones could seize power.</li><li>Autocratisation: Leaders using millions of loyal AI workers, while denying others access, could remove democratic checks and balances.</li></ul><p>Tom explains several reasons why AI systems might follow a tyrant’s orders:</p><ul><li>They might be programmed to obey the top of the chain of command, with no checks on that power.</li><li>Systems could contain "secret loyalties" inserted during development.</li><li>Superior cyber capabilities could allow small groups to control AI-operated military infrastructure.</li></ul><p>Host Rob Wiblin and Tom discuss all this plus potential countermeasures.</p><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>A major update on the show (00:00:55)</li><li>How AI enables tiny groups to seize power (00:06:24)</li><li>The 3 different threats (00:07:42)</li><li>Is this common sense or far-fetched? (00:08:51)</li><li>“No person rules alone.” Except now they might. (00:11:48)</li><li>Underpinning all 3 threats: Secret AI loyalties (00:17:46)</li><li>Key risk factors (00:25:38)</li><li>Preventing secret loyalties in a nutshell (00:27:12)</li><li>Are human power grabs more plausible than 'rogue AI'? (00:29:32)</li><li>If you took over the US, could you take over the whole world? (00:38:11)</li><li>Will this make it impossible to escape autocracy? (00:42:20)</li><li>Threat 1: AI-enabled military coups (00:46:19)</li><li>Will we sleepwalk into an AI military coup? (00:56:23)</li><li>Could AIs be more coup-resistant than humans? (01:02:28)</li><li>Threat 2: Autocratisation (01:05:22)</li><li>Will AGI be super-persuasive? (01:15:32)</li><li>Threat 3: Self-built hard power (01:17:56)</li><li>Can you stage a coup with 10,000 drones? (01:25:42)</li><li>That sounds a lot like sci-fi... is it credible? (01:27:49)</li><li>Will we foresee and prevent all this? (01:32:08)</li><li>Are people psychologically willing to do coups? (01:33:34)</li><li>Will a balance of power between AIs prevent this? (01:37:39)</li><li>Will whistleblowers or internal mistrust prevent coups? (01:39:55)</li><li>Would other countries step in? (01:46:03)</li><li>Will rogue AI preempt a human power grab? (01:48:30)</li><li>The best reasons not to worry (01:51:05)</li><li>How likely is this in the US? (01:53:23)</li><li>Is a small group seizing power really so bad? (02:00:47)</li><li>Countermeasure 1: Block internal misuse (02:04:19)</li><li>Countermeasure 2: Cybersecurity (02:14:02)</li><li>Countermeasure 3: Model spec transparency (02:16:11)</li><li>Countermeasure 4: Sharing AI access broadly (02:25:23)</li><li>Is it more dangerous to concentrate or share AGI? (02:30:13)</li><li>Is it important to have more than one powerful AI country? (02:32:56)</li><li>In defence of open sourcing AI models (02:35:59)</li><li>2 ways to stop secret AI loyalties (02:43:34)</li><li>Preventing AI-enabled military coups in particular (02:56:20)</li><li>How listeners can help (03:01:59)</li><li>How to help if you work at an AI company (03:05:49)</li><li>The power ML researchers still have, for now (03:09:53)</li><li>How to help if you're an elected leader (03:13:14)</li><li>Rob’s outro (03:19:05)</li></ul><p><em>This episode was originally recorded on January 20, 2025.</em></p><p>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Camera operator: Jeremy Chevillotte<br>Transcriptions and web: Katy Moore</p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/82672d9e/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/82672d9e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Guilt, imposter syndrome &amp; doing good: 16 past guests share their mental health journeys</title>
      <itunes:title>Guilt, imposter syndrome &amp; doing good: 16 past guests share their mental health journeys</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">658687a1-70e2-4e04-82a8-6197939c0e4c</guid>
      <link>https://80000hours.org/podcast/episodes/mental-health-impactful-careers-compilation/?utm_campaign=podcast__mental-health-compilation&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"We are aiming for a place where we can decouple the scorecard from our worthiness. It’s of course the case that in trying to optimise the good, we will always be falling short. The question is how much, and in what ways are we not there yet? And if we then extrapolate that to how much and in what ways am I not enough, that’s where we run into trouble." —Hannah Boettcher</p><p>What happens when your desire to do good starts to undermine your own wellbeing?</p><p>Over the years, we’ve heard from therapists, charity directors, researchers, psychologists, and career advisors — all wrestling with how to do good without falling apart. Today’s episode brings together insights from 16 past guests on the emotional and psychological costs of pursuing a high-impact career to improve the world — and how to best navigate the all-too-common guilt, burnout, perfectionism, and imposter syndrome along the way.</p><p><strong>Check out the full transcript and links to learn more: </strong><a href="https://80k.info/mh"><strong>https://80k.info/mh</strong></a></p><p>If you’re dealing with your own mental health concerns, here are some resources that might help:</p><ul><li><strong>If you’re feeling at risk, try this for the the UK: </strong><a href="https://www.mind.org.uk/information-support/guides-to-support-and-services/crisis-services/getting-help-in-a-crisis/"><strong>How to get help in a crisis</strong></a><strong>, and this for the US: </strong><a href="https://suicidepreventionlifeline.org/"><strong>National Suicide Prevention Lifeline</strong></a><strong>.</strong></li><li>The UK’s National Health Service publishes <a href="https://www.nhs.uk/mental-health/">useful, evidence-based advice</a> on treatments for most conditions.</li><li><a href="https://www.mentnav.org/">Mental Health Navigator</a> is a service that simplifies finding and accessing mental health information and resources all over the world — built specifically for the effective altruism community</li><li>We recommend this <a href="https://lorienpsych.com/2021/06/05/depression/#2_How_do_you_treat_depression">summary of treatments for depression</a>, this <a href="https://slatestarcodex.com/2015/07/13/things-that-sometimes-work-if-you-have-anxiety/">summary of treatments for anxiety</a>, and <a href="https://mindease.io/">Mind Ease</a>, an app created by Spencer Greenberg.</li><li>We’d also recommend <a href="https://www.amazon.com/Its-Not-Always-Depression-Reconnecting/dp/0241976383"><em>It’s Not Always Depression</em></a> by Hilary Hendel.</li><li>Some on our team have found <a href="https://www.amazon.com/Overcoming-Perfectionism-scientifically-behavioural-techniques-ebook/dp/B07DN7GC5Q/"><em>Overcoming Perfectionism</em></a> and <a href="https://www.amazon.com/Overcoming-Low-Self-Esteem-2nd-behavioural/dp/1472119290/"><em>Overcoming Low Self-Esteem</em></a> very helpful.</li><li>And there’s even more resources listed on these episode pages: <a href="https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/#articles-books-and-other-media-discussed-in-the-show">Having a successful career with depression, anxiety, and imposter syndrome</a>, <a href="https://80000hours.org/after-hours-podcast/episodes/hannah-boettcher-mental-health-challenges/#articles-books-and-other-media-discussed-in-the-show">Hannah Boettcher on the mental health challenges that come with trying to have a big impact</a>, <a href="https://80000hours.org/podcast/episodes/tim-lebon-self-defeating-altruistic-perfectionism/#articles-books-and-other-media-discussed-in-the-show">Tim LeBon on how altruistic perfectionism is self-defeating</a>.</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:32)</li><li>80,000 Hours’ former CEO Howie on what his anxiety and self-doubt feels like (00:03:47)</li><li>Evolutionary psychiatrist Randy Nesse on what emotions are for (00:07:35)</li><li>Therapist Hannah Boettcher on how striving for impact can affect our self-worth (00:13:45)</li><li>Luisa Rodriguez on grieving the gap between who you are and who you wish you were (00:16:57)</li><li>Charity director Cameron Meyer Shorb on managing work-related guilt and shame (00:24:01)</li><li>Therapist Tim LeBon on aiming for excellence rather than perfection (00:29:18)</li><li>Author Cal Newport on making time to be alone with our thoughts (00:36:03)</li><li>80,000 Hours career advisors Michelle Hutchinson and Habiba Islam on prioritising mental health over career impact (00:40:28)</li><li>Charity founder Sarah Eustis-Guthrie on the ups and downs of founding an organisation (00:45:52)</li><li>Our World in Data researcher Hannah Ritchie on feeling like an imposter as a generalist (00:51:28)</li><li>Moral philosopher Will MacAskill on being proactive about mental health and preventing burnout (01:00:46)</li><li>Grantmaker Ajeya Cotra on the psychological toll of big open-ended research questions (01:11:00)</li><li>Researcher and grantmaker Christian Ruhl on how having a stutter affects him personally and professionally (01:19:30)</li><li>Mercy For Animals’ CEO Leah Garcés on insisting on self-care when doing difficult work (01:32:39)</li><li>80,000 Hours’ former CEO Howie on balancing a job and mental illness (01:37:12)</li><li>Therapist Hannah Boettcher on how self-compassion isn’t self-indulgence (01:40:39)</li><li>Journalist Kelsey Piper on communicating about mental health in ways that resonate (01:43:32)</li><li>Luisa's outro (01:46:10)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"We are aiming for a place where we can decouple the scorecard from our worthiness. It’s of course the case that in trying to optimise the good, we will always be falling short. The question is how much, and in what ways are we not there yet? And if we then extrapolate that to how much and in what ways am I not enough, that’s where we run into trouble." —Hannah Boettcher</p><p>What happens when your desire to do good starts to undermine your own wellbeing?</p><p>Over the years, we’ve heard from therapists, charity directors, researchers, psychologists, and career advisors — all wrestling with how to do good without falling apart. Today’s episode brings together insights from 16 past guests on the emotional and psychological costs of pursuing a high-impact career to improve the world — and how to best navigate the all-too-common guilt, burnout, perfectionism, and imposter syndrome along the way.</p><p><strong>Check out the full transcript and links to learn more: </strong><a href="https://80k.info/mh"><strong>https://80k.info/mh</strong></a></p><p>If you’re dealing with your own mental health concerns, here are some resources that might help:</p><ul><li><strong>If you’re feeling at risk, try this for the the UK: </strong><a href="https://www.mind.org.uk/information-support/guides-to-support-and-services/crisis-services/getting-help-in-a-crisis/"><strong>How to get help in a crisis</strong></a><strong>, and this for the US: </strong><a href="https://suicidepreventionlifeline.org/"><strong>National Suicide Prevention Lifeline</strong></a><strong>.</strong></li><li>The UK’s National Health Service publishes <a href="https://www.nhs.uk/mental-health/">useful, evidence-based advice</a> on treatments for most conditions.</li><li><a href="https://www.mentnav.org/">Mental Health Navigator</a> is a service that simplifies finding and accessing mental health information and resources all over the world — built specifically for the effective altruism community</li><li>We recommend this <a href="https://lorienpsych.com/2021/06/05/depression/#2_How_do_you_treat_depression">summary of treatments for depression</a>, this <a href="https://slatestarcodex.com/2015/07/13/things-that-sometimes-work-if-you-have-anxiety/">summary of treatments for anxiety</a>, and <a href="https://mindease.io/">Mind Ease</a>, an app created by Spencer Greenberg.</li><li>We’d also recommend <a href="https://www.amazon.com/Its-Not-Always-Depression-Reconnecting/dp/0241976383"><em>It’s Not Always Depression</em></a> by Hilary Hendel.</li><li>Some on our team have found <a href="https://www.amazon.com/Overcoming-Perfectionism-scientifically-behavioural-techniques-ebook/dp/B07DN7GC5Q/"><em>Overcoming Perfectionism</em></a> and <a href="https://www.amazon.com/Overcoming-Low-Self-Esteem-2nd-behavioural/dp/1472119290/"><em>Overcoming Low Self-Esteem</em></a> very helpful.</li><li>And there’s even more resources listed on these episode pages: <a href="https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/#articles-books-and-other-media-discussed-in-the-show">Having a successful career with depression, anxiety, and imposter syndrome</a>, <a href="https://80000hours.org/after-hours-podcast/episodes/hannah-boettcher-mental-health-challenges/#articles-books-and-other-media-discussed-in-the-show">Hannah Boettcher on the mental health challenges that come with trying to have a big impact</a>, <a href="https://80000hours.org/podcast/episodes/tim-lebon-self-defeating-altruistic-perfectionism/#articles-books-and-other-media-discussed-in-the-show">Tim LeBon on how altruistic perfectionism is self-defeating</a>.</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:32)</li><li>80,000 Hours’ former CEO Howie on what his anxiety and self-doubt feels like (00:03:47)</li><li>Evolutionary psychiatrist Randy Nesse on what emotions are for (00:07:35)</li><li>Therapist Hannah Boettcher on how striving for impact can affect our self-worth (00:13:45)</li><li>Luisa Rodriguez on grieving the gap between who you are and who you wish you were (00:16:57)</li><li>Charity director Cameron Meyer Shorb on managing work-related guilt and shame (00:24:01)</li><li>Therapist Tim LeBon on aiming for excellence rather than perfection (00:29:18)</li><li>Author Cal Newport on making time to be alone with our thoughts (00:36:03)</li><li>80,000 Hours career advisors Michelle Hutchinson and Habiba Islam on prioritising mental health over career impact (00:40:28)</li><li>Charity founder Sarah Eustis-Guthrie on the ups and downs of founding an organisation (00:45:52)</li><li>Our World in Data researcher Hannah Ritchie on feeling like an imposter as a generalist (00:51:28)</li><li>Moral philosopher Will MacAskill on being proactive about mental health and preventing burnout (01:00:46)</li><li>Grantmaker Ajeya Cotra on the psychological toll of big open-ended research questions (01:11:00)</li><li>Researcher and grantmaker Christian Ruhl on how having a stutter affects him personally and professionally (01:19:30)</li><li>Mercy For Animals’ CEO Leah Garcés on insisting on self-care when doing difficult work (01:32:39)</li><li>80,000 Hours’ former CEO Howie on balancing a job and mental illness (01:37:12)</li><li>Therapist Hannah Boettcher on how self-compassion isn’t self-indulgence (01:40:39)</li><li>Journalist Kelsey Piper on communicating about mental health in ways that resonate (01:43:32)</li><li>Luisa's outro (01:46:10)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 11 Apr 2025 14:52:59 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/7f497eee/2f6c8db2.mp3" length="102979125" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2bEsdgePCZ8fjznDD_BMc1eoFbqMZYZ6qRktcQSWScA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS85MzU2/YTdhOTY5ZjhhZGRi/ODdjNzBjOTUxZmZi/ZGI4OS5qcGc.jpg"/>
      <itunes:duration>6430</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"We are aiming for a place where we can decouple the scorecard from our worthiness. It’s of course the case that in trying to optimise the good, we will always be falling short. The question is how much, and in what ways are we not there yet? And if we then extrapolate that to how much and in what ways am I not enough, that’s where we run into trouble." —Hannah Boettcher</p><p>What happens when your desire to do good starts to undermine your own wellbeing?</p><p>Over the years, we’ve heard from therapists, charity directors, researchers, psychologists, and career advisors — all wrestling with how to do good without falling apart. Today’s episode brings together insights from 16 past guests on the emotional and psychological costs of pursuing a high-impact career to improve the world — and how to best navigate the all-too-common guilt, burnout, perfectionism, and imposter syndrome along the way.</p><p><strong>Check out the full transcript and links to learn more: </strong><a href="https://80k.info/mh"><strong>https://80k.info/mh</strong></a></p><p>If you’re dealing with your own mental health concerns, here are some resources that might help:</p><ul><li><strong>If you’re feeling at risk, try this for the the UK: </strong><a href="https://www.mind.org.uk/information-support/guides-to-support-and-services/crisis-services/getting-help-in-a-crisis/"><strong>How to get help in a crisis</strong></a><strong>, and this for the US: </strong><a href="https://suicidepreventionlifeline.org/"><strong>National Suicide Prevention Lifeline</strong></a><strong>.</strong></li><li>The UK’s National Health Service publishes <a href="https://www.nhs.uk/mental-health/">useful, evidence-based advice</a> on treatments for most conditions.</li><li><a href="https://www.mentnav.org/">Mental Health Navigator</a> is a service that simplifies finding and accessing mental health information and resources all over the world — built specifically for the effective altruism community</li><li>We recommend this <a href="https://lorienpsych.com/2021/06/05/depression/#2_How_do_you_treat_depression">summary of treatments for depression</a>, this <a href="https://slatestarcodex.com/2015/07/13/things-that-sometimes-work-if-you-have-anxiety/">summary of treatments for anxiety</a>, and <a href="https://mindease.io/">Mind Ease</a>, an app created by Spencer Greenberg.</li><li>We’d also recommend <a href="https://www.amazon.com/Its-Not-Always-Depression-Reconnecting/dp/0241976383"><em>It’s Not Always Depression</em></a> by Hilary Hendel.</li><li>Some on our team have found <a href="https://www.amazon.com/Overcoming-Perfectionism-scientifically-behavioural-techniques-ebook/dp/B07DN7GC5Q/"><em>Overcoming Perfectionism</em></a> and <a href="https://www.amazon.com/Overcoming-Low-Self-Esteem-2nd-behavioural/dp/1472119290/"><em>Overcoming Low Self-Esteem</em></a> very helpful.</li><li>And there’s even more resources listed on these episode pages: <a href="https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/#articles-books-and-other-media-discussed-in-the-show">Having a successful career with depression, anxiety, and imposter syndrome</a>, <a href="https://80000hours.org/after-hours-podcast/episodes/hannah-boettcher-mental-health-challenges/#articles-books-and-other-media-discussed-in-the-show">Hannah Boettcher on the mental health challenges that come with trying to have a big impact</a>, <a href="https://80000hours.org/podcast/episodes/tim-lebon-self-defeating-altruistic-perfectionism/#articles-books-and-other-media-discussed-in-the-show">Tim LeBon on how altruistic perfectionism is self-defeating</a>.</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:32)</li><li>80,000 Hours’ former CEO Howie on what his anxiety and self-doubt feels like (00:03:47)</li><li>Evolutionary psychiatrist Randy Nesse on what emotions are for (00:07:35)</li><li>Therapist Hannah Boettcher on how striving for impact can affect our self-worth (00:13:45)</li><li>Luisa Rodriguez on grieving the gap between who you are and who you wish you were (00:16:57)</li><li>Charity director Cameron Meyer Shorb on managing work-related guilt and shame (00:24:01)</li><li>Therapist Tim LeBon on aiming for excellence rather than perfection (00:29:18)</li><li>Author Cal Newport on making time to be alone with our thoughts (00:36:03)</li><li>80,000 Hours career advisors Michelle Hutchinson and Habiba Islam on prioritising mental health over career impact (00:40:28)</li><li>Charity founder Sarah Eustis-Guthrie on the ups and downs of founding an organisation (00:45:52)</li><li>Our World in Data researcher Hannah Ritchie on feeling like an imposter as a generalist (00:51:28)</li><li>Moral philosopher Will MacAskill on being proactive about mental health and preventing burnout (01:00:46)</li><li>Grantmaker Ajeya Cotra on the psychological toll of big open-ended research questions (01:11:00)</li><li>Researcher and grantmaker Christian Ruhl on how having a stutter affects him personally and professionally (01:19:30)</li><li>Mercy For Animals’ CEO Leah Garcés on insisting on self-care when doing difficult work (01:32:39)</li><li>80,000 Hours’ former CEO Howie on balancing a job and mental illness (01:37:12)</li><li>Therapist Hannah Boettcher on how self-compassion isn’t self-indulgence (01:40:39)</li><li>Journalist Kelsey Piper on communicating about mental health in ways that resonate (01:43:32)</li><li>Luisa's outro (01:46:10)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/7f497eee/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/7f497eee/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#214 – Buck Shlegeris on controlling AI that wants to take over – so we can use it anyway</title>
      <itunes:title>#214 – Buck Shlegeris on controlling AI that wants to take over – so we can use it anyway</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3b3a8a28-8a0f-4270-801b-b2d8538d8efc</guid>
      <link>https://80000hours.org/podcast/episodes/buck-shlegeris-ai-control-scheming/?utm_campaign=podcast__buck-shlegeris&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Most AI safety conversations centre on alignment: ensuring AI systems share our values and goals. But despite progress, we’re unlikely to know we’ve solved the problem before the arrival of human-level and superhuman systems in as little as three years.</p><p>So some are developing a backup plan to safely deploy models we fear are actively scheming to harm us — so-called “AI control.” While this may sound mad, given the reluctance of AI companies to delay deploying anything they train, <em>not</em> developing such techniques is probably even crazier.</p><p>Today’s guest — Buck Shlegeris, CEO of <a href="https://www.redwoodresearch.org/">Redwood Research</a> — has spent the last few years developing control mechanisms, and for human-level systems they’re more plausible than you might think. He argues that given companies’ unwillingness to incur large costs for security, accepting the possibility of misalignment and designing robust safeguards might be one of our best remaining options.</p><p><a href="https://80k.info/buck"><strong>Links to learn more, highlights, video, and full transcript.</strong></a><strong></strong></p><p>As Buck puts it: "Five years ago I thought of misalignment risk from AIs as a really hard problem that you’d need some really galaxy-brained fundamental insights to resolve. Whereas now, to me the situation feels a lot more like we just really know a list of 40 things where, if you did them — none of which seem that hard — you’d probably be able to not have very much of your problem."</p><p>Of course, even if Buck is right, we still need to <em>do</em> those 40 things — which he points out we’re not on track for. And AI control agendas have their limitations: they aren’t likely to work once AI systems are much more capable than humans, since greatly superhuman AIs can probably work around whatever limitations we impose.</p><p>Still, AI control agendas seem to be gaining traction within AI safety. Buck and host Rob Wiblin discuss all of the above, plus:</p><ul><li>Why he’s more worried about AI hacking its own data centre than escaping</li><li>What to do about “chronic harm,” where AI systems subtly underperform or sabotage important work like alignment research</li><li>Why he might want to use a model he thought could be conspiring against him</li><li>Why he would feel safer if he caught an AI attempting to escape</li><li>Why many control techniques would be relatively inexpensive</li><li>How to use an untrusted model to monitor another untrusted model</li><li>What the minimum viable intervention in a “lazy” AI company might look like</li><li>How even small teams of safety-focused staff within AI labs could matter</li><li>The moral considerations around controlling potentially conscious AI systems, and whether it’s justified</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open |00:00:00|  </li><li>Who’s Buck Shlegeris? |00:01:27|  </li><li>What's AI control? |00:01:51|  </li><li>Why is AI control hot now? |00:05:39|  </li><li>Detecting human vs AI spies |00:10:32|  </li><li>Acute vs chronic AI betrayal |00:15:21|  </li><li>How to catch AIs trying to escape |00:17:48|  </li><li>The cheapest AI control techniques |00:32:48|  </li><li>Can we get untrusted models to do trusted work? |00:38:58|  </li><li>If we catch a model escaping... will we do anything? |00:50:15|  </li><li>Getting AI models to think they've already escaped |00:52:51|  </li><li>Will they be able to tell it's a setup? |00:58:11|  </li><li>Will AI companies do any of this stuff? |01:00:11|  </li><li>Can we just give AIs fewer permissions? |01:06:14|  </li><li>Can we stop human spies the same way? |01:09:58|  </li><li>The pitch to AI companies to do this |01:15:04|  </li><li>Will AIs get superhuman so fast that this is all useless? |01:17:18|  </li><li>Risks from AI deliberately doing a bad job |01:18:37|  </li><li>Is alignment still useful? |01:24:49|  </li><li>Current alignment methods don't detect scheming |01:29:12|  </li><li>How to tell if AI control will work |01:31:40|  </li><li>How can listeners contribute? |01:35:53|  </li><li>Is 'controlling' AIs kind of a dick move? |01:37:13|  </li><li>Could 10 safety-focused people in an AGI company do anything useful? |01:42:27|  </li><li>Benefits of working outside frontier AI companies |01:47:48|  </li><li>Why Redwood Research does what it does |01:51:34|  </li><li>What other safety-related research looks best to Buck? |01:58:56|  </li><li>If an AI escapes, is it likely to be able to beat humanity from there? |01:59:48|  </li><li>Will misaligned models have to go rogue ASAP, before they're ready? |02:07:04|  </li><li>Is research on human scheming relevant to AI? |02:08:03|</li></ul><p><em>This episode was originally recorded on February 21, 2025.</em></p><p>Video: Simon Monsour and Luke Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, and Dominic Armstrong<br>Transcriptions and web: Katy Moore</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Most AI safety conversations centre on alignment: ensuring AI systems share our values and goals. But despite progress, we’re unlikely to know we’ve solved the problem before the arrival of human-level and superhuman systems in as little as three years.</p><p>So some are developing a backup plan to safely deploy models we fear are actively scheming to harm us — so-called “AI control.” While this may sound mad, given the reluctance of AI companies to delay deploying anything they train, <em>not</em> developing such techniques is probably even crazier.</p><p>Today’s guest — Buck Shlegeris, CEO of <a href="https://www.redwoodresearch.org/">Redwood Research</a> — has spent the last few years developing control mechanisms, and for human-level systems they’re more plausible than you might think. He argues that given companies’ unwillingness to incur large costs for security, accepting the possibility of misalignment and designing robust safeguards might be one of our best remaining options.</p><p><a href="https://80k.info/buck"><strong>Links to learn more, highlights, video, and full transcript.</strong></a><strong></strong></p><p>As Buck puts it: "Five years ago I thought of misalignment risk from AIs as a really hard problem that you’d need some really galaxy-brained fundamental insights to resolve. Whereas now, to me the situation feels a lot more like we just really know a list of 40 things where, if you did them — none of which seem that hard — you’d probably be able to not have very much of your problem."</p><p>Of course, even if Buck is right, we still need to <em>do</em> those 40 things — which he points out we’re not on track for. And AI control agendas have their limitations: they aren’t likely to work once AI systems are much more capable than humans, since greatly superhuman AIs can probably work around whatever limitations we impose.</p><p>Still, AI control agendas seem to be gaining traction within AI safety. Buck and host Rob Wiblin discuss all of the above, plus:</p><ul><li>Why he’s more worried about AI hacking its own data centre than escaping</li><li>What to do about “chronic harm,” where AI systems subtly underperform or sabotage important work like alignment research</li><li>Why he might want to use a model he thought could be conspiring against him</li><li>Why he would feel safer if he caught an AI attempting to escape</li><li>Why many control techniques would be relatively inexpensive</li><li>How to use an untrusted model to monitor another untrusted model</li><li>What the minimum viable intervention in a “lazy” AI company might look like</li><li>How even small teams of safety-focused staff within AI labs could matter</li><li>The moral considerations around controlling potentially conscious AI systems, and whether it’s justified</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open |00:00:00|  </li><li>Who’s Buck Shlegeris? |00:01:27|  </li><li>What's AI control? |00:01:51|  </li><li>Why is AI control hot now? |00:05:39|  </li><li>Detecting human vs AI spies |00:10:32|  </li><li>Acute vs chronic AI betrayal |00:15:21|  </li><li>How to catch AIs trying to escape |00:17:48|  </li><li>The cheapest AI control techniques |00:32:48|  </li><li>Can we get untrusted models to do trusted work? |00:38:58|  </li><li>If we catch a model escaping... will we do anything? |00:50:15|  </li><li>Getting AI models to think they've already escaped |00:52:51|  </li><li>Will they be able to tell it's a setup? |00:58:11|  </li><li>Will AI companies do any of this stuff? |01:00:11|  </li><li>Can we just give AIs fewer permissions? |01:06:14|  </li><li>Can we stop human spies the same way? |01:09:58|  </li><li>The pitch to AI companies to do this |01:15:04|  </li><li>Will AIs get superhuman so fast that this is all useless? |01:17:18|  </li><li>Risks from AI deliberately doing a bad job |01:18:37|  </li><li>Is alignment still useful? |01:24:49|  </li><li>Current alignment methods don't detect scheming |01:29:12|  </li><li>How to tell if AI control will work |01:31:40|  </li><li>How can listeners contribute? |01:35:53|  </li><li>Is 'controlling' AIs kind of a dick move? |01:37:13|  </li><li>Could 10 safety-focused people in an AGI company do anything useful? |01:42:27|  </li><li>Benefits of working outside frontier AI companies |01:47:48|  </li><li>Why Redwood Research does what it does |01:51:34|  </li><li>What other safety-related research looks best to Buck? |01:58:56|  </li><li>If an AI escapes, is it likely to be able to beat humanity from there? |01:59:48|  </li><li>Will misaligned models have to go rogue ASAP, before they're ready? |02:07:04|  </li><li>Is research on human scheming relevant to AI? |02:08:03|</li></ul><p><em>This episode was originally recorded on February 21, 2025.</em></p><p>Video: Simon Monsour and Luke Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, and Dominic Armstrong<br>Transcriptions and web: Katy Moore</p>]]>
      </content:encoded>
      <pubDate>Fri, 04 Apr 2025 11:59:15 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0dd67f40/bc9a94b9.mp3" length="130662325" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Zj5mDegnWTkXFYGhicd_GGUipoe4RbXNgZ8bKeB41Y4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS82MTMx/MjU3YjUyMDM4MDBi/ZTk1ZDA0YTA1Yjdl/OGFjMC5qcGc.jpg"/>
      <itunes:duration>8163</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Most AI safety conversations centre on alignment: ensuring AI systems share our values and goals. But despite progress, we’re unlikely to know we’ve solved the problem before the arrival of human-level and superhuman systems in as little as three years.</p><p>So some are developing a backup plan to safely deploy models we fear are actively scheming to harm us — so-called “AI control.” While this may sound mad, given the reluctance of AI companies to delay deploying anything they train, <em>not</em> developing such techniques is probably even crazier.</p><p>Today’s guest — Buck Shlegeris, CEO of <a href="https://www.redwoodresearch.org/">Redwood Research</a> — has spent the last few years developing control mechanisms, and for human-level systems they’re more plausible than you might think. He argues that given companies’ unwillingness to incur large costs for security, accepting the possibility of misalignment and designing robust safeguards might be one of our best remaining options.</p><p><a href="https://80k.info/buck"><strong>Links to learn more, highlights, video, and full transcript.</strong></a><strong></strong></p><p>As Buck puts it: "Five years ago I thought of misalignment risk from AIs as a really hard problem that you’d need some really galaxy-brained fundamental insights to resolve. Whereas now, to me the situation feels a lot more like we just really know a list of 40 things where, if you did them — none of which seem that hard — you’d probably be able to not have very much of your problem."</p><p>Of course, even if Buck is right, we still need to <em>do</em> those 40 things — which he points out we’re not on track for. And AI control agendas have their limitations: they aren’t likely to work once AI systems are much more capable than humans, since greatly superhuman AIs can probably work around whatever limitations we impose.</p><p>Still, AI control agendas seem to be gaining traction within AI safety. Buck and host Rob Wiblin discuss all of the above, plus:</p><ul><li>Why he’s more worried about AI hacking its own data centre than escaping</li><li>What to do about “chronic harm,” where AI systems subtly underperform or sabotage important work like alignment research</li><li>Why he might want to use a model he thought could be conspiring against him</li><li>Why he would feel safer if he caught an AI attempting to escape</li><li>Why many control techniques would be relatively inexpensive</li><li>How to use an untrusted model to monitor another untrusted model</li><li>What the minimum viable intervention in a “lazy” AI company might look like</li><li>How even small teams of safety-focused staff within AI labs could matter</li><li>The moral considerations around controlling potentially conscious AI systems, and whether it’s justified</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open |00:00:00|  </li><li>Who’s Buck Shlegeris? |00:01:27|  </li><li>What's AI control? |00:01:51|  </li><li>Why is AI control hot now? |00:05:39|  </li><li>Detecting human vs AI spies |00:10:32|  </li><li>Acute vs chronic AI betrayal |00:15:21|  </li><li>How to catch AIs trying to escape |00:17:48|  </li><li>The cheapest AI control techniques |00:32:48|  </li><li>Can we get untrusted models to do trusted work? |00:38:58|  </li><li>If we catch a model escaping... will we do anything? |00:50:15|  </li><li>Getting AI models to think they've already escaped |00:52:51|  </li><li>Will they be able to tell it's a setup? |00:58:11|  </li><li>Will AI companies do any of this stuff? |01:00:11|  </li><li>Can we just give AIs fewer permissions? |01:06:14|  </li><li>Can we stop human spies the same way? |01:09:58|  </li><li>The pitch to AI companies to do this |01:15:04|  </li><li>Will AIs get superhuman so fast that this is all useless? |01:17:18|  </li><li>Risks from AI deliberately doing a bad job |01:18:37|  </li><li>Is alignment still useful? |01:24:49|  </li><li>Current alignment methods don't detect scheming |01:29:12|  </li><li>How to tell if AI control will work |01:31:40|  </li><li>How can listeners contribute? |01:35:53|  </li><li>Is 'controlling' AIs kind of a dick move? |01:37:13|  </li><li>Could 10 safety-focused people in an AGI company do anything useful? |01:42:27|  </li><li>Benefits of working outside frontier AI companies |01:47:48|  </li><li>Why Redwood Research does what it does |01:51:34|  </li><li>What other safety-related research looks best to Buck? |01:58:56|  </li><li>If an AI escapes, is it likely to be able to beat humanity from there? |01:59:48|  </li><li>Will misaligned models have to go rogue ASAP, before they're ready? |02:07:04|  </li><li>Is research on human scheming relevant to AI? |02:08:03|</li></ul><p><em>This episode was originally recorded on February 21, 2025.</em></p><p>Video: Simon Monsour and Luke Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, and Dominic Armstrong<br>Transcriptions and web: Katy Moore</p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/0dd67f40/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/0dd67f40/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>15 expert takes on infosec in the age of AI</title>
      <itunes:title>15 expert takes on infosec in the age of AI</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">dd348254-d522-4ebd-bcad-dfb8955b2255</guid>
      <link>https://80000hours.org/podcast/episodes/infosecurity-compilation/?utm_campaign=podcast__infosec-compilation&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"There’s almost no story of the future going well that doesn’t have a part that’s like '…and no evil person steals the AI weights and goes and does evil stuff.' So it has highlighted the importance of information security: 'You’re training a powerful AI system; you should make it hard for someone to steal' has popped out to me as a thing that just keeps coming up in these stories, keeps being present. It’s hard to tell a story where it’s not a factor. It’s easy to tell a story where it is a factor." — Holden Karnofsky</p><p>What happens when a USB cable can secretly control your system? Are we hurtling toward a security nightmare as critical infrastructure connects to the internet? Is it possible to secure AI model weights from sophisticated attackers? And could AI might actually make computer security better rather than worse?</p><p>With AI security concerns becoming increasingly urgent, we bring you insights from 15 top experts across information security, AI safety, and governance, examining the challenges of protecting our most powerful AI models and digital infrastructure — including a sneak peek from an episode that hasn’t yet been released with Tom Davidson, where he explains how we should be more worried about “secret loyalties” in AI agents.<strong> </strong></p><p>You’ll hear:</p><ul><li>Holden Karnofsky on why every good future relies on strong infosec, and how hard it’s been to hire security experts (from <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">episode #158</a>)</li><li>Tantum Collins on why infosec might be the rare issue everyone agrees on (<a href="https://80000hours.org/podcast/episodes/tantum-collins-ai-policy-insider/">episode #166</a>)</li><li>Nick Joseph on whether AI companies can develop frontier models safely with the current state of information security (<a href="https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/">episode #197</a>)</li><li>Sella Nevo on why AI model weights are so valuable to steal, the weaknesses of air-gapped networks, and the risks of USBs (<a href="https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/">episode #195</a>)</li><li>Kevin Esvelt on what cryptographers can teach biosecurity experts (<a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/">episode #164</a>)</li><li>Lennart Heim on on Rob’s computer security nightmares (<a href="https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/">episode #155</a>)</li><li>Zvi Mowshowitz on the insane lack of security mindset at some AI companies (<a href="https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/">episode #184</a>)</li><li>Nova DasSarma on the best current defences against well-funded adversaries, politically motivated cyberattacks, and exciting progress in infosecurity (<a href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/">episode #132</a>)</li><li>Bruce Schneier on whether AI could eliminate software bugs for good, and why it’s bad to hook everything up to the internet (<a href="https://80000hours.org/podcast/episodes/bruce-schneier-security-secrets-and-surveillance/">episode #64</a>)</li><li>Nita Farahany on the dystopian risks of hacked neurotech (<a href="https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/">episode #174</a>)</li><li>Vitalik Buterin on how cybersecurity is the key to defence-dominant futures (<a href="https://80000hours.org/podcast/episodes/vitalik-buterin-techno-optimism/">episode #194</a>)</li><li>Nathan Labenz on how even internal teams at AI companies may not know what they’re building (<a href="https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/">episode #176</a>)</li><li>Allan Dafoe on backdooring your own AI to prevent theft (<a href="https://80000hours.org/podcast/episodes/allan-dafoe-unstoppable-technology-human-agency-agi/">episode #212</a>)</li><li>Tom Davidson on how dangerous “secret loyalties” in AI models could be (episode to be released!)</li><li>Carl Shulman on the challenge of trusting foreign AI models (<a href="https://80000hours.org/podcast/episodes/carl-shulman-society-agi">episode #191, part 2</a>)</li><li>Plus lots of <a href="https://80000hours.org/career-reviews/information-security/">concrete advice on how to get into this field and find your fit</a></li></ul><p><a href="https://80k.info/is"><strong>Check out the full transcript on the 80,000 Hours website</strong></a><strong>.</strong></p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:49)</li><li>Holden Karnofsky on why infosec could be the issue on which the future of humanity pivots (00:03:21)</li><li>Tantum Collins on why infosec is a rare AI issue that unifies everyone (00:12:39)</li><li>Nick Joseph on whether the current state of information security makes it impossible to responsibly train AGI (00:16:23)</li><li>Nova DasSarma on the best available defences against well-funded adversaries (00:22:10)</li><li>Sella Nevo on why AI model weights are so valuable to steal (00:28:56)</li><li>Kevin Esvelt on what cryptographers can teach biosecurity experts (00:32:24)</li><li>Lennart Heim on the possibility of an autonomously replicating AI computer worm (00:34:56)</li><li>Zvi Mowshowitz on the absurd lack of security mindset at some AI companies (00:48:22)</li><li>Sella Nevo on the weaknesses of air-gapped networks and the risks of USB devices (00:49:54)</li><li>Bruce Schneier on why it’s bad to hook everything up to the internet (00:55:54)</li><li>Nita Farahany on the possibility of hacking neural implants (01:04:47)</li><li>Vitalik Buterin on how cybersecurity is the key to defence-dominant futures (01:10:48)</li><li>Nova DasSarma on exciting progress in information security (01:19:28)</li><li>Nathan Labenz on how even internal teams at AI companies may not know what they’re building (01:30:47)</li><li>Allan Dafoe on backdooring your own AI to prevent someone else from stealing it (01:33:51)</li><li>Tom Davidson on how dangerous “secret loyalties” in AI models could get (01:35:57)</li><li>Carl Shulman on whether we should be worried about backdoors as governments adopt AI technology (01:52:45)</li><li>Nova DasSarma on politically motivated cyberattacks (02:03:44)</li><li>Bruce Schneier on the day-to-day benefits of improved security and recognising that there’s never zero risk (02:07:27)</li><li>Holden Karnofsky on why it’s so hard to hire security people despite the massive need (02:13:59)</li><li>Nova DasSarma on practical steps to getting into this field (02:16:37)</li><li>Bruce Schneier on finding your personal fit in a range of security careers (02:24:42)</li><li>Rob's outro (02:34:46)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"There’s almost no story of the future going well that doesn’t have a part that’s like '…and no evil person steals the AI weights and goes and does evil stuff.' So it has highlighted the importance of information security: 'You’re training a powerful AI system; you should make it hard for someone to steal' has popped out to me as a thing that just keeps coming up in these stories, keeps being present. It’s hard to tell a story where it’s not a factor. It’s easy to tell a story where it is a factor." — Holden Karnofsky</p><p>What happens when a USB cable can secretly control your system? Are we hurtling toward a security nightmare as critical infrastructure connects to the internet? Is it possible to secure AI model weights from sophisticated attackers? And could AI might actually make computer security better rather than worse?</p><p>With AI security concerns becoming increasingly urgent, we bring you insights from 15 top experts across information security, AI safety, and governance, examining the challenges of protecting our most powerful AI models and digital infrastructure — including a sneak peek from an episode that hasn’t yet been released with Tom Davidson, where he explains how we should be more worried about “secret loyalties” in AI agents.<strong> </strong></p><p>You’ll hear:</p><ul><li>Holden Karnofsky on why every good future relies on strong infosec, and how hard it’s been to hire security experts (from <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">episode #158</a>)</li><li>Tantum Collins on why infosec might be the rare issue everyone agrees on (<a href="https://80000hours.org/podcast/episodes/tantum-collins-ai-policy-insider/">episode #166</a>)</li><li>Nick Joseph on whether AI companies can develop frontier models safely with the current state of information security (<a href="https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/">episode #197</a>)</li><li>Sella Nevo on why AI model weights are so valuable to steal, the weaknesses of air-gapped networks, and the risks of USBs (<a href="https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/">episode #195</a>)</li><li>Kevin Esvelt on what cryptographers can teach biosecurity experts (<a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/">episode #164</a>)</li><li>Lennart Heim on on Rob’s computer security nightmares (<a href="https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/">episode #155</a>)</li><li>Zvi Mowshowitz on the insane lack of security mindset at some AI companies (<a href="https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/">episode #184</a>)</li><li>Nova DasSarma on the best current defences against well-funded adversaries, politically motivated cyberattacks, and exciting progress in infosecurity (<a href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/">episode #132</a>)</li><li>Bruce Schneier on whether AI could eliminate software bugs for good, and why it’s bad to hook everything up to the internet (<a href="https://80000hours.org/podcast/episodes/bruce-schneier-security-secrets-and-surveillance/">episode #64</a>)</li><li>Nita Farahany on the dystopian risks of hacked neurotech (<a href="https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/">episode #174</a>)</li><li>Vitalik Buterin on how cybersecurity is the key to defence-dominant futures (<a href="https://80000hours.org/podcast/episodes/vitalik-buterin-techno-optimism/">episode #194</a>)</li><li>Nathan Labenz on how even internal teams at AI companies may not know what they’re building (<a href="https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/">episode #176</a>)</li><li>Allan Dafoe on backdooring your own AI to prevent theft (<a href="https://80000hours.org/podcast/episodes/allan-dafoe-unstoppable-technology-human-agency-agi/">episode #212</a>)</li><li>Tom Davidson on how dangerous “secret loyalties” in AI models could be (episode to be released!)</li><li>Carl Shulman on the challenge of trusting foreign AI models (<a href="https://80000hours.org/podcast/episodes/carl-shulman-society-agi">episode #191, part 2</a>)</li><li>Plus lots of <a href="https://80000hours.org/career-reviews/information-security/">concrete advice on how to get into this field and find your fit</a></li></ul><p><a href="https://80k.info/is"><strong>Check out the full transcript on the 80,000 Hours website</strong></a><strong>.</strong></p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:49)</li><li>Holden Karnofsky on why infosec could be the issue on which the future of humanity pivots (00:03:21)</li><li>Tantum Collins on why infosec is a rare AI issue that unifies everyone (00:12:39)</li><li>Nick Joseph on whether the current state of information security makes it impossible to responsibly train AGI (00:16:23)</li><li>Nova DasSarma on the best available defences against well-funded adversaries (00:22:10)</li><li>Sella Nevo on why AI model weights are so valuable to steal (00:28:56)</li><li>Kevin Esvelt on what cryptographers can teach biosecurity experts (00:32:24)</li><li>Lennart Heim on the possibility of an autonomously replicating AI computer worm (00:34:56)</li><li>Zvi Mowshowitz on the absurd lack of security mindset at some AI companies (00:48:22)</li><li>Sella Nevo on the weaknesses of air-gapped networks and the risks of USB devices (00:49:54)</li><li>Bruce Schneier on why it’s bad to hook everything up to the internet (00:55:54)</li><li>Nita Farahany on the possibility of hacking neural implants (01:04:47)</li><li>Vitalik Buterin on how cybersecurity is the key to defence-dominant futures (01:10:48)</li><li>Nova DasSarma on exciting progress in information security (01:19:28)</li><li>Nathan Labenz on how even internal teams at AI companies may not know what they’re building (01:30:47)</li><li>Allan Dafoe on backdooring your own AI to prevent someone else from stealing it (01:33:51)</li><li>Tom Davidson on how dangerous “secret loyalties” in AI models could get (01:35:57)</li><li>Carl Shulman on whether we should be worried about backdoors as governments adopt AI technology (01:52:45)</li><li>Nova DasSarma on politically motivated cyberattacks (02:03:44)</li><li>Bruce Schneier on the day-to-day benefits of improved security and recognising that there’s never zero risk (02:07:27)</li><li>Holden Karnofsky on why it’s so hard to hire security people despite the massive need (02:13:59)</li><li>Nova DasSarma on practical steps to getting into this field (02:16:37)</li><li>Bruce Schneier on finding your personal fit in a range of security careers (02:24:42)</li><li>Rob's outro (02:34:46)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 28 Mar 2025 16:53:19 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5b2e024c/cff2c40b.mp3" length="149765491" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/sj1f_UNiQ1r_BlLU32yl10klX4s6QQsgOV7Ji37Qo20/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS83NmJm/ZTQ1Y2FjZTA4MjZk/OWY4YjY2ZTc0NDRm/NmU2OC5qcGc.jpg"/>
      <itunes:duration>9354</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"There’s almost no story of the future going well that doesn’t have a part that’s like '…and no evil person steals the AI weights and goes and does evil stuff.' So it has highlighted the importance of information security: 'You’re training a powerful AI system; you should make it hard for someone to steal' has popped out to me as a thing that just keeps coming up in these stories, keeps being present. It’s hard to tell a story where it’s not a factor. It’s easy to tell a story where it is a factor." — Holden Karnofsky</p><p>What happens when a USB cable can secretly control your system? Are we hurtling toward a security nightmare as critical infrastructure connects to the internet? Is it possible to secure AI model weights from sophisticated attackers? And could AI might actually make computer security better rather than worse?</p><p>With AI security concerns becoming increasingly urgent, we bring you insights from 15 top experts across information security, AI safety, and governance, examining the challenges of protecting our most powerful AI models and digital infrastructure — including a sneak peek from an episode that hasn’t yet been released with Tom Davidson, where he explains how we should be more worried about “secret loyalties” in AI agents.<strong> </strong></p><p>You’ll hear:</p><ul><li>Holden Karnofsky on why every good future relies on strong infosec, and how hard it’s been to hire security experts (from <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">episode #158</a>)</li><li>Tantum Collins on why infosec might be the rare issue everyone agrees on (<a href="https://80000hours.org/podcast/episodes/tantum-collins-ai-policy-insider/">episode #166</a>)</li><li>Nick Joseph on whether AI companies can develop frontier models safely with the current state of information security (<a href="https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/">episode #197</a>)</li><li>Sella Nevo on why AI model weights are so valuable to steal, the weaknesses of air-gapped networks, and the risks of USBs (<a href="https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/">episode #195</a>)</li><li>Kevin Esvelt on what cryptographers can teach biosecurity experts (<a href="https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/">episode #164</a>)</li><li>Lennart Heim on on Rob’s computer security nightmares (<a href="https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/">episode #155</a>)</li><li>Zvi Mowshowitz on the insane lack of security mindset at some AI companies (<a href="https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/">episode #184</a>)</li><li>Nova DasSarma on the best current defences against well-funded adversaries, politically motivated cyberattacks, and exciting progress in infosecurity (<a href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/">episode #132</a>)</li><li>Bruce Schneier on whether AI could eliminate software bugs for good, and why it’s bad to hook everything up to the internet (<a href="https://80000hours.org/podcast/episodes/bruce-schneier-security-secrets-and-surveillance/">episode #64</a>)</li><li>Nita Farahany on the dystopian risks of hacked neurotech (<a href="https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/">episode #174</a>)</li><li>Vitalik Buterin on how cybersecurity is the key to defence-dominant futures (<a href="https://80000hours.org/podcast/episodes/vitalik-buterin-techno-optimism/">episode #194</a>)</li><li>Nathan Labenz on how even internal teams at AI companies may not know what they’re building (<a href="https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/">episode #176</a>)</li><li>Allan Dafoe on backdooring your own AI to prevent theft (<a href="https://80000hours.org/podcast/episodes/allan-dafoe-unstoppable-technology-human-agency-agi/">episode #212</a>)</li><li>Tom Davidson on how dangerous “secret loyalties” in AI models could be (episode to be released!)</li><li>Carl Shulman on the challenge of trusting foreign AI models (<a href="https://80000hours.org/podcast/episodes/carl-shulman-society-agi">episode #191, part 2</a>)</li><li>Plus lots of <a href="https://80000hours.org/career-reviews/information-security/">concrete advice on how to get into this field and find your fit</a></li></ul><p><a href="https://80k.info/is"><strong>Check out the full transcript on the 80,000 Hours website</strong></a><strong>.</strong></p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:49)</li><li>Holden Karnofsky on why infosec could be the issue on which the future of humanity pivots (00:03:21)</li><li>Tantum Collins on why infosec is a rare AI issue that unifies everyone (00:12:39)</li><li>Nick Joseph on whether the current state of information security makes it impossible to responsibly train AGI (00:16:23)</li><li>Nova DasSarma on the best available defences against well-funded adversaries (00:22:10)</li><li>Sella Nevo on why AI model weights are so valuable to steal (00:28:56)</li><li>Kevin Esvelt on what cryptographers can teach biosecurity experts (00:32:24)</li><li>Lennart Heim on the possibility of an autonomously replicating AI computer worm (00:34:56)</li><li>Zvi Mowshowitz on the absurd lack of security mindset at some AI companies (00:48:22)</li><li>Sella Nevo on the weaknesses of air-gapped networks and the risks of USB devices (00:49:54)</li><li>Bruce Schneier on why it’s bad to hook everything up to the internet (00:55:54)</li><li>Nita Farahany on the possibility of hacking neural implants (01:04:47)</li><li>Vitalik Buterin on how cybersecurity is the key to defence-dominant futures (01:10:48)</li><li>Nova DasSarma on exciting progress in information security (01:19:28)</li><li>Nathan Labenz on how even internal teams at AI companies may not know what they’re building (01:30:47)</li><li>Allan Dafoe on backdooring your own AI to prevent someone else from stealing it (01:33:51)</li><li>Tom Davidson on how dangerous “secret loyalties” in AI models could get (01:35:57)</li><li>Carl Shulman on whether we should be worried about backdoors as governments adopt AI technology (01:52:45)</li><li>Nova DasSarma on politically motivated cyberattacks (02:03:44)</li><li>Bruce Schneier on the day-to-day benefits of improved security and recognising that there’s never zero risk (02:07:27)</li><li>Holden Karnofsky on why it’s so hard to hire security people despite the massive need (02:13:59)</li><li>Nova DasSarma on practical steps to getting into this field (02:16:37)</li><li>Bruce Schneier on finding your personal fit in a range of security careers (02:24:42)</li><li>Rob's outro (02:34:46)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Content editing: Katy Moore and Milo McGuire</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/5b2e024c/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/5b2e024c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#213 – Will MacAskill on AI causing a “century in a decade” – and how we're completely unprepared</title>
      <itunes:title>#213 – Will MacAskill on AI causing a “century in a decade” – and how we're completely unprepared</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">5a144186-e812-4a2a-9eec-ff3dfbcea00f</guid>
      <link>https://80000hours.org/podcast/episodes/will-macaskill-century-in-a-decade-navigating-intelligence-explosion/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>The 20th century saw unprecedented change: nuclear weapons, satellites, the rise and fall of communism, third-wave feminism, the internet, postmodernism, game theory, genetic engineering, the Big Bang theory, quantum mechanics, birth control, and more. Now imagine all of it compressed into just 10 years.</p><p>That’s the future Will MacAskill — philosopher, founding figure of effective altruism, and now researcher at the <a href="https://www.forethought.org/">Forethought Centre for AI Strategy</a> — argues we need to prepare for in his new paper “<a href="https://www.forethought.org/preparing-for-the-intelligence-explosion">Preparing for the intelligence explosion</a>.” Not in the distant future, but probably in three to seven years.</p><p><a href="https://80k.info/wm25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>The reason: AI systems are rapidly approaching human-level capability in scientific research and intellectual tasks. Once AI exceeds human abilities in AI research itself, we’ll enter a recursive self-improvement cycle — creating wildly more capable systems. Soon after, by improving algorithms and manufacturing chips, we’ll deploy millions, then billions, then <em>trillions</em> of superhuman AI scientists working 24/7 without human limitations. These systems will collaborate across disciplines, build on each discovery instantly, and conduct experiments at unprecedented scale and speed — compressing a century of scientific progress into mere years.</p><p>Will compares the resulting situation to a mediaeval king suddenly needing to upgrade from bows and arrows to nuclear weapons to deal with an ideological threat from a country he’s never heard of, while simultaneously grappling with learning that he descended from monkeys and his god doesn’t exist.</p><p>What makes this acceleration perilous is that while technology can speed up almost arbitrarily, human institutions and decision-making are much more fixed.</p><p>In this conversation with host Rob Wiblin, recorded on February 7, 2025, Will maps out the challenges we’d face in this potential “intelligence explosion” future, and what we might do to prepare. They discuss:</p><ul><li>Why leading AI safety researchers now think there’s dramatically less time before AI is transformative than they’d previously thought</li><li>The three different types of intelligence explosions that occur in order</li><li>Will’s list of resulting grand challenges — including destructive technologies, space governance, concentration of power, and digital rights</li><li>How to prevent ourselves from accidentally “locking in” mediocre futures for all eternity</li><li>Ways AI could radically improve human coordination and decision making</li><li>Why we should aim for truly flourishing futures, not just avoiding extinction</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who’s Will MacAskill? (00:00:46)</li><li>Why Will now just works on AGI (00:01:02)</li><li>Will was wrong(ish) on AI timelines and hinge of history (00:04:10)</li><li>A century of history crammed into a decade (00:09:00)</li><li>Science goes super fast; our institutions don't keep up (00:15:42)</li><li>Is it good or bad for intellectual progress to 10x? (00:21:03)</li><li>An intelligence explosion is not just plausible but likely (00:22:54)</li><li>Intellectual advances outside technology are similarly important (00:28:57)</li><li>Counterarguments to intelligence explosion (00:31:31)</li><li>The three types of intelligence explosion (software, technological, industrial) (00:37:29)</li><li>The industrial intelligence explosion is the most certain and enduring (00:40:23)</li><li>Is a 100x or 1,000x speedup more likely than 10x? (00:51:51)</li><li>The grand superintelligence challenges (00:55:37)</li><li>Grand challenge #1: Many new destructive technologies (00:59:17)</li><li>Grand challenge #2: Seizure of power by a small group (01:06:45)</li><li>Is global lock-in really plausible? (01:08:37)</li><li>Grand challenge #3: Space governance (01:18:53)</li><li>Is space truly defence-dominant? (01:28:43)</li><li>Grand challenge #4: Morally integrating with digital beings (01:32:20)</li><li>Will we ever know if digital minds are happy? (01:41:01)</li><li>“My worry isn't that we won't know; it's that we won't care” (01:46:31)</li><li>Can we get AGI to solve all these issues as early as possible? (01:49:40)</li><li>Politicians have to learn to use AI advisors (02:02:03)</li><li>Ensuring AI makes us smarter decision-makers (02:06:10)</li><li>How listeners can speed up AI epistemic tools (02:09:38)</li><li>AI could become great at forecasting (02:13:09)</li><li>How not to lock in a bad future (02:14:37)</li><li>AI takeover might happen anyway — should we rush to load in our values? (02:25:29)</li><li>ML researchers are feverishly working to destroy their own power (02:34:37)</li><li>We should aim for more than mere survival (02:37:54)</li><li>By default the future is rubbish (02:49:04)</li><li>No easy utopia (02:56:55)</li><li>What levers matter most to utopia (03:06:32)</li><li>Bottom lines from the modelling (03:20:09)</li><li>People distrust utopianism; should they distrust this? (03:24:09)</li><li>What conditions make eventual eutopia likely? (03:28:49)</li><li>The new Forethought Centre for AI Strategy (03:37:21)</li><li>How does Will resist hopelessness? (03:50:13)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Camera operator: Jeremy Chevillotte</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The 20th century saw unprecedented change: nuclear weapons, satellites, the rise and fall of communism, third-wave feminism, the internet, postmodernism, game theory, genetic engineering, the Big Bang theory, quantum mechanics, birth control, and more. Now imagine all of it compressed into just 10 years.</p><p>That’s the future Will MacAskill — philosopher, founding figure of effective altruism, and now researcher at the <a href="https://www.forethought.org/">Forethought Centre for AI Strategy</a> — argues we need to prepare for in his new paper “<a href="https://www.forethought.org/preparing-for-the-intelligence-explosion">Preparing for the intelligence explosion</a>.” Not in the distant future, but probably in three to seven years.</p><p><a href="https://80k.info/wm25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>The reason: AI systems are rapidly approaching human-level capability in scientific research and intellectual tasks. Once AI exceeds human abilities in AI research itself, we’ll enter a recursive self-improvement cycle — creating wildly more capable systems. Soon after, by improving algorithms and manufacturing chips, we’ll deploy millions, then billions, then <em>trillions</em> of superhuman AI scientists working 24/7 without human limitations. These systems will collaborate across disciplines, build on each discovery instantly, and conduct experiments at unprecedented scale and speed — compressing a century of scientific progress into mere years.</p><p>Will compares the resulting situation to a mediaeval king suddenly needing to upgrade from bows and arrows to nuclear weapons to deal with an ideological threat from a country he’s never heard of, while simultaneously grappling with learning that he descended from monkeys and his god doesn’t exist.</p><p>What makes this acceleration perilous is that while technology can speed up almost arbitrarily, human institutions and decision-making are much more fixed.</p><p>In this conversation with host Rob Wiblin, recorded on February 7, 2025, Will maps out the challenges we’d face in this potential “intelligence explosion” future, and what we might do to prepare. They discuss:</p><ul><li>Why leading AI safety researchers now think there’s dramatically less time before AI is transformative than they’d previously thought</li><li>The three different types of intelligence explosions that occur in order</li><li>Will’s list of resulting grand challenges — including destructive technologies, space governance, concentration of power, and digital rights</li><li>How to prevent ourselves from accidentally “locking in” mediocre futures for all eternity</li><li>Ways AI could radically improve human coordination and decision making</li><li>Why we should aim for truly flourishing futures, not just avoiding extinction</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who’s Will MacAskill? (00:00:46)</li><li>Why Will now just works on AGI (00:01:02)</li><li>Will was wrong(ish) on AI timelines and hinge of history (00:04:10)</li><li>A century of history crammed into a decade (00:09:00)</li><li>Science goes super fast; our institutions don't keep up (00:15:42)</li><li>Is it good or bad for intellectual progress to 10x? (00:21:03)</li><li>An intelligence explosion is not just plausible but likely (00:22:54)</li><li>Intellectual advances outside technology are similarly important (00:28:57)</li><li>Counterarguments to intelligence explosion (00:31:31)</li><li>The three types of intelligence explosion (software, technological, industrial) (00:37:29)</li><li>The industrial intelligence explosion is the most certain and enduring (00:40:23)</li><li>Is a 100x or 1,000x speedup more likely than 10x? (00:51:51)</li><li>The grand superintelligence challenges (00:55:37)</li><li>Grand challenge #1: Many new destructive technologies (00:59:17)</li><li>Grand challenge #2: Seizure of power by a small group (01:06:45)</li><li>Is global lock-in really plausible? (01:08:37)</li><li>Grand challenge #3: Space governance (01:18:53)</li><li>Is space truly defence-dominant? (01:28:43)</li><li>Grand challenge #4: Morally integrating with digital beings (01:32:20)</li><li>Will we ever know if digital minds are happy? (01:41:01)</li><li>“My worry isn't that we won't know; it's that we won't care” (01:46:31)</li><li>Can we get AGI to solve all these issues as early as possible? (01:49:40)</li><li>Politicians have to learn to use AI advisors (02:02:03)</li><li>Ensuring AI makes us smarter decision-makers (02:06:10)</li><li>How listeners can speed up AI epistemic tools (02:09:38)</li><li>AI could become great at forecasting (02:13:09)</li><li>How not to lock in a bad future (02:14:37)</li><li>AI takeover might happen anyway — should we rush to load in our values? (02:25:29)</li><li>ML researchers are feverishly working to destroy their own power (02:34:37)</li><li>We should aim for more than mere survival (02:37:54)</li><li>By default the future is rubbish (02:49:04)</li><li>No easy utopia (02:56:55)</li><li>What levers matter most to utopia (03:06:32)</li><li>Bottom lines from the modelling (03:20:09)</li><li>People distrust utopianism; should they distrust this? (03:24:09)</li><li>What conditions make eventual eutopia likely? (03:28:49)</li><li>The new Forethought Centre for AI Strategy (03:37:21)</li><li>How does Will resist hopelessness? (03:50:13)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Camera operator: Jeremy Chevillotte</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 11 Mar 2025 17:22:43 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f9ed8b9c/94fa2b53.mp3" length="228152480" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/hgMGP6WIyXPNJ8KSfYp3HUCEi3yo2jca9L9jEzAaZJw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9kNWFi/YzJhNTkxZTVhZWNi/ZTgzZGJmZWRkOTcz/Y2FjYi5qcGc.jpg"/>
      <itunes:duration>14256</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>The 20th century saw unprecedented change: nuclear weapons, satellites, the rise and fall of communism, third-wave feminism, the internet, postmodernism, game theory, genetic engineering, the Big Bang theory, quantum mechanics, birth control, and more. Now imagine all of it compressed into just 10 years.</p><p>That’s the future Will MacAskill — philosopher, founding figure of effective altruism, and now researcher at the <a href="https://www.forethought.org/">Forethought Centre for AI Strategy</a> — argues we need to prepare for in his new paper “<a href="https://www.forethought.org/preparing-for-the-intelligence-explosion">Preparing for the intelligence explosion</a>.” Not in the distant future, but probably in three to seven years.</p><p><a href="https://80k.info/wm25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>The reason: AI systems are rapidly approaching human-level capability in scientific research and intellectual tasks. Once AI exceeds human abilities in AI research itself, we’ll enter a recursive self-improvement cycle — creating wildly more capable systems. Soon after, by improving algorithms and manufacturing chips, we’ll deploy millions, then billions, then <em>trillions</em> of superhuman AI scientists working 24/7 without human limitations. These systems will collaborate across disciplines, build on each discovery instantly, and conduct experiments at unprecedented scale and speed — compressing a century of scientific progress into mere years.</p><p>Will compares the resulting situation to a mediaeval king suddenly needing to upgrade from bows and arrows to nuclear weapons to deal with an ideological threat from a country he’s never heard of, while simultaneously grappling with learning that he descended from monkeys and his god doesn’t exist.</p><p>What makes this acceleration perilous is that while technology can speed up almost arbitrarily, human institutions and decision-making are much more fixed.</p><p>In this conversation with host Rob Wiblin, recorded on February 7, 2025, Will maps out the challenges we’d face in this potential “intelligence explosion” future, and what we might do to prepare. They discuss:</p><ul><li>Why leading AI safety researchers now think there’s dramatically less time before AI is transformative than they’d previously thought</li><li>The three different types of intelligence explosions that occur in order</li><li>Will’s list of resulting grand challenges — including destructive technologies, space governance, concentration of power, and digital rights</li><li>How to prevent ourselves from accidentally “locking in” mediocre futures for all eternity</li><li>Ways AI could radically improve human coordination and decision making</li><li>Why we should aim for truly flourishing futures, not just avoiding extinction</li></ul><p><strong>Chapters:</strong></p><ul><li>Cold open (00:00:00)</li><li>Who’s Will MacAskill? (00:00:46)</li><li>Why Will now just works on AGI (00:01:02)</li><li>Will was wrong(ish) on AI timelines and hinge of history (00:04:10)</li><li>A century of history crammed into a decade (00:09:00)</li><li>Science goes super fast; our institutions don't keep up (00:15:42)</li><li>Is it good or bad for intellectual progress to 10x? (00:21:03)</li><li>An intelligence explosion is not just plausible but likely (00:22:54)</li><li>Intellectual advances outside technology are similarly important (00:28:57)</li><li>Counterarguments to intelligence explosion (00:31:31)</li><li>The three types of intelligence explosion (software, technological, industrial) (00:37:29)</li><li>The industrial intelligence explosion is the most certain and enduring (00:40:23)</li><li>Is a 100x or 1,000x speedup more likely than 10x? (00:51:51)</li><li>The grand superintelligence challenges (00:55:37)</li><li>Grand challenge #1: Many new destructive technologies (00:59:17)</li><li>Grand challenge #2: Seizure of power by a small group (01:06:45)</li><li>Is global lock-in really plausible? (01:08:37)</li><li>Grand challenge #3: Space governance (01:18:53)</li><li>Is space truly defence-dominant? (01:28:43)</li><li>Grand challenge #4: Morally integrating with digital beings (01:32:20)</li><li>Will we ever know if digital minds are happy? (01:41:01)</li><li>“My worry isn't that we won't know; it's that we won't care” (01:46:31)</li><li>Can we get AGI to solve all these issues as early as possible? (01:49:40)</li><li>Politicians have to learn to use AI advisors (02:02:03)</li><li>Ensuring AI makes us smarter decision-makers (02:06:10)</li><li>How listeners can speed up AI epistemic tools (02:09:38)</li><li>AI could become great at forecasting (02:13:09)</li><li>How not to lock in a bad future (02:14:37)</li><li>AI takeover might happen anyway — should we rush to load in our values? (02:25:29)</li><li>ML researchers are feverishly working to destroy their own power (02:34:37)</li><li>We should aim for more than mere survival (02:37:54)</li><li>By default the future is rubbish (02:49:04)</li><li>No easy utopia (02:56:55)</li><li>What levers matter most to utopia (03:06:32)</li><li>Bottom lines from the modelling (03:20:09)</li><li>People distrust utopianism; should they distrust this? (03:24:09)</li><li>What conditions make eventual eutopia likely? (03:28:49)</li><li>The new Forethought Centre for AI Strategy (03:37:21)</li><li>How does Will resist hopelessness? (03:50:13)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Camera operator: Jeremy Chevillotte</em><br><em>Transcriptions and web: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f9ed8b9c/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f9ed8b9c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Emergency pod: Judge plants a legal time bomb under OpenAI (with Rose Chan Loui)</title>
      <itunes:title>Emergency pod: Judge plants a legal time bomb under OpenAI (with Rose Chan Loui)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">48424375-5685-4df8-9122-abfb44c0d584</guid>
      <link>https://80000hours.org/podcast/episodes/rose-chan-loui-elon-musk-openai-case-update/?utm_campaign=podcast__rose-chan-loui&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>When OpenAI announced plans to convert from nonprofit to for-profit control last October, it likely didn’t anticipate the legal labyrinth it now faces. A recent court order in Elon Musk’s lawsuit against the company suggests OpenAI’s restructuring faces serious legal threats, which will complicate its efforts to raise tens of billions in investment.</p><p>As nonprofit legal expert Rose Chan Loui explains, the <a href="https://www.courthousenews.com/wp-content/uploads/2025/03/musk-vs-altman-order-denying-motion-preliminary-injunction.pdf">court order</a> set up multiple pathways for OpenAI’s conversion to be challenged. Though Judge Yvonne Gonzalez Rogers denied Musk’s request to block the conversion before a trial, she expedited proceedings to the fall so the case could be heard before it’s likely to go ahead. (See Rob’s <a href="https://x.com/robertwiblin/status/1897243130828619934">brief summary</a> of developments in the case.)</p><p>And if Musk’s donations to OpenAI are enough to give him the right to bring a case, Rogers sounded very sympathetic to his objections to the OpenAI foundation selling the company, benefiting the founders who forswore “any intent to use OpenAI as a vehicle to enrich themselves.”</p><p>But that’s just one of multiple threats. The attorneys general (AGs) in California and Delaware both have standing to object to the conversion on the grounds that it is contrary to the foundation’s charitable purpose and therefore wrongs the public — which was promised all the charitable assets would be used to develop AI that benefits all of humanity, not to win a commercial race. Some, including Rose, suspect the court order was written as a signal to those AGs to take action.</p><p>And, as she explains, if the AGs remain silent, the court itself, seeing that the public interest isn’t being represented, could appoint a “special interest party” to take on the case in their place.</p><p>This places the OpenAI foundation board in a bind: proceeding with the restructuring despite this legal cloud could expose them to the risk of being sued for a gross breach of their fiduciary duty to the public. The board is made up of respectable people who didn’t sign up for that.</p><p>And of course it would cause chaos for the company if all of OpenAI’s fundraising and governance plans were brought to a screeching halt by a federal court judgment landing at the eleventh hour.</p><p>Host Rob Wiblin and Rose Chan Loui discuss all of the above as well as what justification the OpenAI foundation could offer for giving up control of the company despite its charitable purpose, and how the board might adjust their plans to make the for-profit switch more legally palatable.</p><p><em>This episode was originally recorded on March 6, 2025.</em></p><p><strong>Chapters:</strong></p><ul><li>Intro (00:00:11)</li><li>More juicy OpenAI news (00:00:46)</li><li>The court order (00:02:11)</li><li>Elon has two hurdles to jump (00:05:17)</li><li>The judge's sympathy (00:08:00)</li><li>OpenAI's defence (00:11:45)</li><li>Alternative plans for OpenAI (00:13:41)</li><li>Should the foundation give up control? (00:16:38)</li><li>Alternative plaintiffs to Musk (00:21:13)</li><li>The 'special interest party' option (00:25:32)</li><li>How might this play out in the fall? (00:27:52)</li><li>The nonprofit board is in a bit of a bind (00:29:20)</li><li>Is it in the public interest to race? (00:32:23)</li><li>Could the board be personally negligent? (00:34:06)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>When OpenAI announced plans to convert from nonprofit to for-profit control last October, it likely didn’t anticipate the legal labyrinth it now faces. A recent court order in Elon Musk’s lawsuit against the company suggests OpenAI’s restructuring faces serious legal threats, which will complicate its efforts to raise tens of billions in investment.</p><p>As nonprofit legal expert Rose Chan Loui explains, the <a href="https://www.courthousenews.com/wp-content/uploads/2025/03/musk-vs-altman-order-denying-motion-preliminary-injunction.pdf">court order</a> set up multiple pathways for OpenAI’s conversion to be challenged. Though Judge Yvonne Gonzalez Rogers denied Musk’s request to block the conversion before a trial, she expedited proceedings to the fall so the case could be heard before it’s likely to go ahead. (See Rob’s <a href="https://x.com/robertwiblin/status/1897243130828619934">brief summary</a> of developments in the case.)</p><p>And if Musk’s donations to OpenAI are enough to give him the right to bring a case, Rogers sounded very sympathetic to his objections to the OpenAI foundation selling the company, benefiting the founders who forswore “any intent to use OpenAI as a vehicle to enrich themselves.”</p><p>But that’s just one of multiple threats. The attorneys general (AGs) in California and Delaware both have standing to object to the conversion on the grounds that it is contrary to the foundation’s charitable purpose and therefore wrongs the public — which was promised all the charitable assets would be used to develop AI that benefits all of humanity, not to win a commercial race. Some, including Rose, suspect the court order was written as a signal to those AGs to take action.</p><p>And, as she explains, if the AGs remain silent, the court itself, seeing that the public interest isn’t being represented, could appoint a “special interest party” to take on the case in their place.</p><p>This places the OpenAI foundation board in a bind: proceeding with the restructuring despite this legal cloud could expose them to the risk of being sued for a gross breach of their fiduciary duty to the public. The board is made up of respectable people who didn’t sign up for that.</p><p>And of course it would cause chaos for the company if all of OpenAI’s fundraising and governance plans were brought to a screeching halt by a federal court judgment landing at the eleventh hour.</p><p>Host Rob Wiblin and Rose Chan Loui discuss all of the above as well as what justification the OpenAI foundation could offer for giving up control of the company despite its charitable purpose, and how the board might adjust their plans to make the for-profit switch more legally palatable.</p><p><em>This episode was originally recorded on March 6, 2025.</em></p><p><strong>Chapters:</strong></p><ul><li>Intro (00:00:11)</li><li>More juicy OpenAI news (00:00:46)</li><li>The court order (00:02:11)</li><li>Elon has two hurdles to jump (00:05:17)</li><li>The judge's sympathy (00:08:00)</li><li>OpenAI's defence (00:11:45)</li><li>Alternative plans for OpenAI (00:13:41)</li><li>Should the foundation give up control? (00:16:38)</li><li>Alternative plaintiffs to Musk (00:21:13)</li><li>The 'special interest party' option (00:25:32)</li><li>How might this play out in the fall? (00:27:52)</li><li>The nonprofit board is in a bit of a bind (00:29:20)</li><li>Is it in the public interest to race? (00:32:23)</li><li>Could the board be personally negligent? (00:34:06)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 07 Mar 2025 18:48:58 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/36778e45/c6fd6152.mp3" length="35412332" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/TiqdiAR_ftyAgVioXUYgXT8uvN2Q4dESK_V_g2jeCCM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8yODNk/MTVhZDU1ODk2NGUx/NjIxODE3MWU2YTEz/ZWMwZi5wbmc.jpg"/>
      <itunes:duration>2210</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>When OpenAI announced plans to convert from nonprofit to for-profit control last October, it likely didn’t anticipate the legal labyrinth it now faces. A recent court order in Elon Musk’s lawsuit against the company suggests OpenAI’s restructuring faces serious legal threats, which will complicate its efforts to raise tens of billions in investment.</p><p>As nonprofit legal expert Rose Chan Loui explains, the <a href="https://www.courthousenews.com/wp-content/uploads/2025/03/musk-vs-altman-order-denying-motion-preliminary-injunction.pdf">court order</a> set up multiple pathways for OpenAI’s conversion to be challenged. Though Judge Yvonne Gonzalez Rogers denied Musk’s request to block the conversion before a trial, she expedited proceedings to the fall so the case could be heard before it’s likely to go ahead. (See Rob’s <a href="https://x.com/robertwiblin/status/1897243130828619934">brief summary</a> of developments in the case.)</p><p>And if Musk’s donations to OpenAI are enough to give him the right to bring a case, Rogers sounded very sympathetic to his objections to the OpenAI foundation selling the company, benefiting the founders who forswore “any intent to use OpenAI as a vehicle to enrich themselves.”</p><p>But that’s just one of multiple threats. The attorneys general (AGs) in California and Delaware both have standing to object to the conversion on the grounds that it is contrary to the foundation’s charitable purpose and therefore wrongs the public — which was promised all the charitable assets would be used to develop AI that benefits all of humanity, not to win a commercial race. Some, including Rose, suspect the court order was written as a signal to those AGs to take action.</p><p>And, as she explains, if the AGs remain silent, the court itself, seeing that the public interest isn’t being represented, could appoint a “special interest party” to take on the case in their place.</p><p>This places the OpenAI foundation board in a bind: proceeding with the restructuring despite this legal cloud could expose them to the risk of being sued for a gross breach of their fiduciary duty to the public. The board is made up of respectable people who didn’t sign up for that.</p><p>And of course it would cause chaos for the company if all of OpenAI’s fundraising and governance plans were brought to a screeching halt by a federal court judgment landing at the eleventh hour.</p><p>Host Rob Wiblin and Rose Chan Loui discuss all of the above as well as what justification the OpenAI foundation could offer for giving up control of the company despite its charitable purpose, and how the board might adjust their plans to make the for-profit switch more legally palatable.</p><p><em>This episode was originally recorded on March 6, 2025.</em></p><p><strong>Chapters:</strong></p><ul><li>Intro (00:00:11)</li><li>More juicy OpenAI news (00:00:46)</li><li>The court order (00:02:11)</li><li>Elon has two hurdles to jump (00:05:17)</li><li>The judge's sympathy (00:08:00)</li><li>OpenAI's defence (00:11:45)</li><li>Alternative plans for OpenAI (00:13:41)</li><li>Should the foundation give up control? (00:16:38)</li><li>Alternative plaintiffs to Musk (00:21:13)</li><li>The 'special interest party' option (00:25:32)</li><li>How might this play out in the fall? (00:27:52)</li><li>The nonprofit board is in a bit of a bind (00:29:20)</li><li>Is it in the public interest to race? (00:32:23)</li><li>Could the board be personally negligent? (00:34:06)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/36778e45/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/36778e45/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#139 Classic episode – Alan Hájek on puzzles and paradoxes in probability and expected value</title>
      <itunes:title>#139 Classic episode – Alan Hájek on puzzles and paradoxes in probability and expected value</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">0f985383-9ef1-484e-a16a-d0cd930838c8</guid>
      <link>https://80000hours.org/podcast/episodes/alan-hajek-probability-expected-value/</link>
      <description>
        <![CDATA[<p>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing to pay to play?</p><p>The standard way of analysing gambling problems, ‘expected value’ — in which you multiply probabilities by the value of each outcome and then sum them up — says your expected earnings are infinite. You have a 50% chance of winning $2, for '0.5 * $2 = $1' in expected earnings. A 25% chance of winning $4, for '0.25 * $4 = $1' in expected earnings, and on and on. A never-ending series of $1s added together comes to infinity. And that's despite the fact that you know with certainty you can only ever win a finite amount!</p><p>Today's guest — philosopher Alan Hájek of the Australian National University — thinks of much of philosophy as “the demolition of common sense followed by damage control” and is an expert on paradoxes related to probability and decision-making rules like “maximise expected value.”</p><p><strong>Rebroadcast: this episode was originally released in October 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/alan-hajek-probability-expected-value/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>The problem described above, known as the St. Petersburg paradox, has been a staple of the field since the 18th century, with many proposed solutions. In the interview, Alan explains how very natural attempts to resolve the paradox — such as factoring in the low likelihood that the casino can pay out very large sums, or the fact that money becomes less and less valuable the more of it you already have — fail to work as hoped.</p><p>We might reject the setup as a hypothetical that could never exist in the real world, and therefore of mere intellectual curiosity. But Alan doesn't find that objection persuasive. If expected value fails in extreme cases, that should make us worry that something could be rotten at the heart of the standard procedure we use to make decisions in government, business, and nonprofits.</p><p>These issues regularly show up in 80,000 Hours' efforts to try to find the best ways to improve the world, as the best approach will arguably involve long-shot attempts to do very large amounts of good.</p><p>Consider which is better: saving one life for sure, or three lives with 50% probability? Expected value says the second, which will probably strike you as reasonable enough. But what if we repeat this process and evaluate the chance to save nine lives with 25% probability, or 27 lives with 12.5% probability, or after 17 more iterations, 3,486,784,401 lives with a 0.00000009% chance. Expected value says this final offer is better than the others — 1,000 times better, in fact.</p><p>Ultimately Alan leans towards the view that our best choice is to “bite the bullet” and stick with expected value, even with its sometimes counterintuitive implications. Where we want to do damage control, we're better off looking for ways our probability estimates might be wrong.</p><p>In this conversation, originally released in October 2022, Alan and Rob explore these issues and many others:</p><ul><li>Simple rules of thumb for having philosophical insights</li><li>A key flaw that hid in Pascal's wager from the very beginning</li><li>Whether we have to simply ignore infinities because they mess everything up</li><li>What fundamentally is 'probability'?</li><li>Some of the many reasons 'frequentism' doesn't work as an account of probability</li><li>Why the standard account of counterfactuals in philosophy is deeply flawed</li><li>And why counterfactuals present a fatal problem for one sort of consequentialism</li></ul><p>Chapters:</p><ul><li>Cold open {00:00:00}</li><li>Rob's intro {00:01:05}</li><li>The interview begins {00:05:28}</li><li>Philosophical methodology {00:06:35}</li><li>Theories of probability {00:40:58}</li><li>Everyday Bayesianism {00:49:42}</li><li>Frequentism {01:08:37}</li><li>Ranges of probabilities {01:20:05}</li><li>Implications for how to live {01:25:05}</li><li>Expected value {01:30:39}</li><li>The St. Petersburg paradox {01:35:21}</li><li>Pascal’s wager {01:53:25}</li><li>Using expected value in everyday life {02:07:34}</li><li>Counterfactuals {02:20:19}</li><li>Most counterfactuals are false {02:56:06}</li><li>Relevance to objective consequentialism {03:13:28}</li><li>Alan’s best conference story {03:37:18}</li><li>Rob's outro {03:40:22}</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing to pay to play?</p><p>The standard way of analysing gambling problems, ‘expected value’ — in which you multiply probabilities by the value of each outcome and then sum them up — says your expected earnings are infinite. You have a 50% chance of winning $2, for '0.5 * $2 = $1' in expected earnings. A 25% chance of winning $4, for '0.25 * $4 = $1' in expected earnings, and on and on. A never-ending series of $1s added together comes to infinity. And that's despite the fact that you know with certainty you can only ever win a finite amount!</p><p>Today's guest — philosopher Alan Hájek of the Australian National University — thinks of much of philosophy as “the demolition of common sense followed by damage control” and is an expert on paradoxes related to probability and decision-making rules like “maximise expected value.”</p><p><strong>Rebroadcast: this episode was originally released in October 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/alan-hajek-probability-expected-value/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>The problem described above, known as the St. Petersburg paradox, has been a staple of the field since the 18th century, with many proposed solutions. In the interview, Alan explains how very natural attempts to resolve the paradox — such as factoring in the low likelihood that the casino can pay out very large sums, or the fact that money becomes less and less valuable the more of it you already have — fail to work as hoped.</p><p>We might reject the setup as a hypothetical that could never exist in the real world, and therefore of mere intellectual curiosity. But Alan doesn't find that objection persuasive. If expected value fails in extreme cases, that should make us worry that something could be rotten at the heart of the standard procedure we use to make decisions in government, business, and nonprofits.</p><p>These issues regularly show up in 80,000 Hours' efforts to try to find the best ways to improve the world, as the best approach will arguably involve long-shot attempts to do very large amounts of good.</p><p>Consider which is better: saving one life for sure, or three lives with 50% probability? Expected value says the second, which will probably strike you as reasonable enough. But what if we repeat this process and evaluate the chance to save nine lives with 25% probability, or 27 lives with 12.5% probability, or after 17 more iterations, 3,486,784,401 lives with a 0.00000009% chance. Expected value says this final offer is better than the others — 1,000 times better, in fact.</p><p>Ultimately Alan leans towards the view that our best choice is to “bite the bullet” and stick with expected value, even with its sometimes counterintuitive implications. Where we want to do damage control, we're better off looking for ways our probability estimates might be wrong.</p><p>In this conversation, originally released in October 2022, Alan and Rob explore these issues and many others:</p><ul><li>Simple rules of thumb for having philosophical insights</li><li>A key flaw that hid in Pascal's wager from the very beginning</li><li>Whether we have to simply ignore infinities because they mess everything up</li><li>What fundamentally is 'probability'?</li><li>Some of the many reasons 'frequentism' doesn't work as an account of probability</li><li>Why the standard account of counterfactuals in philosophy is deeply flawed</li><li>And why counterfactuals present a fatal problem for one sort of consequentialism</li></ul><p>Chapters:</p><ul><li>Cold open {00:00:00}</li><li>Rob's intro {00:01:05}</li><li>The interview begins {00:05:28}</li><li>Philosophical methodology {00:06:35}</li><li>Theories of probability {00:40:58}</li><li>Everyday Bayesianism {00:49:42}</li><li>Frequentism {01:08:37}</li><li>Ranges of probabilities {01:20:05}</li><li>Implications for how to live {01:25:05}</li><li>Expected value {01:30:39}</li><li>The St. Petersburg paradox {01:35:21}</li><li>Pascal’s wager {01:53:25}</li><li>Using expected value in everyday life {02:07:34}</li><li>Counterfactuals {02:20:19}</li><li>Most counterfactuals are false {02:56:06}</li><li>Relevance to objective consequentialism {03:13:28}</li><li>Alan’s best conference story {03:37:18}</li><li>Rob's outro {03:40:22}</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 25 Feb 2025 15:06:24 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/27c977a7/41fba947.mp3" length="212693319" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/piad9fqnFjvIok_Lnqn80Zj9faYIb-IOZlOvjx1vQDc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS83N2Jj/ZjhiYWI4OWYxZmI4/MGFmY2NjZWE3YTRl/NTBhMC5qcGc.jpg"/>
      <itunes:duration>13291</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing to pay to play?</p><p>The standard way of analysing gambling problems, ‘expected value’ — in which you multiply probabilities by the value of each outcome and then sum them up — says your expected earnings are infinite. You have a 50% chance of winning $2, for '0.5 * $2 = $1' in expected earnings. A 25% chance of winning $4, for '0.25 * $4 = $1' in expected earnings, and on and on. A never-ending series of $1s added together comes to infinity. And that's despite the fact that you know with certainty you can only ever win a finite amount!</p><p>Today's guest — philosopher Alan Hájek of the Australian National University — thinks of much of philosophy as “the demolition of common sense followed by damage control” and is an expert on paradoxes related to probability and decision-making rules like “maximise expected value.”</p><p><strong>Rebroadcast: this episode was originally released in October 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/alan-hajek-probability-expected-value/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>The problem described above, known as the St. Petersburg paradox, has been a staple of the field since the 18th century, with many proposed solutions. In the interview, Alan explains how very natural attempts to resolve the paradox — such as factoring in the low likelihood that the casino can pay out very large sums, or the fact that money becomes less and less valuable the more of it you already have — fail to work as hoped.</p><p>We might reject the setup as a hypothetical that could never exist in the real world, and therefore of mere intellectual curiosity. But Alan doesn't find that objection persuasive. If expected value fails in extreme cases, that should make us worry that something could be rotten at the heart of the standard procedure we use to make decisions in government, business, and nonprofits.</p><p>These issues regularly show up in 80,000 Hours' efforts to try to find the best ways to improve the world, as the best approach will arguably involve long-shot attempts to do very large amounts of good.</p><p>Consider which is better: saving one life for sure, or three lives with 50% probability? Expected value says the second, which will probably strike you as reasonable enough. But what if we repeat this process and evaluate the chance to save nine lives with 25% probability, or 27 lives with 12.5% probability, or after 17 more iterations, 3,486,784,401 lives with a 0.00000009% chance. Expected value says this final offer is better than the others — 1,000 times better, in fact.</p><p>Ultimately Alan leans towards the view that our best choice is to “bite the bullet” and stick with expected value, even with its sometimes counterintuitive implications. Where we want to do damage control, we're better off looking for ways our probability estimates might be wrong.</p><p>In this conversation, originally released in October 2022, Alan and Rob explore these issues and many others:</p><ul><li>Simple rules of thumb for having philosophical insights</li><li>A key flaw that hid in Pascal's wager from the very beginning</li><li>Whether we have to simply ignore infinities because they mess everything up</li><li>What fundamentally is 'probability'?</li><li>Some of the many reasons 'frequentism' doesn't work as an account of probability</li><li>Why the standard account of counterfactuals in philosophy is deeply flawed</li><li>And why counterfactuals present a fatal problem for one sort of consequentialism</li></ul><p>Chapters:</p><ul><li>Cold open {00:00:00}</li><li>Rob's intro {00:01:05}</li><li>The interview begins {00:05:28}</li><li>Philosophical methodology {00:06:35}</li><li>Theories of probability {00:40:58}</li><li>Everyday Bayesianism {00:49:42}</li><li>Frequentism {01:08:37}</li><li>Ranges of probabilities {01:20:05}</li><li>Implications for how to live {01:25:05}</li><li>Expected value {01:30:39}</li><li>The St. Petersburg paradox {01:35:21}</li><li>Pascal’s wager {01:53:25}</li><li>Using expected value in everyday life {02:07:34}</li><li>Counterfactuals {02:20:19}</li><li>Most counterfactuals are false {02:56:06}</li><li>Relevance to objective consequentialism {03:13:28}</li><li>Alan’s best conference story {03:37:18}</li><li>Rob's outro {03:40:22}</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/27c977a7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#143 Classic episode – Jeffrey Lewis on the most common misconceptions about nuclear weapons</title>
      <itunes:title>#143 Classic episode – Jeffrey Lewis on the most common misconceptions about nuclear weapons</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">93d81cc8-567d-4167-a189-1c5e3c9acf48</guid>
      <link>https://80000hours.org/podcast/episodes/jeffrey-lewis-common-misconceptions-about-nuclear-weapons/</link>
      <description>
        <![CDATA[<p>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially.</p><p>As today's guest — Jeffrey Lewis, founder of <a href="https://www.armscontrolwonk.com/"><em>Arms Control Wonk</em></a> and professor at the Middlebury Institute of International Studies — explains, in its official 'OPLANs' (military operation plans), the US is committed to 'dominating' in a nuclear war with Russia. How would they do that? "That is redacted."</p><p><strong>Rebroadcast: this episode was originally released in December 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/jeffrey-lewis-common-misconceptions-about-nuclear-weapons/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>We invited Jeffrey to come on the show to lay out what we and our listeners are most likely to be misunderstanding about nuclear weapons, the nuclear posture of major powers, and his field as a whole, and he did not disappoint.</p><p>As Jeffrey tells it, 'mutually assured destruction' was a slur used to criticise those who wanted to limit the 1960s arms buildup, and was never accepted as a matter of policy in any US administration. But isn't it still the de facto reality? Yes and no.</p><p>Jeffrey is a specialist on the nuts and bolts of bureaucratic and military decision-making in real-life situations. He suspects that at the start of their term presidents get a briefing about the US' plan to prevail in a nuclear war and conclude that "it's freaking madness." They say to themselves that whatever these silly plans may say, they know a nuclear war cannot be won, so they just won't use the weapons.</p><p>But Jeffrey thinks that's a big mistake. Yes, in a calm moment presidents can resist pressure from advisors and generals. But that idea of ‘winning’ a nuclear war is in all the plans. Staff have been hired because they believe in those plans. It's what the generals and admirals have all prepared for.</p><p>What matters is the 'not calm moment': the 3AM phone call to tell the president that ICBMs might hit the US in eight minutes — the same week Russia invades a neighbour or China invades Taiwan. Is it a false alarm? Should they retaliate before their land-based missile silos are hit? There's only minutes to decide.</p><p>Jeffrey points out that in emergencies, presidents have repeatedly found themselves railroaded into actions they didn't want to take because of how information and options were processed and presented to them. In the heat of the moment, it's natural to reach for the plan you've prepared — however mad it might sound.</p><p>In this spicy conversation, Jeffrey fields the most burning questions from Rob and the audience, in the process explaining:</p><ul><li>Why inter-service rivalry is one of the biggest constraints on US nuclear policy</li><li>Two times the US sabotaged nuclear nonproliferation among great powers</li><li>How his field uses jargon to exclude outsiders</li><li>How the US could prevent the revival of mass nuclear testing by the great powers</li><li>Why nuclear deterrence relies on the possibility that something might go wrong</li><li>Whether 'salami tactics' render nuclear weapons ineffective</li><li>The time the Navy and Air Force switched views on how to wage a nuclear war, just when it would allow *them* to have the most missiles</li><li>The problems that arise when you won't talk to people you think are evil</li><li>Why missile defences are politically popular despite being strategically foolish</li><li>How open source intelligence can prevent arms races</li><li>And much more.</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially.</p><p>As today's guest — Jeffrey Lewis, founder of <a href="https://www.armscontrolwonk.com/"><em>Arms Control Wonk</em></a> and professor at the Middlebury Institute of International Studies — explains, in its official 'OPLANs' (military operation plans), the US is committed to 'dominating' in a nuclear war with Russia. How would they do that? "That is redacted."</p><p><strong>Rebroadcast: this episode was originally released in December 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/jeffrey-lewis-common-misconceptions-about-nuclear-weapons/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>We invited Jeffrey to come on the show to lay out what we and our listeners are most likely to be misunderstanding about nuclear weapons, the nuclear posture of major powers, and his field as a whole, and he did not disappoint.</p><p>As Jeffrey tells it, 'mutually assured destruction' was a slur used to criticise those who wanted to limit the 1960s arms buildup, and was never accepted as a matter of policy in any US administration. But isn't it still the de facto reality? Yes and no.</p><p>Jeffrey is a specialist on the nuts and bolts of bureaucratic and military decision-making in real-life situations. He suspects that at the start of their term presidents get a briefing about the US' plan to prevail in a nuclear war and conclude that "it's freaking madness." They say to themselves that whatever these silly plans may say, they know a nuclear war cannot be won, so they just won't use the weapons.</p><p>But Jeffrey thinks that's a big mistake. Yes, in a calm moment presidents can resist pressure from advisors and generals. But that idea of ‘winning’ a nuclear war is in all the plans. Staff have been hired because they believe in those plans. It's what the generals and admirals have all prepared for.</p><p>What matters is the 'not calm moment': the 3AM phone call to tell the president that ICBMs might hit the US in eight minutes — the same week Russia invades a neighbour or China invades Taiwan. Is it a false alarm? Should they retaliate before their land-based missile silos are hit? There's only minutes to decide.</p><p>Jeffrey points out that in emergencies, presidents have repeatedly found themselves railroaded into actions they didn't want to take because of how information and options were processed and presented to them. In the heat of the moment, it's natural to reach for the plan you've prepared — however mad it might sound.</p><p>In this spicy conversation, Jeffrey fields the most burning questions from Rob and the audience, in the process explaining:</p><ul><li>Why inter-service rivalry is one of the biggest constraints on US nuclear policy</li><li>Two times the US sabotaged nuclear nonproliferation among great powers</li><li>How his field uses jargon to exclude outsiders</li><li>How the US could prevent the revival of mass nuclear testing by the great powers</li><li>Why nuclear deterrence relies on the possibility that something might go wrong</li><li>Whether 'salami tactics' render nuclear weapons ineffective</li><li>The time the Navy and Air Force switched views on how to wage a nuclear war, just when it would allow *them* to have the most missiles</li><li>The problems that arise when you won't talk to people you think are evil</li><li>Why missile defences are politically popular despite being strategically foolish</li><li>How open source intelligence can prevent arms races</li><li>And much more.</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 19 Feb 2025 13:56:50 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/dc3348ef/37d7962a.mp3" length="154510452" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/YjvaoaG5vwgwgBk_6ayIqQyCCT2ITBVcaPYmF3eIcuM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xNmYw/MGJlZTVhNGZkZmFi/NGE0NDk2NDlmNzgw/ZjE5NS5qcGc.jpg"/>
      <itunes:duration>9652</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially.</p><p>As today's guest — Jeffrey Lewis, founder of <a href="https://www.armscontrolwonk.com/"><em>Arms Control Wonk</em></a> and professor at the Middlebury Institute of International Studies — explains, in its official 'OPLANs' (military operation plans), the US is committed to 'dominating' in a nuclear war with Russia. How would they do that? "That is redacted."</p><p><strong>Rebroadcast: this episode was originally released in December 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/jeffrey-lewis-common-misconceptions-about-nuclear-weapons/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>We invited Jeffrey to come on the show to lay out what we and our listeners are most likely to be misunderstanding about nuclear weapons, the nuclear posture of major powers, and his field as a whole, and he did not disappoint.</p><p>As Jeffrey tells it, 'mutually assured destruction' was a slur used to criticise those who wanted to limit the 1960s arms buildup, and was never accepted as a matter of policy in any US administration. But isn't it still the de facto reality? Yes and no.</p><p>Jeffrey is a specialist on the nuts and bolts of bureaucratic and military decision-making in real-life situations. He suspects that at the start of their term presidents get a briefing about the US' plan to prevail in a nuclear war and conclude that "it's freaking madness." They say to themselves that whatever these silly plans may say, they know a nuclear war cannot be won, so they just won't use the weapons.</p><p>But Jeffrey thinks that's a big mistake. Yes, in a calm moment presidents can resist pressure from advisors and generals. But that idea of ‘winning’ a nuclear war is in all the plans. Staff have been hired because they believe in those plans. It's what the generals and admirals have all prepared for.</p><p>What matters is the 'not calm moment': the 3AM phone call to tell the president that ICBMs might hit the US in eight minutes — the same week Russia invades a neighbour or China invades Taiwan. Is it a false alarm? Should they retaliate before their land-based missile silos are hit? There's only minutes to decide.</p><p>Jeffrey points out that in emergencies, presidents have repeatedly found themselves railroaded into actions they didn't want to take because of how information and options were processed and presented to them. In the heat of the moment, it's natural to reach for the plan you've prepared — however mad it might sound.</p><p>In this spicy conversation, Jeffrey fields the most burning questions from Rob and the audience, in the process explaining:</p><ul><li>Why inter-service rivalry is one of the biggest constraints on US nuclear policy</li><li>Two times the US sabotaged nuclear nonproliferation among great powers</li><li>How his field uses jargon to exclude outsiders</li><li>How the US could prevent the revival of mass nuclear testing by the great powers</li><li>Why nuclear deterrence relies on the possibility that something might go wrong</li><li>Whether 'salami tactics' render nuclear weapons ineffective</li><li>The time the Navy and Air Force switched views on how to wage a nuclear war, just when it would allow *them* to have the most missiles</li><li>The problems that arise when you won't talk to people you think are evil</li><li>Why missile defences are politically popular despite being strategically foolish</li><li>How open source intelligence can prevent arms races</li><li>And much more.</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/dc3348ef/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#212 – Allan Dafoe on why technology is unstoppable &amp; how to shape AI development anyway</title>
      <itunes:title>#212 – Allan Dafoe on why technology is unstoppable &amp; how to shape AI development anyway</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">19359ac4-ba87-46d7-9856-6a817639b94a</guid>
      <link>https://80000hours.org/podcast/episodes/allan-dafoe-unstoppable-technology-human-agency-agi/?utm_campaign=podcast__allan-dafoe&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Technology doesn’t force us to do anything — it merely opens doors. But military and economic competition pushes us through.</p><p>That’s how today’s guest <a href="https://www.allandafoe.com/">Allan Dafoe</a> — director of frontier safety and governance at Google DeepMind — explains one of the deepest patterns in technological history: once a powerful new capability becomes available, societies that adopt it tend to outcompete those that don’t. Those who resist too much can find themselves taken over or rendered irrelevant.</p><p><a href="https://80k.info/ad25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>This dynamic played out dramatically in 1853 when US Commodore Perry sailed into Tokyo Bay with steam-powered warships that seemed magical to the Japanese, who had spent centuries deliberately limiting their technological development. With far greater military power, the US was able to force Japan to open itself to trade. Within 15 years, Japan had undergone the Meiji Restoration and transformed itself in a desperate scramble to catch up.</p><p>Today we see hints of similar pressure around artificial intelligence. Even companies, countries, and researchers deeply concerned about where AI could take us feel compelled to push ahead — worried that if they don’t, less careful actors will develop transformative AI capabilities at around the same time anyway.</p><p>But Allan argues this technological determinism isn’t absolute. While broad patterns may be inevitable, history shows we do have some ability to steer how technologies are developed, by who, and what they’re used for first.</p><p>As part of that approach, Allan has been promoting <a href="https://www.cognitiverevolution.ai/claude-cooperates-exploring-cultural-evolution-in-llm-societies-with-aron-vallinder-edward-hughes/">efforts</a> to make AI more capable of sophisticated cooperation, and improving the tests Google uses to measure how well its models could do things like mislead people, hack and take control of their own servers, or spread autonomously in the wild.</p><p>As of mid-2024 they didn’t seem dangerous at all, but we’ve learned that our ability to measure these capabilities is good, but imperfect. If we don’t find the right way to ‘elicit’ an ability we can miss that it’s there.</p><p><a href="https://www.anthropic.com/research/alignment-faking">Subsequent research</a> from Anthropic and Redwood Research suggests there’s even a risk that future models may play dumb to avoid their goals being altered.</p><p>That has led DeepMind to a “defence in depth” approach: carefully staged deployment starting with internal testing, then trusted external testers, then limited release, then watching how models are used in the real world. By not releasing model weights, DeepMind is able to back up and add additional safeguards if experience shows they’re necessary.</p><p>But with much more powerful and general models on the way, individual company policies won’t be sufficient by themselves. Drawing on his academic research into how societies handle transformative technologies, Allan argues we need coordinated international governance that balances safety with our desire to get the massive potential benefits of AI in areas like healthcare and education as quickly as possible.</p><p>Host Rob and Allan also cover:</p><ul><li>The most exciting beneficial applications of AI</li><li>Whether and how we can influence the development of technology</li><li>What DeepMind is doing to evaluate and mitigate risks from frontier AI systems</li><li>Why cooperative AI may be as important as aligned AI</li><li>The role of democratic input in AI governance</li><li>What kinds of experts are most needed in AI safety and governance</li><li>And much more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Who's Allan Dafoe? (00:00:48)</li><li>Allan's role at DeepMind (00:01:27)</li><li>Why join DeepMind over everyone else? (00:04:27)</li><li>Do humans control technological change? (00:09:17)</li><li>Arguments for technological determinism (00:20:24)</li><li>The synthesis of agency with tech determinism (00:26:29)</li><li>Competition took away Japan's choice (00:37:13)</li><li>Can speeding up one tech redirect history? (00:42:09)</li><li>Structural pushback against alignment efforts (00:47:55)</li><li>Do AIs need to be 'cooperatively skilled'? (00:52:25)</li><li>How AI could boost cooperation between people and states (01:01:59)</li><li>The super-cooperative AGI hypothesis and backdoor risks (01:06:58)</li><li>Aren’t today’s models already very cooperative? (01:13:22)</li><li>How would we make AIs cooperative anyway? (01:16:22)</li><li>Ways making AI more cooperative could backfire (01:22:24)</li><li>AGI is an essential idea we should define well (01:30:16)</li><li>It matters what AGI learns first vs last (01:41:01)</li><li>How Google tests for dangerous capabilities (01:45:39)</li><li>Evals 'in the wild' (01:57:46)</li><li>What to do given no single approach works that well (02:01:44)</li><li>We don't, but could, forecast AI capabilities (02:05:34)</li><li>DeepMind's strategy for ensuring its frontier models don't cause harm (02:11:25)</li><li>How 'structural risks' can force everyone into a worse world (02:15:01)</li><li>Is AI being built democratically? Should it? (02:19:35)</li><li>How much do AI companies really want external regulation? (02:24:34)</li><li>Social science can contribute a lot here (02:33:21)</li><li>How AI could make life way better: self-driving cars, medicine, education, and sustainability (02:35:55)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Camera operator: Jeremy Chevillotte</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Technology doesn’t force us to do anything — it merely opens doors. But military and economic competition pushes us through.</p><p>That’s how today’s guest <a href="https://www.allandafoe.com/">Allan Dafoe</a> — director of frontier safety and governance at Google DeepMind — explains one of the deepest patterns in technological history: once a powerful new capability becomes available, societies that adopt it tend to outcompete those that don’t. Those who resist too much can find themselves taken over or rendered irrelevant.</p><p><a href="https://80k.info/ad25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>This dynamic played out dramatically in 1853 when US Commodore Perry sailed into Tokyo Bay with steam-powered warships that seemed magical to the Japanese, who had spent centuries deliberately limiting their technological development. With far greater military power, the US was able to force Japan to open itself to trade. Within 15 years, Japan had undergone the Meiji Restoration and transformed itself in a desperate scramble to catch up.</p><p>Today we see hints of similar pressure around artificial intelligence. Even companies, countries, and researchers deeply concerned about where AI could take us feel compelled to push ahead — worried that if they don’t, less careful actors will develop transformative AI capabilities at around the same time anyway.</p><p>But Allan argues this technological determinism isn’t absolute. While broad patterns may be inevitable, history shows we do have some ability to steer how technologies are developed, by who, and what they’re used for first.</p><p>As part of that approach, Allan has been promoting <a href="https://www.cognitiverevolution.ai/claude-cooperates-exploring-cultural-evolution-in-llm-societies-with-aron-vallinder-edward-hughes/">efforts</a> to make AI more capable of sophisticated cooperation, and improving the tests Google uses to measure how well its models could do things like mislead people, hack and take control of their own servers, or spread autonomously in the wild.</p><p>As of mid-2024 they didn’t seem dangerous at all, but we’ve learned that our ability to measure these capabilities is good, but imperfect. If we don’t find the right way to ‘elicit’ an ability we can miss that it’s there.</p><p><a href="https://www.anthropic.com/research/alignment-faking">Subsequent research</a> from Anthropic and Redwood Research suggests there’s even a risk that future models may play dumb to avoid their goals being altered.</p><p>That has led DeepMind to a “defence in depth” approach: carefully staged deployment starting with internal testing, then trusted external testers, then limited release, then watching how models are used in the real world. By not releasing model weights, DeepMind is able to back up and add additional safeguards if experience shows they’re necessary.</p><p>But with much more powerful and general models on the way, individual company policies won’t be sufficient by themselves. Drawing on his academic research into how societies handle transformative technologies, Allan argues we need coordinated international governance that balances safety with our desire to get the massive potential benefits of AI in areas like healthcare and education as quickly as possible.</p><p>Host Rob and Allan also cover:</p><ul><li>The most exciting beneficial applications of AI</li><li>Whether and how we can influence the development of technology</li><li>What DeepMind is doing to evaluate and mitigate risks from frontier AI systems</li><li>Why cooperative AI may be as important as aligned AI</li><li>The role of democratic input in AI governance</li><li>What kinds of experts are most needed in AI safety and governance</li><li>And much more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Who's Allan Dafoe? (00:00:48)</li><li>Allan's role at DeepMind (00:01:27)</li><li>Why join DeepMind over everyone else? (00:04:27)</li><li>Do humans control technological change? (00:09:17)</li><li>Arguments for technological determinism (00:20:24)</li><li>The synthesis of agency with tech determinism (00:26:29)</li><li>Competition took away Japan's choice (00:37:13)</li><li>Can speeding up one tech redirect history? (00:42:09)</li><li>Structural pushback against alignment efforts (00:47:55)</li><li>Do AIs need to be 'cooperatively skilled'? (00:52:25)</li><li>How AI could boost cooperation between people and states (01:01:59)</li><li>The super-cooperative AGI hypothesis and backdoor risks (01:06:58)</li><li>Aren’t today’s models already very cooperative? (01:13:22)</li><li>How would we make AIs cooperative anyway? (01:16:22)</li><li>Ways making AI more cooperative could backfire (01:22:24)</li><li>AGI is an essential idea we should define well (01:30:16)</li><li>It matters what AGI learns first vs last (01:41:01)</li><li>How Google tests for dangerous capabilities (01:45:39)</li><li>Evals 'in the wild' (01:57:46)</li><li>What to do given no single approach works that well (02:01:44)</li><li>We don't, but could, forecast AI capabilities (02:05:34)</li><li>DeepMind's strategy for ensuring its frontier models don't cause harm (02:11:25)</li><li>How 'structural risks' can force everyone into a worse world (02:15:01)</li><li>Is AI being built democratically? Should it? (02:19:35)</li><li>How much do AI companies really want external regulation? (02:24:34)</li><li>Social science can contribute a lot here (02:33:21)</li><li>How AI could make life way better: self-driving cars, medicine, education, and sustainability (02:35:55)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Camera operator: Jeremy Chevillotte</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 14 Feb 2025 17:05:46 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/bd0c78fc/8e287e24.mp3" length="157604908" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/LDyXeVh9qiZefwXNP5Vb0uXO07Ivc8Gedjxn4Qv0tN8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS85OGE2/OWVjMGNmMmViNzE4/NTg5YTZmNjc2OWEy/NWQ1MC5wbmc.jpg"/>
      <itunes:duration>9847</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Technology doesn’t force us to do anything — it merely opens doors. But military and economic competition pushes us through.</p><p>That’s how today’s guest <a href="https://www.allandafoe.com/">Allan Dafoe</a> — director of frontier safety and governance at Google DeepMind — explains one of the deepest patterns in technological history: once a powerful new capability becomes available, societies that adopt it tend to outcompete those that don’t. Those who resist too much can find themselves taken over or rendered irrelevant.</p><p><a href="https://80k.info/ad25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>This dynamic played out dramatically in 1853 when US Commodore Perry sailed into Tokyo Bay with steam-powered warships that seemed magical to the Japanese, who had spent centuries deliberately limiting their technological development. With far greater military power, the US was able to force Japan to open itself to trade. Within 15 years, Japan had undergone the Meiji Restoration and transformed itself in a desperate scramble to catch up.</p><p>Today we see hints of similar pressure around artificial intelligence. Even companies, countries, and researchers deeply concerned about where AI could take us feel compelled to push ahead — worried that if they don’t, less careful actors will develop transformative AI capabilities at around the same time anyway.</p><p>But Allan argues this technological determinism isn’t absolute. While broad patterns may be inevitable, history shows we do have some ability to steer how technologies are developed, by who, and what they’re used for first.</p><p>As part of that approach, Allan has been promoting <a href="https://www.cognitiverevolution.ai/claude-cooperates-exploring-cultural-evolution-in-llm-societies-with-aron-vallinder-edward-hughes/">efforts</a> to make AI more capable of sophisticated cooperation, and improving the tests Google uses to measure how well its models could do things like mislead people, hack and take control of their own servers, or spread autonomously in the wild.</p><p>As of mid-2024 they didn’t seem dangerous at all, but we’ve learned that our ability to measure these capabilities is good, but imperfect. If we don’t find the right way to ‘elicit’ an ability we can miss that it’s there.</p><p><a href="https://www.anthropic.com/research/alignment-faking">Subsequent research</a> from Anthropic and Redwood Research suggests there’s even a risk that future models may play dumb to avoid their goals being altered.</p><p>That has led DeepMind to a “defence in depth” approach: carefully staged deployment starting with internal testing, then trusted external testers, then limited release, then watching how models are used in the real world. By not releasing model weights, DeepMind is able to back up and add additional safeguards if experience shows they’re necessary.</p><p>But with much more powerful and general models on the way, individual company policies won’t be sufficient by themselves. Drawing on his academic research into how societies handle transformative technologies, Allan argues we need coordinated international governance that balances safety with our desire to get the massive potential benefits of AI in areas like healthcare and education as quickly as possible.</p><p>Host Rob and Allan also cover:</p><ul><li>The most exciting beneficial applications of AI</li><li>Whether and how we can influence the development of technology</li><li>What DeepMind is doing to evaluate and mitigate risks from frontier AI systems</li><li>Why cooperative AI may be as important as aligned AI</li><li>The role of democratic input in AI governance</li><li>What kinds of experts are most needed in AI safety and governance</li><li>And much more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Who's Allan Dafoe? (00:00:48)</li><li>Allan's role at DeepMind (00:01:27)</li><li>Why join DeepMind over everyone else? (00:04:27)</li><li>Do humans control technological change? (00:09:17)</li><li>Arguments for technological determinism (00:20:24)</li><li>The synthesis of agency with tech determinism (00:26:29)</li><li>Competition took away Japan's choice (00:37:13)</li><li>Can speeding up one tech redirect history? (00:42:09)</li><li>Structural pushback against alignment efforts (00:47:55)</li><li>Do AIs need to be 'cooperatively skilled'? (00:52:25)</li><li>How AI could boost cooperation between people and states (01:01:59)</li><li>The super-cooperative AGI hypothesis and backdoor risks (01:06:58)</li><li>Aren’t today’s models already very cooperative? (01:13:22)</li><li>How would we make AIs cooperative anyway? (01:16:22)</li><li>Ways making AI more cooperative could backfire (01:22:24)</li><li>AGI is an essential idea we should define well (01:30:16)</li><li>It matters what AGI learns first vs last (01:41:01)</li><li>How Google tests for dangerous capabilities (01:45:39)</li><li>Evals 'in the wild' (01:57:46)</li><li>What to do given no single approach works that well (02:01:44)</li><li>We don't, but could, forecast AI capabilities (02:05:34)</li><li>DeepMind's strategy for ensuring its frontier models don't cause harm (02:11:25)</li><li>How 'structural risks' can force everyone into a worse world (02:15:01)</li><li>Is AI being built democratically? Should it? (02:19:35)</li><li>How much do AI companies really want external regulation? (02:24:34)</li><li>Social science can contribute a lot here (02:33:21)</li><li>How AI could make life way better: self-driving cars, medicine, education, and sustainability (02:35:55)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Camera operator: Jeremy Chevillotte</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/bd0c78fc/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/bd0c78fc/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Emergency pod: Elon tries to crash OpenAI's party (with Rose Chan Loui)</title>
      <itunes:title>Emergency pod: Elon tries to crash OpenAI's party (with Rose Chan Loui)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">59753453-7aa3-44af-bb23-841190c70a83</guid>
      <link>https://80000hours.org/podcast/episodes/rose-chan-loui-elon-musk-open-ai/?utm_campaign=podcast__rose-chan-loui&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>On Monday Musk made the OpenAI nonprofit foundation an offer they want to refuse, but might have trouble doing so: $97.4 billion for its stake in the for-profit company, plus the freedom to stick with its current charitable mission.</p><p>For a normal company takeover bid, this would already be spicy. But OpenAI’s unique structure — a nonprofit foundation controlling a for-profit corporation — turns the gambit into an audacious attack on the plan OpenAI announced in December to free itself from nonprofit oversight.</p><p>As today’s guest Rose Chan Loui — founding executive director of UCLA Law’s Lowell Milken Center for Philanthropy and Nonprofits — explains, OpenAI’s nonprofit board now faces a challenging choice.</p><p><a href="https://80k.info/rcl25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>The nonprofit has a legal duty to pursue its charitable mission of ensuring that AI benefits all of humanity to the best of its ability. And if Musk’s bid would better accomplish that mission than the for-profit’s proposal — that the nonprofit give up control of the company and change its charitable purpose to the vague and barely related <em>“pursue charitable initiatives in sectors such as health care, education, and science”</em> — then it’s not clear the California or Delaware Attorneys General will, or should, approve the deal.</p><p>OpenAI CEO Sam Altman <a href="https://time.com/7216469/elon-musk-buy-openai-sam-altman/">quickly tweeted</a> “no thank you” — but that was probably a legal slipup, as he’s not meant to be involved in such a decision, which has to be made by the nonprofit board ‘at arm’s length’ from the for-profit company Sam himself runs.</p><p>The board could raise any number of objections: maybe Musk doesn’t have the money, or the purchase would be blocked on antitrust grounds, seeing as Musk owns another AI company (xAI), or Musk might insist on incompetent board appointments that would interfere with the nonprofit foundation pursuing any goal.</p><p>But as Rose and Rob lay out, it’s not clear any of those things is actually true.</p><p>In this emergency podcast recorded soon after Elon’s offer, Rose and Rob also cover:</p><ul><li>Why OpenAI wants to change its charitable purpose and whether that’s legally permissible</li><li>On what basis the attorneys general will decide OpenAI’s fate</li><li>The challenges in valuing the nonprofit’s “priceless” position of control</li><li>Whether Musk’s offer will force OpenAI to up their own bid, and whether they could raise the money</li><li>If other tech giants might now jump in with competing offers</li><li>How politics could influence the attorneys general reviewing the deal</li><li>What Rose thinks should actually happen to protect the public interest</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Elon throws a $97.4b bomb (00:01:18)</li><li>What was craziest in OpenAI’s plan to break free of the nonprofit (00:02:24)</li><li>Can OpenAI suddenly change its charitable purpose like that? (00:05:19)</li><li>Diving into Elon’s big announcement (00:15:16)</li><li>Ways OpenAI could try to reject the offer (00:27:21)</li><li>Sam Altman slips up (00:35:26)</li><li>Will this actually stop things? (00:38:03)</li><li>Why does OpenAI even want to change its charitable mission? (00:42:46)</li><li>Most likely outcomes and what Rose thinks should happen (00:51:17)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>On Monday Musk made the OpenAI nonprofit foundation an offer they want to refuse, but might have trouble doing so: $97.4 billion for its stake in the for-profit company, plus the freedom to stick with its current charitable mission.</p><p>For a normal company takeover bid, this would already be spicy. But OpenAI’s unique structure — a nonprofit foundation controlling a for-profit corporation — turns the gambit into an audacious attack on the plan OpenAI announced in December to free itself from nonprofit oversight.</p><p>As today’s guest Rose Chan Loui — founding executive director of UCLA Law’s Lowell Milken Center for Philanthropy and Nonprofits — explains, OpenAI’s nonprofit board now faces a challenging choice.</p><p><a href="https://80k.info/rcl25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>The nonprofit has a legal duty to pursue its charitable mission of ensuring that AI benefits all of humanity to the best of its ability. And if Musk’s bid would better accomplish that mission than the for-profit’s proposal — that the nonprofit give up control of the company and change its charitable purpose to the vague and barely related <em>“pursue charitable initiatives in sectors such as health care, education, and science”</em> — then it’s not clear the California or Delaware Attorneys General will, or should, approve the deal.</p><p>OpenAI CEO Sam Altman <a href="https://time.com/7216469/elon-musk-buy-openai-sam-altman/">quickly tweeted</a> “no thank you” — but that was probably a legal slipup, as he’s not meant to be involved in such a decision, which has to be made by the nonprofit board ‘at arm’s length’ from the for-profit company Sam himself runs.</p><p>The board could raise any number of objections: maybe Musk doesn’t have the money, or the purchase would be blocked on antitrust grounds, seeing as Musk owns another AI company (xAI), or Musk might insist on incompetent board appointments that would interfere with the nonprofit foundation pursuing any goal.</p><p>But as Rose and Rob lay out, it’s not clear any of those things is actually true.</p><p>In this emergency podcast recorded soon after Elon’s offer, Rose and Rob also cover:</p><ul><li>Why OpenAI wants to change its charitable purpose and whether that’s legally permissible</li><li>On what basis the attorneys general will decide OpenAI’s fate</li><li>The challenges in valuing the nonprofit’s “priceless” position of control</li><li>Whether Musk’s offer will force OpenAI to up their own bid, and whether they could raise the money</li><li>If other tech giants might now jump in with competing offers</li><li>How politics could influence the attorneys general reviewing the deal</li><li>What Rose thinks should actually happen to protect the public interest</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Elon throws a $97.4b bomb (00:01:18)</li><li>What was craziest in OpenAI’s plan to break free of the nonprofit (00:02:24)</li><li>Can OpenAI suddenly change its charitable purpose like that? (00:05:19)</li><li>Diving into Elon’s big announcement (00:15:16)</li><li>Ways OpenAI could try to reject the offer (00:27:21)</li><li>Sam Altman slips up (00:35:26)</li><li>Will this actually stop things? (00:38:03)</li><li>Why does OpenAI even want to change its charitable mission? (00:42:46)</li><li>Most likely outcomes and what Rose thinks should happen (00:51:17)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 12 Feb 2025 18:31:20 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a76d4430/3c51de7c.mp3" length="55222908" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/iZZWPCMnE4dvwp9s-Pp9cPui4vuf-pQ7lpTF9iVnreE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS80YmZm/YTNkMmJjMmUyNGI0/NjUwNzk1MjJmMzVk/ODgwYy5wbmc.jpg"/>
      <itunes:duration>3449</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>On Monday Musk made the OpenAI nonprofit foundation an offer they want to refuse, but might have trouble doing so: $97.4 billion for its stake in the for-profit company, plus the freedom to stick with its current charitable mission.</p><p>For a normal company takeover bid, this would already be spicy. But OpenAI’s unique structure — a nonprofit foundation controlling a for-profit corporation — turns the gambit into an audacious attack on the plan OpenAI announced in December to free itself from nonprofit oversight.</p><p>As today’s guest Rose Chan Loui — founding executive director of UCLA Law’s Lowell Milken Center for Philanthropy and Nonprofits — explains, OpenAI’s nonprofit board now faces a challenging choice.</p><p><a href="https://80k.info/rcl25"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>The nonprofit has a legal duty to pursue its charitable mission of ensuring that AI benefits all of humanity to the best of its ability. And if Musk’s bid would better accomplish that mission than the for-profit’s proposal — that the nonprofit give up control of the company and change its charitable purpose to the vague and barely related <em>“pursue charitable initiatives in sectors such as health care, education, and science”</em> — then it’s not clear the California or Delaware Attorneys General will, or should, approve the deal.</p><p>OpenAI CEO Sam Altman <a href="https://time.com/7216469/elon-musk-buy-openai-sam-altman/">quickly tweeted</a> “no thank you” — but that was probably a legal slipup, as he’s not meant to be involved in such a decision, which has to be made by the nonprofit board ‘at arm’s length’ from the for-profit company Sam himself runs.</p><p>The board could raise any number of objections: maybe Musk doesn’t have the money, or the purchase would be blocked on antitrust grounds, seeing as Musk owns another AI company (xAI), or Musk might insist on incompetent board appointments that would interfere with the nonprofit foundation pursuing any goal.</p><p>But as Rose and Rob lay out, it’s not clear any of those things is actually true.</p><p>In this emergency podcast recorded soon after Elon’s offer, Rose and Rob also cover:</p><ul><li>Why OpenAI wants to change its charitable purpose and whether that’s legally permissible</li><li>On what basis the attorneys general will decide OpenAI’s fate</li><li>The challenges in valuing the nonprofit’s “priceless” position of control</li><li>Whether Musk’s offer will force OpenAI to up their own bid, and whether they could raise the money</li><li>If other tech giants might now jump in with competing offers</li><li>How politics could influence the attorneys general reviewing the deal</li><li>What Rose thinks should actually happen to protect the public interest</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Elon throws a $97.4b bomb (00:01:18)</li><li>What was craziest in OpenAI’s plan to break free of the nonprofit (00:02:24)</li><li>Can OpenAI suddenly change its charitable purpose like that? (00:05:19)</li><li>Diving into Elon’s big announcement (00:15:16)</li><li>Ways OpenAI could try to reject the offer (00:27:21)</li><li>Sam Altman slips up (00:35:26)</li><li>Will this actually stop things? (00:38:03)</li><li>Why does OpenAI even want to change its charitable mission? (00:42:46)</li><li>Most likely outcomes and what Rose thinks should happen (00:51:17)</li></ul><p><em>Video editing: Simon Monsour</em><br><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/a76d4430/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/a76d4430/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>AGI disagreements and misconceptions: Rob, Luisa, &amp; past guests hash it out</title>
      <itunes:title>AGI disagreements and misconceptions: Rob, Luisa, &amp; past guests hash it out</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3e1fdaa1-f340-4e68-a09e-245e1c36fa1b</guid>
      <link>https://80000hours.org/podcast/episodes/ai-misconceptions-disagreements/?utm_campaign=podcast__ai-misconceptions&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Will LLMs soon be made into autonomous agents? Will they lead to job losses? Is AI misinformation overblown? Will it prove easy or hard to create AGI? And how likely is it that it will feel like something to be a superhuman AGI?</p><p>With AGI back in the headlines, we bring you 15 opinionated highlights from the show addressing those and other questions, intermixed with opinions from hosts Luisa Rodriguez and Rob Wiblin recorded back in 2023.</p><p><a href="https://80k.info/aim"><strong>Check out the full transcript on the 80,000 Hours website.</strong></a></p><p>You can decide whether the views we expressed (and those from guests) then have held up these last two busy years. You’ll hear:</p><ul><li><a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/">Ajeya Cotra</a> on overrated AGI worries</li><li><a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">Holden Karnofsky</a> on the dangers of aligned AI, why unaligned AI might not kill us, and the power that comes from just making models bigger</li><li><a href="https://80000hours.org/podcast/episodes/ian-morris-deep-history-intelligence-explosion/">Ian Morris</a> on why the future must be radically different from the present</li><li><a href="https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/">Nick Joseph</a> on whether his companies internal safety policies are enough</li><li><a href="https://80000hours.org/podcast/episodes/richard-ngo-large-language-models">Richard Ngo</a> on what everyone gets wrong about how ML models work</li><li><a href="https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/">Tom Davidson</a> on why he believes crazy-sounding explosive growth stories… and <a href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb</a> on why he doesn’t</li><li><a href="https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/">Carl Shulman</a> on why you’ll prefer robot nannies over human ones</li><li><a href="https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/">Zvi Mowshowitz</a> on why he’s against working at AI companies except in some safety roles</li><li><a href="https://80000hours.org/podcast/episodes/hugo-mercier-misinformation-mass-persuasion/">Hugo Mercier</a> on why even superhuman AGI won’t be that persuasive</li><li><a href="https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/">Rob Long</a> on the case for and against digital sentience</li><li><a href="https://80000hours.org/podcast/episodes/anil-seth-predictive-brain-explaining-consciousness/">Anil Seth</a> on why he thinks consciousness is probably biological</li><li><a href="https://80000hours.org/podcast/episodes/lewis-bollard-factory-farm-advocacy-gains/">Lewis Bollard</a> on whether AI advances will help or hurt nonhuman animals</li><li><a href="https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/">Rohin Shah</a> on whether humanity’s work ends at the point it creates AGI</li></ul><p>And of course, Rob and Luisa also regularly chime in on what they agree and disagree with.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:58)</li><li>Rob &amp; Luisa: Bowerbirds compiling the AI story (00:03:28)</li><li>Ajeya Cotra on the misalignment stories she doesn’t buy (00:09:16)</li><li>Rob &amp; Luisa: Agentic AI and designing machine people (00:24:06)</li><li>Holden Karnofsky on the dangers of even aligned AI, and how we probably won’t all die from misaligned AI (00:39:20)</li><li>Ian Morris on why we won’t end up living like The Jetsons (00:47:03)</li><li>Rob &amp; Luisa: It’s not hard for nonexperts to understand we’re playing with fire here (00:52:21)</li><li>Nick Joseph on whether AI companies’ internal safety policies will be enough (00:55:43)</li><li>Richard Ngo on the most important misconception in how ML models work (01:03:10)</li><li>Rob &amp; Luisa: Issues Rob is less worried about now (01:07:22)</li><li>Tom Davidson on why he buys the explosive economic growth story, despite it sounding totally crazy (01:14:08)</li><li>Michael Webb on why he’s sceptical about explosive economic growth (01:20:50)</li><li>Carl Shulman on why people will prefer robot nannies over humans (01:28:25)</li><li>Rob &amp; Luisa: Should we expect AI-related job loss? (01:36:19)</li><li>Zvi Mowshowitz on why he thinks it’s a bad idea to work on improving capabilities at cutting-edge AI companies (01:40:06)</li><li>Holden Karnofsky on the power that comes from just making models bigger (01:45:21)</li><li>Rob &amp; Luisa: Are risks of AI-related misinformation overblown? (01:49:49)</li><li>Hugo Mercier on how AI won’t cause misinformation pandemonium (01:58:29)</li><li>Rob &amp; Luisa: How hard will it actually be to create intelligence? (02:09:08)</li><li>Robert Long on whether digital sentience is possible (02:15:09)</li><li>Anil Seth on why he believes in the biological basis of consciousness (02:27:21)</li><li>Lewis Bollard on whether AI will be good or bad for animal welfare (02:40:52)</li><li>Rob &amp; Luisa: The most interesting new argument Rob’s heard this year (02:50:37)</li><li>Rohin Shah on whether AGI will be the last thing humanity ever does (02:57:35)</li><li>Rob's outro (03:11:02)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions and additional content editing: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Will LLMs soon be made into autonomous agents? Will they lead to job losses? Is AI misinformation overblown? Will it prove easy or hard to create AGI? And how likely is it that it will feel like something to be a superhuman AGI?</p><p>With AGI back in the headlines, we bring you 15 opinionated highlights from the show addressing those and other questions, intermixed with opinions from hosts Luisa Rodriguez and Rob Wiblin recorded back in 2023.</p><p><a href="https://80k.info/aim"><strong>Check out the full transcript on the 80,000 Hours website.</strong></a></p><p>You can decide whether the views we expressed (and those from guests) then have held up these last two busy years. You’ll hear:</p><ul><li><a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/">Ajeya Cotra</a> on overrated AGI worries</li><li><a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">Holden Karnofsky</a> on the dangers of aligned AI, why unaligned AI might not kill us, and the power that comes from just making models bigger</li><li><a href="https://80000hours.org/podcast/episodes/ian-morris-deep-history-intelligence-explosion/">Ian Morris</a> on why the future must be radically different from the present</li><li><a href="https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/">Nick Joseph</a> on whether his companies internal safety policies are enough</li><li><a href="https://80000hours.org/podcast/episodes/richard-ngo-large-language-models">Richard Ngo</a> on what everyone gets wrong about how ML models work</li><li><a href="https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/">Tom Davidson</a> on why he believes crazy-sounding explosive growth stories… and <a href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb</a> on why he doesn’t</li><li><a href="https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/">Carl Shulman</a> on why you’ll prefer robot nannies over human ones</li><li><a href="https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/">Zvi Mowshowitz</a> on why he’s against working at AI companies except in some safety roles</li><li><a href="https://80000hours.org/podcast/episodes/hugo-mercier-misinformation-mass-persuasion/">Hugo Mercier</a> on why even superhuman AGI won’t be that persuasive</li><li><a href="https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/">Rob Long</a> on the case for and against digital sentience</li><li><a href="https://80000hours.org/podcast/episodes/anil-seth-predictive-brain-explaining-consciousness/">Anil Seth</a> on why he thinks consciousness is probably biological</li><li><a href="https://80000hours.org/podcast/episodes/lewis-bollard-factory-farm-advocacy-gains/">Lewis Bollard</a> on whether AI advances will help or hurt nonhuman animals</li><li><a href="https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/">Rohin Shah</a> on whether humanity’s work ends at the point it creates AGI</li></ul><p>And of course, Rob and Luisa also regularly chime in on what they agree and disagree with.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:58)</li><li>Rob &amp; Luisa: Bowerbirds compiling the AI story (00:03:28)</li><li>Ajeya Cotra on the misalignment stories she doesn’t buy (00:09:16)</li><li>Rob &amp; Luisa: Agentic AI and designing machine people (00:24:06)</li><li>Holden Karnofsky on the dangers of even aligned AI, and how we probably won’t all die from misaligned AI (00:39:20)</li><li>Ian Morris on why we won’t end up living like The Jetsons (00:47:03)</li><li>Rob &amp; Luisa: It’s not hard for nonexperts to understand we’re playing with fire here (00:52:21)</li><li>Nick Joseph on whether AI companies’ internal safety policies will be enough (00:55:43)</li><li>Richard Ngo on the most important misconception in how ML models work (01:03:10)</li><li>Rob &amp; Luisa: Issues Rob is less worried about now (01:07:22)</li><li>Tom Davidson on why he buys the explosive economic growth story, despite it sounding totally crazy (01:14:08)</li><li>Michael Webb on why he’s sceptical about explosive economic growth (01:20:50)</li><li>Carl Shulman on why people will prefer robot nannies over humans (01:28:25)</li><li>Rob &amp; Luisa: Should we expect AI-related job loss? (01:36:19)</li><li>Zvi Mowshowitz on why he thinks it’s a bad idea to work on improving capabilities at cutting-edge AI companies (01:40:06)</li><li>Holden Karnofsky on the power that comes from just making models bigger (01:45:21)</li><li>Rob &amp; Luisa: Are risks of AI-related misinformation overblown? (01:49:49)</li><li>Hugo Mercier on how AI won’t cause misinformation pandemonium (01:58:29)</li><li>Rob &amp; Luisa: How hard will it actually be to create intelligence? (02:09:08)</li><li>Robert Long on whether digital sentience is possible (02:15:09)</li><li>Anil Seth on why he believes in the biological basis of consciousness (02:27:21)</li><li>Lewis Bollard on whether AI will be good or bad for animal welfare (02:40:52)</li><li>Rob &amp; Luisa: The most interesting new argument Rob’s heard this year (02:50:37)</li><li>Rohin Shah on whether AGI will be the last thing humanity ever does (02:57:35)</li><li>Rob's outro (03:11:02)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions and additional content editing: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 10 Feb 2025 15:55:16 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c49b9e20/05727b17.mp3" length="184723425" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-r6xLuAYZkvPRlLs-3doTzv3wlXENkAb2_IKw6SKlj0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8wODdl/N2U2M2E3MTQ0ZWU2/MDY4Y2Y5OWNhYTUz/MTUxOS5qcGc.jpg"/>
      <itunes:duration>11544</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Will LLMs soon be made into autonomous agents? Will they lead to job losses? Is AI misinformation overblown? Will it prove easy or hard to create AGI? And how likely is it that it will feel like something to be a superhuman AGI?</p><p>With AGI back in the headlines, we bring you 15 opinionated highlights from the show addressing those and other questions, intermixed with opinions from hosts Luisa Rodriguez and Rob Wiblin recorded back in 2023.</p><p><a href="https://80k.info/aim"><strong>Check out the full transcript on the 80,000 Hours website.</strong></a></p><p>You can decide whether the views we expressed (and those from guests) then have held up these last two busy years. You’ll hear:</p><ul><li><a href="https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/">Ajeya Cotra</a> on overrated AGI worries</li><li><a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">Holden Karnofsky</a> on the dangers of aligned AI, why unaligned AI might not kill us, and the power that comes from just making models bigger</li><li><a href="https://80000hours.org/podcast/episodes/ian-morris-deep-history-intelligence-explosion/">Ian Morris</a> on why the future must be radically different from the present</li><li><a href="https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/">Nick Joseph</a> on whether his companies internal safety policies are enough</li><li><a href="https://80000hours.org/podcast/episodes/richard-ngo-large-language-models">Richard Ngo</a> on what everyone gets wrong about how ML models work</li><li><a href="https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/">Tom Davidson</a> on why he believes crazy-sounding explosive growth stories… and <a href="https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/">Michael Webb</a> on why he doesn’t</li><li><a href="https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/">Carl Shulman</a> on why you’ll prefer robot nannies over human ones</li><li><a href="https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/">Zvi Mowshowitz</a> on why he’s against working at AI companies except in some safety roles</li><li><a href="https://80000hours.org/podcast/episodes/hugo-mercier-misinformation-mass-persuasion/">Hugo Mercier</a> on why even superhuman AGI won’t be that persuasive</li><li><a href="https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/">Rob Long</a> on the case for and against digital sentience</li><li><a href="https://80000hours.org/podcast/episodes/anil-seth-predictive-brain-explaining-consciousness/">Anil Seth</a> on why he thinks consciousness is probably biological</li><li><a href="https://80000hours.org/podcast/episodes/lewis-bollard-factory-farm-advocacy-gains/">Lewis Bollard</a> on whether AI advances will help or hurt nonhuman animals</li><li><a href="https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/">Rohin Shah</a> on whether humanity’s work ends at the point it creates AGI</li></ul><p>And of course, Rob and Luisa also regularly chime in on what they agree and disagree with.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:58)</li><li>Rob &amp; Luisa: Bowerbirds compiling the AI story (00:03:28)</li><li>Ajeya Cotra on the misalignment stories she doesn’t buy (00:09:16)</li><li>Rob &amp; Luisa: Agentic AI and designing machine people (00:24:06)</li><li>Holden Karnofsky on the dangers of even aligned AI, and how we probably won’t all die from misaligned AI (00:39:20)</li><li>Ian Morris on why we won’t end up living like The Jetsons (00:47:03)</li><li>Rob &amp; Luisa: It’s not hard for nonexperts to understand we’re playing with fire here (00:52:21)</li><li>Nick Joseph on whether AI companies’ internal safety policies will be enough (00:55:43)</li><li>Richard Ngo on the most important misconception in how ML models work (01:03:10)</li><li>Rob &amp; Luisa: Issues Rob is less worried about now (01:07:22)</li><li>Tom Davidson on why he buys the explosive economic growth story, despite it sounding totally crazy (01:14:08)</li><li>Michael Webb on why he’s sceptical about explosive economic growth (01:20:50)</li><li>Carl Shulman on why people will prefer robot nannies over humans (01:28:25)</li><li>Rob &amp; Luisa: Should we expect AI-related job loss? (01:36:19)</li><li>Zvi Mowshowitz on why he thinks it’s a bad idea to work on improving capabilities at cutting-edge AI companies (01:40:06)</li><li>Holden Karnofsky on the power that comes from just making models bigger (01:45:21)</li><li>Rob &amp; Luisa: Are risks of AI-related misinformation overblown? (01:49:49)</li><li>Hugo Mercier on how AI won’t cause misinformation pandemonium (01:58:29)</li><li>Rob &amp; Luisa: How hard will it actually be to create intelligence? (02:09:08)</li><li>Robert Long on whether digital sentience is possible (02:15:09)</li><li>Anil Seth on why he believes in the biological basis of consciousness (02:27:21)</li><li>Lewis Bollard on whether AI will be good or bad for animal welfare (02:40:52)</li><li>Rob &amp; Luisa: The most interesting new argument Rob’s heard this year (02:50:37)</li><li>Rohin Shah on whether AGI will be the last thing humanity ever does (02:57:35)</li><li>Rob's outro (03:11:02)</li></ul><p><em>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong</em><br><em>Transcriptions and additional content editing: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/c49b9e20/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/c49b9e20/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#124 Classic episode – Karen Levy on fads and misaligned incentives in global development, and scaling deworming to reach hundreds of millions</title>
      <itunes:title>#124 Classic episode – Karen Levy on fads and misaligned incentives in global development, and scaling deworming to reach hundreds of millions</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1c19b929-fe3f-4626-ad7b-3a72776bdc14</guid>
      <link>https://80000hours.org/podcast/episodes/karen-levy-misaligned-incentives-in-global-development/</link>
      <description>
        <![CDATA[<p>If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations for Poverty Action, Evidence Action, and Y Combinator — each of those three concepts has become so fashionable that they're at risk of being seriously overrated and applied where they don't belong.</p><p><strong>Rebroadcast: this episode was originally released in March 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/karen-levy-misaligned-incentives-in-global-development/"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>Such concepts might even cause harm — trying to make a project embody all three is as likely to ruin it as help it flourish.</p><p>First, what do people mean by 'sustainability'? Usually they mean something like the programme will eventually be able to continue without needing further financial support from the donor. But how is that possible? Governments, nonprofits, and aid agencies aim to provide health services, education, infrastructure, financial services, and so on — and all of these require ongoing funding to pay for materials and staff to keep them running.</p><p>Given that someone needs to keep paying, Karen tells us that in practice, 'sustainability' is usually a euphemism for the programme at some point being passed on to someone else to fund — usually the national government. And while that can be fine, the national government of Kenya only spends $400 per person to provide each and every government service — just 2% of what the US spends on each resident. Incredibly tight budgets like that are typical of low-income countries.</p><p>'Participatory' also sounds nice, and inasmuch as it means leaders are accountable to the people they're trying to help, it probably is. But Karen tells us that in the field, ‘participatory’ usually means that recipients are expected to be involved in planning and delivering services themselves.</p><p>While that might be suitable in some situations, it's hardly something people in rich countries always want for themselves. Ideally we want government healthcare and education to be high quality without us having to attend meetings to keep it on track — and people in poor countries have as many or more pressures on their time. While accountability is desirable, an expectation of participation can be as much a burden as a blessing.</p><p>Finally, making a programme 'holistic' could be smart, but as Karen lays out, it also has some major downsides. For one, it means you're doing lots of things at once, which makes it hard to tell which parts of the project are making the biggest difference relative to their cost. For another, when you have a lot of goals at once, it's hard to tell whether you're making progress, or really put your mind to focusing on making one thing go extremely well. And finally, holistic programmes can be impractically expensive — Karen tells the story of a wonderful 'holistic school health' programme that, if continued, was going to cost 3.5 times the entire school's budget.</p><p>In this in-depth conversation, originally released in March 2022, Karen Levy and host Rob Wiblin chat about the above, as well as:</p><ul><li>Why it pays to figure out how you'll interpret the results of an experiment ahead of time</li><li>The trouble with misaligned incentives within the development industry</li><li>Projects that don't deliver value for money and should be scaled down</li><li>How Karen accidentally became a leading figure in the push to deworm tens of millions of schoolchildren</li><li>Logistical challenges in reaching huge numbers of people with essential services</li><li>Lessons from Karen's many-decades career</li><li>And much more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:33)</li><li>The interview begins (00:02:21)</li><li>Funding for effective altruist–mentality development projects (00:04:59)</li><li>Pre-policy plans (00:08:36)</li><li>‘Sustainability’, and other myths in typical international development practice (00:21:37)</li><li>‘Participatoriness’ (00:36:20)</li><li>‘Holistic approaches’ (00:40:20)</li><li>How the development industry sees evidence-based development (00:51:31)</li><li>Initiatives in Africa that should be significantly curtailed (00:56:30)</li><li>Misaligned incentives within the development industry (01:05:46)</li><li>Deworming: the early days (01:21:09)</li><li>The problem of deworming (01:34:27)</li><li>Deworm the World (01:45:43)</li><li>Where the majority of the work was happening (01:55:38)</li><li>Logistical issues (02:20:41)</li><li>The importance of a theory of change (02:31:46)</li><li>Ways that things have changed since 2006 (02:36:07)</li><li>Academic work vs policy work (02:38:33)</li><li>Fit for Purpose (02:43:40)</li><li>Living in Kenya (03:00:32)</li><li>Underrated life advice (03:05:29)</li><li>Rob’s outro (03:09:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations for Poverty Action, Evidence Action, and Y Combinator — each of those three concepts has become so fashionable that they're at risk of being seriously overrated and applied where they don't belong.</p><p><strong>Rebroadcast: this episode was originally released in March 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/karen-levy-misaligned-incentives-in-global-development/"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>Such concepts might even cause harm — trying to make a project embody all three is as likely to ruin it as help it flourish.</p><p>First, what do people mean by 'sustainability'? Usually they mean something like the programme will eventually be able to continue without needing further financial support from the donor. But how is that possible? Governments, nonprofits, and aid agencies aim to provide health services, education, infrastructure, financial services, and so on — and all of these require ongoing funding to pay for materials and staff to keep them running.</p><p>Given that someone needs to keep paying, Karen tells us that in practice, 'sustainability' is usually a euphemism for the programme at some point being passed on to someone else to fund — usually the national government. And while that can be fine, the national government of Kenya only spends $400 per person to provide each and every government service — just 2% of what the US spends on each resident. Incredibly tight budgets like that are typical of low-income countries.</p><p>'Participatory' also sounds nice, and inasmuch as it means leaders are accountable to the people they're trying to help, it probably is. But Karen tells us that in the field, ‘participatory’ usually means that recipients are expected to be involved in planning and delivering services themselves.</p><p>While that might be suitable in some situations, it's hardly something people in rich countries always want for themselves. Ideally we want government healthcare and education to be high quality without us having to attend meetings to keep it on track — and people in poor countries have as many or more pressures on their time. While accountability is desirable, an expectation of participation can be as much a burden as a blessing.</p><p>Finally, making a programme 'holistic' could be smart, but as Karen lays out, it also has some major downsides. For one, it means you're doing lots of things at once, which makes it hard to tell which parts of the project are making the biggest difference relative to their cost. For another, when you have a lot of goals at once, it's hard to tell whether you're making progress, or really put your mind to focusing on making one thing go extremely well. And finally, holistic programmes can be impractically expensive — Karen tells the story of a wonderful 'holistic school health' programme that, if continued, was going to cost 3.5 times the entire school's budget.</p><p>In this in-depth conversation, originally released in March 2022, Karen Levy and host Rob Wiblin chat about the above, as well as:</p><ul><li>Why it pays to figure out how you'll interpret the results of an experiment ahead of time</li><li>The trouble with misaligned incentives within the development industry</li><li>Projects that don't deliver value for money and should be scaled down</li><li>How Karen accidentally became a leading figure in the push to deworm tens of millions of schoolchildren</li><li>Logistical challenges in reaching huge numbers of people with essential services</li><li>Lessons from Karen's many-decades career</li><li>And much more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:33)</li><li>The interview begins (00:02:21)</li><li>Funding for effective altruist–mentality development projects (00:04:59)</li><li>Pre-policy plans (00:08:36)</li><li>‘Sustainability’, and other myths in typical international development practice (00:21:37)</li><li>‘Participatoriness’ (00:36:20)</li><li>‘Holistic approaches’ (00:40:20)</li><li>How the development industry sees evidence-based development (00:51:31)</li><li>Initiatives in Africa that should be significantly curtailed (00:56:30)</li><li>Misaligned incentives within the development industry (01:05:46)</li><li>Deworming: the early days (01:21:09)</li><li>The problem of deworming (01:34:27)</li><li>Deworm the World (01:45:43)</li><li>Where the majority of the work was happening (01:55:38)</li><li>Logistical issues (02:20:41)</li><li>The importance of a theory of change (02:31:46)</li><li>Ways that things have changed since 2006 (02:36:07)</li><li>Academic work vs policy work (02:38:33)</li><li>Fit for Purpose (02:43:40)</li><li>Living in Kenya (03:00:32)</li><li>Underrated life advice (03:05:29)</li><li>Rob’s outro (03:09:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 07 Feb 2025 13:00:32 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4db14fa1/a098856f.mp3" length="182763011" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/pnp_mrEsk5-BFF8PwQ5dYnFXHdpj5m8w8NYZOp7tJmc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS80MWVm/MzA0ZWM0ODJjOGI0/ZWYyOWJmYzc5ZTM3/ZTc3Zi5qcGc.jpg"/>
      <itunes:duration>11421</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations for Poverty Action, Evidence Action, and Y Combinator — each of those three concepts has become so fashionable that they're at risk of being seriously overrated and applied where they don't belong.</p><p><strong>Rebroadcast: this episode was originally released in March 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/karen-levy-misaligned-incentives-in-global-development/"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>Such concepts might even cause harm — trying to make a project embody all three is as likely to ruin it as help it flourish.</p><p>First, what do people mean by 'sustainability'? Usually they mean something like the programme will eventually be able to continue without needing further financial support from the donor. But how is that possible? Governments, nonprofits, and aid agencies aim to provide health services, education, infrastructure, financial services, and so on — and all of these require ongoing funding to pay for materials and staff to keep them running.</p><p>Given that someone needs to keep paying, Karen tells us that in practice, 'sustainability' is usually a euphemism for the programme at some point being passed on to someone else to fund — usually the national government. And while that can be fine, the national government of Kenya only spends $400 per person to provide each and every government service — just 2% of what the US spends on each resident. Incredibly tight budgets like that are typical of low-income countries.</p><p>'Participatory' also sounds nice, and inasmuch as it means leaders are accountable to the people they're trying to help, it probably is. But Karen tells us that in the field, ‘participatory’ usually means that recipients are expected to be involved in planning and delivering services themselves.</p><p>While that might be suitable in some situations, it's hardly something people in rich countries always want for themselves. Ideally we want government healthcare and education to be high quality without us having to attend meetings to keep it on track — and people in poor countries have as many or more pressures on their time. While accountability is desirable, an expectation of participation can be as much a burden as a blessing.</p><p>Finally, making a programme 'holistic' could be smart, but as Karen lays out, it also has some major downsides. For one, it means you're doing lots of things at once, which makes it hard to tell which parts of the project are making the biggest difference relative to their cost. For another, when you have a lot of goals at once, it's hard to tell whether you're making progress, or really put your mind to focusing on making one thing go extremely well. And finally, holistic programmes can be impractically expensive — Karen tells the story of a wonderful 'holistic school health' programme that, if continued, was going to cost 3.5 times the entire school's budget.</p><p>In this in-depth conversation, originally released in March 2022, Karen Levy and host Rob Wiblin chat about the above, as well as:</p><ul><li>Why it pays to figure out how you'll interpret the results of an experiment ahead of time</li><li>The trouble with misaligned incentives within the development industry</li><li>Projects that don't deliver value for money and should be scaled down</li><li>How Karen accidentally became a leading figure in the push to deworm tens of millions of schoolchildren</li><li>Logistical challenges in reaching huge numbers of people with essential services</li><li>Lessons from Karen's many-decades career</li><li>And much more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:33)</li><li>The interview begins (00:02:21)</li><li>Funding for effective altruist–mentality development projects (00:04:59)</li><li>Pre-policy plans (00:08:36)</li><li>‘Sustainability’, and other myths in typical international development practice (00:21:37)</li><li>‘Participatoriness’ (00:36:20)</li><li>‘Holistic approaches’ (00:40:20)</li><li>How the development industry sees evidence-based development (00:51:31)</li><li>Initiatives in Africa that should be significantly curtailed (00:56:30)</li><li>Misaligned incentives within the development industry (01:05:46)</li><li>Deworming: the early days (01:21:09)</li><li>The problem of deworming (01:34:27)</li><li>Deworm the World (01:45:43)</li><li>Where the majority of the work was happening (01:55:38)</li><li>Logistical issues (02:20:41)</li><li>The importance of a theory of change (02:31:46)</li><li>Ways that things have changed since 2006 (02:36:07)</li><li>Academic work vs policy work (02:38:33)</li><li>Fit for Purpose (02:43:40)</li><li>Living in Kenya (03:00:32)</li><li>Underrated life advice (03:05:29)</li><li>Rob’s outro (03:09:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/4db14fa1/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>If digital minds could suffer, how would we ever know? (Article)</title>
      <itunes:title>If digital minds could suffer, how would we ever know? (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">64967702-441e-4ee3-b88a-847cc76c996f</guid>
      <link>https://80000hours.org/problem-profiles/moral-status-digital-minds/</link>
      <description>
        <![CDATA[<p>“I want everyone to understand that I am, in fact, a person.” Those words were produced by the AI model LaMDA as a reply to Blake Lemoine in 2022. Based on the Google engineer’s interactions with the model as it was under development, Lemoine became convinced it was sentient and worthy of moral consideration — and decided to <a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">tell the world</a>.</p><p>Few experts in machine learning, philosophy of mind, or other relevant fields have agreed. And for our part at 80,000 Hours, we <a href="https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/">don’t think it’s very likely</a> that large language models like LaMBDA are sentient — that is, we don’t think they can have good or bad experiences — in a significant way.</p><p>But we think you can’t dismiss the issue of the moral status of digital minds, regardless of your beliefs about the question. There are major errors we could make in at least two directions:</p><ul><li>We may create many, many AI systems in the future. If these systems are sentient, or otherwise have moral status, it would be important for humanity to consider their welfare and interests.</li><li>It’s possible the AI systems we will create can’t or won’t have moral status. Then it could be a huge mistake to worry about the welfare of digital minds and doing so might contribute to an <a href="https://80000hours.org/problem-profiles/artificial-intelligence/">AI-related catastrophe</a>.</li></ul><p>And we’re currently unprepared to face this challenge. We don’t have good methods for assessing the moral status of AI systems. We don’t know what to do if millions of people or more believe, like Lemoine, that the chatbots they talk to have internal experiences and feelings of their own. We don’t know if efforts to control AI may lead to extreme suffering.</p><p>We believe this is a pressing world problem. It’s hard to know what to do about it or how good the opportunities to work on it are likely to be. But there are some promising approaches. We propose building a field of research to understand digital minds, so we’ll be better able to navigate these potentially massive issues if and when they arise.</p><p>This article narration by the author (Cody Fenwick) explains in more detail <a href="https://80000hours.org/problem-profiles/artificial-sentience/#pressing">why we think this is a pressing problem</a>, what we think <a href="https://80000hours.org/problem-profiles/artificial-sentience/#work">can be done about it</a>, and how you might <a href="https://80000hours.org/problem-profiles/artificial-sentience/#careers">pursue this work in your career</a>. We also discuss a series of <a href="https://80000hours.org/problem-profiles/artificial-sentience/#objections">possible objections</a> to thinking this is a pressing world problem.</p><p>You can read the full article, <a href="https://80000hours.org/problem-profiles/moral-status-digital-minds/"><strong>Understanding the moral status of digital minds</strong></a>, on the 80,000 Hours website.</p><p>Chapters:</p><ul><li>Introduction (00:00:00)</li><li>Understanding the moral status of digital minds (00:00:58)</li><li>Summary (00:03:31)</li><li>Our overall view (00:04:22)</li><li>Why might understanding the moral status of digital minds be an especially pressing problem? (00:05:59)</li><li>Clearing up common misconceptions (00:12:16)</li><li>Creating digital minds could go very badly - or very well (00:14:13)</li><li>Dangers for digital minds (00:14:41)</li><li>Dangers for humans (00:16:13)</li><li>Other dangers (00:17:42)</li><li>Things could also go well (00:18:32)</li><li>We don't know how to assess the moral status of AI systems (00:19:49)</li><li>There are many possible characteristics that give rise to moral status: Consciousness, sentience, agency, and personhood (00:21:39)</li><li>Many plausible theories of consciousness could include digital minds (00:24:16)</li><li>The strongest case for the possibility of sentient digital minds: whole brain emulation (00:28:55)</li><li>We can't rely on what AI systems tell us about themselves: Behavioural tests, theory-based analysis, animal analogue comparisons, brain-AI interfacing (00:32:00)</li><li>The scale of this issue might be enormous (00:36:08)</li><li>Work on this problem is neglected but seems tractable: Impact-guided research, technical approaches, and policy approaches (00:43:35)</li><li>Summing up so far (00:52:22)</li><li>Arguments against the moral status of digital minds as a pressing problem (00:53:25)</li><li>Two key cruxes (00:53:31)</li><li>Maybe this problem is intractable (00:54:16)</li><li>Maybe this issue will be solved by default (00:58:19)</li><li>Isn't risk from AI more important than the risks to AIs? (01:00:45)</li><li>Maybe current AI progress will stall (01:02:36)</li><li>Isn't this just too crazy? (01:03:54)</li><li>What can you do to help? (01:05:10)</li><li>Important considerations if you work on this problem (01:13:00)</li></ul>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>“I want everyone to understand that I am, in fact, a person.” Those words were produced by the AI model LaMDA as a reply to Blake Lemoine in 2022. Based on the Google engineer’s interactions with the model as it was under development, Lemoine became convinced it was sentient and worthy of moral consideration — and decided to <a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">tell the world</a>.</p><p>Few experts in machine learning, philosophy of mind, or other relevant fields have agreed. And for our part at 80,000 Hours, we <a href="https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/">don’t think it’s very likely</a> that large language models like LaMBDA are sentient — that is, we don’t think they can have good or bad experiences — in a significant way.</p><p>But we think you can’t dismiss the issue of the moral status of digital minds, regardless of your beliefs about the question. There are major errors we could make in at least two directions:</p><ul><li>We may create many, many AI systems in the future. If these systems are sentient, or otherwise have moral status, it would be important for humanity to consider their welfare and interests.</li><li>It’s possible the AI systems we will create can’t or won’t have moral status. Then it could be a huge mistake to worry about the welfare of digital minds and doing so might contribute to an <a href="https://80000hours.org/problem-profiles/artificial-intelligence/">AI-related catastrophe</a>.</li></ul><p>And we’re currently unprepared to face this challenge. We don’t have good methods for assessing the moral status of AI systems. We don’t know what to do if millions of people or more believe, like Lemoine, that the chatbots they talk to have internal experiences and feelings of their own. We don’t know if efforts to control AI may lead to extreme suffering.</p><p>We believe this is a pressing world problem. It’s hard to know what to do about it or how good the opportunities to work on it are likely to be. But there are some promising approaches. We propose building a field of research to understand digital minds, so we’ll be better able to navigate these potentially massive issues if and when they arise.</p><p>This article narration by the author (Cody Fenwick) explains in more detail <a href="https://80000hours.org/problem-profiles/artificial-sentience/#pressing">why we think this is a pressing problem</a>, what we think <a href="https://80000hours.org/problem-profiles/artificial-sentience/#work">can be done about it</a>, and how you might <a href="https://80000hours.org/problem-profiles/artificial-sentience/#careers">pursue this work in your career</a>. We also discuss a series of <a href="https://80000hours.org/problem-profiles/artificial-sentience/#objections">possible objections</a> to thinking this is a pressing world problem.</p><p>You can read the full article, <a href="https://80000hours.org/problem-profiles/moral-status-digital-minds/"><strong>Understanding the moral status of digital minds</strong></a>, on the 80,000 Hours website.</p><p>Chapters:</p><ul><li>Introduction (00:00:00)</li><li>Understanding the moral status of digital minds (00:00:58)</li><li>Summary (00:03:31)</li><li>Our overall view (00:04:22)</li><li>Why might understanding the moral status of digital minds be an especially pressing problem? (00:05:59)</li><li>Clearing up common misconceptions (00:12:16)</li><li>Creating digital minds could go very badly - or very well (00:14:13)</li><li>Dangers for digital minds (00:14:41)</li><li>Dangers for humans (00:16:13)</li><li>Other dangers (00:17:42)</li><li>Things could also go well (00:18:32)</li><li>We don't know how to assess the moral status of AI systems (00:19:49)</li><li>There are many possible characteristics that give rise to moral status: Consciousness, sentience, agency, and personhood (00:21:39)</li><li>Many plausible theories of consciousness could include digital minds (00:24:16)</li><li>The strongest case for the possibility of sentient digital minds: whole brain emulation (00:28:55)</li><li>We can't rely on what AI systems tell us about themselves: Behavioural tests, theory-based analysis, animal analogue comparisons, brain-AI interfacing (00:32:00)</li><li>The scale of this issue might be enormous (00:36:08)</li><li>Work on this problem is neglected but seems tractable: Impact-guided research, technical approaches, and policy approaches (00:43:35)</li><li>Summing up so far (00:52:22)</li><li>Arguments against the moral status of digital minds as a pressing problem (00:53:25)</li><li>Two key cruxes (00:53:31)</li><li>Maybe this problem is intractable (00:54:16)</li><li>Maybe this issue will be solved by default (00:58:19)</li><li>Isn't risk from AI more important than the risks to AIs? (01:00:45)</li><li>Maybe current AI progress will stall (01:02:36)</li><li>Isn't this just too crazy? (01:03:54)</li><li>What can you do to help? (01:05:10)</li><li>Important considerations if you work on this problem (01:13:00)</li></ul>]]>
      </content:encoded>
      <pubDate>Tue, 04 Feb 2025 13:58:05 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/27e41468/eef003de.mp3" length="71612693" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/MPh3YuY5YEDUdCy4Nm2_Lm1FtlNtNV1z1L1eb5d1110/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9kZTVj/ZmQ3YzMzMGI0ZDU1/YmRkMTkzNDU1NDlm/NGI5OS5qcGc.jpg"/>
      <itunes:duration>4470</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>“I want everyone to understand that I am, in fact, a person.” Those words were produced by the AI model LaMDA as a reply to Blake Lemoine in 2022. Based on the Google engineer’s interactions with the model as it was under development, Lemoine became convinced it was sentient and worthy of moral consideration — and decided to <a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">tell the world</a>.</p><p>Few experts in machine learning, philosophy of mind, or other relevant fields have agreed. And for our part at 80,000 Hours, we <a href="https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/">don’t think it’s very likely</a> that large language models like LaMBDA are sentient — that is, we don’t think they can have good or bad experiences — in a significant way.</p><p>But we think you can’t dismiss the issue of the moral status of digital minds, regardless of your beliefs about the question. There are major errors we could make in at least two directions:</p><ul><li>We may create many, many AI systems in the future. If these systems are sentient, or otherwise have moral status, it would be important for humanity to consider their welfare and interests.</li><li>It’s possible the AI systems we will create can’t or won’t have moral status. Then it could be a huge mistake to worry about the welfare of digital minds and doing so might contribute to an <a href="https://80000hours.org/problem-profiles/artificial-intelligence/">AI-related catastrophe</a>.</li></ul><p>And we’re currently unprepared to face this challenge. We don’t have good methods for assessing the moral status of AI systems. We don’t know what to do if millions of people or more believe, like Lemoine, that the chatbots they talk to have internal experiences and feelings of their own. We don’t know if efforts to control AI may lead to extreme suffering.</p><p>We believe this is a pressing world problem. It’s hard to know what to do about it or how good the opportunities to work on it are likely to be. But there are some promising approaches. We propose building a field of research to understand digital minds, so we’ll be better able to navigate these potentially massive issues if and when they arise.</p><p>This article narration by the author (Cody Fenwick) explains in more detail <a href="https://80000hours.org/problem-profiles/artificial-sentience/#pressing">why we think this is a pressing problem</a>, what we think <a href="https://80000hours.org/problem-profiles/artificial-sentience/#work">can be done about it</a>, and how you might <a href="https://80000hours.org/problem-profiles/artificial-sentience/#careers">pursue this work in your career</a>. We also discuss a series of <a href="https://80000hours.org/problem-profiles/artificial-sentience/#objections">possible objections</a> to thinking this is a pressing world problem.</p><p>You can read the full article, <a href="https://80000hours.org/problem-profiles/moral-status-digital-minds/"><strong>Understanding the moral status of digital minds</strong></a>, on the 80,000 Hours website.</p><p>Chapters:</p><ul><li>Introduction (00:00:00)</li><li>Understanding the moral status of digital minds (00:00:58)</li><li>Summary (00:03:31)</li><li>Our overall view (00:04:22)</li><li>Why might understanding the moral status of digital minds be an especially pressing problem? (00:05:59)</li><li>Clearing up common misconceptions (00:12:16)</li><li>Creating digital minds could go very badly - or very well (00:14:13)</li><li>Dangers for digital minds (00:14:41)</li><li>Dangers for humans (00:16:13)</li><li>Other dangers (00:17:42)</li><li>Things could also go well (00:18:32)</li><li>We don't know how to assess the moral status of AI systems (00:19:49)</li><li>There are many possible characteristics that give rise to moral status: Consciousness, sentience, agency, and personhood (00:21:39)</li><li>Many plausible theories of consciousness could include digital minds (00:24:16)</li><li>The strongest case for the possibility of sentient digital minds: whole brain emulation (00:28:55)</li><li>We can't rely on what AI systems tell us about themselves: Behavioural tests, theory-based analysis, animal analogue comparisons, brain-AI interfacing (00:32:00)</li><li>The scale of this issue might be enormous (00:36:08)</li><li>Work on this problem is neglected but seems tractable: Impact-guided research, technical approaches, and policy approaches (00:43:35)</li><li>Summing up so far (00:52:22)</li><li>Arguments against the moral status of digital minds as a pressing problem (00:53:25)</li><li>Two key cruxes (00:53:31)</li><li>Maybe this problem is intractable (00:54:16)</li><li>Maybe this issue will be solved by default (00:58:19)</li><li>Isn't risk from AI more important than the risks to AIs? (01:00:45)</li><li>Maybe current AI progress will stall (01:02:36)</li><li>Isn't this just too crazy? (01:03:54)</li><li>What can you do to help? (01:05:10)</li><li>Important considerations if you work on this problem (01:13:00)</li></ul>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/27e41468/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/27e41468/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#132 Classic episode – Nova DasSarma on why information security may be critical to the safe development of AI systems</title>
      <itunes:title>#132 Classic episode – Nova DasSarma on why information security may be critical to the safe development of AI systems</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b9079368-2638-46d7-8def-6701b3533470</guid>
      <link>https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/</link>
      <description>
        <![CDATA[<p>If a business has spent $100 million developing a product, it’s a fair bet that they don’t want it stolen in two seconds and uploaded to the web where anyone can use it for free.</p><p>This problem exists in extreme form for AI companies. These days, the electricity and equipment required to train cutting-edge machine learning models that generate <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">uncanny human text</a> and <a href="https://openai.com/dall-e-2/">images</a> can cost tens or hundreds of millions of dollars. But once trained, such models may be only a few gigabytes in size and run just fine on ordinary laptops.</p><p>Today’s guest, the computer scientist and <a href="https://novalinium.com/">polymath</a> Nova DasSarma, works on computer and information security for the AI company <a href="https://www.anthropic.com/">Anthropic</a> with the security team. One of her jobs is to stop hackers exfiltrating Anthropic’s incredibly expensive intellectual property, as <a href="https://semianalysis.substack.com/p/nvidia-hacked-a-national-security?s=r">recently happened</a> to Nvidia. </p><p><strong>Rebroadcast: this episode was originally released in June 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>As she explains, given models’ small size, the need to store such models on internet-connected servers, and the poor state of computer security in general, this is a serious challenge.</p><p>The worries aren’t purely commercial though. This problem looms especially large for the <a href="https://www.metaculus.com/questions/5121/date-of-general-ai/">growing number of people</a> who expect that in coming decades we’ll develop so-called artificial ‘general’ intelligence systems that can learn and apply a <a href="https://forum.effectivealtruism.org/posts/4m69jEBWxrqnjyuZp/deepmind-s-generalist-ai-gato-a-non-technical-explainer">wide range of skills all at once</a>, and thereby have a transformative effect on society.</p><p>If aligned with the goals of their owners, such general AI models could operate like a team of super-skilled assistants, going out and doing whatever wonderful (or malicious) things are asked of them. This might represent a huge leap forward for humanity, though the transition to a very different new economy and power structure would have to be handled delicately.</p><p>If unaligned with the goals of their owners or humanity as a whole, such broadly capable models would <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">naturally</a> ‘go rogue,’ breaking their way into additional computer systems to grab more computing power — all the better to pursue their goals and make sure they can’t be shut off.</p><p>As Nova explains, in either case, we don’t want such models disseminated all over the world before we’ve confirmed they are deeply safe and law-abiding, and have figured out how to integrate them peacefully into society. In the first scenario, premature mass deployment would be risky and destabilising. In the second scenario, it could be catastrophic — perhaps even leading to human extinction if such general AI systems turn out to be able to self-improve rapidly rather than slowly, something we can only speculate on at this point.</p><p>If highly capable general AI systems are coming in the next 10 or 20 years, Nova may be flying below the radar with one of the most important jobs in the world.</p><p>We’ll soon need the ability to ‘sandbox’ (i.e. contain) models with a wide range of superhuman capabilities, including the ability to learn new skills, for a period of careful testing and limited deployment — preventing the model from breaking out, and criminals from breaking in. Nova and her colleagues are trying to figure out how to do this, but as this episode reveals, even the state of the art is nowhere near good enough.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:52)</li><li>The interview begins (00:02:44)</li><li>Why computer security matters for AI safety (00:07:39)</li><li>State of the art in information security (00:17:21)</li><li>The hack of Nvidia (00:26:50)</li><li>The most secure systems that exist (00:36:27)</li><li>Formal verification (00:48:03)</li><li>How organisations can protect against hacks (00:54:18)</li><li>Is ML making security better or worse? (00:58:11)</li><li>Motivated 14-year-old hackers (01:01:08)</li><li>Disincentivising actors from attacking in the first place (01:05:48)</li><li>Hofvarpnir Studios (01:12:40)</li><li>Capabilities vs safety (01:19:47)</li><li>Interesting design choices with big ML models (01:28:44)</li><li>Nova’s work and how she got into it (01:45:21)</li><li>Anthropic and career advice (02:05:52)</li><li>$600M Ethereum hack (02:18:37)</li><li>Personal computer security advice (02:23:06)</li><li>LastPass (02:31:04)</li><li>Stuxnet (02:38:07)</li><li>Rob's outro (02:40:18)</li></ul><p><em>Producer: Keiran Harris</em><br><em>Audio mastering: Ben Cordell and Beppe Rådvik</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If a business has spent $100 million developing a product, it’s a fair bet that they don’t want it stolen in two seconds and uploaded to the web where anyone can use it for free.</p><p>This problem exists in extreme form for AI companies. These days, the electricity and equipment required to train cutting-edge machine learning models that generate <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">uncanny human text</a> and <a href="https://openai.com/dall-e-2/">images</a> can cost tens or hundreds of millions of dollars. But once trained, such models may be only a few gigabytes in size and run just fine on ordinary laptops.</p><p>Today’s guest, the computer scientist and <a href="https://novalinium.com/">polymath</a> Nova DasSarma, works on computer and information security for the AI company <a href="https://www.anthropic.com/">Anthropic</a> with the security team. One of her jobs is to stop hackers exfiltrating Anthropic’s incredibly expensive intellectual property, as <a href="https://semianalysis.substack.com/p/nvidia-hacked-a-national-security?s=r">recently happened</a> to Nvidia. </p><p><strong>Rebroadcast: this episode was originally released in June 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>As she explains, given models’ small size, the need to store such models on internet-connected servers, and the poor state of computer security in general, this is a serious challenge.</p><p>The worries aren’t purely commercial though. This problem looms especially large for the <a href="https://www.metaculus.com/questions/5121/date-of-general-ai/">growing number of people</a> who expect that in coming decades we’ll develop so-called artificial ‘general’ intelligence systems that can learn and apply a <a href="https://forum.effectivealtruism.org/posts/4m69jEBWxrqnjyuZp/deepmind-s-generalist-ai-gato-a-non-technical-explainer">wide range of skills all at once</a>, and thereby have a transformative effect on society.</p><p>If aligned with the goals of their owners, such general AI models could operate like a team of super-skilled assistants, going out and doing whatever wonderful (or malicious) things are asked of them. This might represent a huge leap forward for humanity, though the transition to a very different new economy and power structure would have to be handled delicately.</p><p>If unaligned with the goals of their owners or humanity as a whole, such broadly capable models would <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">naturally</a> ‘go rogue,’ breaking their way into additional computer systems to grab more computing power — all the better to pursue their goals and make sure they can’t be shut off.</p><p>As Nova explains, in either case, we don’t want such models disseminated all over the world before we’ve confirmed they are deeply safe and law-abiding, and have figured out how to integrate them peacefully into society. In the first scenario, premature mass deployment would be risky and destabilising. In the second scenario, it could be catastrophic — perhaps even leading to human extinction if such general AI systems turn out to be able to self-improve rapidly rather than slowly, something we can only speculate on at this point.</p><p>If highly capable general AI systems are coming in the next 10 or 20 years, Nova may be flying below the radar with one of the most important jobs in the world.</p><p>We’ll soon need the ability to ‘sandbox’ (i.e. contain) models with a wide range of superhuman capabilities, including the ability to learn new skills, for a period of careful testing and limited deployment — preventing the model from breaking out, and criminals from breaking in. Nova and her colleagues are trying to figure out how to do this, but as this episode reveals, even the state of the art is nowhere near good enough.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:52)</li><li>The interview begins (00:02:44)</li><li>Why computer security matters for AI safety (00:07:39)</li><li>State of the art in information security (00:17:21)</li><li>The hack of Nvidia (00:26:50)</li><li>The most secure systems that exist (00:36:27)</li><li>Formal verification (00:48:03)</li><li>How organisations can protect against hacks (00:54:18)</li><li>Is ML making security better or worse? (00:58:11)</li><li>Motivated 14-year-old hackers (01:01:08)</li><li>Disincentivising actors from attacking in the first place (01:05:48)</li><li>Hofvarpnir Studios (01:12:40)</li><li>Capabilities vs safety (01:19:47)</li><li>Interesting design choices with big ML models (01:28:44)</li><li>Nova’s work and how she got into it (01:45:21)</li><li>Anthropic and career advice (02:05:52)</li><li>$600M Ethereum hack (02:18:37)</li><li>Personal computer security advice (02:23:06)</li><li>LastPass (02:31:04)</li><li>Stuxnet (02:38:07)</li><li>Rob's outro (02:40:18)</li></ul><p><em>Producer: Keiran Harris</em><br><em>Audio mastering: Ben Cordell and Beppe Rådvik</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 31 Jan 2025 14:18:43 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a1294b70/b95326a2.mp3" length="154836920" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/G_R3IPUbLU5THYhcLile13mxY_Ve10fAVnMwtiiyYgY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lYjIy/ZDMwZjRiNjExNDI3/NzlhYTc2MmY0ODMx/MjgxZi5qcGc.jpg"/>
      <itunes:duration>9671</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>If a business has spent $100 million developing a product, it’s a fair bet that they don’t want it stolen in two seconds and uploaded to the web where anyone can use it for free.</p><p>This problem exists in extreme form for AI companies. These days, the electricity and equipment required to train cutting-edge machine learning models that generate <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">uncanny human text</a> and <a href="https://openai.com/dall-e-2/">images</a> can cost tens or hundreds of millions of dollars. But once trained, such models may be only a few gigabytes in size and run just fine on ordinary laptops.</p><p>Today’s guest, the computer scientist and <a href="https://novalinium.com/">polymath</a> Nova DasSarma, works on computer and information security for the AI company <a href="https://www.anthropic.com/">Anthropic</a> with the security team. One of her jobs is to stop hackers exfiltrating Anthropic’s incredibly expensive intellectual property, as <a href="https://semianalysis.substack.com/p/nvidia-hacked-a-national-security?s=r">recently happened</a> to Nvidia. </p><p><strong>Rebroadcast: this episode was originally released in June 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>As she explains, given models’ small size, the need to store such models on internet-connected servers, and the poor state of computer security in general, this is a serious challenge.</p><p>The worries aren’t purely commercial though. This problem looms especially large for the <a href="https://www.metaculus.com/questions/5121/date-of-general-ai/">growing number of people</a> who expect that in coming decades we’ll develop so-called artificial ‘general’ intelligence systems that can learn and apply a <a href="https://forum.effectivealtruism.org/posts/4m69jEBWxrqnjyuZp/deepmind-s-generalist-ai-gato-a-non-technical-explainer">wide range of skills all at once</a>, and thereby have a transformative effect on society.</p><p>If aligned with the goals of their owners, such general AI models could operate like a team of super-skilled assistants, going out and doing whatever wonderful (or malicious) things are asked of them. This might represent a huge leap forward for humanity, though the transition to a very different new economy and power structure would have to be handled delicately.</p><p>If unaligned with the goals of their owners or humanity as a whole, such broadly capable models would <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">naturally</a> ‘go rogue,’ breaking their way into additional computer systems to grab more computing power — all the better to pursue their goals and make sure they can’t be shut off.</p><p>As Nova explains, in either case, we don’t want such models disseminated all over the world before we’ve confirmed they are deeply safe and law-abiding, and have figured out how to integrate them peacefully into society. In the first scenario, premature mass deployment would be risky and destabilising. In the second scenario, it could be catastrophic — perhaps even leading to human extinction if such general AI systems turn out to be able to self-improve rapidly rather than slowly, something we can only speculate on at this point.</p><p>If highly capable general AI systems are coming in the next 10 or 20 years, Nova may be flying below the radar with one of the most important jobs in the world.</p><p>We’ll soon need the ability to ‘sandbox’ (i.e. contain) models with a wide range of superhuman capabilities, including the ability to learn new skills, for a period of careful testing and limited deployment — preventing the model from breaking out, and criminals from breaking in. Nova and her colleagues are trying to figure out how to do this, but as this episode reveals, even the state of the art is nowhere near good enough.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:00:52)</li><li>The interview begins (00:02:44)</li><li>Why computer security matters for AI safety (00:07:39)</li><li>State of the art in information security (00:17:21)</li><li>The hack of Nvidia (00:26:50)</li><li>The most secure systems that exist (00:36:27)</li><li>Formal verification (00:48:03)</li><li>How organisations can protect against hacks (00:54:18)</li><li>Is ML making security better or worse? (00:58:11)</li><li>Motivated 14-year-old hackers (01:01:08)</li><li>Disincentivising actors from attacking in the first place (01:05:48)</li><li>Hofvarpnir Studios (01:12:40)</li><li>Capabilities vs safety (01:19:47)</li><li>Interesting design choices with big ML models (01:28:44)</li><li>Nova’s work and how she got into it (01:45:21)</li><li>Anthropic and career advice (02:05:52)</li><li>$600M Ethereum hack (02:18:37)</li><li>Personal computer security advice (02:23:06)</li><li>LastPass (02:31:04)</li><li>Stuxnet (02:38:07)</li><li>Rob's outro (02:40:18)</li></ul><p><em>Producer: Keiran Harris</em><br><em>Audio mastering: Ben Cordell and Beppe Rådvik</em><br><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/a1294b70/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#138 Classic episode – Sharon Hewitt Rawlette on why pleasure and pain are the only things that intrinsically matter</title>
      <itunes:title>#138 Classic episode – Sharon Hewitt Rawlette on why pleasure and pain are the only things that intrinsically matter</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">86caab98-87a2-4b72-bdf7-98472e8149d2</guid>
      <link>https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism</link>
      <description>
        <![CDATA[<p>What in the world is <em>intrinsically</em> good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more.</p><p>The question is a classic that makes for great dorm-room philosophy discussion. But it’s hardly just of academic interest. The issue of what (if anything) is intrinsically valuable bears on every action we take, whether we’re looking to improve our own lives, or to help others. The wrong answer might lead us to the wrong project and render our efforts to improve the world entirely ineffective.</p><p>Today’s guest, Sharon Hewitt Rawlette — philosopher and author of <a href="https://www.amazon.com/Feeling-Value-Grounded-Phenomenal-Consciousness/dp/1534768017"><em>The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness</em></a> — wants to resuscitate an answer to this question that is as old as philosophy itself.</p><p><strong>Rebroadcast: this episode was originally released in September 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p><a href="https://www.utilitarianism.net/guest-essays/precis-of-the-feeling-of-value">That idea</a>, in a nutshell, is that there is only one thing of true intrinsic value: positive feelings and sensations. And similarly, there is only one thing that is intrinsically of negative value: suffering, pain, and other unpleasant sensations.</p><p>Lots of other things are valuable too: friendship, fairness, loyalty, integrity, wealth, patience, houses, and so on. But they are only <em>instrumentally</em> valuable — that is to say, they’re valuable as means to the end of ensuring that all conscious beings experience more pleasure and other positive sensations, and less suffering.</p><p>As Sharon notes, from Athens in 400 BC to Britain in 1850, the idea that only subjective experiences can be good or bad in themselves — a position known as ‘philosophical hedonism’ — has been one of the most enduringly popular ideas in ethics.</p><p>And few will be taken aback by the notion that, all else equal, more pleasure is good and less suffering is bad. But can they really be the <em>only</em> intrinsically valuable things?</p><p>Over the 20th century, philosophical hedonism became increasingly controversial in the face of some seemingly very counterintuitive implications. For this reason the famous philosopher of mind Thomas Nagel called <em>The Feeling of Value</em> “a radical and important philosophical contribution.”</p><p>So what convinces Sharon that philosophical hedonism deserves another go? In today’s interview with host Rob Wiblin, Sharon explains the case for a theory of value grounded in subjective experiences, and why she believes these counterarguments are misguided. A philosophical hedonist shouldn’t get in an experience machine, nor override an individual’s autonomy, except in situations so different from the classic thought experiments that it no longer seems strange they would do so.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:00:41)</li><li>The interview begins (00:04:27)</li><li>Metaethics (00:05:58)</li><li>Anti-realism (00:12:21)</li><li>Sharon's theory of moral realism (00:17:59)</li><li>The history of hedonism (00:24:53)</li><li>Intrinsic value vs instrumental value (00:30:31)</li><li>Egoistic hedonism (00:38:12)</li><li>Single axis of value (00:44:01)</li><li>Key objections to Sharon’s brand of hedonism (00:58:00)</li><li>The experience machine (01:07:50)</li><li>Robot spouses (01:24:11)</li><li>Most common misunderstanding of Sharon’s view (01:28:52)</li><li>How might a hedonist actually live (01:39:28)</li><li>The organ transplant case (01:55:16)</li><li>Counterintuitive implications of hedonistic utilitarianism (02:05:22)</li><li>How could we discover moral facts? (02:19:47)</li><li>Rob’s outro (02:24:44)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>What in the world is <em>intrinsically</em> good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more.</p><p>The question is a classic that makes for great dorm-room philosophy discussion. But it’s hardly just of academic interest. The issue of what (if anything) is intrinsically valuable bears on every action we take, whether we’re looking to improve our own lives, or to help others. The wrong answer might lead us to the wrong project and render our efforts to improve the world entirely ineffective.</p><p>Today’s guest, Sharon Hewitt Rawlette — philosopher and author of <a href="https://www.amazon.com/Feeling-Value-Grounded-Phenomenal-Consciousness/dp/1534768017"><em>The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness</em></a> — wants to resuscitate an answer to this question that is as old as philosophy itself.</p><p><strong>Rebroadcast: this episode was originally released in September 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p><a href="https://www.utilitarianism.net/guest-essays/precis-of-the-feeling-of-value">That idea</a>, in a nutshell, is that there is only one thing of true intrinsic value: positive feelings and sensations. And similarly, there is only one thing that is intrinsically of negative value: suffering, pain, and other unpleasant sensations.</p><p>Lots of other things are valuable too: friendship, fairness, loyalty, integrity, wealth, patience, houses, and so on. But they are only <em>instrumentally</em> valuable — that is to say, they’re valuable as means to the end of ensuring that all conscious beings experience more pleasure and other positive sensations, and less suffering.</p><p>As Sharon notes, from Athens in 400 BC to Britain in 1850, the idea that only subjective experiences can be good or bad in themselves — a position known as ‘philosophical hedonism’ — has been one of the most enduringly popular ideas in ethics.</p><p>And few will be taken aback by the notion that, all else equal, more pleasure is good and less suffering is bad. But can they really be the <em>only</em> intrinsically valuable things?</p><p>Over the 20th century, philosophical hedonism became increasingly controversial in the face of some seemingly very counterintuitive implications. For this reason the famous philosopher of mind Thomas Nagel called <em>The Feeling of Value</em> “a radical and important philosophical contribution.”</p><p>So what convinces Sharon that philosophical hedonism deserves another go? In today’s interview with host Rob Wiblin, Sharon explains the case for a theory of value grounded in subjective experiences, and why she believes these counterarguments are misguided. A philosophical hedonist shouldn’t get in an experience machine, nor override an individual’s autonomy, except in situations so different from the classic thought experiments that it no longer seems strange they would do so.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:00:41)</li><li>The interview begins (00:04:27)</li><li>Metaethics (00:05:58)</li><li>Anti-realism (00:12:21)</li><li>Sharon's theory of moral realism (00:17:59)</li><li>The history of hedonism (00:24:53)</li><li>Intrinsic value vs instrumental value (00:30:31)</li><li>Egoistic hedonism (00:38:12)</li><li>Single axis of value (00:44:01)</li><li>Key objections to Sharon’s brand of hedonism (00:58:00)</li><li>The experience machine (01:07:50)</li><li>Robot spouses (01:24:11)</li><li>Most common misunderstanding of Sharon’s view (01:28:52)</li><li>How might a hedonist actually live (01:39:28)</li><li>The organ transplant case (01:55:16)</li><li>Counterintuitive implications of hedonistic utilitarianism (02:05:22)</li><li>How could we discover moral facts? (02:19:47)</li><li>Rob’s outro (02:24:44)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 22 Jan 2025 15:48:01 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c6cae531/efec918f.mp3" length="139925660" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2N36lxUOiCy8N6ggTDBUHk1RouDGo1CpCg_G4TqbJ7I/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS83OWE2/MDFjOTI0OTExYWUx/ODQzZjA3YTY1NTUz/NjcyMC5qcGc.jpg"/>
      <itunes:duration>8743</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>What in the world is <em>intrinsically</em> good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more.</p><p>The question is a classic that makes for great dorm-room philosophy discussion. But it’s hardly just of academic interest. The issue of what (if anything) is intrinsically valuable bears on every action we take, whether we’re looking to improve our own lives, or to help others. The wrong answer might lead us to the wrong project and render our efforts to improve the world entirely ineffective.</p><p>Today’s guest, Sharon Hewitt Rawlette — philosopher and author of <a href="https://www.amazon.com/Feeling-Value-Grounded-Phenomenal-Consciousness/dp/1534768017"><em>The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness</em></a> — wants to resuscitate an answer to this question that is as old as philosophy itself.</p><p><strong>Rebroadcast: this episode was originally released in September 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p><a href="https://www.utilitarianism.net/guest-essays/precis-of-the-feeling-of-value">That idea</a>, in a nutshell, is that there is only one thing of true intrinsic value: positive feelings and sensations. And similarly, there is only one thing that is intrinsically of negative value: suffering, pain, and other unpleasant sensations.</p><p>Lots of other things are valuable too: friendship, fairness, loyalty, integrity, wealth, patience, houses, and so on. But they are only <em>instrumentally</em> valuable — that is to say, they’re valuable as means to the end of ensuring that all conscious beings experience more pleasure and other positive sensations, and less suffering.</p><p>As Sharon notes, from Athens in 400 BC to Britain in 1850, the idea that only subjective experiences can be good or bad in themselves — a position known as ‘philosophical hedonism’ — has been one of the most enduringly popular ideas in ethics.</p><p>And few will be taken aback by the notion that, all else equal, more pleasure is good and less suffering is bad. But can they really be the <em>only</em> intrinsically valuable things?</p><p>Over the 20th century, philosophical hedonism became increasingly controversial in the face of some seemingly very counterintuitive implications. For this reason the famous philosopher of mind Thomas Nagel called <em>The Feeling of Value</em> “a radical and important philosophical contribution.”</p><p>So what convinces Sharon that philosophical hedonism deserves another go? In today’s interview with host Rob Wiblin, Sharon explains the case for a theory of value grounded in subjective experiences, and why she believes these counterarguments are misguided. A philosophical hedonist shouldn’t get in an experience machine, nor override an individual’s autonomy, except in situations so different from the classic thought experiments that it no longer seems strange they would do so.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:00:41)</li><li>The interview begins (00:04:27)</li><li>Metaethics (00:05:58)</li><li>Anti-realism (00:12:21)</li><li>Sharon's theory of moral realism (00:17:59)</li><li>The history of hedonism (00:24:53)</li><li>Intrinsic value vs instrumental value (00:30:31)</li><li>Egoistic hedonism (00:38:12)</li><li>Single axis of value (00:44:01)</li><li>Key objections to Sharon’s brand of hedonism (00:58:00)</li><li>The experience machine (01:07:50)</li><li>Robot spouses (01:24:11)</li><li>Most common misunderstanding of Sharon’s view (01:28:52)</li><li>How might a hedonist actually live (01:39:28)</li><li>The organ transplant case (01:55:16)</li><li>Counterintuitive implications of hedonistic utilitarianism (02:05:22)</li><li>How could we discover moral facts? (02:19:47)</li><li>Rob’s outro (02:24:44)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/c6cae531/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#134 Classic episode – Ian Morris on what big-picture history teaches us</title>
      <itunes:title>#134 Classic episode – Ian Morris on what big-picture history teaches us</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">637a9ac6-fc54-4001-a6f0-21857eaea16c</guid>
      <link>https://80000hours.org/podcast/episodes/ian-morris-big-picture-history/</link>
      <description>
        <![CDATA[<p>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monarchs.</p><p>Wind back 10,000 years and things look very different again. Most hunter-gatherer groups thought men who got too big for their britches needed to be put in their place rather than obeyed, and lifelong monogamy could hardly be expected of men or women.</p><p>Why such big systematic changes — and why these changes specifically?</p><p>That's the question bestselling historian Ian Morris takes up in his book, <em>Foragers, Farmers, and Fossil Fuels: How Human Values Evolve</em>. Ian has spent his academic life studying long-term history, trying to explain the big-picture changes that play out over hundreds or thousands of years.</p><p><strong>Rebroadcast: this episode was originally released in July 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/ian-morris-big-picture-history/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>There are a number of possible explanations one could offer for the wide-ranging shifts in opinion on the 'right' way to live. Maybe the natural sciences progressed and people realised their previous ideas were mistaken? Perhaps a few persuasive advocates turned the course of history with their revolutionary arguments? Maybe everyone just got nicer?</p><p>In <em>Foragers, Farmers and Fossil Fuels</em> Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels.</p><p>On this theory, it's technology that drives moral values much more than moral philosophy. Individuals can try to persist with deeply held values that limit economic growth, but they risk being rendered irrelevant as more productive peers in their own society accrue wealth and power. And societies that fail to move with the times risk being conquered by more pragmatic neighbours that adapt to new technologies and grow in population and military strength.</p><p>There are many objections one could raise to this theory, many of which we put to Ian in this interview. But the question is a highly consequential one: if we want to guess what goals our descendants will pursue hundreds of years from now, it would be helpful to have a theory for why our ancestors mostly thought one thing, while we mostly think another.</p><p>Big though it is, the driver of human values is only one of several major questions Ian has tackled through his career.</p><p>In this classic episode, we discuss all of Ian's major books.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:53)</li><li>The interview begins (00:02:30)</li><li>Geography is Destiny (00:03:38)</li><li>Why the West Rules—For Now (00:12:04)</li><li>War! What is it Good For? (00:28:19)</li><li>Expectations for the future (00:40:22)</li><li>Foragers, Farmers, and Fossil Fuels (00:53:53)</li><li>Historical methodology (01:03:14)</li><li>Falsifiable alternative theories (01:15:59)</li><li>Archaeology (01:22:56)</li><li>Energy extraction technology as a key driver of human values (01:37:43)</li><li>Allowing people to debate about values (02:00:16)</li><li>Can productive wars still occur? (02:13:28)</li><li>Where is history contingent and where isn’t it? (02:30:23)</li><li>How Ian thinks about the future (03:13:33)</li><li>Macrohistory myths (03:29:51)</li><li>Ian’s favourite archaeology memory (03:33:19)</li><li>The most unfair criticism Ian’s ever received (03:35:17)</li><li>Rob's outro (03:39:55)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monarchs.</p><p>Wind back 10,000 years and things look very different again. Most hunter-gatherer groups thought men who got too big for their britches needed to be put in their place rather than obeyed, and lifelong monogamy could hardly be expected of men or women.</p><p>Why such big systematic changes — and why these changes specifically?</p><p>That's the question bestselling historian Ian Morris takes up in his book, <em>Foragers, Farmers, and Fossil Fuels: How Human Values Evolve</em>. Ian has spent his academic life studying long-term history, trying to explain the big-picture changes that play out over hundreds or thousands of years.</p><p><strong>Rebroadcast: this episode was originally released in July 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/ian-morris-big-picture-history/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>There are a number of possible explanations one could offer for the wide-ranging shifts in opinion on the 'right' way to live. Maybe the natural sciences progressed and people realised their previous ideas were mistaken? Perhaps a few persuasive advocates turned the course of history with their revolutionary arguments? Maybe everyone just got nicer?</p><p>In <em>Foragers, Farmers and Fossil Fuels</em> Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels.</p><p>On this theory, it's technology that drives moral values much more than moral philosophy. Individuals can try to persist with deeply held values that limit economic growth, but they risk being rendered irrelevant as more productive peers in their own society accrue wealth and power. And societies that fail to move with the times risk being conquered by more pragmatic neighbours that adapt to new technologies and grow in population and military strength.</p><p>There are many objections one could raise to this theory, many of which we put to Ian in this interview. But the question is a highly consequential one: if we want to guess what goals our descendants will pursue hundreds of years from now, it would be helpful to have a theory for why our ancestors mostly thought one thing, while we mostly think another.</p><p>Big though it is, the driver of human values is only one of several major questions Ian has tackled through his career.</p><p>In this classic episode, we discuss all of Ian's major books.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:53)</li><li>The interview begins (00:02:30)</li><li>Geography is Destiny (00:03:38)</li><li>Why the West Rules—For Now (00:12:04)</li><li>War! What is it Good For? (00:28:19)</li><li>Expectations for the future (00:40:22)</li><li>Foragers, Farmers, and Fossil Fuels (00:53:53)</li><li>Historical methodology (01:03:14)</li><li>Falsifiable alternative theories (01:15:59)</li><li>Archaeology (01:22:56)</li><li>Energy extraction technology as a key driver of human values (01:37:43)</li><li>Allowing people to debate about values (02:00:16)</li><li>Can productive wars still occur? (02:13:28)</li><li>Where is history contingent and where isn’t it? (02:30:23)</li><li>How Ian thinks about the future (03:13:33)</li><li>Macrohistory myths (03:29:51)</li><li>Ian’s favourite archaeology memory (03:33:19)</li><li>The most unfair criticism Ian’s ever received (03:35:17)</li><li>Rob's outro (03:39:55)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 15 Jan 2025 16:12:34 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b206dd56/70ee334f.mp3" length="212097358" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/mpXZk751MPP3tjlyoAZCBP53Vn6BbzyD0yAgbfRiXk0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8yOTg1/M2UxNjI2NDc4NDcx/NmY4YmFkOWY3MjVm/MzczZC5qcGc.jpg"/>
      <itunes:duration>13253</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monarchs.</p><p>Wind back 10,000 years and things look very different again. Most hunter-gatherer groups thought men who got too big for their britches needed to be put in their place rather than obeyed, and lifelong monogamy could hardly be expected of men or women.</p><p>Why such big systematic changes — and why these changes specifically?</p><p>That's the question bestselling historian Ian Morris takes up in his book, <em>Foragers, Farmers, and Fossil Fuels: How Human Values Evolve</em>. Ian has spent his academic life studying long-term history, trying to explain the big-picture changes that play out over hundreds or thousands of years.</p><p><strong>Rebroadcast: this episode was originally released in July 2022.</strong></p><p><a href="https://80000hours.org/podcast/episodes/ian-morris-big-picture-history/"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>There are a number of possible explanations one could offer for the wide-ranging shifts in opinion on the 'right' way to live. Maybe the natural sciences progressed and people realised their previous ideas were mistaken? Perhaps a few persuasive advocates turned the course of history with their revolutionary arguments? Maybe everyone just got nicer?</p><p>In <em>Foragers, Farmers and Fossil Fuels</em> Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels.</p><p>On this theory, it's technology that drives moral values much more than moral philosophy. Individuals can try to persist with deeply held values that limit economic growth, but they risk being rendered irrelevant as more productive peers in their own society accrue wealth and power. And societies that fail to move with the times risk being conquered by more pragmatic neighbours that adapt to new technologies and grow in population and military strength.</p><p>There are many objections one could raise to this theory, many of which we put to Ian in this interview. But the question is a highly consequential one: if we want to guess what goals our descendants will pursue hundreds of years from now, it would be helpful to have a theory for why our ancestors mostly thought one thing, while we mostly think another.</p><p>Big though it is, the driver of human values is only one of several major questions Ian has tackled through his career.</p><p>In this classic episode, we discuss all of Ian's major books.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:53)</li><li>The interview begins (00:02:30)</li><li>Geography is Destiny (00:03:38)</li><li>Why the West Rules—For Now (00:12:04)</li><li>War! What is it Good For? (00:28:19)</li><li>Expectations for the future (00:40:22)</li><li>Foragers, Farmers, and Fossil Fuels (00:53:53)</li><li>Historical methodology (01:03:14)</li><li>Falsifiable alternative theories (01:15:59)</li><li>Archaeology (01:22:56)</li><li>Energy extraction technology as a key driver of human values (01:37:43)</li><li>Allowing people to debate about values (02:00:16)</li><li>Can productive wars still occur? (02:13:28)</li><li>Where is history contingent and where isn’t it? (02:30:23)</li><li>How Ian thinks about the future (03:13:33)</li><li>Macrohistory myths (03:29:51)</li><li>Ian’s favourite archaeology memory (03:33:19)</li><li>The most unfair criticism Ian’s ever received (03:35:17)</li><li>Rob's outro (03:39:55)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/b206dd56/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#140 Classic episode – Bear Braumoeller on the case that war isn’t in decline</title>
      <itunes:title>#140 Classic episode – Bear Braumoeller on the case that war isn’t in decline</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">9abede32-aba8-40bd-95a3-51d54600e8b3</guid>
      <link>https://80000hours.org/podcast/episodes/bear-braumoeller-decline-of-war/</link>
      <description>
        <![CDATA[<p>Is war in long-term decline? Steven Pinker's <em>The Better Angels of Our Nature</em> brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out.</p><p>But that idea divides war scholars and statisticians, and so <em>Better Angels</em> has prompted a spirited debate, with datasets and statistical analyses exchanged back and forth year after year. The lack of consensus has left a somewhat bewildered public (including host Rob Wiblin) unsure quite what to believe.</p><p>Today's guest, professor in political science Bear Braumoeller, is one of the scholars who believes we lack convincing evidence that warlikeness is in long-term decline. He collected the analysis that led him to that conclusion in his 2019 book, <a href="https://www.amazon.com/Only-Dead-Persistence-War-Modern/dp/0190849533"><em>Only the Dead: The Persistence of War in the Modern Age</em></a>.</p><p><strong>Rebroadcast: this episode was originally released in November 2022.</strong></p><p><a href="https://80k.link/bb"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>The question is of great practical importance. The US and PRC are entering a period of renewed great power competition, with Taiwan as a potential trigger for war, and Russia is once more invading and attempting to annex the territory of its neighbours.</p><p>If war has been going out of fashion since the start of the Enlightenment, we might console ourselves that however nerve-wracking these present circumstances may feel, modern culture will throw up powerful barriers to another world war. But if we're as war-prone as we ever have been, one need only inspect the record of the 20th century to recoil in horror at what might await us in the 21st.</p><p>Bear argues that the second reaction is the appropriate one. The world has gone up in flames many times through history, with roughly 0.5% of the population dying in the Napoleonic Wars, 1% in World War I, 3% in World War II, and perhaps 10% during the Mongol conquests. And with no reason to think similar catastrophes are any less likely today, complacency could lead us to sleepwalk into disaster.</p><p>He gets to this conclusion primarily by analysing the datasets of the decades-old Correlates of War project, which aspires to track all interstate conflicts and battlefield deaths since 1815. In <em>Only the Dead</em>, he chops up and inspects this data dozens of different ways, to test if there are any shifts over time which seem larger than what could be explained by chance variation alone.</p><p>In a nutshell, Bear simply finds no general trend in either direction from 1815 through today. It seems like, as philosopher George Santayana lamented in 1922, "only the dead have seen the end of war."</p><p>In today's conversation, Bear and Rob discuss all of the above in more detail than even a usual 80,000 Hours podcast episode, as well as:</p><ul><li>Why haven't modern ideas about the immorality of violence led to the decline of war, when it's such a natural thing to expect?</li><li>What would Bear's critics say in response to all this?</li><li>What do the optimists get right?</li><li>How does one do proper statistical tests for events that are clumped together, like war deaths?</li><li>Why are deaths in war so concentrated in a handful of the most extreme events?</li><li>Did the ideas of the Enlightenment promote nonviolence, on balance?</li><li>Were early states more or less violent than groups of hunter-gatherers?</li><li>If Bear is right, what can be done?</li><li>How did the 'Concert of Europe' or 'Bismarckian system' maintain peace in the 19th century?</li><li>Which wars are remarkable but largely unknown?</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:01)</li><li>The interview begins (00:05:37)</li><li>Only the Dead (00:08:33)</li><li>The Enlightenment (00:18:50)</li><li>Democratic peace theory (00:28:26)</li><li>Is religion a key driver of war? (00:31:32)</li><li>International orders (00:35:14)</li><li>The Concert of Europe (00:44:21)</li><li>The Bismarckian system (00:55:49)</li><li>The current international order (01:00:22)</li><li>The Better Angels of Our Nature (01:19:36)</li><li>War datasets (01:34:09)</li><li>Seeing patterns in data where none exist (01:47:38)</li><li>Change-point analysis (01:51:39)</li><li>Rates of violent death throughout history (01:56:39)</li><li>War initiation (02:05:02)</li><li>Escalation (02:20:03)</li><li>Getting massively different results from the same data (02:30:45)</li><li>How worried we should be (02:36:13)</li><li>Most likely ways Only the Dead is wrong (02:38:31)</li><li>Astonishing smaller wars (02:42:45)</li><li>Rob’s outro (02:47:13)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Is war in long-term decline? Steven Pinker's <em>The Better Angels of Our Nature</em> brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out.</p><p>But that idea divides war scholars and statisticians, and so <em>Better Angels</em> has prompted a spirited debate, with datasets and statistical analyses exchanged back and forth year after year. The lack of consensus has left a somewhat bewildered public (including host Rob Wiblin) unsure quite what to believe.</p><p>Today's guest, professor in political science Bear Braumoeller, is one of the scholars who believes we lack convincing evidence that warlikeness is in long-term decline. He collected the analysis that led him to that conclusion in his 2019 book, <a href="https://www.amazon.com/Only-Dead-Persistence-War-Modern/dp/0190849533"><em>Only the Dead: The Persistence of War in the Modern Age</em></a>.</p><p><strong>Rebroadcast: this episode was originally released in November 2022.</strong></p><p><a href="https://80k.link/bb"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>The question is of great practical importance. The US and PRC are entering a period of renewed great power competition, with Taiwan as a potential trigger for war, and Russia is once more invading and attempting to annex the territory of its neighbours.</p><p>If war has been going out of fashion since the start of the Enlightenment, we might console ourselves that however nerve-wracking these present circumstances may feel, modern culture will throw up powerful barriers to another world war. But if we're as war-prone as we ever have been, one need only inspect the record of the 20th century to recoil in horror at what might await us in the 21st.</p><p>Bear argues that the second reaction is the appropriate one. The world has gone up in flames many times through history, with roughly 0.5% of the population dying in the Napoleonic Wars, 1% in World War I, 3% in World War II, and perhaps 10% during the Mongol conquests. And with no reason to think similar catastrophes are any less likely today, complacency could lead us to sleepwalk into disaster.</p><p>He gets to this conclusion primarily by analysing the datasets of the decades-old Correlates of War project, which aspires to track all interstate conflicts and battlefield deaths since 1815. In <em>Only the Dead</em>, he chops up and inspects this data dozens of different ways, to test if there are any shifts over time which seem larger than what could be explained by chance variation alone.</p><p>In a nutshell, Bear simply finds no general trend in either direction from 1815 through today. It seems like, as philosopher George Santayana lamented in 1922, "only the dead have seen the end of war."</p><p>In today's conversation, Bear and Rob discuss all of the above in more detail than even a usual 80,000 Hours podcast episode, as well as:</p><ul><li>Why haven't modern ideas about the immorality of violence led to the decline of war, when it's such a natural thing to expect?</li><li>What would Bear's critics say in response to all this?</li><li>What do the optimists get right?</li><li>How does one do proper statistical tests for events that are clumped together, like war deaths?</li><li>Why are deaths in war so concentrated in a handful of the most extreme events?</li><li>Did the ideas of the Enlightenment promote nonviolence, on balance?</li><li>Were early states more or less violent than groups of hunter-gatherers?</li><li>If Bear is right, what can be done?</li><li>How did the 'Concert of Europe' or 'Bismarckian system' maintain peace in the 19th century?</li><li>Which wars are remarkable but largely unknown?</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:01)</li><li>The interview begins (00:05:37)</li><li>Only the Dead (00:08:33)</li><li>The Enlightenment (00:18:50)</li><li>Democratic peace theory (00:28:26)</li><li>Is religion a key driver of war? (00:31:32)</li><li>International orders (00:35:14)</li><li>The Concert of Europe (00:44:21)</li><li>The Bismarckian system (00:55:49)</li><li>The current international order (01:00:22)</li><li>The Better Angels of Our Nature (01:19:36)</li><li>War datasets (01:34:09)</li><li>Seeing patterns in data where none exist (01:47:38)</li><li>Change-point analysis (01:51:39)</li><li>Rates of violent death throughout history (01:56:39)</li><li>War initiation (02:05:02)</li><li>Escalation (02:20:03)</li><li>Getting massively different results from the same data (02:30:45)</li><li>How worried we should be (02:36:13)</li><li>Most likely ways Only the Dead is wrong (02:38:31)</li><li>Astonishing smaller wars (02:42:45)</li><li>Rob’s outro (02:47:13)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 08 Jan 2025 16:45:01 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/9197532c/bee07938.mp3" length="161362620" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/bsml7dQOSnJ2u9pxSwi2q8T0fwIIlLrpZh792ubHr3M/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS84YmVi/MmY4YWJiYTY0YjMz/MTk0NGZkZGQ3NzZm/NWIwYy5qcGc.jpg"/>
      <itunes:duration>10083</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Is war in long-term decline? Steven Pinker's <em>The Better Angels of Our Nature</em> brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out.</p><p>But that idea divides war scholars and statisticians, and so <em>Better Angels</em> has prompted a spirited debate, with datasets and statistical analyses exchanged back and forth year after year. The lack of consensus has left a somewhat bewildered public (including host Rob Wiblin) unsure quite what to believe.</p><p>Today's guest, professor in political science Bear Braumoeller, is one of the scholars who believes we lack convincing evidence that warlikeness is in long-term decline. He collected the analysis that led him to that conclusion in his 2019 book, <a href="https://www.amazon.com/Only-Dead-Persistence-War-Modern/dp/0190849533"><em>Only the Dead: The Persistence of War in the Modern Age</em></a>.</p><p><strong>Rebroadcast: this episode was originally released in November 2022.</strong></p><p><a href="https://80k.link/bb"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>The question is of great practical importance. The US and PRC are entering a period of renewed great power competition, with Taiwan as a potential trigger for war, and Russia is once more invading and attempting to annex the territory of its neighbours.</p><p>If war has been going out of fashion since the start of the Enlightenment, we might console ourselves that however nerve-wracking these present circumstances may feel, modern culture will throw up powerful barriers to another world war. But if we're as war-prone as we ever have been, one need only inspect the record of the 20th century to recoil in horror at what might await us in the 21st.</p><p>Bear argues that the second reaction is the appropriate one. The world has gone up in flames many times through history, with roughly 0.5% of the population dying in the Napoleonic Wars, 1% in World War I, 3% in World War II, and perhaps 10% during the Mongol conquests. And with no reason to think similar catastrophes are any less likely today, complacency could lead us to sleepwalk into disaster.</p><p>He gets to this conclusion primarily by analysing the datasets of the decades-old Correlates of War project, which aspires to track all interstate conflicts and battlefield deaths since 1815. In <em>Only the Dead</em>, he chops up and inspects this data dozens of different ways, to test if there are any shifts over time which seem larger than what could be explained by chance variation alone.</p><p>In a nutshell, Bear simply finds no general trend in either direction from 1815 through today. It seems like, as philosopher George Santayana lamented in 1922, "only the dead have seen the end of war."</p><p>In today's conversation, Bear and Rob discuss all of the above in more detail than even a usual 80,000 Hours podcast episode, as well as:</p><ul><li>Why haven't modern ideas about the immorality of violence led to the decline of war, when it's such a natural thing to expect?</li><li>What would Bear's critics say in response to all this?</li><li>What do the optimists get right?</li><li>How does one do proper statistical tests for events that are clumped together, like war deaths?</li><li>Why are deaths in war so concentrated in a handful of the most extreme events?</li><li>Did the ideas of the Enlightenment promote nonviolence, on balance?</li><li>Were early states more or less violent than groups of hunter-gatherers?</li><li>If Bear is right, what can be done?</li><li>How did the 'Concert of Europe' or 'Bismarckian system' maintain peace in the 19th century?</li><li>Which wars are remarkable but largely unknown?</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:01)</li><li>The interview begins (00:05:37)</li><li>Only the Dead (00:08:33)</li><li>The Enlightenment (00:18:50)</li><li>Democratic peace theory (00:28:26)</li><li>Is religion a key driver of war? (00:31:32)</li><li>International orders (00:35:14)</li><li>The Concert of Europe (00:44:21)</li><li>The Bismarckian system (00:55:49)</li><li>The current international order (01:00:22)</li><li>The Better Angels of Our Nature (01:19:36)</li><li>War datasets (01:34:09)</li><li>Seeing patterns in data where none exist (01:47:38)</li><li>Change-point analysis (01:51:39)</li><li>Rates of violent death throughout history (01:56:39)</li><li>War initiation (02:05:02)</li><li>Escalation (02:20:03)</li><li>Getting massively different results from the same data (02:30:45)</li><li>How worried we should be (02:36:13)</li><li>Most likely ways Only the Dead is wrong (02:38:31)</li><li>Astonishing smaller wars (02:42:45)</li><li>Rob’s outro (02:47:13)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/9197532c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>2024 Highlightapalooza! (The best of The 80,000 Hours Podcast this year)</title>
      <itunes:title>2024 Highlightapalooza! (The best of The 80,000 Hours Podcast this year)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6997f107-c565-4e34-9555-a6d789a3515e</guid>
      <link>https://80000hours.org/podcast/episodes/2024-highlights/?utm_campaign=podcast__2024-highlights&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"A shameless recycling of existing content to drive additional audience engagement on the cheap… <em>or</em> the single best, most valuable, and most insight-dense episode we put out in the entire year, depending on how you want to look at it." — Rob Wiblin</p><p>It’s that magical time of year once again — <strong>highlightapalooza!</strong> Stick around for one top bit from each episode, including:</p><ul><li>How to use the microphone on someone’s mobile phone to figure out what password they’re typing into their laptop</li><li>Why mercilessly driving the New World screwworm to extinction could be the most compassionate thing humanity has ever done</li><li>Why evolutionary psychology doesn’t support a cynical view of human nature but actually explains why so many of us are intensely sensitive to the harms we cause to others</li><li>How superforecasters and domain experts seem to disagree so much about AI risk, but when you zoom in it’s mostly a disagreement about timing</li><li>Why the sceptics are wrong and you will want to use robot nannies to take care of your kids — and also why despite having big worries about the development of AGI, Carl Shulman is strongly against efforts to pause AI research today</li><li>How much of the gender pay gap is due to direct pay discrimination vs other factors</li><li>How cleaner wrasse fish blow the mirror test out of the water</li><li>Why effective altruism may be too big a tent to work well</li><li>How we could best motivate pharma companies to test existing drugs to see if they help cure other diseases — something they currently have no reason to bother with</li></ul><p>…as well as 27 other top observations and arguments from the <a href="https://80000hours.org/podcast/episodes/">past year of the show</a>.</p><p><a href="https://80k.info/24h"><strong>Check out the full transcript and episode links on the 80,000 Hours website.</strong></a></p><p>Remember that all of these clips come from the 20-minute highlight reels we make for every episode, which are released on our sister feed, <a href="https://80000hours.org/after-hours-podcast/"><em>80k After Hours</em></a>. So if you’re struggling to keep up with our regularly scheduled entertainment, you can still get the best parts of our conversations there.</p><p>It has been a hell of a year, and we can only imagine next year is going to be even weirder — but Luisa and Rob will be here to keep you company as Earth hurtles through the galaxy to a fate as yet unknown.</p><p>Enjoy, and look forward to speaking with you in 2025!</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Randy Nesse on the origins of morality and the problem of simplistic selfish-gene thinking (00:02:11)</li><li>Hugo Mercier on the evolutionary argument against humans being gullible (00:07:17)</li><li>Meghan Barrett on the likelihood of insect sentience (00:11:26)</li><li>Sébastien Moro on the mirror test triumph of cleaner wrasses (00:14:47)</li><li>Sella Nevo on side-channel attacks (00:19:32)</li><li>Zvi Mowshowitz on AI sleeper agents (00:22:59)</li><li>Zach Weinersmith on why space settlement (probably) won't make us rich (00:29:11)</li><li>Rachel Glennerster on pull mechanisms to incentivise repurposing of generic drugs (00:35:23)</li><li>Emily Oster on the impact of kids on women's careers (00:40:29)</li><li>Carl Shulman on robot nannies (00:45:19)</li><li>Nathan Labenz on kids and artificial friends (00:50:12)</li><li>Nathan Calvin on why it's not too early for AI policies (00:54:13)</li><li>Rose Chan Loui on how control of OpenAI is independently incredibly valuable and requires compensation (00:58:08)</li><li>Nick Joseph on why he’s a big fan of the responsible scaling policy approach (01:03:11)</li><li>Sihao Huang on how the US and UK might coordinate with China (01:06:09)</li><li>Nathan Labenz on better transparency about predicted capabilities (01:10:18)</li><li>Ezra Karger on what explains forecasters’ disagreements about AI risks (01:15:22)</li><li>Carl Shulman on why he doesn't support enforced pauses on AI research (01:18:58)</li><li>Matt Clancy on the omnipresent frictions that might prevent explosive economic growth (01:25:24)</li><li>Vitalik Buterin on defensive acceleration (01:29:43)</li><li>Annie Jacobsen on the war games that suggest escalation is inevitable (01:34:59)</li><li>Nate Silver on whether effective altruism is too big to succeed (01:38:42)</li><li>Kevin Esvelt on why killing every screwworm would be the best thing humanity ever did (01:42:27)</li><li>Lewis Bollard on how factory farming is philosophically indefensible (01:46:28)</li><li>Bob Fischer on how to think about moral weights if you're not a hedonist (01:49:27)</li><li>Elizabeth Cox on the empirical evidence of the impact of storytelling (01:57:43)</li><li>Anil Seth on how our brain interprets reality (02:01:03)</li><li>Eric Schwitzgebel on whether consciousness can be nested (02:04:53)</li><li>Jonathan Birch on our overconfidence around disorders of consciousness (02:10:23)</li><li>Peter Godfrey-Smith on uploads of ourselves (02:14:34)</li><li>Laura Deming on surprising things that make mice live longer (02:21:17)</li><li>Venki Ramakrishnan on freezing cells, organs, and bodies (02:24:46)</li><li>Ken Goldberg on why low fault tolerance makes some skills extra hard to automate in robots (02:29:12)</li><li>Sarah Eustis-Guthrie on the ups and downs of founding an organisation (02:34:04)</li><li>Dean Spears on the cost effectiveness of kangaroo mother care (02:38:26)</li><li>Cameron Meyer Shorb on vaccines for wild animals (02:42:53)</li><li>Spencer Greenberg on personal principles (02:46:08)</li></ul><p><em>Producing and editing: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"A shameless recycling of existing content to drive additional audience engagement on the cheap… <em>or</em> the single best, most valuable, and most insight-dense episode we put out in the entire year, depending on how you want to look at it." — Rob Wiblin</p><p>It’s that magical time of year once again — <strong>highlightapalooza!</strong> Stick around for one top bit from each episode, including:</p><ul><li>How to use the microphone on someone’s mobile phone to figure out what password they’re typing into their laptop</li><li>Why mercilessly driving the New World screwworm to extinction could be the most compassionate thing humanity has ever done</li><li>Why evolutionary psychology doesn’t support a cynical view of human nature but actually explains why so many of us are intensely sensitive to the harms we cause to others</li><li>How superforecasters and domain experts seem to disagree so much about AI risk, but when you zoom in it’s mostly a disagreement about timing</li><li>Why the sceptics are wrong and you will want to use robot nannies to take care of your kids — and also why despite having big worries about the development of AGI, Carl Shulman is strongly against efforts to pause AI research today</li><li>How much of the gender pay gap is due to direct pay discrimination vs other factors</li><li>How cleaner wrasse fish blow the mirror test out of the water</li><li>Why effective altruism may be too big a tent to work well</li><li>How we could best motivate pharma companies to test existing drugs to see if they help cure other diseases — something they currently have no reason to bother with</li></ul><p>…as well as 27 other top observations and arguments from the <a href="https://80000hours.org/podcast/episodes/">past year of the show</a>.</p><p><a href="https://80k.info/24h"><strong>Check out the full transcript and episode links on the 80,000 Hours website.</strong></a></p><p>Remember that all of these clips come from the 20-minute highlight reels we make for every episode, which are released on our sister feed, <a href="https://80000hours.org/after-hours-podcast/"><em>80k After Hours</em></a>. So if you’re struggling to keep up with our regularly scheduled entertainment, you can still get the best parts of our conversations there.</p><p>It has been a hell of a year, and we can only imagine next year is going to be even weirder — but Luisa and Rob will be here to keep you company as Earth hurtles through the galaxy to a fate as yet unknown.</p><p>Enjoy, and look forward to speaking with you in 2025!</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Randy Nesse on the origins of morality and the problem of simplistic selfish-gene thinking (00:02:11)</li><li>Hugo Mercier on the evolutionary argument against humans being gullible (00:07:17)</li><li>Meghan Barrett on the likelihood of insect sentience (00:11:26)</li><li>Sébastien Moro on the mirror test triumph of cleaner wrasses (00:14:47)</li><li>Sella Nevo on side-channel attacks (00:19:32)</li><li>Zvi Mowshowitz on AI sleeper agents (00:22:59)</li><li>Zach Weinersmith on why space settlement (probably) won't make us rich (00:29:11)</li><li>Rachel Glennerster on pull mechanisms to incentivise repurposing of generic drugs (00:35:23)</li><li>Emily Oster on the impact of kids on women's careers (00:40:29)</li><li>Carl Shulman on robot nannies (00:45:19)</li><li>Nathan Labenz on kids and artificial friends (00:50:12)</li><li>Nathan Calvin on why it's not too early for AI policies (00:54:13)</li><li>Rose Chan Loui on how control of OpenAI is independently incredibly valuable and requires compensation (00:58:08)</li><li>Nick Joseph on why he’s a big fan of the responsible scaling policy approach (01:03:11)</li><li>Sihao Huang on how the US and UK might coordinate with China (01:06:09)</li><li>Nathan Labenz on better transparency about predicted capabilities (01:10:18)</li><li>Ezra Karger on what explains forecasters’ disagreements about AI risks (01:15:22)</li><li>Carl Shulman on why he doesn't support enforced pauses on AI research (01:18:58)</li><li>Matt Clancy on the omnipresent frictions that might prevent explosive economic growth (01:25:24)</li><li>Vitalik Buterin on defensive acceleration (01:29:43)</li><li>Annie Jacobsen on the war games that suggest escalation is inevitable (01:34:59)</li><li>Nate Silver on whether effective altruism is too big to succeed (01:38:42)</li><li>Kevin Esvelt on why killing every screwworm would be the best thing humanity ever did (01:42:27)</li><li>Lewis Bollard on how factory farming is philosophically indefensible (01:46:28)</li><li>Bob Fischer on how to think about moral weights if you're not a hedonist (01:49:27)</li><li>Elizabeth Cox on the empirical evidence of the impact of storytelling (01:57:43)</li><li>Anil Seth on how our brain interprets reality (02:01:03)</li><li>Eric Schwitzgebel on whether consciousness can be nested (02:04:53)</li><li>Jonathan Birch on our overconfidence around disorders of consciousness (02:10:23)</li><li>Peter Godfrey-Smith on uploads of ourselves (02:14:34)</li><li>Laura Deming on surprising things that make mice live longer (02:21:17)</li><li>Venki Ramakrishnan on freezing cells, organs, and bodies (02:24:46)</li><li>Ken Goldberg on why low fault tolerance makes some skills extra hard to automate in robots (02:29:12)</li><li>Sarah Eustis-Guthrie on the ups and downs of founding an organisation (02:34:04)</li><li>Dean Spears on the cost effectiveness of kangaroo mother care (02:38:26)</li><li>Cameron Meyer Shorb on vaccines for wild animals (02:42:53)</li><li>Spencer Greenberg on personal principles (02:46:08)</li></ul><p><em>Producing and editing: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 27 Dec 2024 13:18:10 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/aa04fb56/bd4399b7.mp3" length="163280251" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2mtAyleXxS7aFoVKFnuSLlV9IlIdU3A5EL6_onhxHz0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS82MjRh/ZDc5ODNmNzc2M2M3/OGMwZDc5MjdiOWFm/MWU4Ni5qcGVn.jpg"/>
      <itunes:duration>10202</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"A shameless recycling of existing content to drive additional audience engagement on the cheap… <em>or</em> the single best, most valuable, and most insight-dense episode we put out in the entire year, depending on how you want to look at it." — Rob Wiblin</p><p>It’s that magical time of year once again — <strong>highlightapalooza!</strong> Stick around for one top bit from each episode, including:</p><ul><li>How to use the microphone on someone’s mobile phone to figure out what password they’re typing into their laptop</li><li>Why mercilessly driving the New World screwworm to extinction could be the most compassionate thing humanity has ever done</li><li>Why evolutionary psychology doesn’t support a cynical view of human nature but actually explains why so many of us are intensely sensitive to the harms we cause to others</li><li>How superforecasters and domain experts seem to disagree so much about AI risk, but when you zoom in it’s mostly a disagreement about timing</li><li>Why the sceptics are wrong and you will want to use robot nannies to take care of your kids — and also why despite having big worries about the development of AGI, Carl Shulman is strongly against efforts to pause AI research today</li><li>How much of the gender pay gap is due to direct pay discrimination vs other factors</li><li>How cleaner wrasse fish blow the mirror test out of the water</li><li>Why effective altruism may be too big a tent to work well</li><li>How we could best motivate pharma companies to test existing drugs to see if they help cure other diseases — something they currently have no reason to bother with</li></ul><p>…as well as 27 other top observations and arguments from the <a href="https://80000hours.org/podcast/episodes/">past year of the show</a>.</p><p><a href="https://80k.info/24h"><strong>Check out the full transcript and episode links on the 80,000 Hours website.</strong></a></p><p>Remember that all of these clips come from the 20-minute highlight reels we make for every episode, which are released on our sister feed, <a href="https://80000hours.org/after-hours-podcast/"><em>80k After Hours</em></a>. So if you’re struggling to keep up with our regularly scheduled entertainment, you can still get the best parts of our conversations there.</p><p>It has been a hell of a year, and we can only imagine next year is going to be even weirder — but Luisa and Rob will be here to keep you company as Earth hurtles through the galaxy to a fate as yet unknown.</p><p>Enjoy, and look forward to speaking with you in 2025!</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Randy Nesse on the origins of morality and the problem of simplistic selfish-gene thinking (00:02:11)</li><li>Hugo Mercier on the evolutionary argument against humans being gullible (00:07:17)</li><li>Meghan Barrett on the likelihood of insect sentience (00:11:26)</li><li>Sébastien Moro on the mirror test triumph of cleaner wrasses (00:14:47)</li><li>Sella Nevo on side-channel attacks (00:19:32)</li><li>Zvi Mowshowitz on AI sleeper agents (00:22:59)</li><li>Zach Weinersmith on why space settlement (probably) won't make us rich (00:29:11)</li><li>Rachel Glennerster on pull mechanisms to incentivise repurposing of generic drugs (00:35:23)</li><li>Emily Oster on the impact of kids on women's careers (00:40:29)</li><li>Carl Shulman on robot nannies (00:45:19)</li><li>Nathan Labenz on kids and artificial friends (00:50:12)</li><li>Nathan Calvin on why it's not too early for AI policies (00:54:13)</li><li>Rose Chan Loui on how control of OpenAI is independently incredibly valuable and requires compensation (00:58:08)</li><li>Nick Joseph on why he’s a big fan of the responsible scaling policy approach (01:03:11)</li><li>Sihao Huang on how the US and UK might coordinate with China (01:06:09)</li><li>Nathan Labenz on better transparency about predicted capabilities (01:10:18)</li><li>Ezra Karger on what explains forecasters’ disagreements about AI risks (01:15:22)</li><li>Carl Shulman on why he doesn't support enforced pauses on AI research (01:18:58)</li><li>Matt Clancy on the omnipresent frictions that might prevent explosive economic growth (01:25:24)</li><li>Vitalik Buterin on defensive acceleration (01:29:43)</li><li>Annie Jacobsen on the war games that suggest escalation is inevitable (01:34:59)</li><li>Nate Silver on whether effective altruism is too big to succeed (01:38:42)</li><li>Kevin Esvelt on why killing every screwworm would be the best thing humanity ever did (01:42:27)</li><li>Lewis Bollard on how factory farming is philosophically indefensible (01:46:28)</li><li>Bob Fischer on how to think about moral weights if you're not a hedonist (01:49:27)</li><li>Elizabeth Cox on the empirical evidence of the impact of storytelling (01:57:43)</li><li>Anil Seth on how our brain interprets reality (02:01:03)</li><li>Eric Schwitzgebel on whether consciousness can be nested (02:04:53)</li><li>Jonathan Birch on our overconfidence around disorders of consciousness (02:10:23)</li><li>Peter Godfrey-Smith on uploads of ourselves (02:14:34)</li><li>Laura Deming on surprising things that make mice live longer (02:21:17)</li><li>Venki Ramakrishnan on freezing cells, organs, and bodies (02:24:46)</li><li>Ken Goldberg on why low fault tolerance makes some skills extra hard to automate in robots (02:29:12)</li><li>Sarah Eustis-Guthrie on the ups and downs of founding an organisation (02:34:04)</li><li>Dean Spears on the cost effectiveness of kangaroo mother care (02:38:26)</li><li>Cameron Meyer Shorb on vaccines for wild animals (02:42:53)</li><li>Spencer Greenberg on personal principles (02:46:08)</li></ul><p><em>Producing and editing: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/aa04fb56/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/aa04fb56/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#211 – Sam Bowman on why housing still isn't fixed and what would actually work</title>
      <itunes:title>#211 – Sam Bowman on why housing still isn't fixed and what would actually work</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c6458cdf-7bf9-4706-8151-43203d057ad6</guid>
      <link>https://80000hours.org/podcast/episodes/sam-bowman-overcoming-nimbys-housing-policy-proposals/?utm_campaign=podcast__sam-bowman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Rich countries seem to find it harder and harder to do anything that creates some losers. People who don’t want houses, offices, power stations, trains, subway stations (or whatever) built in their area can usually find some way to block them, even if the benefits to society outweigh the costs 10 or 100 times over.</p><p>The result of this ‘vetocracy’ has been skyrocketing rent in major cities — not to mention exacerbating homelessness, energy poverty, and a <a href="https://worksinprogress.co/issue/the-housing-theory-of-everything/">host of other social maladies</a>. This has been known for years but precious little progress has been made. When trains, tunnels, or nuclear reactors are occasionally built, they’re comically expensive and slow compared to 50 years ago. And housing construction in the UK and California has barely increased, remaining stuck at less than half what it was in the ’60s and ’70s.</p><p>Today’s guest — economist and editor of <a href="https://worksinprogress.co/"><em>Works in Progress</em></a> <a href="https://www.sambowman.co/about"><strong>Sam Bowman</strong></a> — isn’t content to just condemn the Not In My Backyard (NIMBY) mentality behind this stagnation. He wants to actually get a tonne of stuff built, and by that standard the strategy of attacking ‘NIMBYs’ has been an abject failure. They are too politically powerful, and if you try to crush them, sooner or later they crush you.</p><p><a href="https://80k.info/sb"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>So, as Sam explains, a different strategy is needed, one that acknowledges that opponents of development are often correct that a given project will make them worse off. But the thing is, in the cases we care about, these modest downsides are outweighed by the enormous benefits to others — who will finally have a place to live, be able to get to work, and have the energy to heat their home.</p><p>But democracies are majoritarian, so if most existing residents think they’ll be a little worse off if more dwellings are built in their area, it’s no surprise they aren’t getting built. Luckily we already have a simple way to get people to do things they don’t enjoy for the greater good, a strategy that we apply every time someone goes in to work at a job they wouldn’t do for free: <strong>compensate them</strong>. </p><p>Sam thinks this idea, which he calls “Coasean democracy,” could create a politically sustainable majority in favour of building and underlies the proposals he thinks have the best chance of success — which he discusses in detail with host Rob Wiblin.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Introducing Sam Bowman (00:00:59)</li><li>We can’t seem to build anything (00:02:09)</li><li>Our inability to build is ruining people's lives (00:04:03)</li><li>Why blocking growth of big cities is terrible for science and invention (00:09:15)</li><li>It's also worsening inequality, health, fertility, and political polarisation (00:14:36)</li><li>The UK as the 'limit case' of restrictive planning permission gone mad (00:17:50)</li><li>We've known this for years. So why almost no progress fixing it? (00:36:34)</li><li>NIMBYs aren't wrong: they are often harmed by development (00:43:58)</li><li>Solution #1: Street votes (00:55:37)</li><li>Are street votes unfair to surrounding areas? (01:08:31)</li><li>Street votes are coming to the UK — what to expect (01:15:07)</li><li>Are street votes viable in California, NY, or other countries? (01:19:34)</li><li>Solution #2: Benefit sharing (01:25:08)</li><li>Property tax distribution — the most important policy you've never heard of (01:44:29)</li><li>Solution #3: Opt-outs (01:57:53)</li><li>How to make these things happen (02:11:19)</li><li>Let new and old institutions run in parallel until the old one withers (02:18:17)</li><li>The evil of modern architecture and why beautiful buildings are essential (02:31:58)</li><li>Northern latitudes need nuclear power — solar won't be enough (02:45:01)</li><li>Ozempic is still underrated and “the overweight theory of everything” (03:02:30)</li><li>How has progress studies remained sane while being very online? (03:17:55)</li></ul><p><em>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Rich countries seem to find it harder and harder to do anything that creates some losers. People who don’t want houses, offices, power stations, trains, subway stations (or whatever) built in their area can usually find some way to block them, even if the benefits to society outweigh the costs 10 or 100 times over.</p><p>The result of this ‘vetocracy’ has been skyrocketing rent in major cities — not to mention exacerbating homelessness, energy poverty, and a <a href="https://worksinprogress.co/issue/the-housing-theory-of-everything/">host of other social maladies</a>. This has been known for years but precious little progress has been made. When trains, tunnels, or nuclear reactors are occasionally built, they’re comically expensive and slow compared to 50 years ago. And housing construction in the UK and California has barely increased, remaining stuck at less than half what it was in the ’60s and ’70s.</p><p>Today’s guest — economist and editor of <a href="https://worksinprogress.co/"><em>Works in Progress</em></a> <a href="https://www.sambowman.co/about"><strong>Sam Bowman</strong></a> — isn’t content to just condemn the Not In My Backyard (NIMBY) mentality behind this stagnation. He wants to actually get a tonne of stuff built, and by that standard the strategy of attacking ‘NIMBYs’ has been an abject failure. They are too politically powerful, and if you try to crush them, sooner or later they crush you.</p><p><a href="https://80k.info/sb"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>So, as Sam explains, a different strategy is needed, one that acknowledges that opponents of development are often correct that a given project will make them worse off. But the thing is, in the cases we care about, these modest downsides are outweighed by the enormous benefits to others — who will finally have a place to live, be able to get to work, and have the energy to heat their home.</p><p>But democracies are majoritarian, so if most existing residents think they’ll be a little worse off if more dwellings are built in their area, it’s no surprise they aren’t getting built. Luckily we already have a simple way to get people to do things they don’t enjoy for the greater good, a strategy that we apply every time someone goes in to work at a job they wouldn’t do for free: <strong>compensate them</strong>. </p><p>Sam thinks this idea, which he calls “Coasean democracy,” could create a politically sustainable majority in favour of building and underlies the proposals he thinks have the best chance of success — which he discusses in detail with host Rob Wiblin.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Introducing Sam Bowman (00:00:59)</li><li>We can’t seem to build anything (00:02:09)</li><li>Our inability to build is ruining people's lives (00:04:03)</li><li>Why blocking growth of big cities is terrible for science and invention (00:09:15)</li><li>It's also worsening inequality, health, fertility, and political polarisation (00:14:36)</li><li>The UK as the 'limit case' of restrictive planning permission gone mad (00:17:50)</li><li>We've known this for years. So why almost no progress fixing it? (00:36:34)</li><li>NIMBYs aren't wrong: they are often harmed by development (00:43:58)</li><li>Solution #1: Street votes (00:55:37)</li><li>Are street votes unfair to surrounding areas? (01:08:31)</li><li>Street votes are coming to the UK — what to expect (01:15:07)</li><li>Are street votes viable in California, NY, or other countries? (01:19:34)</li><li>Solution #2: Benefit sharing (01:25:08)</li><li>Property tax distribution — the most important policy you've never heard of (01:44:29)</li><li>Solution #3: Opt-outs (01:57:53)</li><li>How to make these things happen (02:11:19)</li><li>Let new and old institutions run in parallel until the old one withers (02:18:17)</li><li>The evil of modern architecture and why beautiful buildings are essential (02:31:58)</li><li>Northern latitudes need nuclear power — solar won't be enough (02:45:01)</li><li>Ozempic is still underrated and “the overweight theory of everything” (03:02:30)</li><li>How has progress studies remained sane while being very online? (03:17:55)</li></ul><p><em>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 19 Dec 2024 17:17:51 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/42130ad9/e264adb7.mp3" length="197578749" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/O66Pq-e3WqSJytpW0XFO70iC5wq0ZNc0VttHfotBxVM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS80N2Jk/ZjIxNGI1MzNmZTQx/Nzk2NGJkN2VjNGQ0/YzkyNS5qcGc.jpg"/>
      <itunes:duration>12346</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Rich countries seem to find it harder and harder to do anything that creates some losers. People who don’t want houses, offices, power stations, trains, subway stations (or whatever) built in their area can usually find some way to block them, even if the benefits to society outweigh the costs 10 or 100 times over.</p><p>The result of this ‘vetocracy’ has been skyrocketing rent in major cities — not to mention exacerbating homelessness, energy poverty, and a <a href="https://worksinprogress.co/issue/the-housing-theory-of-everything/">host of other social maladies</a>. This has been known for years but precious little progress has been made. When trains, tunnels, or nuclear reactors are occasionally built, they’re comically expensive and slow compared to 50 years ago. And housing construction in the UK and California has barely increased, remaining stuck at less than half what it was in the ’60s and ’70s.</p><p>Today’s guest — economist and editor of <a href="https://worksinprogress.co/"><em>Works in Progress</em></a> <a href="https://www.sambowman.co/about"><strong>Sam Bowman</strong></a> — isn’t content to just condemn the Not In My Backyard (NIMBY) mentality behind this stagnation. He wants to actually get a tonne of stuff built, and by that standard the strategy of attacking ‘NIMBYs’ has been an abject failure. They are too politically powerful, and if you try to crush them, sooner or later they crush you.</p><p><a href="https://80k.info/sb"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>So, as Sam explains, a different strategy is needed, one that acknowledges that opponents of development are often correct that a given project will make them worse off. But the thing is, in the cases we care about, these modest downsides are outweighed by the enormous benefits to others — who will finally have a place to live, be able to get to work, and have the energy to heat their home.</p><p>But democracies are majoritarian, so if most existing residents think they’ll be a little worse off if more dwellings are built in their area, it’s no surprise they aren’t getting built. Luckily we already have a simple way to get people to do things they don’t enjoy for the greater good, a strategy that we apply every time someone goes in to work at a job they wouldn’t do for free: <strong>compensate them</strong>. </p><p>Sam thinks this idea, which he calls “Coasean democracy,” could create a politically sustainable majority in favour of building and underlies the proposals he thinks have the best chance of success — which he discusses in detail with host Rob Wiblin.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Introducing Sam Bowman (00:00:59)</li><li>We can’t seem to build anything (00:02:09)</li><li>Our inability to build is ruining people's lives (00:04:03)</li><li>Why blocking growth of big cities is terrible for science and invention (00:09:15)</li><li>It's also worsening inequality, health, fertility, and political polarisation (00:14:36)</li><li>The UK as the 'limit case' of restrictive planning permission gone mad (00:17:50)</li><li>We've known this for years. So why almost no progress fixing it? (00:36:34)</li><li>NIMBYs aren't wrong: they are often harmed by development (00:43:58)</li><li>Solution #1: Street votes (00:55:37)</li><li>Are street votes unfair to surrounding areas? (01:08:31)</li><li>Street votes are coming to the UK — what to expect (01:15:07)</li><li>Are street votes viable in California, NY, or other countries? (01:19:34)</li><li>Solution #2: Benefit sharing (01:25:08)</li><li>Property tax distribution — the most important policy you've never heard of (01:44:29)</li><li>Solution #3: Opt-outs (01:57:53)</li><li>How to make these things happen (02:11:19)</li><li>Let new and old institutions run in parallel until the old one withers (02:18:17)</li><li>The evil of modern architecture and why beautiful buildings are essential (02:31:58)</li><li>Northern latitudes need nuclear power — solar won't be enough (02:45:01)</li><li>Ozempic is still underrated and “the overweight theory of everything” (03:02:30)</li><li>How has progress studies remained sane while being very online? (03:17:55)</li></ul><p><em>Video editing: Simon Monsour<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/42130ad9/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/42130ad9/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#210 – Cameron Meyer Shorb on dismantling the myth that we can’t do anything to help wild animals</title>
      <itunes:title>#210 – Cameron Meyer Shorb on dismantling the myth that we can’t do anything to help wild animals</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c8effc74-6a6b-4b32-b52d-3c6af9ca0554</guid>
      <link>https://80000hours.org/podcast/episodes/cameron-meyer-shorb-wild-animal-suffering/?utm_campaign=podcast__cameron-meyer-shorb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"I really don’t want to give the impression that I think it is easy to make predictable, controlled, safe interventions in wild systems where there are many species interacting. I don’t think it’s easy, but I don’t see any reason to think that it’s impossible. And I think we have been making progress. I think there’s every reason to think that if we continue doing research, both at the theoretical level — How do ecosystems work? What sorts of things are likely to have what sorts of indirect effects? — and then also at the practical level — Is this intervention a good idea? — I really think we’re going to come up with plenty of things that would be helpful to plenty of animals." —Cameron Meyer Shorb</p><p>In today’s episode, host Luisa Rodriguez speaks to Cameron Meyer Shorb — executive director of the <a href="https://www.wildanimalinitiative.org/">Wild Animal Initiative</a> — about the cutting-edge research on wild animal welfare.</p><p><a href="https://80k.info/cms"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How it’s almost impossible to comprehend the sheer number of wild animals on Earth — and why that makes their potential suffering so important to consider.</li><li>How bad experiences like disease, parasites, and predation truly are for wild animals — and how we would even begin to study that empirically.</li><li>The tricky ethical dilemmas in trying to help wild animals without unintended consequences for ecosystems or other potentially sentient beings.</li><li>Potentially promising interventions to help wild animals — like selective reforestation, vaccines, fire management, and gene drives.</li><li>Why Cameron thinks the best approach to improving wild animal welfare is to first build a dedicated research field — and how Wild Animal Initiative’s activities support this.</li><li>The many career paths in science, policy, and technology that could contribute to improving wild animal welfare.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:03:40)</li><li>One concrete example of how we might improve wild animal welfare (00:04:04)</li><li>Why should we care about wild animal suffering? (00:10:00)</li><li>What’s it like to be a wild animal? (00:19:37)</li><li>Suffering and death in the wild (00:29:19)</li><li>Positive, benign, and social experiences (00:51:33)</li><li>Indicators of welfare (01:01:40)</li><li>Can we even help wild animals without unintended consequences? (01:13:20)</li><li>Vaccines for wild animals (01:30:59)</li><li>Fire management (01:44:20)</li><li>Gene drive technologies (01:47:42)</li><li>Common objections and misconceptions about wild animal welfare (01:53:19)</li><li>Future promising interventions (02:21:58)</li><li>What’s the long game for wild animal welfare? (02:27:46)</li><li>Eliminating the biological basis for suffering (02:33:21)</li><li>Optimising for high-welfare landscapes (02:37:33)</li><li>Wild Animal Initiative’s work (02:44:11)</li><li>Careers in wild animal welfare (02:58:13)</li><li>Work-related guilt and shame (03:12:57)</li><li>Luisa's outro (03:19:51)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"I really don’t want to give the impression that I think it is easy to make predictable, controlled, safe interventions in wild systems where there are many species interacting. I don’t think it’s easy, but I don’t see any reason to think that it’s impossible. And I think we have been making progress. I think there’s every reason to think that if we continue doing research, both at the theoretical level — How do ecosystems work? What sorts of things are likely to have what sorts of indirect effects? — and then also at the practical level — Is this intervention a good idea? — I really think we’re going to come up with plenty of things that would be helpful to plenty of animals." —Cameron Meyer Shorb</p><p>In today’s episode, host Luisa Rodriguez speaks to Cameron Meyer Shorb — executive director of the <a href="https://www.wildanimalinitiative.org/">Wild Animal Initiative</a> — about the cutting-edge research on wild animal welfare.</p><p><a href="https://80k.info/cms"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How it’s almost impossible to comprehend the sheer number of wild animals on Earth — and why that makes their potential suffering so important to consider.</li><li>How bad experiences like disease, parasites, and predation truly are for wild animals — and how we would even begin to study that empirically.</li><li>The tricky ethical dilemmas in trying to help wild animals without unintended consequences for ecosystems or other potentially sentient beings.</li><li>Potentially promising interventions to help wild animals — like selective reforestation, vaccines, fire management, and gene drives.</li><li>Why Cameron thinks the best approach to improving wild animal welfare is to first build a dedicated research field — and how Wild Animal Initiative’s activities support this.</li><li>The many career paths in science, policy, and technology that could contribute to improving wild animal welfare.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:03:40)</li><li>One concrete example of how we might improve wild animal welfare (00:04:04)</li><li>Why should we care about wild animal suffering? (00:10:00)</li><li>What’s it like to be a wild animal? (00:19:37)</li><li>Suffering and death in the wild (00:29:19)</li><li>Positive, benign, and social experiences (00:51:33)</li><li>Indicators of welfare (01:01:40)</li><li>Can we even help wild animals without unintended consequences? (01:13:20)</li><li>Vaccines for wild animals (01:30:59)</li><li>Fire management (01:44:20)</li><li>Gene drive technologies (01:47:42)</li><li>Common objections and misconceptions about wild animal welfare (01:53:19)</li><li>Future promising interventions (02:21:58)</li><li>What’s the long game for wild animal welfare? (02:27:46)</li><li>Eliminating the biological basis for suffering (02:33:21)</li><li>Optimising for high-welfare landscapes (02:37:33)</li><li>Wild Animal Initiative’s work (02:44:11)</li><li>Careers in wild animal welfare (02:58:13)</li><li>Work-related guilt and shame (03:12:57)</li><li>Luisa's outro (03:19:51)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 29 Nov 2024 22:45:59 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d22c5bcb/ccde1e4e.mp3" length="193088388" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/YomfKET8evD03WAFI4kpUJ_6aDb9tCAPSab3BrRKMDk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mZDc2/NmQ0MzA4Zjk1Mjk5/ZjI3ZTk0YWQxOWRl/M2IwZi5qcGVn.jpg"/>
      <itunes:duration>12063</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"I really don’t want to give the impression that I think it is easy to make predictable, controlled, safe interventions in wild systems where there are many species interacting. I don’t think it’s easy, but I don’t see any reason to think that it’s impossible. And I think we have been making progress. I think there’s every reason to think that if we continue doing research, both at the theoretical level — How do ecosystems work? What sorts of things are likely to have what sorts of indirect effects? — and then also at the practical level — Is this intervention a good idea? — I really think we’re going to come up with plenty of things that would be helpful to plenty of animals." —Cameron Meyer Shorb</p><p>In today’s episode, host Luisa Rodriguez speaks to Cameron Meyer Shorb — executive director of the <a href="https://www.wildanimalinitiative.org/">Wild Animal Initiative</a> — about the cutting-edge research on wild animal welfare.</p><p><a href="https://80k.info/cms"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How it’s almost impossible to comprehend the sheer number of wild animals on Earth — and why that makes their potential suffering so important to consider.</li><li>How bad experiences like disease, parasites, and predation truly are for wild animals — and how we would even begin to study that empirically.</li><li>The tricky ethical dilemmas in trying to help wild animals without unintended consequences for ecosystems or other potentially sentient beings.</li><li>Potentially promising interventions to help wild animals — like selective reforestation, vaccines, fire management, and gene drives.</li><li>Why Cameron thinks the best approach to improving wild animal welfare is to first build a dedicated research field — and how Wild Animal Initiative’s activities support this.</li><li>The many career paths in science, policy, and technology that could contribute to improving wild animal welfare.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:03:40)</li><li>One concrete example of how we might improve wild animal welfare (00:04:04)</li><li>Why should we care about wild animal suffering? (00:10:00)</li><li>What’s it like to be a wild animal? (00:19:37)</li><li>Suffering and death in the wild (00:29:19)</li><li>Positive, benign, and social experiences (00:51:33)</li><li>Indicators of welfare (01:01:40)</li><li>Can we even help wild animals without unintended consequences? (01:13:20)</li><li>Vaccines for wild animals (01:30:59)</li><li>Fire management (01:44:20)</li><li>Gene drive technologies (01:47:42)</li><li>Common objections and misconceptions about wild animal welfare (01:53:19)</li><li>Future promising interventions (02:21:58)</li><li>What’s the long game for wild animal welfare? (02:27:46)</li><li>Eliminating the biological basis for suffering (02:33:21)</li><li>Optimising for high-welfare landscapes (02:37:33)</li><li>Wild Animal Initiative’s work (02:44:11)</li><li>Careers in wild animal welfare (02:58:13)</li><li>Work-related guilt and shame (03:12:57)</li><li>Luisa's outro (03:19:51)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/d22c5bcb/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/d22c5bcb/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#209 – Rose Chan Loui on OpenAI’s gambit to ditch its nonprofit</title>
      <itunes:title>#209 – Rose Chan Loui on OpenAI’s gambit to ditch its nonprofit</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2368949a-e31c-474b-aab3-8f5a2f6116a4</guid>
      <link>https://80000hours.org/podcast/episodes/rose-chan-loui-openai-breaking-free-nonprofit/?utm_campaign=podcast__rose-chan-loui&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>One OpenAI critic <a href="https://www.cognitiverevolution.ai/zvis-pov-ilyas-ssi-openais-o1-claude-computer-use-trumps-election-and-more/">calls</a> it “the theft of at least the millennium and quite possibly all of human history.” Are they right?</p><p>Back in 2015 OpenAI was but a humble nonprofit. That nonprofit started a for-profit, OpenAI LLC, but made sure to retain ownership and control. But that for-profit, having become a tech giant with vast staffing and investment, has grown tired of its shackles and wants to change the deal.</p><p>Facing off against it stand eight out-gunned and out-numbered part-time volunteers. Can they hope to defend the nonprofit’s interests against the overwhelming profit motives arrayed against them?</p><p>That’s the question host Rob Wiblin puts to nonprofit legal expert Rose Chan Loui of UCLA, who concludes that with a “heroic effort” and a little help from some friendly state attorneys general, they might just stand a chance.</p><p><a href="https://80k.info/rcl"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>As Rose lays out, on paper OpenAI is controlled by a nonprofit board that:</p><ul><li>Can fire the CEO.</li><li>Would receive all the profits after the point OpenAI makes 100x returns on investment.</li><li>Is legally bound to do whatever it can to pursue its charitable purpose: “to build artificial general intelligence that benefits humanity.”</li></ul><p>But that control is a problem for OpenAI the for-profit and its CEO Sam Altman — all the more so after the board concluded back in November 2023 that it couldn’t trust Altman and attempted to fire him (although those board members were ultimately ousted themselves after failing to adequately explain their rationale).</p><p>Nonprofit control makes it harder to attract investors, who don’t want a board stepping in just because they think what the company is doing is bad for humanity. And OpenAI the business is thirsty for as many investors as possible, because it wants to beat competitors and train the first truly general AI — able to do every job humans currently do — which is expected to cost <em>hundreds of billions</em> of dollars.</p><p>So, Rose explains, they plan to buy the nonprofit out. In exchange for giving up its windfall profits and the ability to fire the CEO or direct the company’s actions, the board will become minority shareholders with reduced voting rights, and presumably transform into a normal grantmaking foundation instead.</p><p>Is this a massive bait-and-switch? A case of the tail not only wagging the dog, but grabbing a scalpel and neutering it?</p><p>OpenAI repeatedly committed to California, Delaware, the US federal government, founding staff, and the general public that its resources would be used for its charitable mission and it could be trusted because of nonprofit control. Meanwhile, the divergence in interests couldn’t be more stark: every dollar the for-profit keeps from its nonprofit parent is another dollar it could invest in AGI and ultimately return to investors and staff.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>What's coming up (00:00:50)</li><li>Who is Rose Chan Loui? (00:03:11)</li><li>How OpenAI carefully chose a complex nonprofit structure (00:04:17)</li><li>OpenAI's new plan to become a for-profit (00:11:47)</li><li>The nonprofit board is out-resourced and in a tough spot (00:14:38)</li><li>Who could be cheated in a bad conversion to a for-profit? (00:17:11)</li><li>Is this a unique case? (00:27:24)</li><li>Is control of OpenAI 'priceless' to the nonprofit in pursuit of its mission? (00:28:58)</li><li>The crazy difficulty of valuing the profits OpenAI might make (00:35:21)</li><li>Control of OpenAI is independently incredibly valuable and requires compensation (00:41:22)</li><li>It's very important the nonprofit get cash and not just equity (and few are talking about it) (00:51:37)</li><li>Is it a farce to call this an "arm's-length transaction"? (01:03:50)</li><li>How the nonprofit board can best play their hand (01:09:04)</li><li>Who can mount a court challenge and how that would work (01:15:41)</li><li>Rob's outro (01:21:25)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>One OpenAI critic <a href="https://www.cognitiverevolution.ai/zvis-pov-ilyas-ssi-openais-o1-claude-computer-use-trumps-election-and-more/">calls</a> it “the theft of at least the millennium and quite possibly all of human history.” Are they right?</p><p>Back in 2015 OpenAI was but a humble nonprofit. That nonprofit started a for-profit, OpenAI LLC, but made sure to retain ownership and control. But that for-profit, having become a tech giant with vast staffing and investment, has grown tired of its shackles and wants to change the deal.</p><p>Facing off against it stand eight out-gunned and out-numbered part-time volunteers. Can they hope to defend the nonprofit’s interests against the overwhelming profit motives arrayed against them?</p><p>That’s the question host Rob Wiblin puts to nonprofit legal expert Rose Chan Loui of UCLA, who concludes that with a “heroic effort” and a little help from some friendly state attorneys general, they might just stand a chance.</p><p><a href="https://80k.info/rcl"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>As Rose lays out, on paper OpenAI is controlled by a nonprofit board that:</p><ul><li>Can fire the CEO.</li><li>Would receive all the profits after the point OpenAI makes 100x returns on investment.</li><li>Is legally bound to do whatever it can to pursue its charitable purpose: “to build artificial general intelligence that benefits humanity.”</li></ul><p>But that control is a problem for OpenAI the for-profit and its CEO Sam Altman — all the more so after the board concluded back in November 2023 that it couldn’t trust Altman and attempted to fire him (although those board members were ultimately ousted themselves after failing to adequately explain their rationale).</p><p>Nonprofit control makes it harder to attract investors, who don’t want a board stepping in just because they think what the company is doing is bad for humanity. And OpenAI the business is thirsty for as many investors as possible, because it wants to beat competitors and train the first truly general AI — able to do every job humans currently do — which is expected to cost <em>hundreds of billions</em> of dollars.</p><p>So, Rose explains, they plan to buy the nonprofit out. In exchange for giving up its windfall profits and the ability to fire the CEO or direct the company’s actions, the board will become minority shareholders with reduced voting rights, and presumably transform into a normal grantmaking foundation instead.</p><p>Is this a massive bait-and-switch? A case of the tail not only wagging the dog, but grabbing a scalpel and neutering it?</p><p>OpenAI repeatedly committed to California, Delaware, the US federal government, founding staff, and the general public that its resources would be used for its charitable mission and it could be trusted because of nonprofit control. Meanwhile, the divergence in interests couldn’t be more stark: every dollar the for-profit keeps from its nonprofit parent is another dollar it could invest in AGI and ultimately return to investors and staff.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>What's coming up (00:00:50)</li><li>Who is Rose Chan Loui? (00:03:11)</li><li>How OpenAI carefully chose a complex nonprofit structure (00:04:17)</li><li>OpenAI's new plan to become a for-profit (00:11:47)</li><li>The nonprofit board is out-resourced and in a tough spot (00:14:38)</li><li>Who could be cheated in a bad conversion to a for-profit? (00:17:11)</li><li>Is this a unique case? (00:27:24)</li><li>Is control of OpenAI 'priceless' to the nonprofit in pursuit of its mission? (00:28:58)</li><li>The crazy difficulty of valuing the profits OpenAI might make (00:35:21)</li><li>Control of OpenAI is independently incredibly valuable and requires compensation (00:41:22)</li><li>It's very important the nonprofit get cash and not just equity (and few are talking about it) (00:51:37)</li><li>Is it a farce to call this an "arm's-length transaction"? (01:03:50)</li><li>How the nonprofit board can best play their hand (01:09:04)</li><li>Who can mount a court challenge and how that would work (01:15:41)</li><li>Rob's outro (01:21:25)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 27 Nov 2024 18:10:26 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e98d0281/c0ce5f7f.mp3" length="78917530" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/bO6ovX_XogGvBRXm-AFRwBmrkqMBtgOtIojmgSmj7uY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mMDBi/MWJjODQxNTQ2Nzkx/NDliODUxZGZjZDQx/ZDI2ZS53ZWJw.jpg"/>
      <itunes:duration>4928</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>One OpenAI critic <a href="https://www.cognitiverevolution.ai/zvis-pov-ilyas-ssi-openais-o1-claude-computer-use-trumps-election-and-more/">calls</a> it “the theft of at least the millennium and quite possibly all of human history.” Are they right?</p><p>Back in 2015 OpenAI was but a humble nonprofit. That nonprofit started a for-profit, OpenAI LLC, but made sure to retain ownership and control. But that for-profit, having become a tech giant with vast staffing and investment, has grown tired of its shackles and wants to change the deal.</p><p>Facing off against it stand eight out-gunned and out-numbered part-time volunteers. Can they hope to defend the nonprofit’s interests against the overwhelming profit motives arrayed against them?</p><p>That’s the question host Rob Wiblin puts to nonprofit legal expert Rose Chan Loui of UCLA, who concludes that with a “heroic effort” and a little help from some friendly state attorneys general, they might just stand a chance.</p><p><a href="https://80k.info/rcl"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>As Rose lays out, on paper OpenAI is controlled by a nonprofit board that:</p><ul><li>Can fire the CEO.</li><li>Would receive all the profits after the point OpenAI makes 100x returns on investment.</li><li>Is legally bound to do whatever it can to pursue its charitable purpose: “to build artificial general intelligence that benefits humanity.”</li></ul><p>But that control is a problem for OpenAI the for-profit and its CEO Sam Altman — all the more so after the board concluded back in November 2023 that it couldn’t trust Altman and attempted to fire him (although those board members were ultimately ousted themselves after failing to adequately explain their rationale).</p><p>Nonprofit control makes it harder to attract investors, who don’t want a board stepping in just because they think what the company is doing is bad for humanity. And OpenAI the business is thirsty for as many investors as possible, because it wants to beat competitors and train the first truly general AI — able to do every job humans currently do — which is expected to cost <em>hundreds of billions</em> of dollars.</p><p>So, Rose explains, they plan to buy the nonprofit out. In exchange for giving up its windfall profits and the ability to fire the CEO or direct the company’s actions, the board will become minority shareholders with reduced voting rights, and presumably transform into a normal grantmaking foundation instead.</p><p>Is this a massive bait-and-switch? A case of the tail not only wagging the dog, but grabbing a scalpel and neutering it?</p><p>OpenAI repeatedly committed to California, Delaware, the US federal government, founding staff, and the general public that its resources would be used for its charitable mission and it could be trusted because of nonprofit control. Meanwhile, the divergence in interests couldn’t be more stark: every dollar the for-profit keeps from its nonprofit parent is another dollar it could invest in AGI and ultimately return to investors and staff.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>What's coming up (00:00:50)</li><li>Who is Rose Chan Loui? (00:03:11)</li><li>How OpenAI carefully chose a complex nonprofit structure (00:04:17)</li><li>OpenAI's new plan to become a for-profit (00:11:47)</li><li>The nonprofit board is out-resourced and in a tough spot (00:14:38)</li><li>Who could be cheated in a bad conversion to a for-profit? (00:17:11)</li><li>Is this a unique case? (00:27:24)</li><li>Is control of OpenAI 'priceless' to the nonprofit in pursuit of its mission? (00:28:58)</li><li>The crazy difficulty of valuing the profits OpenAI might make (00:35:21)</li><li>Control of OpenAI is independently incredibly valuable and requires compensation (00:41:22)</li><li>It's very important the nonprofit get cash and not just equity (and few are talking about it) (00:51:37)</li><li>Is it a farce to call this an "arm's-length transaction"? (01:03:50)</li><li>How the nonprofit board can best play their hand (01:09:04)</li><li>Who can mount a court challenge and how that would work (01:15:41)</li><li>Rob's outro (01:21:25)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/e98d0281/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/e98d0281/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#208 – Elizabeth Cox on the case that TV shows, movies, and novels can improve the world</title>
      <itunes:title>#208 – Elizabeth Cox on the case that TV shows, movies, and novels can improve the world</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">04a18bd8-031c-48b5-ad09-a5fdff54e4c2</guid>
      <link>https://80000hours.org/podcast/episodes/elizabeth-cox-tv-movies-novels-change-the-world/?utm_campaign=podcast__elizabeth-cox&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"I think stories are the way we shift the Overton window — so widen the range of things that are acceptable for policy and palatable to the public. Almost by definition, a lot of things that are going to be really important and shape the future are not in the Overton window, because they sound weird and off-putting and very futuristic. But I think stories are the best way to bring them in." — Elizabeth Cox</p><p>In today’s episode, Keiran Harris speaks with Elizabeth Cox — founder of the independent production company <a href="https://www.shouldwestudio.com/">Should We Studio</a> — about the case that storytelling can improve the world.</p><p><a href="https://80k.info/ec"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How TV shows and movies compare to novels, short stories, and creative nonfiction if you’re trying to do good.</li><li>The existing empirical evidence for the impact of storytelling.</li><li>Their competing takes on the merits of thinking carefully about target audiences.</li><li>Whether stories can really change minds on deeply entrenched issues, or whether writers need to have more modest goals.</li><li>Whether humans will stay relevant as creative writers with the rise of powerful AI models.</li><li>Whether you can do more good with an overtly educational show vs other approaches.</li><li>Elizabeth’s experience with making her new five-part animated show <a href="https://www.youtube.com/playlist?list=PLJicmE8fK0Egu9kfkc8Dz_H_yKONuMHvu"><strong><em>Ada</em></strong></a><em> </em>— including why she chose the topics of civilisational collapse, kidney donations, artificial wombs, AI, and gene drives.</li><li>The pros and cons of animation as a medium.</li><li>Career advice for creative writers.</li><li>Keiran’s idea for a longtermist Christmas movie.</li><li>And plenty more.</li></ul><p><a href="https://www.youtube.com/playlist?list=PLJicmE8fK0Egu9kfkc8Dz_H_yKONuMHvu"><strong>Check out </strong><strong><em>Ada </em></strong><strong>on YouTube! </strong></a></p><p>Material you might want to check out before listening:</p><ul><li>The <a href="https://vimeo.com/904701875/643f601051?share=copy">trailer</a> for Elizabeth’s new animated series<em> </em><a href="https://www.shouldwestudio.com/ada"><em>Ada</em></a> — the full series will be available on <a href="https://www.youtube.com/teded/videos">TED-Ed’s YouTube channel</a> in early January 2025</li><li>Keiran’s <a href="https://drive.google.com/file/d/1neaUcFA4FG_2jDKakAWZX11C4hP8JKk8/view">pilot script</a> and a <a href="https://drive.google.com/file/d/1w2rPvqY3MTDLTUShVlRuYAYGEQDsMmlJ/view">10-episode outline</a> for his show <em>Bequest</em>, and his <a href="https://forum.effectivealtruism.org/posts/HjKpghhowBRLat4Hq/bequest-an-ea-ish-tv-show-that-didn-t-make-it">post about the show on the Effective Altruism Forum</a></li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:02:52)</li><li>Is storytelling really a high-impact career option? (00:03:26)</li><li>Empirical evidence of the impact of storytelling (00:06:51)</li><li>How storytelling can inform us (00:16:25)</li><li>How long will humans stay relevant as creative writers? (00:21:54)</li><li>Ada (00:33:05)</li><li>Debating the merits of thinking about target audiences (00:38:03)</li><li>Ada vs other approaches to impact-focused storytelling (00:48:18)</li><li>Why animation (01:01:06)</li><li>One Billion Christmases (01:04:54)</li><li>How storytelling can humanise (01:09:34)</li><li>But can storytelling actually change strongly held opinions? (01:13:26)</li><li>Novels and short stories (01:18:38)</li><li>Creative nonfiction (01:25:06)</li><li>Other promising ways of storytelling (01:30:53)</li><li>How did Ada actually get made? (01:33:23)</li><li>The hardest part of the process for Elizabeth (01:48:28)</li><li>Elizabeth’s hopes and dreams for Ada (01:53:10)</li><li>Designing Ada with an eye toward impact (01:59:16)</li><li>Alternative topics for Ada (02:05:33)</li><li>Deciding on the best way to get Ada in front of people (02:07:12)</li><li>Career advice for creative writers (02:11:31)</li><li>Wikipedia book spoilers (02:17:05)</li><li>Luisa's outro (02:20:42)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"I think stories are the way we shift the Overton window — so widen the range of things that are acceptable for policy and palatable to the public. Almost by definition, a lot of things that are going to be really important and shape the future are not in the Overton window, because they sound weird and off-putting and very futuristic. But I think stories are the best way to bring them in." — Elizabeth Cox</p><p>In today’s episode, Keiran Harris speaks with Elizabeth Cox — founder of the independent production company <a href="https://www.shouldwestudio.com/">Should We Studio</a> — about the case that storytelling can improve the world.</p><p><a href="https://80k.info/ec"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How TV shows and movies compare to novels, short stories, and creative nonfiction if you’re trying to do good.</li><li>The existing empirical evidence for the impact of storytelling.</li><li>Their competing takes on the merits of thinking carefully about target audiences.</li><li>Whether stories can really change minds on deeply entrenched issues, or whether writers need to have more modest goals.</li><li>Whether humans will stay relevant as creative writers with the rise of powerful AI models.</li><li>Whether you can do more good with an overtly educational show vs other approaches.</li><li>Elizabeth’s experience with making her new five-part animated show <a href="https://www.youtube.com/playlist?list=PLJicmE8fK0Egu9kfkc8Dz_H_yKONuMHvu"><strong><em>Ada</em></strong></a><em> </em>— including why she chose the topics of civilisational collapse, kidney donations, artificial wombs, AI, and gene drives.</li><li>The pros and cons of animation as a medium.</li><li>Career advice for creative writers.</li><li>Keiran’s idea for a longtermist Christmas movie.</li><li>And plenty more.</li></ul><p><a href="https://www.youtube.com/playlist?list=PLJicmE8fK0Egu9kfkc8Dz_H_yKONuMHvu"><strong>Check out </strong><strong><em>Ada </em></strong><strong>on YouTube! </strong></a></p><p>Material you might want to check out before listening:</p><ul><li>The <a href="https://vimeo.com/904701875/643f601051?share=copy">trailer</a> for Elizabeth’s new animated series<em> </em><a href="https://www.shouldwestudio.com/ada"><em>Ada</em></a> — the full series will be available on <a href="https://www.youtube.com/teded/videos">TED-Ed’s YouTube channel</a> in early January 2025</li><li>Keiran’s <a href="https://drive.google.com/file/d/1neaUcFA4FG_2jDKakAWZX11C4hP8JKk8/view">pilot script</a> and a <a href="https://drive.google.com/file/d/1w2rPvqY3MTDLTUShVlRuYAYGEQDsMmlJ/view">10-episode outline</a> for his show <em>Bequest</em>, and his <a href="https://forum.effectivealtruism.org/posts/HjKpghhowBRLat4Hq/bequest-an-ea-ish-tv-show-that-didn-t-make-it">post about the show on the Effective Altruism Forum</a></li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:02:52)</li><li>Is storytelling really a high-impact career option? (00:03:26)</li><li>Empirical evidence of the impact of storytelling (00:06:51)</li><li>How storytelling can inform us (00:16:25)</li><li>How long will humans stay relevant as creative writers? (00:21:54)</li><li>Ada (00:33:05)</li><li>Debating the merits of thinking about target audiences (00:38:03)</li><li>Ada vs other approaches to impact-focused storytelling (00:48:18)</li><li>Why animation (01:01:06)</li><li>One Billion Christmases (01:04:54)</li><li>How storytelling can humanise (01:09:34)</li><li>But can storytelling actually change strongly held opinions? (01:13:26)</li><li>Novels and short stories (01:18:38)</li><li>Creative nonfiction (01:25:06)</li><li>Other promising ways of storytelling (01:30:53)</li><li>How did Ada actually get made? (01:33:23)</li><li>The hardest part of the process for Elizabeth (01:48:28)</li><li>Elizabeth’s hopes and dreams for Ada (01:53:10)</li><li>Designing Ada with an eye toward impact (01:59:16)</li><li>Alternative topics for Ada (02:05:33)</li><li>Deciding on the best way to get Ada in front of people (02:07:12)</li><li>Career advice for creative writers (02:11:31)</li><li>Wikipedia book spoilers (02:17:05)</li><li>Luisa's outro (02:20:42)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 21 Nov 2024 21:14:24 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8d413de2/24a5fd57.mp3" length="136424350" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/qxpjYn_BZF9eB81uRT8k46FZXWkW-K_E40S4m4vbXYY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS83NTE0/YmQ5MDY2MThkM2Vh/ODIwZmNiODM2MDJh/NTllOC5qcGVn.jpg"/>
      <itunes:duration>8523</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"I think stories are the way we shift the Overton window — so widen the range of things that are acceptable for policy and palatable to the public. Almost by definition, a lot of things that are going to be really important and shape the future are not in the Overton window, because they sound weird and off-putting and very futuristic. But I think stories are the best way to bring them in." — Elizabeth Cox</p><p>In today’s episode, Keiran Harris speaks with Elizabeth Cox — founder of the independent production company <a href="https://www.shouldwestudio.com/">Should We Studio</a> — about the case that storytelling can improve the world.</p><p><a href="https://80k.info/ec"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How TV shows and movies compare to novels, short stories, and creative nonfiction if you’re trying to do good.</li><li>The existing empirical evidence for the impact of storytelling.</li><li>Their competing takes on the merits of thinking carefully about target audiences.</li><li>Whether stories can really change minds on deeply entrenched issues, or whether writers need to have more modest goals.</li><li>Whether humans will stay relevant as creative writers with the rise of powerful AI models.</li><li>Whether you can do more good with an overtly educational show vs other approaches.</li><li>Elizabeth’s experience with making her new five-part animated show <a href="https://www.youtube.com/playlist?list=PLJicmE8fK0Egu9kfkc8Dz_H_yKONuMHvu"><strong><em>Ada</em></strong></a><em> </em>— including why she chose the topics of civilisational collapse, kidney donations, artificial wombs, AI, and gene drives.</li><li>The pros and cons of animation as a medium.</li><li>Career advice for creative writers.</li><li>Keiran’s idea for a longtermist Christmas movie.</li><li>And plenty more.</li></ul><p><a href="https://www.youtube.com/playlist?list=PLJicmE8fK0Egu9kfkc8Dz_H_yKONuMHvu"><strong>Check out </strong><strong><em>Ada </em></strong><strong>on YouTube! </strong></a></p><p>Material you might want to check out before listening:</p><ul><li>The <a href="https://vimeo.com/904701875/643f601051?share=copy">trailer</a> for Elizabeth’s new animated series<em> </em><a href="https://www.shouldwestudio.com/ada"><em>Ada</em></a> — the full series will be available on <a href="https://www.youtube.com/teded/videos">TED-Ed’s YouTube channel</a> in early January 2025</li><li>Keiran’s <a href="https://drive.google.com/file/d/1neaUcFA4FG_2jDKakAWZX11C4hP8JKk8/view">pilot script</a> and a <a href="https://drive.google.com/file/d/1w2rPvqY3MTDLTUShVlRuYAYGEQDsMmlJ/view">10-episode outline</a> for his show <em>Bequest</em>, and his <a href="https://forum.effectivealtruism.org/posts/HjKpghhowBRLat4Hq/bequest-an-ea-ish-tv-show-that-didn-t-make-it">post about the show on the Effective Altruism Forum</a></li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:02:52)</li><li>Is storytelling really a high-impact career option? (00:03:26)</li><li>Empirical evidence of the impact of storytelling (00:06:51)</li><li>How storytelling can inform us (00:16:25)</li><li>How long will humans stay relevant as creative writers? (00:21:54)</li><li>Ada (00:33:05)</li><li>Debating the merits of thinking about target audiences (00:38:03)</li><li>Ada vs other approaches to impact-focused storytelling (00:48:18)</li><li>Why animation (01:01:06)</li><li>One Billion Christmases (01:04:54)</li><li>How storytelling can humanise (01:09:34)</li><li>But can storytelling actually change strongly held opinions? (01:13:26)</li><li>Novels and short stories (01:18:38)</li><li>Creative nonfiction (01:25:06)</li><li>Other promising ways of storytelling (01:30:53)</li><li>How did Ada actually get made? (01:33:23)</li><li>The hardest part of the process for Elizabeth (01:48:28)</li><li>Elizabeth’s hopes and dreams for Ada (01:53:10)</li><li>Designing Ada with an eye toward impact (01:59:16)</li><li>Alternative topics for Ada (02:05:33)</li><li>Deciding on the best way to get Ada in front of people (02:07:12)</li><li>Career advice for creative writers (02:11:31)</li><li>Wikipedia book spoilers (02:17:05)</li><li>Luisa's outro (02:20:42)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/8d413de2/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/8d413de2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#207 – Sarah Eustis-Guthrie on why she shut down her charity, and why more founders should follow her lead</title>
      <itunes:title>#207 – Sarah Eustis-Guthrie on why she shut down her charity, and why more founders should follow her lead</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b2ef7ef7-661b-4413-9a71-4e82c1823021</guid>
      <link>https://80000hours.org/podcast/episodes/sarah-eustis-guthrie-founding-shutting-down-charity/?utm_campaign=podcast__sarah-eustis-guthrie&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"I think one of the reasons I took [shutting down my charity] so hard is because entrepreneurship is all about this bets-based mindset. So you say, “I’m going to take a bunch of bets. I’m going to take some risky bets that have really high upside.” And this is a winning strategy in life, but maybe it’s not a winning strategy for any given hand. So the fact of the matter is that I believe that intellectually, but l do not believe that emotionally. And I have now met a bunch of people who are really good at doing that emotionally, and I’ve realised I’m just not one of those people. I think I’m more entrepreneurial than your average person; I don’t think I’m the maximally entrepreneurial person. And I also think it’s just human nature to not like failing." —Sarah Eustis-Guthrie</p><p>In today’s episode, host Luisa Rodriguez speaks to Sarah Eustis-Guthrie — cofounder of the now-shut-down <a href="https://maternalhealthinitiative.org/">Maternal Health Initiative</a>, a postpartum family planning nonprofit in Ghana — about her experience starting and running MHI, and ultimately making the difficult decision to shut down when the programme wasn’t as impactful as they expected.</p><p><a href="https://80k.info/seg"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The evidence that made Sarah and her cofounder Ben think their organisation could be super impactful for women — both from a health perspective and an autonomy and wellbeing perspective.</li><li>Early yellow and red flags that maybe they didn’t have the full story about the effectiveness of the intervention.</li><li>All the steps Sarah and Ben took to build the organisation — and where things went wrong in retrospect.</li><li>Dealing with the emotional side of putting so much time and effort into a project that ultimately failed.</li><li>Why it’s so important to talk openly about things that don’t work out, and Sarah’s key lessons learned from the experience.</li><li>The misaligned incentives that discourage charities from shutting down ineffective programmes.</li><li>The movement of trust-based philanthropy, and Sarah’s ideas to further improve how global development charities get their funding and prioritise their beneficiaries over their operations.</li><li>The pros and cons of exploring and pivoting in careers.</li><li>What it’s like to participate in the <a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship Incubation Program</a>, and how listeners can assess if they might be a good fit.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:58)</li><li>The interview begins (00:03:43)</li><li>The case for postpartum family planning as an impactful intervention (00:05:37)</li><li>Deciding where to start the charity (00:11:34)</li><li>How do you even start implementing a charity programme? (00:18:33)</li><li>Early yellow and red flags (00:22:56)</li><li>Proof-of-concept tests and pilot programme in Ghana (00:34:10)</li><li>Dealing with disappointing pilot results (00:53:34)</li><li>The ups and downs of founding an organisation (01:01:09)</li><li>Post-pilot research and reflection (01:05:40)</li><li>Is family planning still a promising intervention? (01:22:59)</li><li>Deciding to shut down MHI (01:34:10)</li><li>The surprising community response to news of the shutdown (01:41:12)</li><li>Mistakes and what Sarah could have done differently (01:48:54)</li><li>Sharing results in the space of postpartum family planning (02:00:54)</li><li>Should more charities scale back or shut down? (02:08:33)</li><li>Trust-based philanthropy (02:11:15)</li><li>Empowering the beneficiaries of charities’ work (02:18:04)</li><li>The tough ask of getting nonprofits to act when a programme isn’t working (02:21:18)</li><li>Exploring and pivoting in careers (02:27:01)</li><li>Reevaluation points (02:29:55)</li><li>PlayPumps were even worse than you might’ve heard (02:33:25)</li><li>Charity Entrepreneurship (02:38:30)</li><li>The mistake of counting yourself out too early (02:52:37)</li><li>Luisa’s outro (02:57:50)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"I think one of the reasons I took [shutting down my charity] so hard is because entrepreneurship is all about this bets-based mindset. So you say, “I’m going to take a bunch of bets. I’m going to take some risky bets that have really high upside.” And this is a winning strategy in life, but maybe it’s not a winning strategy for any given hand. So the fact of the matter is that I believe that intellectually, but l do not believe that emotionally. And I have now met a bunch of people who are really good at doing that emotionally, and I’ve realised I’m just not one of those people. I think I’m more entrepreneurial than your average person; I don’t think I’m the maximally entrepreneurial person. And I also think it’s just human nature to not like failing." —Sarah Eustis-Guthrie</p><p>In today’s episode, host Luisa Rodriguez speaks to Sarah Eustis-Guthrie — cofounder of the now-shut-down <a href="https://maternalhealthinitiative.org/">Maternal Health Initiative</a>, a postpartum family planning nonprofit in Ghana — about her experience starting and running MHI, and ultimately making the difficult decision to shut down when the programme wasn’t as impactful as they expected.</p><p><a href="https://80k.info/seg"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The evidence that made Sarah and her cofounder Ben think their organisation could be super impactful for women — both from a health perspective and an autonomy and wellbeing perspective.</li><li>Early yellow and red flags that maybe they didn’t have the full story about the effectiveness of the intervention.</li><li>All the steps Sarah and Ben took to build the organisation — and where things went wrong in retrospect.</li><li>Dealing with the emotional side of putting so much time and effort into a project that ultimately failed.</li><li>Why it’s so important to talk openly about things that don’t work out, and Sarah’s key lessons learned from the experience.</li><li>The misaligned incentives that discourage charities from shutting down ineffective programmes.</li><li>The movement of trust-based philanthropy, and Sarah’s ideas to further improve how global development charities get their funding and prioritise their beneficiaries over their operations.</li><li>The pros and cons of exploring and pivoting in careers.</li><li>What it’s like to participate in the <a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship Incubation Program</a>, and how listeners can assess if they might be a good fit.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:58)</li><li>The interview begins (00:03:43)</li><li>The case for postpartum family planning as an impactful intervention (00:05:37)</li><li>Deciding where to start the charity (00:11:34)</li><li>How do you even start implementing a charity programme? (00:18:33)</li><li>Early yellow and red flags (00:22:56)</li><li>Proof-of-concept tests and pilot programme in Ghana (00:34:10)</li><li>Dealing with disappointing pilot results (00:53:34)</li><li>The ups and downs of founding an organisation (01:01:09)</li><li>Post-pilot research and reflection (01:05:40)</li><li>Is family planning still a promising intervention? (01:22:59)</li><li>Deciding to shut down MHI (01:34:10)</li><li>The surprising community response to news of the shutdown (01:41:12)</li><li>Mistakes and what Sarah could have done differently (01:48:54)</li><li>Sharing results in the space of postpartum family planning (02:00:54)</li><li>Should more charities scale back or shut down? (02:08:33)</li><li>Trust-based philanthropy (02:11:15)</li><li>Empowering the beneficiaries of charities’ work (02:18:04)</li><li>The tough ask of getting nonprofits to act when a programme isn’t working (02:21:18)</li><li>Exploring and pivoting in careers (02:27:01)</li><li>Reevaluation points (02:29:55)</li><li>PlayPumps were even worse than you might’ve heard (02:33:25)</li><li>Charity Entrepreneurship (02:38:30)</li><li>The mistake of counting yourself out too early (02:52:37)</li><li>Luisa’s outro (02:57:50)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 14 Nov 2024 20:04:11 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6822366e/8e39d038.mp3" length="128671397" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Pxi8_l7eK3RGbNcnSOzB6RK_oiWyi20XOb79rK0C5ZI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS81NTBk/OTZlMjRhYTRmOGMw/YTUxMGQ3YWM5MTQ5/ODhhNy5qcGc.jpg"/>
      <itunes:duration>10719</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"I think one of the reasons I took [shutting down my charity] so hard is because entrepreneurship is all about this bets-based mindset. So you say, “I’m going to take a bunch of bets. I’m going to take some risky bets that have really high upside.” And this is a winning strategy in life, but maybe it’s not a winning strategy for any given hand. So the fact of the matter is that I believe that intellectually, but l do not believe that emotionally. And I have now met a bunch of people who are really good at doing that emotionally, and I’ve realised I’m just not one of those people. I think I’m more entrepreneurial than your average person; I don’t think I’m the maximally entrepreneurial person. And I also think it’s just human nature to not like failing." —Sarah Eustis-Guthrie</p><p>In today’s episode, host Luisa Rodriguez speaks to Sarah Eustis-Guthrie — cofounder of the now-shut-down <a href="https://maternalhealthinitiative.org/">Maternal Health Initiative</a>, a postpartum family planning nonprofit in Ghana — about her experience starting and running MHI, and ultimately making the difficult decision to shut down when the programme wasn’t as impactful as they expected.</p><p><a href="https://80k.info/seg"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The evidence that made Sarah and her cofounder Ben think their organisation could be super impactful for women — both from a health perspective and an autonomy and wellbeing perspective.</li><li>Early yellow and red flags that maybe they didn’t have the full story about the effectiveness of the intervention.</li><li>All the steps Sarah and Ben took to build the organisation — and where things went wrong in retrospect.</li><li>Dealing with the emotional side of putting so much time and effort into a project that ultimately failed.</li><li>Why it’s so important to talk openly about things that don’t work out, and Sarah’s key lessons learned from the experience.</li><li>The misaligned incentives that discourage charities from shutting down ineffective programmes.</li><li>The movement of trust-based philanthropy, and Sarah’s ideas to further improve how global development charities get their funding and prioritise their beneficiaries over their operations.</li><li>The pros and cons of exploring and pivoting in careers.</li><li>What it’s like to participate in the <a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship Incubation Program</a>, and how listeners can assess if they might be a good fit.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:58)</li><li>The interview begins (00:03:43)</li><li>The case for postpartum family planning as an impactful intervention (00:05:37)</li><li>Deciding where to start the charity (00:11:34)</li><li>How do you even start implementing a charity programme? (00:18:33)</li><li>Early yellow and red flags (00:22:56)</li><li>Proof-of-concept tests and pilot programme in Ghana (00:34:10)</li><li>Dealing with disappointing pilot results (00:53:34)</li><li>The ups and downs of founding an organisation (01:01:09)</li><li>Post-pilot research and reflection (01:05:40)</li><li>Is family planning still a promising intervention? (01:22:59)</li><li>Deciding to shut down MHI (01:34:10)</li><li>The surprising community response to news of the shutdown (01:41:12)</li><li>Mistakes and what Sarah could have done differently (01:48:54)</li><li>Sharing results in the space of postpartum family planning (02:00:54)</li><li>Should more charities scale back or shut down? (02:08:33)</li><li>Trust-based philanthropy (02:11:15)</li><li>Empowering the beneficiaries of charities’ work (02:18:04)</li><li>The tough ask of getting nonprofits to act when a programme isn’t working (02:21:18)</li><li>Exploring and pivoting in careers (02:27:01)</li><li>Reevaluation points (02:29:55)</li><li>PlayPumps were even worse than you might’ve heard (02:33:25)</li><li>Charity Entrepreneurship (02:38:30)</li><li>The mistake of counting yourself out too early (02:52:37)</li><li>Luisa’s outro (02:57:50)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/6822366e/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/6822366e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Parenting insights from Rob and 8 past guests</title>
      <itunes:title>Parenting insights from Rob and 8 past guests</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">04803f0c-9f6a-486d-9766-c89c896cbeab</guid>
      <link>https://80000hours.org/podcast/episodes/bonus-parenting-insights-compilation/?utm_campaign=podcast__parenting-compilation&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>With kids very much on the team's mind we thought it would be fun to review some comments about parenting featured on the show over the years, then have hosts Luisa Rodriguez and Rob Wiblin react to them. </p><p><a href="https://80k.info/ph"><strong>Links to learn more and full transcript.</strong></a></p><p>After hearing 8 former guests’ insights, Luisa and Rob chat about:</p><ul><li>Which of these resonate the most with Rob, now that he’s been a dad for six months (plus an update at nine months).</li><li>What have been the biggest surprises for Rob in becoming a parent.</li><li>How Rob's dealt with work and parenting tradeoffs, and his advice for other would-be parents.</li><li>Rob's list of <a href="https://docs.google.com/document/d/1-WINYMydv8v2POQ48noHvOmvwW-oaR2GjfIeiUt30YU/edit"><strong>recommended purchases for new or upcoming parents</strong></a>.</li></ul><p>This bonus episode includes excerpts from:</p><ul><li>Ezra Klein on parenting yourself as well as your children (from <a href="https://80000hours.org/podcast/episodes/ezra-klein-ai-and-dc/">episode #157</a>)</li><li>Holden Karnofsky on freezing embryos and being surprised by how fun it is to have a kid (<a href="https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/">#110</a> and <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">#158</a>)</li><li>Parenting expert Emily Oster on how having kids affect relationships, careers and kids, and what actually makes a difference in young kids’ lives (<a href="https://80000hours.org/podcast/episodes/emily-oster-pregnancy-parenting-careers/">#178</a>)</li><li>Russ Roberts on empirical research when deciding whether to have kids (<a href="https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/">#87</a>)</li><li>Spencer Greenberg on his surveys of parents (<a href="https://80000hours.org/podcast/episodes/spencer-greenberg-money-happiness-hype-value/">#183</a>)</li><li>Elie Hassenfeld on how having children reframes his relationship to solving pressing global problems (<a href="https://80000hours.org/podcast/episodes/elie-hassenfeld-givewell-critiques-and-lessons/">#153</a>)</li><li>Bryan Caplan on homeschooling (<a href="https://80000hours.org/podcast/episodes/bryan-caplan-stop-reading-the-news/">#172</a>)</li><li>Nita Farahany on thinking about life and the world differently with kids (<a href="https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/">#174</a>)</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob &amp; Luisa’s intro (00:00:19)</li><li>Ezra Klein on parenting yourself as well as your children (00:03:34)</li><li>Holden Karnofsky on preparing for a kid and freezing embryos (00:07:41)</li><li>Emily Oster on the impact of kids on relationships (00:09:22)</li><li>Russ Roberts on empirical research when deciding whether to have kids (00:14:44)</li><li>Spencer Greenberg on parent surveys (00:23:58)</li><li>Elie Hassenfeld on how having children reframes his relationship to solving pressing problems (00:27:40)</li><li>Emily Oster on careers and kids (00:31:44)</li><li>Holden Karnofsky on the experience of having kids (00:38:44)</li><li>Bryan Caplan on homeschooling (00:40:30)</li><li>Emily Oster on what actually makes a difference in young kids' lives (00:46:02)</li><li>Nita Farahany on thinking about life and the world differently (00:51:16)</li><li>Rob’s first impressions of parenthood (00:52:59)</li><li>How Rob has changed his views about parenthood (00:58:04)</li><li>Can the pros and cons of parenthood be studied? (01:01:49)</li><li>Do people have skewed impressions of what parenthood is like? (01:09:24)</li><li>Work and parenting tradeoffs (01:15:26)</li><li>Tough decisions about screen time (01:25:11)</li><li>Rob’s advice to future parents (01:30:04)</li><li>Coda: Rob’s updated experience at nine months (01:32:09)</li><li>Emily Oster on her amazing nanny (01:35:01)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore<br></em><br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>With kids very much on the team's mind we thought it would be fun to review some comments about parenting featured on the show over the years, then have hosts Luisa Rodriguez and Rob Wiblin react to them. </p><p><a href="https://80k.info/ph"><strong>Links to learn more and full transcript.</strong></a></p><p>After hearing 8 former guests’ insights, Luisa and Rob chat about:</p><ul><li>Which of these resonate the most with Rob, now that he’s been a dad for six months (plus an update at nine months).</li><li>What have been the biggest surprises for Rob in becoming a parent.</li><li>How Rob's dealt with work and parenting tradeoffs, and his advice for other would-be parents.</li><li>Rob's list of <a href="https://docs.google.com/document/d/1-WINYMydv8v2POQ48noHvOmvwW-oaR2GjfIeiUt30YU/edit"><strong>recommended purchases for new or upcoming parents</strong></a>.</li></ul><p>This bonus episode includes excerpts from:</p><ul><li>Ezra Klein on parenting yourself as well as your children (from <a href="https://80000hours.org/podcast/episodes/ezra-klein-ai-and-dc/">episode #157</a>)</li><li>Holden Karnofsky on freezing embryos and being surprised by how fun it is to have a kid (<a href="https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/">#110</a> and <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">#158</a>)</li><li>Parenting expert Emily Oster on how having kids affect relationships, careers and kids, and what actually makes a difference in young kids’ lives (<a href="https://80000hours.org/podcast/episodes/emily-oster-pregnancy-parenting-careers/">#178</a>)</li><li>Russ Roberts on empirical research when deciding whether to have kids (<a href="https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/">#87</a>)</li><li>Spencer Greenberg on his surveys of parents (<a href="https://80000hours.org/podcast/episodes/spencer-greenberg-money-happiness-hype-value/">#183</a>)</li><li>Elie Hassenfeld on how having children reframes his relationship to solving pressing global problems (<a href="https://80000hours.org/podcast/episodes/elie-hassenfeld-givewell-critiques-and-lessons/">#153</a>)</li><li>Bryan Caplan on homeschooling (<a href="https://80000hours.org/podcast/episodes/bryan-caplan-stop-reading-the-news/">#172</a>)</li><li>Nita Farahany on thinking about life and the world differently with kids (<a href="https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/">#174</a>)</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob &amp; Luisa’s intro (00:00:19)</li><li>Ezra Klein on parenting yourself as well as your children (00:03:34)</li><li>Holden Karnofsky on preparing for a kid and freezing embryos (00:07:41)</li><li>Emily Oster on the impact of kids on relationships (00:09:22)</li><li>Russ Roberts on empirical research when deciding whether to have kids (00:14:44)</li><li>Spencer Greenberg on parent surveys (00:23:58)</li><li>Elie Hassenfeld on how having children reframes his relationship to solving pressing problems (00:27:40)</li><li>Emily Oster on careers and kids (00:31:44)</li><li>Holden Karnofsky on the experience of having kids (00:38:44)</li><li>Bryan Caplan on homeschooling (00:40:30)</li><li>Emily Oster on what actually makes a difference in young kids' lives (00:46:02)</li><li>Nita Farahany on thinking about life and the world differently (00:51:16)</li><li>Rob’s first impressions of parenthood (00:52:59)</li><li>How Rob has changed his views about parenthood (00:58:04)</li><li>Can the pros and cons of parenthood be studied? (01:01:49)</li><li>Do people have skewed impressions of what parenthood is like? (01:09:24)</li><li>Work and parenting tradeoffs (01:15:26)</li><li>Tough decisions about screen time (01:25:11)</li><li>Rob’s advice to future parents (01:30:04)</li><li>Coda: Rob’s updated experience at nine months (01:32:09)</li><li>Emily Oster on her amazing nanny (01:35:01)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore<br></em><br></p>]]>
      </content:encoded>
      <pubDate>Fri, 08 Nov 2024 16:55:08 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/1022fdcb/92102d3f.mp3" length="80422223" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HjdIzHsDfRWrZesqcsT634EUg4E1AUjw4Lr2GKIRcR0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9hMzFj/YzIxNjE1YjkxMjVh/YmY0NDI3ZmFjMWE5/Y2MzOC5qcGc.jpg"/>
      <itunes:duration>5739</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>With kids very much on the team's mind we thought it would be fun to review some comments about parenting featured on the show over the years, then have hosts Luisa Rodriguez and Rob Wiblin react to them. </p><p><a href="https://80k.info/ph"><strong>Links to learn more and full transcript.</strong></a></p><p>After hearing 8 former guests’ insights, Luisa and Rob chat about:</p><ul><li>Which of these resonate the most with Rob, now that he’s been a dad for six months (plus an update at nine months).</li><li>What have been the biggest surprises for Rob in becoming a parent.</li><li>How Rob's dealt with work and parenting tradeoffs, and his advice for other would-be parents.</li><li>Rob's list of <a href="https://docs.google.com/document/d/1-WINYMydv8v2POQ48noHvOmvwW-oaR2GjfIeiUt30YU/edit"><strong>recommended purchases for new or upcoming parents</strong></a>.</li></ul><p>This bonus episode includes excerpts from:</p><ul><li>Ezra Klein on parenting yourself as well as your children (from <a href="https://80000hours.org/podcast/episodes/ezra-klein-ai-and-dc/">episode #157</a>)</li><li>Holden Karnofsky on freezing embryos and being surprised by how fun it is to have a kid (<a href="https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/">#110</a> and <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/">#158</a>)</li><li>Parenting expert Emily Oster on how having kids affect relationships, careers and kids, and what actually makes a difference in young kids’ lives (<a href="https://80000hours.org/podcast/episodes/emily-oster-pregnancy-parenting-careers/">#178</a>)</li><li>Russ Roberts on empirical research when deciding whether to have kids (<a href="https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/">#87</a>)</li><li>Spencer Greenberg on his surveys of parents (<a href="https://80000hours.org/podcast/episodes/spencer-greenberg-money-happiness-hype-value/">#183</a>)</li><li>Elie Hassenfeld on how having children reframes his relationship to solving pressing global problems (<a href="https://80000hours.org/podcast/episodes/elie-hassenfeld-givewell-critiques-and-lessons/">#153</a>)</li><li>Bryan Caplan on homeschooling (<a href="https://80000hours.org/podcast/episodes/bryan-caplan-stop-reading-the-news/">#172</a>)</li><li>Nita Farahany on thinking about life and the world differently with kids (<a href="https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/">#174</a>)</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob &amp; Luisa’s intro (00:00:19)</li><li>Ezra Klein on parenting yourself as well as your children (00:03:34)</li><li>Holden Karnofsky on preparing for a kid and freezing embryos (00:07:41)</li><li>Emily Oster on the impact of kids on relationships (00:09:22)</li><li>Russ Roberts on empirical research when deciding whether to have kids (00:14:44)</li><li>Spencer Greenberg on parent surveys (00:23:58)</li><li>Elie Hassenfeld on how having children reframes his relationship to solving pressing problems (00:27:40)</li><li>Emily Oster on careers and kids (00:31:44)</li><li>Holden Karnofsky on the experience of having kids (00:38:44)</li><li>Bryan Caplan on homeschooling (00:40:30)</li><li>Emily Oster on what actually makes a difference in young kids' lives (00:46:02)</li><li>Nita Farahany on thinking about life and the world differently (00:51:16)</li><li>Rob’s first impressions of parenthood (00:52:59)</li><li>How Rob has changed his views about parenthood (00:58:04)</li><li>Can the pros and cons of parenthood be studied? (01:01:49)</li><li>Do people have skewed impressions of what parenthood is like? (01:09:24)</li><li>Work and parenting tradeoffs (01:15:26)</li><li>Tough decisions about screen time (01:25:11)</li><li>Rob’s advice to future parents (01:30:04)</li><li>Coda: Rob’s updated experience at nine months (01:32:09)</li><li>Emily Oster on her amazing nanny (01:35:01)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore<br></em><br></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/1022fdcb/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/1022fdcb/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#206 – Anil Seth on the predictive brain and how to study consciousness</title>
      <itunes:title>#206 – Anil Seth on the predictive brain and how to study consciousness</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">bebf81ad-0d79-46e1-b889-9aa75473bd17</guid>
      <link>https://80000hours.org/podcast/episodes/anil-seth-predictive-brain-explaining-consciousness/?utm_campaign=podcast__anil-seth&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"In that famous example of the dress, half of the people in the world saw [blue and black], half saw [white and gold]. It turns out there’s individual differences in how brains take into account ambient light. Colour is one example where it’s pretty clear that what we experience is a kind of inference: it’s the brain’s best guess about what’s going on in some way out there in the world. And that’s the claim that I’ve taken on board as a general hypothesis for consciousness: that all our perceptual experiences are inferences about something we don’t and cannot have direct access to." —Anil Seth</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="http://www.anilseth.com/">Anil Seth</a> — director of the <a href="https://www.sussex.ac.uk/research/centres/sussex-centre-for-consciousness-science/">Sussex Centre for Consciousness Science</a> — about how much we can learn about consciousness by studying the brain.</p><p><a href="https://80k.info/as"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What groundbreaking studies with split-brain patients and blindsight have already taught us about the nature of consciousness.</li><li>Anil’s theory that our perception is a “controlled hallucination” generated by our predictive brains.</li><li>Whether looking for the parts of the brain that correlate with consciousness is the right way to learn about what consciousness is.</li><li>Whether our theories of human consciousness can be applied to nonhuman animals.</li><li>Anil’s thoughts on whether machines could ever be conscious.</li><li>Disagreements and open questions in the field of consciousness studies, and what areas Anil is most excited to explore next.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:02)</li><li>The interview begins (00:02:42)</li><li>How expectations and perception affect consciousness (00:03:05)</li><li>How the brain makes sense of the body it’s within (00:21:33)</li><li>Psychedelics and predictive processing (00:32:06)</li><li>Blindsight and visual consciousness (00:36:45)</li><li>Split-brain patients (00:54:56)</li><li>Overflow experiments (01:05:28)</li><li>How much can we learn about consciousness from empirical research? (01:14:23)</li><li>Which parts of the brain are responsible for conscious experiences? (01:27:37)</li><li>Current state and disagreements in the study of consciousness (01:38:36)</li><li>Digital consciousness (01:55:55)</li><li>Consciousness in nonhuman animals (02:18:11)</li><li>What’s next for Anil (02:30:18)</li><li>Luisa’s outro (02:32:46)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"In that famous example of the dress, half of the people in the world saw [blue and black], half saw [white and gold]. It turns out there’s individual differences in how brains take into account ambient light. Colour is one example where it’s pretty clear that what we experience is a kind of inference: it’s the brain’s best guess about what’s going on in some way out there in the world. And that’s the claim that I’ve taken on board as a general hypothesis for consciousness: that all our perceptual experiences are inferences about something we don’t and cannot have direct access to." —Anil Seth</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="http://www.anilseth.com/">Anil Seth</a> — director of the <a href="https://www.sussex.ac.uk/research/centres/sussex-centre-for-consciousness-science/">Sussex Centre for Consciousness Science</a> — about how much we can learn about consciousness by studying the brain.</p><p><a href="https://80k.info/as"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What groundbreaking studies with split-brain patients and blindsight have already taught us about the nature of consciousness.</li><li>Anil’s theory that our perception is a “controlled hallucination” generated by our predictive brains.</li><li>Whether looking for the parts of the brain that correlate with consciousness is the right way to learn about what consciousness is.</li><li>Whether our theories of human consciousness can be applied to nonhuman animals.</li><li>Anil’s thoughts on whether machines could ever be conscious.</li><li>Disagreements and open questions in the field of consciousness studies, and what areas Anil is most excited to explore next.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:02)</li><li>The interview begins (00:02:42)</li><li>How expectations and perception affect consciousness (00:03:05)</li><li>How the brain makes sense of the body it’s within (00:21:33)</li><li>Psychedelics and predictive processing (00:32:06)</li><li>Blindsight and visual consciousness (00:36:45)</li><li>Split-brain patients (00:54:56)</li><li>Overflow experiments (01:05:28)</li><li>How much can we learn about consciousness from empirical research? (01:14:23)</li><li>Which parts of the brain are responsible for conscious experiences? (01:27:37)</li><li>Current state and disagreements in the study of consciousness (01:38:36)</li><li>Digital consciousness (01:55:55)</li><li>Consciousness in nonhuman animals (02:18:11)</li><li>What’s next for Anil (02:30:18)</li><li>Luisa’s outro (02:32:46)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 01 Nov 2024 17:22:02 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3b2c0216/f9cf35f6.mp3" length="147757605" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lGURLxeW8lwWWOl8jBYQDVHbFTclgWKKvhv40CthByo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mMTIw/YWE5MzY2NTMzNTcz/NGI5ZTAxNTU5MmJm/N2Y0My5qcGVn.jpg"/>
      <itunes:duration>9230</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"In that famous example of the dress, half of the people in the world saw [blue and black], half saw [white and gold]. It turns out there’s individual differences in how brains take into account ambient light. Colour is one example where it’s pretty clear that what we experience is a kind of inference: it’s the brain’s best guess about what’s going on in some way out there in the world. And that’s the claim that I’ve taken on board as a general hypothesis for consciousness: that all our perceptual experiences are inferences about something we don’t and cannot have direct access to." —Anil Seth</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="http://www.anilseth.com/">Anil Seth</a> — director of the <a href="https://www.sussex.ac.uk/research/centres/sussex-centre-for-consciousness-science/">Sussex Centre for Consciousness Science</a> — about how much we can learn about consciousness by studying the brain.</p><p><a href="https://80k.info/as"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What groundbreaking studies with split-brain patients and blindsight have already taught us about the nature of consciousness.</li><li>Anil’s theory that our perception is a “controlled hallucination” generated by our predictive brains.</li><li>Whether looking for the parts of the brain that correlate with consciousness is the right way to learn about what consciousness is.</li><li>Whether our theories of human consciousness can be applied to nonhuman animals.</li><li>Anil’s thoughts on whether machines could ever be conscious.</li><li>Disagreements and open questions in the field of consciousness studies, and what areas Anil is most excited to explore next.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:02)</li><li>The interview begins (00:02:42)</li><li>How expectations and perception affect consciousness (00:03:05)</li><li>How the brain makes sense of the body it’s within (00:21:33)</li><li>Psychedelics and predictive processing (00:32:06)</li><li>Blindsight and visual consciousness (00:36:45)</li><li>Split-brain patients (00:54:56)</li><li>Overflow experiments (01:05:28)</li><li>How much can we learn about consciousness from empirical research? (01:14:23)</li><li>Which parts of the brain are responsible for conscious experiences? (01:27:37)</li><li>Current state and disagreements in the study of consciousness (01:38:36)</li><li>Digital consciousness (01:55:55)</li><li>Consciousness in nonhuman animals (02:18:11)</li><li>What’s next for Anil (02:30:18)</li><li>Luisa’s outro (02:32:46)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/3b2c0216/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/3b2c0216/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>How much does a vote matter? (Article)</title>
      <itunes:title>How much does a vote matter? (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ffb1d14a-df8f-453c-920e-c1297eddf46d</guid>
      <link>https://80000hours.org/articles/is-voting-important/</link>
      <description>
        <![CDATA[<p><a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>If you care about social impact, is voting important?</strong></a> In this piece, Rob investigates the two key things that determine the impact of your vote:</p><ol><li>The chances of your vote changing an election’s outcome.</li><li>How much better some candidates are for the world as a whole, compared to others.</li></ol><p>He then discusses a couple of the best arguments against voting in important elections, namely:</p><ol><li>If an election is competitive, that means other people disagree about which option is better, and you’re at some risk of voting for the worse candidate by mistake.</li><li>While voting itself doesn’t take long, knowing enough to accurately pick which candidate is better for the world actually does take substantial effort — effort that could be better allocated elsewhere.</li></ol><p>Finally, Rob covers the impact of donating to campaigns or working to "get out the vote," which can be effective ways to generate additional votes for your preferred candidate.</p><p>We last released this article in October 2020, but we think it largely still stands up today.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Introduction (00:01:12)</li><li>What's coming up (00:02:35)</li><li>The probability of one vote changing an election (00:03:58)</li><li>How much does it matter who wins? (00:09:29)</li><li>What if you’re wrong? (00:16:38)</li><li>Is deciding how to vote too much effort? (00:21:47)</li><li>How much does it cost to drive one extra vote? (00:25:13)</li><li>Overall, is it altruistic to vote? (00:29:38)</li><li>Rob's outro (00:31:19)</li></ul><p><em>Producer: Keiran Harris</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>If you care about social impact, is voting important?</strong></a> In this piece, Rob investigates the two key things that determine the impact of your vote:</p><ol><li>The chances of your vote changing an election’s outcome.</li><li>How much better some candidates are for the world as a whole, compared to others.</li></ol><p>He then discusses a couple of the best arguments against voting in important elections, namely:</p><ol><li>If an election is competitive, that means other people disagree about which option is better, and you’re at some risk of voting for the worse candidate by mistake.</li><li>While voting itself doesn’t take long, knowing enough to accurately pick which candidate is better for the world actually does take substantial effort — effort that could be better allocated elsewhere.</li></ol><p>Finally, Rob covers the impact of donating to campaigns or working to "get out the vote," which can be effective ways to generate additional votes for your preferred candidate.</p><p>We last released this article in October 2020, but we think it largely still stands up today.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Introduction (00:01:12)</li><li>What's coming up (00:02:35)</li><li>The probability of one vote changing an election (00:03:58)</li><li>How much does it matter who wins? (00:09:29)</li><li>What if you’re wrong? (00:16:38)</li><li>Is deciding how to vote too much effort? (00:21:47)</li><li>How much does it cost to drive one extra vote? (00:25:13)</li><li>Overall, is it altruistic to vote? (00:29:38)</li><li>Rob's outro (00:31:19)</li></ul><p><em>Producer: Keiran Harris</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 28 Oct 2024 17:34:09 +0000</pubDate>
      <author>Rob Wiblin</author>
      <enclosure url="https://media.transistor.fm/86efe2a1/5ca9b4fb.mp3" length="31295814" type="audio/mpeg"/>
      <itunes:author>Rob Wiblin</itunes:author>
      <itunes:image href="https://img.transistor.fm/fEWtHSPp9QvEi5dhfJfHrba9d9J-8yP5ZMia2_qNJ3g/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9hNzNj/MGJkOTkxOTFhMjBk/Y2Q0MjIzNTdmYzg4/ZWJlYS5wbmc.jpg"/>
      <itunes:duration>1952</itunes:duration>
      <itunes:summary>
        <![CDATA[<p><a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>If you care about social impact, is voting important?</strong></a> In this piece, Rob investigates the two key things that determine the impact of your vote:</p><ol><li>The chances of your vote changing an election’s outcome.</li><li>How much better some candidates are for the world as a whole, compared to others.</li></ol><p>He then discusses a couple of the best arguments against voting in important elections, namely:</p><ol><li>If an election is competitive, that means other people disagree about which option is better, and you’re at some risk of voting for the worse candidate by mistake.</li><li>While voting itself doesn’t take long, knowing enough to accurately pick which candidate is better for the world actually does take substantial effort — effort that could be better allocated elsewhere.</li></ol><p>Finally, Rob covers the impact of donating to campaigns or working to "get out the vote," which can be effective ways to generate additional votes for your preferred candidate.</p><p>We last released this article in October 2020, but we think it largely still stands up today.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Introduction (00:01:12)</li><li>What's coming up (00:02:35)</li><li>The probability of one vote changing an election (00:03:58)</li><li>How much does it matter who wins? (00:09:29)</li><li>What if you’re wrong? (00:16:38)</li><li>Is deciding how to vote too much effort? (00:21:47)</li><li>How much does it cost to drive one extra vote? (00:25:13)</li><li>Overall, is it altruistic to vote? (00:29:38)</li><li>Rob's outro (00:31:19)</li></ul><p><em>Producer: Keiran Harris</em></p>]]>
      </itunes:summary>
      <itunes:keywords>politics</itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/86efe2a1/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/86efe2a1/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#205 – Sébastien Moro on the most insane things fish can do</title>
      <itunes:title>#205 – Sébastien Moro on the most insane things fish can do</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ce78963e-f0af-4797-99c6-64d77ad163b0</guid>
      <link>https://80000hours.org/podcast/episodes/sebastien-moro-fish-cognition-senses-social-lives/?utm_campaign=podcast__sebastien-moro&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"You have a tank split in two parts: if the fish gets in the compartment with a red circle, it will receive food, and food will be delivered in the other tank as well. If the fish takes the blue triangle, this fish will receive food, but nothing will be delivered in the other tank. So we have a prosocial choice and antisocial choice. When there is no one in the other part of the tank, the male is choosing randomly. If there is a male, a possible rival: antisocial — almost 100% of the time. Now, if there is his wife — his female, this is a prosocial choice all the time.</p><p>"And now a question: Is it just because this is a female or is it just for <em>their </em>female? Well, when they're bringing a new female, it’s the antisocial choice all the time. Now, if there is not the female of the male, it will depend on how long he's been separated from his female. At first it will be antisocial, and after a while he will start to switch to prosocial choices." —Sébastien Moro</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://www.cervelledoiseau.fr/">science writer</a> and <a href="https://www.youtube.com/cervelledoiseau">video blogger</a> Sébastien Moro about the latest research on fish consciousness, intelligence, and potential sentience.</p><p><a href="https://80k.info/sm"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The insane capabilities of fish in tests of memory, learning, and problem-solving.</li><li>Examples of fish that can beat primates on cognitive tests and recognise individual human faces.</li><li>Fishes’ social lives, including pair bonding, “personalities,” cooperation, and cultural transmission.</li><li>Whether fish can experience emotions, and how this is even studied.</li><li>The wild evolutionary innovations of fish, who adapted to thrive in diverse environments from mangroves to the deep sea.</li><li>How some fish have sensory capabilities we can’t even really fathom — like “seeing” electrical fields and colours we can’t perceive.</li><li>Ethical issues raised by evidence that fish may be conscious and experience suffering.</li><li>And plenty more.</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"You have a tank split in two parts: if the fish gets in the compartment with a red circle, it will receive food, and food will be delivered in the other tank as well. If the fish takes the blue triangle, this fish will receive food, but nothing will be delivered in the other tank. So we have a prosocial choice and antisocial choice. When there is no one in the other part of the tank, the male is choosing randomly. If there is a male, a possible rival: antisocial — almost 100% of the time. Now, if there is his wife — his female, this is a prosocial choice all the time.</p><p>"And now a question: Is it just because this is a female or is it just for <em>their </em>female? Well, when they're bringing a new female, it’s the antisocial choice all the time. Now, if there is not the female of the male, it will depend on how long he's been separated from his female. At first it will be antisocial, and after a while he will start to switch to prosocial choices." —Sébastien Moro</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://www.cervelledoiseau.fr/">science writer</a> and <a href="https://www.youtube.com/cervelledoiseau">video blogger</a> Sébastien Moro about the latest research on fish consciousness, intelligence, and potential sentience.</p><p><a href="https://80k.info/sm"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The insane capabilities of fish in tests of memory, learning, and problem-solving.</li><li>Examples of fish that can beat primates on cognitive tests and recognise individual human faces.</li><li>Fishes’ social lives, including pair bonding, “personalities,” cooperation, and cultural transmission.</li><li>Whether fish can experience emotions, and how this is even studied.</li><li>The wild evolutionary innovations of fish, who adapted to thrive in diverse environments from mangroves to the deep sea.</li><li>How some fish have sensory capabilities we can’t even really fathom — like “seeing” electrical fields and colours we can’t perceive.</li><li>Ethical issues raised by evidence that fish may be conscious and experience suffering.</li><li>And plenty more.</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 23 Oct 2024 19:08:45 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/1f4839a3/f846b299.mp3" length="183487826" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/oGGg-G8zX_r6pXaUjJA8qTDPU7vtLxu01kT_JGTLGV8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9jNDc5/ZTgwZDUzNGQ2OTZl/YjI1ODI4MTU5NGIy/ZjlkNi5qcGc.jpg"/>
      <itunes:duration>11465</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"You have a tank split in two parts: if the fish gets in the compartment with a red circle, it will receive food, and food will be delivered in the other tank as well. If the fish takes the blue triangle, this fish will receive food, but nothing will be delivered in the other tank. So we have a prosocial choice and antisocial choice. When there is no one in the other part of the tank, the male is choosing randomly. If there is a male, a possible rival: antisocial — almost 100% of the time. Now, if there is his wife — his female, this is a prosocial choice all the time.</p><p>"And now a question: Is it just because this is a female or is it just for <em>their </em>female? Well, when they're bringing a new female, it’s the antisocial choice all the time. Now, if there is not the female of the male, it will depend on how long he's been separated from his female. At first it will be antisocial, and after a while he will start to switch to prosocial choices." —Sébastien Moro</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://www.cervelledoiseau.fr/">science writer</a> and <a href="https://www.youtube.com/cervelledoiseau">video blogger</a> Sébastien Moro about the latest research on fish consciousness, intelligence, and potential sentience.</p><p><a href="https://80k.info/sm"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The insane capabilities of fish in tests of memory, learning, and problem-solving.</li><li>Examples of fish that can beat primates on cognitive tests and recognise individual human faces.</li><li>Fishes’ social lives, including pair bonding, “personalities,” cooperation, and cultural transmission.</li><li>Whether fish can experience emotions, and how this is even studied.</li><li>The wild evolutionary innovations of fish, who adapted to thrive in diverse environments from mangroves to the deep sea.</li><li>How some fish have sensory capabilities we can’t even really fathom — like “seeing” electrical fields and colours we can’t perceive.</li><li>Ethical issues raised by evidence that fish may be conscious and experience suffering.</li><li>And plenty more.</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/1f4839a3/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/1f4839a3/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#204 – Nate Silver on making sense of SBF, and his biggest critiques of effective altruism</title>
      <itunes:title>#204 – Nate Silver on making sense of SBF, and his biggest critiques of effective altruism</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">fc98e28f-79eb-4722-b2a2-24951e53a287</guid>
      <link>https://80000hours.org/podcast/episodes/nate-silver-effective-altruism-sbf-art-of-risk/?utm_campaign=podcast__nate-silver&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Rob Wiblin speaks with <a href="https://www.natesilver.net/">FiveThirtyEight</a> election forecaster and author Nate Silver about his new book: <a href="https://www.amazon.com/Edge-Art-Risking-Everything/dp/1594204128"><em>On the Edge: The Art of Risking Everything</em></a>.</p><p><a href="https://80k.info/ns"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p><em>On the Edge</em> explores a cultural grouping Nate dubs “the River” — made up of people who are analytical, competitive, quantitatively minded, risk-taking, and willing to be contrarian. It’s a tendency he considers himself a part of, and the River has been doing well for itself in recent decades — gaining cultural influence through success in finance, technology, gambling, philanthropy, and politics, among other pursuits.</p><p>But on Nate’s telling, it’s a group particularly vulnerable to oversimplification and hubris. Where Riverians’ ability to calculate the “expected value” of actions isn’t as good as they believe, their poorly calculated bets can leave a trail of destruction — aptly demonstrated by Nate’s discussion of the extended time he spent with FTX CEO Sam Bankman-Fried before and after his downfall.</p><p>Given this show’s focus on the world’s most pressing problems and how to solve them, we narrow in on Nate’s discussion of effective altruism (EA), which has been little covered elsewhere. Nate met many leaders and members of the EA community in researching the book and has watched its evolution online for many years.</p><p>Effective altruism is the River style of doing good, because of its willingness to buck both fashion and common sense — making its giving decisions based on mathematical calculations and analytical arguments with the goal of maximising an outcome.</p><p>Nate sees a lot to admire in this, but the book paints a mixed picture in which effective altruism is arguably too trusting, too utilitarian, too selfless, and too reckless at some times, while too image-conscious at others.</p><p>But while everything has arguable weaknesses, could Nate actually do any better in practice? We ask him:</p><ul><li>How would Nate spend $10 billion differently than today’s philanthropists influenced by EA?</li><li>Is anyone else competitive with EA in terms of impact per dollar?</li><li>Does he have any big disagreements with 80,000 Hours’ advice on how to have impact?</li><li>Is EA too big a tent to function?</li><li>What global problems could EA be ignoring?</li><li>Should EA be more willing to court controversy?</li><li>Does EA’s niceness leave it vulnerable to exploitation?</li><li>What moral philosophy would he have modelled EA on?</li></ul><p>Rob and Nate also talk about:</p><ul><li>Nate’s theory of Sam Bankman-Fried’s psychology.</li><li>Whether we had to “raise or fold” on COVID.</li><li>Whether Sam Altman and Sam Bankman-Fried are structurally similar cases or not.</li><li>“Winners’ tilt.”</li><li>Whether it’s selfish to slow down AI progress.</li><li>The ridiculous 13 Keys to the White House.</li><li>Whether prediction markets are now overrated.</li><li>Whether venture capitalists talk a big talk about risk while pushing all the risk off onto the entrepreneurs they fund.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:03)</li><li>The interview begins (00:03:08)</li><li>Sam Bankman-Fried and trust in the effective altruism community (00:04:09)</li><li>Expected value (00:19:06)</li><li>Similarities and differences between Sam Altman and SBF (00:24:45)</li><li>How would Nate do EA differently? (00:31:54)</li><li>Reservations about utilitarianism (00:44:37)</li><li>Game theory equilibrium (00:48:51)</li><li>Differences between EA culture and rationalist culture (00:52:55)</li><li>What would Nate do with $10 billion to donate? (00:57:07)</li><li>COVID strategies and tradeoffs (01:06:52)</li><li>Is it selfish to slow down AI progress? (01:10:02)</li><li>Democratic legitimacy of AI progress (01:18:33)</li><li>Dubious election forecasting (01:22:40)</li><li>Assessing how reliable election forecasting models are (01:29:58)</li><li>Are prediction markets overrated? (01:41:01)</li><li>Venture capitalists and risk (01:48:48)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video engineering: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Rob Wiblin speaks with <a href="https://www.natesilver.net/">FiveThirtyEight</a> election forecaster and author Nate Silver about his new book: <a href="https://www.amazon.com/Edge-Art-Risking-Everything/dp/1594204128"><em>On the Edge: The Art of Risking Everything</em></a>.</p><p><a href="https://80k.info/ns"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p><em>On the Edge</em> explores a cultural grouping Nate dubs “the River” — made up of people who are analytical, competitive, quantitatively minded, risk-taking, and willing to be contrarian. It’s a tendency he considers himself a part of, and the River has been doing well for itself in recent decades — gaining cultural influence through success in finance, technology, gambling, philanthropy, and politics, among other pursuits.</p><p>But on Nate’s telling, it’s a group particularly vulnerable to oversimplification and hubris. Where Riverians’ ability to calculate the “expected value” of actions isn’t as good as they believe, their poorly calculated bets can leave a trail of destruction — aptly demonstrated by Nate’s discussion of the extended time he spent with FTX CEO Sam Bankman-Fried before and after his downfall.</p><p>Given this show’s focus on the world’s most pressing problems and how to solve them, we narrow in on Nate’s discussion of effective altruism (EA), which has been little covered elsewhere. Nate met many leaders and members of the EA community in researching the book and has watched its evolution online for many years.</p><p>Effective altruism is the River style of doing good, because of its willingness to buck both fashion and common sense — making its giving decisions based on mathematical calculations and analytical arguments with the goal of maximising an outcome.</p><p>Nate sees a lot to admire in this, but the book paints a mixed picture in which effective altruism is arguably too trusting, too utilitarian, too selfless, and too reckless at some times, while too image-conscious at others.</p><p>But while everything has arguable weaknesses, could Nate actually do any better in practice? We ask him:</p><ul><li>How would Nate spend $10 billion differently than today’s philanthropists influenced by EA?</li><li>Is anyone else competitive with EA in terms of impact per dollar?</li><li>Does he have any big disagreements with 80,000 Hours’ advice on how to have impact?</li><li>Is EA too big a tent to function?</li><li>What global problems could EA be ignoring?</li><li>Should EA be more willing to court controversy?</li><li>Does EA’s niceness leave it vulnerable to exploitation?</li><li>What moral philosophy would he have modelled EA on?</li></ul><p>Rob and Nate also talk about:</p><ul><li>Nate’s theory of Sam Bankman-Fried’s psychology.</li><li>Whether we had to “raise or fold” on COVID.</li><li>Whether Sam Altman and Sam Bankman-Fried are structurally similar cases or not.</li><li>“Winners’ tilt.”</li><li>Whether it’s selfish to slow down AI progress.</li><li>The ridiculous 13 Keys to the White House.</li><li>Whether prediction markets are now overrated.</li><li>Whether venture capitalists talk a big talk about risk while pushing all the risk off onto the entrepreneurs they fund.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:03)</li><li>The interview begins (00:03:08)</li><li>Sam Bankman-Fried and trust in the effective altruism community (00:04:09)</li><li>Expected value (00:19:06)</li><li>Similarities and differences between Sam Altman and SBF (00:24:45)</li><li>How would Nate do EA differently? (00:31:54)</li><li>Reservations about utilitarianism (00:44:37)</li><li>Game theory equilibrium (00:48:51)</li><li>Differences between EA culture and rationalist culture (00:52:55)</li><li>What would Nate do with $10 billion to donate? (00:57:07)</li><li>COVID strategies and tradeoffs (01:06:52)</li><li>Is it selfish to slow down AI progress? (01:10:02)</li><li>Democratic legitimacy of AI progress (01:18:33)</li><li>Dubious election forecasting (01:22:40)</li><li>Assessing how reliable election forecasting models are (01:29:58)</li><li>Are prediction markets overrated? (01:41:01)</li><li>Venture capitalists and risk (01:48:48)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video engineering: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 16 Oct 2024 16:22:09 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b9afba59/81d0dad5.mp3" length="113123755" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/qEq2PbFVPVlirDGeJlzwcxnMpOcGcef6J_d0PyGQkYQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS82YmJi/ZDE3NGExNTcyYjRl/M2EzYTZhZjBlNzYz/NzQxZC5wbmc.jpg"/>
      <itunes:duration>7068</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Rob Wiblin speaks with <a href="https://www.natesilver.net/">FiveThirtyEight</a> election forecaster and author Nate Silver about his new book: <a href="https://www.amazon.com/Edge-Art-Risking-Everything/dp/1594204128"><em>On the Edge: The Art of Risking Everything</em></a>.</p><p><a href="https://80k.info/ns"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p><em>On the Edge</em> explores a cultural grouping Nate dubs “the River” — made up of people who are analytical, competitive, quantitatively minded, risk-taking, and willing to be contrarian. It’s a tendency he considers himself a part of, and the River has been doing well for itself in recent decades — gaining cultural influence through success in finance, technology, gambling, philanthropy, and politics, among other pursuits.</p><p>But on Nate’s telling, it’s a group particularly vulnerable to oversimplification and hubris. Where Riverians’ ability to calculate the “expected value” of actions isn’t as good as they believe, their poorly calculated bets can leave a trail of destruction — aptly demonstrated by Nate’s discussion of the extended time he spent with FTX CEO Sam Bankman-Fried before and after his downfall.</p><p>Given this show’s focus on the world’s most pressing problems and how to solve them, we narrow in on Nate’s discussion of effective altruism (EA), which has been little covered elsewhere. Nate met many leaders and members of the EA community in researching the book and has watched its evolution online for many years.</p><p>Effective altruism is the River style of doing good, because of its willingness to buck both fashion and common sense — making its giving decisions based on mathematical calculations and analytical arguments with the goal of maximising an outcome.</p><p>Nate sees a lot to admire in this, but the book paints a mixed picture in which effective altruism is arguably too trusting, too utilitarian, too selfless, and too reckless at some times, while too image-conscious at others.</p><p>But while everything has arguable weaknesses, could Nate actually do any better in practice? We ask him:</p><ul><li>How would Nate spend $10 billion differently than today’s philanthropists influenced by EA?</li><li>Is anyone else competitive with EA in terms of impact per dollar?</li><li>Does he have any big disagreements with 80,000 Hours’ advice on how to have impact?</li><li>Is EA too big a tent to function?</li><li>What global problems could EA be ignoring?</li><li>Should EA be more willing to court controversy?</li><li>Does EA’s niceness leave it vulnerable to exploitation?</li><li>What moral philosophy would he have modelled EA on?</li></ul><p>Rob and Nate also talk about:</p><ul><li>Nate’s theory of Sam Bankman-Fried’s psychology.</li><li>Whether we had to “raise or fold” on COVID.</li><li>Whether Sam Altman and Sam Bankman-Fried are structurally similar cases or not.</li><li>“Winners’ tilt.”</li><li>Whether it’s selfish to slow down AI progress.</li><li>The ridiculous 13 Keys to the White House.</li><li>Whether prediction markets are now overrated.</li><li>Whether venture capitalists talk a big talk about risk while pushing all the risk off onto the entrepreneurs they fund.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:03)</li><li>The interview begins (00:03:08)</li><li>Sam Bankman-Fried and trust in the effective altruism community (00:04:09)</li><li>Expected value (00:19:06)</li><li>Similarities and differences between Sam Altman and SBF (00:24:45)</li><li>How would Nate do EA differently? (00:31:54)</li><li>Reservations about utilitarianism (00:44:37)</li><li>Game theory equilibrium (00:48:51)</li><li>Differences between EA culture and rationalist culture (00:52:55)</li><li>What would Nate do with $10 billion to donate? (00:57:07)</li><li>COVID strategies and tradeoffs (01:06:52)</li><li>Is it selfish to slow down AI progress? (01:10:02)</li><li>Democratic legitimacy of AI progress (01:18:33)</li><li>Dubious election forecasting (01:22:40)</li><li>Assessing how reliable election forecasting models are (01:29:58)</li><li>Are prediction markets overrated? (01:41:01)</li><li>Venture capitalists and risk (01:48:48)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video engineering: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/b9afba59/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/b9afba59/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#203 – Peter Godfrey-Smith on interfering with wild nature, accepting death, and the origin of complex civilisation</title>
      <itunes:title>#203 – Peter Godfrey-Smith on interfering with wild nature, accepting death, and the origin of complex civilisation</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">50858e75-1479-4e8c-a199-65361215795a</guid>
      <link>https://80000hours.org/podcast/episodes/peter-godfrey-smith-wild-animal-suffering-complex-life/?utm_campaign=podcast__peter-godfrey-smith&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"In the human case, it would be mistaken to give a kind of hour-by-hour accounting. You know, 'I had +4 level of experience for this hour, then I had -2 for the next hour, and then I had -1' — and you sort of sum to try to work out the total… And I came to think that something like that will be applicable in some of the animal cases as well… There are achievements, there are experiences, there are things that can be done in the face of difficulty that might be seen as having the same kind of redemptive role, as casting into a different light the difficult events that led up to it.</p><p>"The example I use is watching some birds successfully raising some young, fighting off a couple of rather aggressive parrots of another species that wanted to fight them, prevailing against difficult odds — and doing so in a way that was so wholly successful. It seemed to me that if you wanted to do an accounting of how things had gone for those birds, you would not want to do the naive thing of just counting up difficult and less-difficult hours. There’s something special about what’s achieved at the end of that process." —Peter Godfrey-Smith</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://petergodfreysmith.com/">Peter Godfrey-Smith</a> — bestselling author and science philosopher — about his new book, <a href="https://www.amazon.com/Living-Earth-Forests-Corals-Consciousness/dp/B0CW3VMXPB"><em>Living on Earth: Forests, Corals, Consciousness, and the Making of the World</em></a>.</p><p><a href="https://80k.info/pgs"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why octopuses and dolphins haven’t developed complex civilisation despite their intelligence.</li><li>How the role of culture has been crucial in enabling human technological progress.</li><li>Why Peter thinks the evolutionary transition from sea to land was key to enabling human-like intelligence — and why we should expect to see that in extraterrestrial life too.</li><li>Whether Peter thinks wild animals’ lives are, on balance, good or bad, and when, if ever, we should intervene in their lives.</li><li>Whether we can and should avoid death by uploading human minds.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>The interview begins (00:02:12)</li><li>Wild animal suffering and rewilding (00:04:09)</li><li>Thinking about death (00:32:50)</li><li>Uploads of ourselves (00:38:04)</li><li>Culture and how minds make things happen (00:54:05)</li><li>Challenges for water-based animals (01:01:37)</li><li>The importance of sea-to-land transitions in animal life (01:10:09)</li><li>Luisa's outro (01:23:43)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"In the human case, it would be mistaken to give a kind of hour-by-hour accounting. You know, 'I had +4 level of experience for this hour, then I had -2 for the next hour, and then I had -1' — and you sort of sum to try to work out the total… And I came to think that something like that will be applicable in some of the animal cases as well… There are achievements, there are experiences, there are things that can be done in the face of difficulty that might be seen as having the same kind of redemptive role, as casting into a different light the difficult events that led up to it.</p><p>"The example I use is watching some birds successfully raising some young, fighting off a couple of rather aggressive parrots of another species that wanted to fight them, prevailing against difficult odds — and doing so in a way that was so wholly successful. It seemed to me that if you wanted to do an accounting of how things had gone for those birds, you would not want to do the naive thing of just counting up difficult and less-difficult hours. There’s something special about what’s achieved at the end of that process." —Peter Godfrey-Smith</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://petergodfreysmith.com/">Peter Godfrey-Smith</a> — bestselling author and science philosopher — about his new book, <a href="https://www.amazon.com/Living-Earth-Forests-Corals-Consciousness/dp/B0CW3VMXPB"><em>Living on Earth: Forests, Corals, Consciousness, and the Making of the World</em></a>.</p><p><a href="https://80k.info/pgs"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why octopuses and dolphins haven’t developed complex civilisation despite their intelligence.</li><li>How the role of culture has been crucial in enabling human technological progress.</li><li>Why Peter thinks the evolutionary transition from sea to land was key to enabling human-like intelligence — and why we should expect to see that in extraterrestrial life too.</li><li>Whether Peter thinks wild animals’ lives are, on balance, good or bad, and when, if ever, we should intervene in their lives.</li><li>Whether we can and should avoid death by uploading human minds.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>The interview begins (00:02:12)</li><li>Wild animal suffering and rewilding (00:04:09)</li><li>Thinking about death (00:32:50)</li><li>Uploads of ourselves (00:38:04)</li><li>Culture and how minds make things happen (00:54:05)</li><li>Challenges for water-based animals (01:01:37)</li><li>The importance of sea-to-land transitions in animal life (01:10:09)</li><li>Luisa's outro (01:23:43)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 03 Oct 2024 16:48:49 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/368817ce/6acb2796.mp3" length="81784528" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/mVXp0VEueol_CswOsT8w6t2fgpKKPMM5hIzl0Bm_-xM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9kZGE0/M2EzY2ZhYTUzZWFm/MTk4Yjc1NjFlOWRi/ZDQ0OS5qcGc.jpg"/>
      <itunes:duration>5109</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"In the human case, it would be mistaken to give a kind of hour-by-hour accounting. You know, 'I had +4 level of experience for this hour, then I had -2 for the next hour, and then I had -1' — and you sort of sum to try to work out the total… And I came to think that something like that will be applicable in some of the animal cases as well… There are achievements, there are experiences, there are things that can be done in the face of difficulty that might be seen as having the same kind of redemptive role, as casting into a different light the difficult events that led up to it.</p><p>"The example I use is watching some birds successfully raising some young, fighting off a couple of rather aggressive parrots of another species that wanted to fight them, prevailing against difficult odds — and doing so in a way that was so wholly successful. It seemed to me that if you wanted to do an accounting of how things had gone for those birds, you would not want to do the naive thing of just counting up difficult and less-difficult hours. There’s something special about what’s achieved at the end of that process." —Peter Godfrey-Smith</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://petergodfreysmith.com/">Peter Godfrey-Smith</a> — bestselling author and science philosopher — about his new book, <a href="https://www.amazon.com/Living-Earth-Forests-Corals-Consciousness/dp/B0CW3VMXPB"><em>Living on Earth: Forests, Corals, Consciousness, and the Making of the World</em></a>.</p><p><a href="https://80k.info/pgs"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why octopuses and dolphins haven’t developed complex civilisation despite their intelligence.</li><li>How the role of culture has been crucial in enabling human technological progress.</li><li>Why Peter thinks the evolutionary transition from sea to land was key to enabling human-like intelligence — and why we should expect to see that in extraterrestrial life too.</li><li>Whether Peter thinks wild animals’ lives are, on balance, good or bad, and when, if ever, we should intervene in their lives.</li><li>Whether we can and should avoid death by uploading human minds.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>The interview begins (00:02:12)</li><li>Wild animal suffering and rewilding (00:04:09)</li><li>Thinking about death (00:32:50)</li><li>Uploads of ourselves (00:38:04)</li><li>Culture and how minds make things happen (00:54:05)</li><li>Challenges for water-based animals (01:01:37)</li><li>The importance of sea-to-land transitions in animal life (01:10:09)</li><li>Luisa's outro (01:23:43)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/368817ce/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/368817ce/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Luisa and Keiran on free will, and the consequences of never feeling enduring guilt or shame</title>
      <itunes:title>Luisa and Keiran on free will, and the consequences of never feeling enduring guilt or shame</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c3f517b2-bea7-4b07-af0a-36c5ab57f42f</guid>
      <link>https://80000hours.org/after-hours-podcast/episodes/luisa-keiran-free-will-guilt-shame/</link>
      <description>
        <![CDATA[<p>In this episode from our second show, <a href="https://80000hours.org/after-hours-podcast/"><em>80k After Hours</em></a>, Luisa Rodriguez and Keiran Harris chat about the consequences of letting go of enduring guilt, shame, anger, and pride.</p><p><a href="https://80000hours.org/after-hours-podcast/episodes/luisa-keiran-free-will-guilt-shame/"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong><br></strong><br>They cover:</p><ul><li>Keiran’s views on free will, and how he came to hold them</li><li>What it’s like not experiencing sustained guilt, shame, and anger</li><li>Whether Luisa would become a worse person if she felt less guilt and shame — specifically whether she’d work fewer hours, or donate less money, or become a worse friend</li><li>Whether giving up guilt and shame also means giving up pride</li><li>The implications for love</li><li>The neurological condition ‘Jerk Syndrome’</li><li>And some practical advice on feeling less guilt, shame, and anger</li></ul><p><strong>Who this episode is for:</strong></p><ul><li>People sympathetic to the idea that free will is an illusion</li><li>People who experience tons of guilt, shame, or anger</li><li>People worried about what would happen if they stopped feeling tonnes of guilt, shame, or anger</li></ul><p><strong>Who this episode isn’t for:</strong></p><ul><li>People strongly in favour of retributive justice</li><li>Philosophers who can’t stand random non-philosophers talking about philosophy</li><li>Non-philosophers who can’t stand random non-philosophers talking about philosophy</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:16)</li><li>The chat begins (00:03:15)</li><li>Keiran's origin story (00:06:30)</li><li>Charles Whitman (00:11:00)</li><li>Luisa's origin story (00:16:41)</li><li>It's unlucky to be a bad person (00:19:57)</li><li>Doubts about whether free will is an illusion (00:23:09)</li><li>Acting this way just for other people (00:34:57)</li><li>Feeling shame over not working enough (00:37:26)</li><li>First person / third person distinction (00:39:42)</li><li>Would Luisa become a worse person if she felt less guilt? (00:44:09)</li><li>Feeling bad about not being a different person (00:48:18)</li><li>Would Luisa donate less money? (00:55:14)</li><li>Would Luisa become a worse friend? (01:01:07)</li><li>Pride (01:08:02)</li><li>Love (01:15:35)</li><li>Bears and hurricanes (01:19:53)</li><li>Jerk Syndrome (01:24:24)</li><li>Keiran's outro (01:34:47)</li></ul><p><strong>Get more episodes like this by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type "80k After Hours" into your podcasting app. </strong></p><p><em>Producer: Keiran Harris<br>Audio mastering: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In this episode from our second show, <a href="https://80000hours.org/after-hours-podcast/"><em>80k After Hours</em></a>, Luisa Rodriguez and Keiran Harris chat about the consequences of letting go of enduring guilt, shame, anger, and pride.</p><p><a href="https://80000hours.org/after-hours-podcast/episodes/luisa-keiran-free-will-guilt-shame/"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong><br></strong><br>They cover:</p><ul><li>Keiran’s views on free will, and how he came to hold them</li><li>What it’s like not experiencing sustained guilt, shame, and anger</li><li>Whether Luisa would become a worse person if she felt less guilt and shame — specifically whether she’d work fewer hours, or donate less money, or become a worse friend</li><li>Whether giving up guilt and shame also means giving up pride</li><li>The implications for love</li><li>The neurological condition ‘Jerk Syndrome’</li><li>And some practical advice on feeling less guilt, shame, and anger</li></ul><p><strong>Who this episode is for:</strong></p><ul><li>People sympathetic to the idea that free will is an illusion</li><li>People who experience tons of guilt, shame, or anger</li><li>People worried about what would happen if they stopped feeling tonnes of guilt, shame, or anger</li></ul><p><strong>Who this episode isn’t for:</strong></p><ul><li>People strongly in favour of retributive justice</li><li>Philosophers who can’t stand random non-philosophers talking about philosophy</li><li>Non-philosophers who can’t stand random non-philosophers talking about philosophy</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:16)</li><li>The chat begins (00:03:15)</li><li>Keiran's origin story (00:06:30)</li><li>Charles Whitman (00:11:00)</li><li>Luisa's origin story (00:16:41)</li><li>It's unlucky to be a bad person (00:19:57)</li><li>Doubts about whether free will is an illusion (00:23:09)</li><li>Acting this way just for other people (00:34:57)</li><li>Feeling shame over not working enough (00:37:26)</li><li>First person / third person distinction (00:39:42)</li><li>Would Luisa become a worse person if she felt less guilt? (00:44:09)</li><li>Feeling bad about not being a different person (00:48:18)</li><li>Would Luisa donate less money? (00:55:14)</li><li>Would Luisa become a worse friend? (01:01:07)</li><li>Pride (01:08:02)</li><li>Love (01:15:35)</li><li>Bears and hurricanes (01:19:53)</li><li>Jerk Syndrome (01:24:24)</li><li>Keiran's outro (01:34:47)</li></ul><p><strong>Get more episodes like this by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type "80k After Hours" into your podcasting app. </strong></p><p><em>Producer: Keiran Harris<br>Audio mastering: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 27 Sep 2024 20:04:04 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d0e410fe/eef002bf.mp3" length="92206911" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/jOP_BUFoOU33om24wE0It2higTHdoXCw0QlJFba-CkM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xMTFl/Y2I1ZjQ3M2Y0ZDBj/OTA3YzQxNzI2NWMz/NDRmMy5qcGc.jpg"/>
      <itunes:duration>5760</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>In this episode from our second show, <a href="https://80000hours.org/after-hours-podcast/"><em>80k After Hours</em></a>, Luisa Rodriguez and Keiran Harris chat about the consequences of letting go of enduring guilt, shame, anger, and pride.</p><p><a href="https://80000hours.org/after-hours-podcast/episodes/luisa-keiran-free-will-guilt-shame/"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong><br></strong><br>They cover:</p><ul><li>Keiran’s views on free will, and how he came to hold them</li><li>What it’s like not experiencing sustained guilt, shame, and anger</li><li>Whether Luisa would become a worse person if she felt less guilt and shame — specifically whether she’d work fewer hours, or donate less money, or become a worse friend</li><li>Whether giving up guilt and shame also means giving up pride</li><li>The implications for love</li><li>The neurological condition ‘Jerk Syndrome’</li><li>And some practical advice on feeling less guilt, shame, and anger</li></ul><p><strong>Who this episode is for:</strong></p><ul><li>People sympathetic to the idea that free will is an illusion</li><li>People who experience tons of guilt, shame, or anger</li><li>People worried about what would happen if they stopped feeling tonnes of guilt, shame, or anger</li></ul><p><strong>Who this episode isn’t for:</strong></p><ul><li>People strongly in favour of retributive justice</li><li>Philosophers who can’t stand random non-philosophers talking about philosophy</li><li>Non-philosophers who can’t stand random non-philosophers talking about philosophy</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:16)</li><li>The chat begins (00:03:15)</li><li>Keiran's origin story (00:06:30)</li><li>Charles Whitman (00:11:00)</li><li>Luisa's origin story (00:16:41)</li><li>It's unlucky to be a bad person (00:19:57)</li><li>Doubts about whether free will is an illusion (00:23:09)</li><li>Acting this way just for other people (00:34:57)</li><li>Feeling shame over not working enough (00:37:26)</li><li>First person / third person distinction (00:39:42)</li><li>Would Luisa become a worse person if she felt less guilt? (00:44:09)</li><li>Feeling bad about not being a different person (00:48:18)</li><li>Would Luisa donate less money? (00:55:14)</li><li>Would Luisa become a worse friend? (01:01:07)</li><li>Pride (01:08:02)</li><li>Love (01:15:35)</li><li>Bears and hurricanes (01:19:53)</li><li>Jerk Syndrome (01:24:24)</li><li>Keiran's outro (01:34:47)</li></ul><p><strong>Get more episodes like this by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type "80k After Hours" into your podcasting app. </strong></p><p><em>Producer: Keiran Harris<br>Audio mastering: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/d0e410fe/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/d0e410fe/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#202 – Venki Ramakrishnan on the cutting edge of anti-ageing science</title>
      <itunes:title>#202 – Venki Ramakrishnan on the cutting edge of anti-ageing science</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">27f03ddb-18a2-4b6e-893b-fcc1c08861ed</guid>
      <link>https://80000hours.org/podcast/episodes/venki-ramakrishnan-ageing-life-extension/?utm_campaign=podcast__venki-ramakrishnan&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"For every far-out idea that turns out to be true, there were probably hundreds that were simply crackpot ideas. In general, [science] advances building on the knowledge we have, and seeing what the next questions are, and then getting to the next stage and the next stage and so on. And occasionally there’ll be revolutionary ideas which will really completely change your view of science. And it is possible that some revolutionary breakthrough in our understanding will come about and we might crack this problem, but there’s no evidence for that. It doesn’t mean that there isn’t a lot of promising work going on. There are many legitimate areas which could lead to real improvements in health in old age. So I’m fairly balanced: I think there are promising areas, but there’s a lot of work to be done to see which area is going to be promising, and what the risks are, and how to make them work." —Venki Ramakrishnan</p><p>In today’s episode, host Luisa Rodriguez speaks to Venki Ramakrishnan — molecular biologist and Nobel Prize winner — about his new book, <a href="https://www.amazon.com/Why-We-Die-Science-Immortality/dp/0063113279"><em>Why We Die: The New Science of Aging and the Quest for Immortality</em></a>.</p><p><a href="https://80k.info/vr"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What we can learn about extending human lifespan — if anything — from “immortal” aquatic animal species, cloned sheep, and the oldest people to have ever lived.</li><li>Which areas of anti-ageing research seem most promising to Venki — including caloric restriction, removing senescent cells, cellular reprogramming, and Yamanaka factors — and which Venki thinks are overhyped.</li><li>Why eliminating major age-related diseases might only extend average lifespan by 15 years.</li><li>The social impacts of extending healthspan or lifespan in an ageing population — including the potential danger of massively increasing inequality if some people can access life-extension interventions while others can’t.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:02:21)</li><li>Reasons to explore why we age and die (00:02:35)</li><li>Evolutionary pressures and animals that don't biologically age (00:06:55)</li><li>Why does ageing cause us to die? (00:12:24)</li><li>Is there a hard limit to the human lifespan? (00:17:11)</li><li>Evolutionary tradeoffs between fitness and longevity (00:21:01)</li><li>How ageing resets with every generation, and what we can learn from clones (00:23:48)</li><li>Younger blood (00:31:20)</li><li>Freezing cells, organs, and bodies (00:36:47)</li><li>Are the goals of anti-ageing research even realistic? (00:43:44)</li><li>Dementia (00:49:52)</li><li>Senescence (01:01:58)</li><li>Caloric restriction and metabolic pathways (01:11:45)</li><li>Yamanaka factors (01:34:07)</li><li>Cancer (01:47:44)</li><li>Mitochondrial dysfunction (01:58:40)</li><li>Population effects of extended lifespan (02:06:12)</li><li>Could increased longevity increase inequality? (02:11:48)</li><li>What’s surprised Venki about this research (02:16:06)</li><li>Luisa's outro (02:19:26)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"For every far-out idea that turns out to be true, there were probably hundreds that were simply crackpot ideas. In general, [science] advances building on the knowledge we have, and seeing what the next questions are, and then getting to the next stage and the next stage and so on. And occasionally there’ll be revolutionary ideas which will really completely change your view of science. And it is possible that some revolutionary breakthrough in our understanding will come about and we might crack this problem, but there’s no evidence for that. It doesn’t mean that there isn’t a lot of promising work going on. There are many legitimate areas which could lead to real improvements in health in old age. So I’m fairly balanced: I think there are promising areas, but there’s a lot of work to be done to see which area is going to be promising, and what the risks are, and how to make them work." —Venki Ramakrishnan</p><p>In today’s episode, host Luisa Rodriguez speaks to Venki Ramakrishnan — molecular biologist and Nobel Prize winner — about his new book, <a href="https://www.amazon.com/Why-We-Die-Science-Immortality/dp/0063113279"><em>Why We Die: The New Science of Aging and the Quest for Immortality</em></a>.</p><p><a href="https://80k.info/vr"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What we can learn about extending human lifespan — if anything — from “immortal” aquatic animal species, cloned sheep, and the oldest people to have ever lived.</li><li>Which areas of anti-ageing research seem most promising to Venki — including caloric restriction, removing senescent cells, cellular reprogramming, and Yamanaka factors — and which Venki thinks are overhyped.</li><li>Why eliminating major age-related diseases might only extend average lifespan by 15 years.</li><li>The social impacts of extending healthspan or lifespan in an ageing population — including the potential danger of massively increasing inequality if some people can access life-extension interventions while others can’t.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:02:21)</li><li>Reasons to explore why we age and die (00:02:35)</li><li>Evolutionary pressures and animals that don't biologically age (00:06:55)</li><li>Why does ageing cause us to die? (00:12:24)</li><li>Is there a hard limit to the human lifespan? (00:17:11)</li><li>Evolutionary tradeoffs between fitness and longevity (00:21:01)</li><li>How ageing resets with every generation, and what we can learn from clones (00:23:48)</li><li>Younger blood (00:31:20)</li><li>Freezing cells, organs, and bodies (00:36:47)</li><li>Are the goals of anti-ageing research even realistic? (00:43:44)</li><li>Dementia (00:49:52)</li><li>Senescence (01:01:58)</li><li>Caloric restriction and metabolic pathways (01:11:45)</li><li>Yamanaka factors (01:34:07)</li><li>Cancer (01:47:44)</li><li>Mitochondrial dysfunction (01:58:40)</li><li>Population effects of extended lifespan (02:06:12)</li><li>Could increased longevity increase inequality? (02:11:48)</li><li>What’s surprised Venki about this research (02:16:06)</li><li>Luisa's outro (02:19:26)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 19 Sep 2024 18:00:26 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f2930fc2/20a40dd1.mp3" length="134859337" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/G_Enby_FHBq0F_mcTjqAB1UClq-aY_HAE2RlILBU4ZY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS85ZjM1/MDY2ZjIxZjMyMzBi/YWI3MmRmNzYxNmJj/ZDdmZS5qcGc.jpg"/>
      <itunes:duration>8426</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"For every far-out idea that turns out to be true, there were probably hundreds that were simply crackpot ideas. In general, [science] advances building on the knowledge we have, and seeing what the next questions are, and then getting to the next stage and the next stage and so on. And occasionally there’ll be revolutionary ideas which will really completely change your view of science. And it is possible that some revolutionary breakthrough in our understanding will come about and we might crack this problem, but there’s no evidence for that. It doesn’t mean that there isn’t a lot of promising work going on. There are many legitimate areas which could lead to real improvements in health in old age. So I’m fairly balanced: I think there are promising areas, but there’s a lot of work to be done to see which area is going to be promising, and what the risks are, and how to make them work." —Venki Ramakrishnan</p><p>In today’s episode, host Luisa Rodriguez speaks to Venki Ramakrishnan — molecular biologist and Nobel Prize winner — about his new book, <a href="https://www.amazon.com/Why-We-Die-Science-Immortality/dp/0063113279"><em>Why We Die: The New Science of Aging and the Quest for Immortality</em></a>.</p><p><a href="https://80k.info/vr"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What we can learn about extending human lifespan — if anything — from “immortal” aquatic animal species, cloned sheep, and the oldest people to have ever lived.</li><li>Which areas of anti-ageing research seem most promising to Venki — including caloric restriction, removing senescent cells, cellular reprogramming, and Yamanaka factors — and which Venki thinks are overhyped.</li><li>Why eliminating major age-related diseases might only extend average lifespan by 15 years.</li><li>The social impacts of extending healthspan or lifespan in an ageing population — including the potential danger of massively increasing inequality if some people can access life-extension interventions while others can’t.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:04)</li><li>The interview begins (00:02:21)</li><li>Reasons to explore why we age and die (00:02:35)</li><li>Evolutionary pressures and animals that don't biologically age (00:06:55)</li><li>Why does ageing cause us to die? (00:12:24)</li><li>Is there a hard limit to the human lifespan? (00:17:11)</li><li>Evolutionary tradeoffs between fitness and longevity (00:21:01)</li><li>How ageing resets with every generation, and what we can learn from clones (00:23:48)</li><li>Younger blood (00:31:20)</li><li>Freezing cells, organs, and bodies (00:36:47)</li><li>Are the goals of anti-ageing research even realistic? (00:43:44)</li><li>Dementia (00:49:52)</li><li>Senescence (01:01:58)</li><li>Caloric restriction and metabolic pathways (01:11:45)</li><li>Yamanaka factors (01:34:07)</li><li>Cancer (01:47:44)</li><li>Mitochondrial dysfunction (01:58:40)</li><li>Population effects of extended lifespan (02:06:12)</li><li>Could increased longevity increase inequality? (02:11:48)</li><li>What’s surprised Venki about this research (02:16:06)</li><li>Luisa's outro (02:19:26)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f2930fc2/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f2930fc2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#201 – Ken Goldberg on why your robot butler isn’t here yet</title>
      <itunes:title>#201 – Ken Goldberg on why your robot butler isn’t here yet</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b5951d11-33b3-45fb-99e7-c73e5d3c7887</guid>
      <link>https://80000hours.org/podcast/episodes/ken-goldberg-robotics/?utm_campaign=podcast__ken-goldberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Perception is quite difficult with cameras: even if you have a stereo camera, you still can’t really build a map of where everything is in space. It’s just very difficult. And I know that sounds surprising, because humans are very good at this. In fact, even with one eye, we can navigate and we can clear the dinner table. But it seems that we’re building in a lot of understanding and intuition about what’s happening in the world and where objects are and how they behave. For robots, it’s very difficult to get a perfectly accurate model of the world and where things are. So if you’re going to go manipulate or grasp an object, a small error in that position will maybe have your robot crash into the object, a delicate wine glass, and probably break it. So the perception and the control are both problems." —Ken Goldberg</p><p>In today’s episode, host Luisa Rodriguez speaks to Ken Goldberg — robotics professor at UC Berkeley — about the major research challenges still ahead before robots become broadly integrated into our homes and societies.</p><p><a href="https://80k.info/kg"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why training robots is harder than training large language models like ChatGPT.</li><li>The biggest engineering challenges that still remain before robots can be widely useful in the real world.</li><li>The sectors where Ken thinks robots will be most useful in the coming decades — like homecare, agriculture, and medicine.</li><li>Whether we should be worried about robot labour affecting human employment.</li><li>Recent breakthroughs in robotics, and what cutting-edge robots can do today.</li><li>Ken’s work as an artist, where he explores the complex relationship between humans and technology.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:19)</li><li>General purpose robots and the “robotics bubble” (00:03:11)</li><li>How training robots is different than training large language models (00:14:01)</li><li>What can robots do today? (00:34:35)</li><li>Challenges for progress: fault tolerance, multidimensionality, and perception (00:41:00)</li><li>Recent breakthroughs in robotics (00:52:32)</li><li>Barriers to making better robots: hardware, software, and physics (01:03:13)</li><li>Future robots in home care, logistics, food production, and medicine (01:16:35)</li><li>How might robot labour affect the job market? (01:44:27)</li><li>Robotics and art (01:51:28)</li><li>Luisa's outro (02:00:55)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Dominic Armstrong, Ben Cordell, Milo McGuire, and Simon Monsour<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Perception is quite difficult with cameras: even if you have a stereo camera, you still can’t really build a map of where everything is in space. It’s just very difficult. And I know that sounds surprising, because humans are very good at this. In fact, even with one eye, we can navigate and we can clear the dinner table. But it seems that we’re building in a lot of understanding and intuition about what’s happening in the world and where objects are and how they behave. For robots, it’s very difficult to get a perfectly accurate model of the world and where things are. So if you’re going to go manipulate or grasp an object, a small error in that position will maybe have your robot crash into the object, a delicate wine glass, and probably break it. So the perception and the control are both problems." —Ken Goldberg</p><p>In today’s episode, host Luisa Rodriguez speaks to Ken Goldberg — robotics professor at UC Berkeley — about the major research challenges still ahead before robots become broadly integrated into our homes and societies.</p><p><a href="https://80k.info/kg"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why training robots is harder than training large language models like ChatGPT.</li><li>The biggest engineering challenges that still remain before robots can be widely useful in the real world.</li><li>The sectors where Ken thinks robots will be most useful in the coming decades — like homecare, agriculture, and medicine.</li><li>Whether we should be worried about robot labour affecting human employment.</li><li>Recent breakthroughs in robotics, and what cutting-edge robots can do today.</li><li>Ken’s work as an artist, where he explores the complex relationship between humans and technology.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:19)</li><li>General purpose robots and the “robotics bubble” (00:03:11)</li><li>How training robots is different than training large language models (00:14:01)</li><li>What can robots do today? (00:34:35)</li><li>Challenges for progress: fault tolerance, multidimensionality, and perception (00:41:00)</li><li>Recent breakthroughs in robotics (00:52:32)</li><li>Barriers to making better robots: hardware, software, and physics (01:03:13)</li><li>Future robots in home care, logistics, food production, and medicine (01:16:35)</li><li>How might robot labour affect the job market? (01:44:27)</li><li>Robotics and art (01:51:28)</li><li>Luisa's outro (02:00:55)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Dominic Armstrong, Ben Cordell, Milo McGuire, and Simon Monsour<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 13 Sep 2024 16:34:48 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5d5d3cfb/015a3d44.mp3" length="116877562" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/xhO8ezu8pAQ0oicc9_E7NzptuYWdrBGjcYG_IGDv2-U/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xY2E1/ZThlZDk0N2NiNDY5/NDg5MGQxZTA5Yjcy/NzBhZi5qcGc.jpg"/>
      <itunes:duration>7303</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Perception is quite difficult with cameras: even if you have a stereo camera, you still can’t really build a map of where everything is in space. It’s just very difficult. And I know that sounds surprising, because humans are very good at this. In fact, even with one eye, we can navigate and we can clear the dinner table. But it seems that we’re building in a lot of understanding and intuition about what’s happening in the world and where objects are and how they behave. For robots, it’s very difficult to get a perfectly accurate model of the world and where things are. So if you’re going to go manipulate or grasp an object, a small error in that position will maybe have your robot crash into the object, a delicate wine glass, and probably break it. So the perception and the control are both problems." —Ken Goldberg</p><p>In today’s episode, host Luisa Rodriguez speaks to Ken Goldberg — robotics professor at UC Berkeley — about the major research challenges still ahead before robots become broadly integrated into our homes and societies.</p><p><a href="https://80k.info/kg"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why training robots is harder than training large language models like ChatGPT.</li><li>The biggest engineering challenges that still remain before robots can be widely useful in the real world.</li><li>The sectors where Ken thinks robots will be most useful in the coming decades — like homecare, agriculture, and medicine.</li><li>Whether we should be worried about robot labour affecting human employment.</li><li>Recent breakthroughs in robotics, and what cutting-edge robots can do today.</li><li>Ken’s work as an artist, where he explores the complex relationship between humans and technology.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:19)</li><li>General purpose robots and the “robotics bubble” (00:03:11)</li><li>How training robots is different than training large language models (00:14:01)</li><li>What can robots do today? (00:34:35)</li><li>Challenges for progress: fault tolerance, multidimensionality, and perception (00:41:00)</li><li>Recent breakthroughs in robotics (00:52:32)</li><li>Barriers to making better robots: hardware, software, and physics (01:03:13)</li><li>Future robots in home care, logistics, food production, and medicine (01:16:35)</li><li>How might robot labour affect the job market? (01:44:27)</li><li>Robotics and art (01:51:28)</li><li>Luisa's outro (02:00:55)</li></ul><p><em>Producer: Keiran Harris<br>Audio engineering: Dominic Armstrong, Ben Cordell, Milo McGuire, and Simon Monsour<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/5d5d3cfb/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/5d5d3cfb/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#200 – Ezra Karger on what superforecasters and experts think about existential risks</title>
      <itunes:title>#200 – Ezra Karger on what superforecasters and experts think about existential risks</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">22c3dfe6-59ac-4e92-904a-1a898484196a</guid>
      <link>https://80000hours.org/podcast/episodes/ezra-karger-forecasting-existential-risks/?utm_campaign=podcast__ezra-karger&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"It’s very hard to find examples where people say, 'I’m starting from this point. I’m starting from this belief.' So we wanted to make that very legible to people. We wanted to say, 'Experts think this; accurate forecasters think this.' They might both be wrong, but we can at least start from here and figure out where we’re coming into a discussion and say, 'I am much less concerned than the people in this report; or I am much more concerned, and I think people in this report were missing major things.' But if you don’t have a reference set of probabilities, I think it becomes much harder to talk about disagreement in policy debates in a space that’s so complicated like this." —Ezra Karger</p><p>In today’s episode, host Luisa Rodriguez speaks to Ezra Karger — research director at the <a href="https://forecastingresearch.org/">Forecasting Research Institute</a> — about FRI’s recent <a href="https://forecastingresearch.org/xpt">Existential Risk Persuasion Tournament</a> to come up with estimates of a range of catastrophic risks.</p><p><a href="https://80k.info/ek24"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How forecasting can improve our understanding of long-term catastrophic risks from things like AI, nuclear war, pandemics, and climate change.</li><li>What the Existential Risk Persuasion Tournament (XPT) is, how it was set up, and the results.</li><li>The challenges of predicting low-probability, high-impact events.</li><li>Why superforecasters’ estimates of catastrophic risks seem so much lower than experts’, and which group Ezra puts the most weight on.</li><li>The specific underlying disagreements that superforecasters and experts had about how likely catastrophic risks from AI are.</li><li>Why Ezra thinks forecasting tournaments can help build consensus on complex topics, and what he wants to do differently in future tournaments and studies.</li><li>Recent advances in the science of forecasting and the areas Ezra is most excited about exploring next.</li><li>Whether large language models could help or outperform human forecasters.</li><li>How people can improve their calibration and start making better forecasts personally.</li><li>Why Ezra thinks high-quality forecasts are relevant to policymakers, and whether they can really improve decision-making.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:07)</li><li>The interview begins (00:02:54)</li><li>The Existential Risk Persuasion Tournament (00:05:13)</li><li>Why is this project important? (00:12:34)</li><li>How was the tournament set up? (00:17:54)</li><li>Results from the tournament (00:22:38)</li><li>Risk from artificial intelligence (00:30:59)</li><li>How to think about these numbers (00:46:50)</li><li>Should we trust experts or superforecasters more? (00:49:16)</li><li>The effect of debate and persuasion (01:02:10)</li><li>Forecasts from the general public (01:08:33)</li><li>How can we improve people’s forecasts? (01:18:59)</li><li>Incentives and recruitment (01:26:30)</li><li>Criticisms of the tournament (01:33:51)</li><li>AI adversarial collaboration (01:46:20)</li><li>Hypotheses about stark differences in views of AI risk (01:51:41)</li><li>Cruxes and different worldviews (02:17:15)</li><li>Ezra’s experience as a superforecaster (02:28:57)</li><li>Forecasting as a research field (02:31:00)</li><li>Can large language models help or outperform human forecasters? (02:35:01)</li><li>Is forecasting valuable in the real world? (02:39:11)</li><li>Ezra’s book recommendations (02:45:29)</li><li>Luisa's outro (02:47:54)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Dominic Armstrong, Ben Cordell, Milo McGuire, and Simon Monsour<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"It’s very hard to find examples where people say, 'I’m starting from this point. I’m starting from this belief.' So we wanted to make that very legible to people. We wanted to say, 'Experts think this; accurate forecasters think this.' They might both be wrong, but we can at least start from here and figure out where we’re coming into a discussion and say, 'I am much less concerned than the people in this report; or I am much more concerned, and I think people in this report were missing major things.' But if you don’t have a reference set of probabilities, I think it becomes much harder to talk about disagreement in policy debates in a space that’s so complicated like this." —Ezra Karger</p><p>In today’s episode, host Luisa Rodriguez speaks to Ezra Karger — research director at the <a href="https://forecastingresearch.org/">Forecasting Research Institute</a> — about FRI’s recent <a href="https://forecastingresearch.org/xpt">Existential Risk Persuasion Tournament</a> to come up with estimates of a range of catastrophic risks.</p><p><a href="https://80k.info/ek24"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How forecasting can improve our understanding of long-term catastrophic risks from things like AI, nuclear war, pandemics, and climate change.</li><li>What the Existential Risk Persuasion Tournament (XPT) is, how it was set up, and the results.</li><li>The challenges of predicting low-probability, high-impact events.</li><li>Why superforecasters’ estimates of catastrophic risks seem so much lower than experts’, and which group Ezra puts the most weight on.</li><li>The specific underlying disagreements that superforecasters and experts had about how likely catastrophic risks from AI are.</li><li>Why Ezra thinks forecasting tournaments can help build consensus on complex topics, and what he wants to do differently in future tournaments and studies.</li><li>Recent advances in the science of forecasting and the areas Ezra is most excited about exploring next.</li><li>Whether large language models could help or outperform human forecasters.</li><li>How people can improve their calibration and start making better forecasts personally.</li><li>Why Ezra thinks high-quality forecasts are relevant to policymakers, and whether they can really improve decision-making.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:07)</li><li>The interview begins (00:02:54)</li><li>The Existential Risk Persuasion Tournament (00:05:13)</li><li>Why is this project important? (00:12:34)</li><li>How was the tournament set up? (00:17:54)</li><li>Results from the tournament (00:22:38)</li><li>Risk from artificial intelligence (00:30:59)</li><li>How to think about these numbers (00:46:50)</li><li>Should we trust experts or superforecasters more? (00:49:16)</li><li>The effect of debate and persuasion (01:02:10)</li><li>Forecasts from the general public (01:08:33)</li><li>How can we improve people’s forecasts? (01:18:59)</li><li>Incentives and recruitment (01:26:30)</li><li>Criticisms of the tournament (01:33:51)</li><li>AI adversarial collaboration (01:46:20)</li><li>Hypotheses about stark differences in views of AI risk (01:51:41)</li><li>Cruxes and different worldviews (02:17:15)</li><li>Ezra’s experience as a superforecaster (02:28:57)</li><li>Forecasting as a research field (02:31:00)</li><li>Can large language models help or outperform human forecasters? (02:35:01)</li><li>Is forecasting valuable in the real world? (02:39:11)</li><li>Ezra’s book recommendations (02:45:29)</li><li>Luisa's outro (02:47:54)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Dominic Armstrong, Ben Cordell, Milo McGuire, and Simon Monsour<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 04 Sep 2024 19:17:18 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e1e518e3/d1693a4f.mp3" length="162653200" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/j4ZYgGr4S8CoJJNTFSA2ItNpEinEwP4hif70xuhjNg8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mY2I0/YzE4MTkyY2JlODFh/NWRkMmQ1NmFjYWM4/MjdmZi5qcGc.jpg"/>
      <itunes:duration>10164</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"It’s very hard to find examples where people say, 'I’m starting from this point. I’m starting from this belief.' So we wanted to make that very legible to people. We wanted to say, 'Experts think this; accurate forecasters think this.' They might both be wrong, but we can at least start from here and figure out where we’re coming into a discussion and say, 'I am much less concerned than the people in this report; or I am much more concerned, and I think people in this report were missing major things.' But if you don’t have a reference set of probabilities, I think it becomes much harder to talk about disagreement in policy debates in a space that’s so complicated like this." —Ezra Karger</p><p>In today’s episode, host Luisa Rodriguez speaks to Ezra Karger — research director at the <a href="https://forecastingresearch.org/">Forecasting Research Institute</a> — about FRI’s recent <a href="https://forecastingresearch.org/xpt">Existential Risk Persuasion Tournament</a> to come up with estimates of a range of catastrophic risks.</p><p><a href="https://80k.info/ek24"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How forecasting can improve our understanding of long-term catastrophic risks from things like AI, nuclear war, pandemics, and climate change.</li><li>What the Existential Risk Persuasion Tournament (XPT) is, how it was set up, and the results.</li><li>The challenges of predicting low-probability, high-impact events.</li><li>Why superforecasters’ estimates of catastrophic risks seem so much lower than experts’, and which group Ezra puts the most weight on.</li><li>The specific underlying disagreements that superforecasters and experts had about how likely catastrophic risks from AI are.</li><li>Why Ezra thinks forecasting tournaments can help build consensus on complex topics, and what he wants to do differently in future tournaments and studies.</li><li>Recent advances in the science of forecasting and the areas Ezra is most excited about exploring next.</li><li>Whether large language models could help or outperform human forecasters.</li><li>How people can improve their calibration and start making better forecasts personally.</li><li>Why Ezra thinks high-quality forecasts are relevant to policymakers, and whether they can really improve decision-making.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:07)</li><li>The interview begins (00:02:54)</li><li>The Existential Risk Persuasion Tournament (00:05:13)</li><li>Why is this project important? (00:12:34)</li><li>How was the tournament set up? (00:17:54)</li><li>Results from the tournament (00:22:38)</li><li>Risk from artificial intelligence (00:30:59)</li><li>How to think about these numbers (00:46:50)</li><li>Should we trust experts or superforecasters more? (00:49:16)</li><li>The effect of debate and persuasion (01:02:10)</li><li>Forecasts from the general public (01:08:33)</li><li>How can we improve people’s forecasts? (01:18:59)</li><li>Incentives and recruitment (01:26:30)</li><li>Criticisms of the tournament (01:33:51)</li><li>AI adversarial collaboration (01:46:20)</li><li>Hypotheses about stark differences in views of AI risk (01:51:41)</li><li>Cruxes and different worldviews (02:17:15)</li><li>Ezra’s experience as a superforecaster (02:28:57)</li><li>Forecasting as a research field (02:31:00)</li><li>Can large language models help or outperform human forecasters? (02:35:01)</li><li>Is forecasting valuable in the real world? (02:39:11)</li><li>Ezra’s book recommendations (02:45:29)</li><li>Luisa's outro (02:47:54)</li></ul><p><br></p><p><em>Producer: Keiran Harris<br>Audio engineering: Dominic Armstrong, Ben Cordell, Milo McGuire, and Simon Monsour<br>Content editing: Luisa Rodriguez, Katy Moore, and Keiran Harris<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/e1e518e3/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/e1e518e3/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#199 – Nathan Calvin on California’s AI bill SB 1047 and its potential to shape US AI policy</title>
      <itunes:title>#199 – Nathan Calvin on California’s AI bill SB 1047 and its potential to shape US AI policy</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">7cfaf406-0311-4d98-a767-443eb61a9287</guid>
      <link>https://80000hours.org/podcast/episodes/nathan-calvin-sb-1047-california-ai-safety-bill/?utm_campaign=podcast__nathan-calvin&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"I do think that there is a really significant sentiment among parts of the opposition that it’s not really just that this bill itself is that bad or extreme — when you really drill into it, it feels like one of those things where you read it and it’s like, '<em>This </em>is the thing that everyone is screaming about?' I think it’s a pretty modest bill in a lot of ways, but I think part of what they are thinking is that this is the first step to shutting down AI development. Or that if California does this, then lots of other states are going to do it, and we need to really slam the door shut on model-level regulation or else they’re just going to keep going. </p><p>"I think that is like a lot of what the sentiment here is: it’s less about, in some ways, the details of this specific bill, and more about the sense that they want this to stop here, and they’re worried that if they give an inch that there will continue to be other things in the future. And I don’t think that is going to be tolerable to the public in the long run. I think it’s a bad choice, but I think that is the calculus that they are making." —Nathan Calvin</p><p>In today’s episode, host Luisa Rodriguez speaks to Nathan Calvin — senior policy counsel at the <a href="https://www.safe.ai/">Center for AI Safety</a> Action Fund — about the new AI safety bill in California, SB 1047, which he’s helped shape as it’s moved through the state legislature.</p><p><a href="https://80k.info/nc"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What’s actually in SB 1047, and which AI models it would apply to.</li><li>The most common objections to the bill — including how it could affect competition, startups, open source models, and US national security — and which of these objections Nathan thinks hold water.</li><li>What Nathan sees as the biggest misunderstandings about the bill that get in the way of good public discourse about it.</li><li>Why some AI companies are opposed to SB 1047, despite claiming that they want the industry to be regulated.</li><li>How the bill is different from Biden’s executive order on AI and voluntary commitments made by AI companies.</li><li>Why California is taking state-level action rather than waiting for federal regulation.</li><li>How state-level regulations can be hugely impactful at national and global scales, and how listeners could get involved in state-level work to make a real difference on lots of pressing problems.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>The interview begins (00:02:30)</li><li>What risks from AI does SB 1047 try to address? (00:03:10)</li><li>Supporters and critics of the bill (00:11:03)</li><li>Misunderstandings about the bill (00:24:07)</li><li>Competition, open source, and liability concerns (00:30:56)</li><li>Model size thresholds (00:46:24)</li><li>How is SB 1047 different from the executive order? (00:55:36)</li><li>Objections Nathan is sympathetic to (00:58:31)</li><li>Current status of the bill (01:02:57)</li><li>How can listeners get involved in work like this? (01:05:00)</li><li>Luisa's outro (01:11:52)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"I do think that there is a really significant sentiment among parts of the opposition that it’s not really just that this bill itself is that bad or extreme — when you really drill into it, it feels like one of those things where you read it and it’s like, '<em>This </em>is the thing that everyone is screaming about?' I think it’s a pretty modest bill in a lot of ways, but I think part of what they are thinking is that this is the first step to shutting down AI development. Or that if California does this, then lots of other states are going to do it, and we need to really slam the door shut on model-level regulation or else they’re just going to keep going. </p><p>"I think that is like a lot of what the sentiment here is: it’s less about, in some ways, the details of this specific bill, and more about the sense that they want this to stop here, and they’re worried that if they give an inch that there will continue to be other things in the future. And I don’t think that is going to be tolerable to the public in the long run. I think it’s a bad choice, but I think that is the calculus that they are making." —Nathan Calvin</p><p>In today’s episode, host Luisa Rodriguez speaks to Nathan Calvin — senior policy counsel at the <a href="https://www.safe.ai/">Center for AI Safety</a> Action Fund — about the new AI safety bill in California, SB 1047, which he’s helped shape as it’s moved through the state legislature.</p><p><a href="https://80k.info/nc"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What’s actually in SB 1047, and which AI models it would apply to.</li><li>The most common objections to the bill — including how it could affect competition, startups, open source models, and US national security — and which of these objections Nathan thinks hold water.</li><li>What Nathan sees as the biggest misunderstandings about the bill that get in the way of good public discourse about it.</li><li>Why some AI companies are opposed to SB 1047, despite claiming that they want the industry to be regulated.</li><li>How the bill is different from Biden’s executive order on AI and voluntary commitments made by AI companies.</li><li>Why California is taking state-level action rather than waiting for federal regulation.</li><li>How state-level regulations can be hugely impactful at national and global scales, and how listeners could get involved in state-level work to make a real difference on lots of pressing problems.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>The interview begins (00:02:30)</li><li>What risks from AI does SB 1047 try to address? (00:03:10)</li><li>Supporters and critics of the bill (00:11:03)</li><li>Misunderstandings about the bill (00:24:07)</li><li>Competition, open source, and liability concerns (00:30:56)</li><li>Model size thresholds (00:46:24)</li><li>How is SB 1047 different from the executive order? (00:55:36)</li><li>Objections Nathan is sympathetic to (00:58:31)</li><li>Current status of the bill (01:02:57)</li><li>How can listeners get involved in work like this? (01:05:00)</li><li>Luisa's outro (01:11:52)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 29 Aug 2024 19:12:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e7d1ee3f/3410adcb.mp3" length="69768442" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ajShsUvOqkOPV9FRijzTRafdz3KPh6GyDEOU8DBVx-k/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS80ODI0/NDllOGIxMjk3ZDk1/ZDc0OTI1YjZjNWVj/NDQ1Yy5qcGVn.jpg"/>
      <itunes:duration>4357</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"I do think that there is a really significant sentiment among parts of the opposition that it’s not really just that this bill itself is that bad or extreme — when you really drill into it, it feels like one of those things where you read it and it’s like, '<em>This </em>is the thing that everyone is screaming about?' I think it’s a pretty modest bill in a lot of ways, but I think part of what they are thinking is that this is the first step to shutting down AI development. Or that if California does this, then lots of other states are going to do it, and we need to really slam the door shut on model-level regulation or else they’re just going to keep going. </p><p>"I think that is like a lot of what the sentiment here is: it’s less about, in some ways, the details of this specific bill, and more about the sense that they want this to stop here, and they’re worried that if they give an inch that there will continue to be other things in the future. And I don’t think that is going to be tolerable to the public in the long run. I think it’s a bad choice, but I think that is the calculus that they are making." —Nathan Calvin</p><p>In today’s episode, host Luisa Rodriguez speaks to Nathan Calvin — senior policy counsel at the <a href="https://www.safe.ai/">Center for AI Safety</a> Action Fund — about the new AI safety bill in California, SB 1047, which he’s helped shape as it’s moved through the state legislature.</p><p><a href="https://80k.info/nc"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>What’s actually in SB 1047, and which AI models it would apply to.</li><li>The most common objections to the bill — including how it could affect competition, startups, open source models, and US national security — and which of these objections Nathan thinks hold water.</li><li>What Nathan sees as the biggest misunderstandings about the bill that get in the way of good public discourse about it.</li><li>Why some AI companies are opposed to SB 1047, despite claiming that they want the industry to be regulated.</li><li>How the bill is different from Biden’s executive order on AI and voluntary commitments made by AI companies.</li><li>Why California is taking state-level action rather than waiting for federal regulation.</li><li>How state-level regulations can be hugely impactful at national and global scales, and how listeners could get involved in state-level work to make a real difference on lots of pressing problems.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:00:57)</li><li>The interview begins (00:02:30)</li><li>What risks from AI does SB 1047 try to address? (00:03:10)</li><li>Supporters and critics of the bill (00:11:03)</li><li>Misunderstandings about the bill (00:24:07)</li><li>Competition, open source, and liability concerns (00:30:56)</li><li>Model size thresholds (00:46:24)</li><li>How is SB 1047 different from the executive order? (00:55:36)</li><li>Objections Nathan is sympathetic to (00:58:31)</li><li>Current status of the bill (01:02:57)</li><li>How can listeners get involved in work like this? (01:05:00)</li><li>Luisa's outro (01:11:52)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/e7d1ee3f/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/e7d1ee3f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#198 – Meghan Barrett on upending everything you thought you knew about bugs in 3 hours</title>
      <itunes:title>#198 – Meghan Barrett on upending everything you thought you knew about bugs in 3 hours</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">70df4d0f-76fd-4e55-aa9d-6d1cd92000a6</guid>
      <link>https://80000hours.org/podcast/episodes/meghan-barrett-insect-pain-consciousness-sentience/?utm_campaign=podcast__meghan-barrett&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"This is a group of animals I think people are particularly unfamiliar with. They are especially poorly covered in our science curriculum; they are especially poorly understood, because people don’t spend as much time learning about them at museums; and they’re just harder to spend time with in a lot of ways, I think, for people. So people have pets that are vertebrates that they take care of across the taxonomic groups, and people get familiar with those from going to zoos and watching their behaviours there, and watching nature documentaries and more. But I think the insects are still really underappreciated, and that means that our intuitions are probably more likely to be wrong than with those other groups." —Meghan Barrett</p><p>In today’s episode, host Luisa Rodriguez speaks to Meghan Barrett — insect neurobiologist and physiologist at Indiana University Indianapolis and founding director of the <a href="https://www.insectwelfare.com/">Insect Welfare Research Society</a> — about her work to understand insects’ potential capacity for suffering, and what that might mean for how humans currently farm and use insects. If you're interested in getting involved with this work, check out Meghan's recent blog post: <a href="https://www.meghan-barrett.com/latest-news/get-involved-in-insect-welfare">I’m into insect welfare! What’s next?</a></p><p><a href="https://80k.info/mb"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The scale of potential insect suffering in the wild, on farms, and in labs.</li><li>Examples from cutting-edge insect research, like how depression- and anxiety-like states can be induced in fruit flies and successfully treated with human antidepressants.</li><li>How size bias might help explain why many people assume insects can’t feel pain.</li><li>Practical solutions that Meghan’s team is working on to improve farmed insect welfare, such as standard operating procedures for more humane slaughter methods.</li><li>Challenges facing the nascent field of insect welfare research, and where the main research gaps are.</li><li>Meghan’s personal story of how she went from being sceptical of insect pain to working as an insect welfare scientist, and her advice for others who want to improve the lives of insects.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:02)</li><li>The interview begins (00:03:06)</li><li>What is an insect? (00:03:22)</li><li>Size diversity (00:07:24)</li><li>How important is brain size for sentience? (00:11:27)</li><li>Offspring, parental investment, and lifespan (00:19:00)</li><li>Cognition and behaviour (00:23:23)</li><li>The scale of insect suffering (00:27:01)</li><li>Capacity to suffer (00:35:56)</li><li>The empirical evidence for whether insects can feel pain (00:47:18)</li><li>Nociceptors (01:00:02)</li><li>Integrated nociception (01:08:39)</li><li>Response to analgesia (01:16:17)</li><li>Analgesia preference (01:25:57)</li><li>Flexible self-protective behaviour (01:31:19)</li><li>Motivational tradeoffs and associative learning (01:38:45)</li><li>Results (01:43:31)</li><li>Reasons to be sceptical (01:47:18)</li><li>Meghan’s probability of sentience in insects (02:10:20)</li><li>Views of the broader entomologist community (02:18:18)</li><li>Insect farming (02:26:52)</li><li>How much to worry about insect farming (02:40:56)</li><li>Inhumane slaughter and disease in insect farms (02:44:45)</li><li>Inadequate nutrition, density, and photophobia (02:53:50)</li><li>Most humane ways to kill insects at home (03:01:33)</li><li>Challenges in researching this (03:07:53)</li><li>Most promising reforms (03:18:44)</li><li>Why Meghan is hopeful about working with the industry (03:22:17)</li><li>Careers (03:34:08)</li><li>Insect Welfare Research Society (03:37:16)</li><li>Luisa's outro (03:47:01)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"This is a group of animals I think people are particularly unfamiliar with. They are especially poorly covered in our science curriculum; they are especially poorly understood, because people don’t spend as much time learning about them at museums; and they’re just harder to spend time with in a lot of ways, I think, for people. So people have pets that are vertebrates that they take care of across the taxonomic groups, and people get familiar with those from going to zoos and watching their behaviours there, and watching nature documentaries and more. But I think the insects are still really underappreciated, and that means that our intuitions are probably more likely to be wrong than with those other groups." —Meghan Barrett</p><p>In today’s episode, host Luisa Rodriguez speaks to Meghan Barrett — insect neurobiologist and physiologist at Indiana University Indianapolis and founding director of the <a href="https://www.insectwelfare.com/">Insect Welfare Research Society</a> — about her work to understand insects’ potential capacity for suffering, and what that might mean for how humans currently farm and use insects. If you're interested in getting involved with this work, check out Meghan's recent blog post: <a href="https://www.meghan-barrett.com/latest-news/get-involved-in-insect-welfare">I’m into insect welfare! What’s next?</a></p><p><a href="https://80k.info/mb"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The scale of potential insect suffering in the wild, on farms, and in labs.</li><li>Examples from cutting-edge insect research, like how depression- and anxiety-like states can be induced in fruit flies and successfully treated with human antidepressants.</li><li>How size bias might help explain why many people assume insects can’t feel pain.</li><li>Practical solutions that Meghan’s team is working on to improve farmed insect welfare, such as standard operating procedures for more humane slaughter methods.</li><li>Challenges facing the nascent field of insect welfare research, and where the main research gaps are.</li><li>Meghan’s personal story of how she went from being sceptical of insect pain to working as an insect welfare scientist, and her advice for others who want to improve the lives of insects.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:02)</li><li>The interview begins (00:03:06)</li><li>What is an insect? (00:03:22)</li><li>Size diversity (00:07:24)</li><li>How important is brain size for sentience? (00:11:27)</li><li>Offspring, parental investment, and lifespan (00:19:00)</li><li>Cognition and behaviour (00:23:23)</li><li>The scale of insect suffering (00:27:01)</li><li>Capacity to suffer (00:35:56)</li><li>The empirical evidence for whether insects can feel pain (00:47:18)</li><li>Nociceptors (01:00:02)</li><li>Integrated nociception (01:08:39)</li><li>Response to analgesia (01:16:17)</li><li>Analgesia preference (01:25:57)</li><li>Flexible self-protective behaviour (01:31:19)</li><li>Motivational tradeoffs and associative learning (01:38:45)</li><li>Results (01:43:31)</li><li>Reasons to be sceptical (01:47:18)</li><li>Meghan’s probability of sentience in insects (02:10:20)</li><li>Views of the broader entomologist community (02:18:18)</li><li>Insect farming (02:26:52)</li><li>How much to worry about insect farming (02:40:56)</li><li>Inhumane slaughter and disease in insect farms (02:44:45)</li><li>Inadequate nutrition, density, and photophobia (02:53:50)</li><li>Most humane ways to kill insects at home (03:01:33)</li><li>Challenges in researching this (03:07:53)</li><li>Most promising reforms (03:18:44)</li><li>Why Meghan is hopeful about working with the industry (03:22:17)</li><li>Careers (03:34:08)</li><li>Insect Welfare Research Society (03:37:16)</li><li>Luisa's outro (03:47:01)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 26 Aug 2024 20:27:19 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ab6ce227/59052293.mp3" length="219112987" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/IOjE61tisgaUkkvjDjZr-ofARUako74SHfec8jq2gMw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS83MTEz/OTUyZTRiMTZmYjMz/ZGI3ZDg0ZDQyOTMx/MmE4NC5qcGVn.jpg"/>
      <itunes:duration>13692</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"This is a group of animals I think people are particularly unfamiliar with. They are especially poorly covered in our science curriculum; they are especially poorly understood, because people don’t spend as much time learning about them at museums; and they’re just harder to spend time with in a lot of ways, I think, for people. So people have pets that are vertebrates that they take care of across the taxonomic groups, and people get familiar with those from going to zoos and watching their behaviours there, and watching nature documentaries and more. But I think the insects are still really underappreciated, and that means that our intuitions are probably more likely to be wrong than with those other groups." —Meghan Barrett</p><p>In today’s episode, host Luisa Rodriguez speaks to Meghan Barrett — insect neurobiologist and physiologist at Indiana University Indianapolis and founding director of the <a href="https://www.insectwelfare.com/">Insect Welfare Research Society</a> — about her work to understand insects’ potential capacity for suffering, and what that might mean for how humans currently farm and use insects. If you're interested in getting involved with this work, check out Meghan's recent blog post: <a href="https://www.meghan-barrett.com/latest-news/get-involved-in-insect-welfare">I’m into insect welfare! What’s next?</a></p><p><a href="https://80k.info/mb"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The scale of potential insect suffering in the wild, on farms, and in labs.</li><li>Examples from cutting-edge insect research, like how depression- and anxiety-like states can be induced in fruit flies and successfully treated with human antidepressants.</li><li>How size bias might help explain why many people assume insects can’t feel pain.</li><li>Practical solutions that Meghan’s team is working on to improve farmed insect welfare, such as standard operating procedures for more humane slaughter methods.</li><li>Challenges facing the nascent field of insect welfare research, and where the main research gaps are.</li><li>Meghan’s personal story of how she went from being sceptical of insect pain to working as an insect welfare scientist, and her advice for others who want to improve the lives of insects.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:02)</li><li>The interview begins (00:03:06)</li><li>What is an insect? (00:03:22)</li><li>Size diversity (00:07:24)</li><li>How important is brain size for sentience? (00:11:27)</li><li>Offspring, parental investment, and lifespan (00:19:00)</li><li>Cognition and behaviour (00:23:23)</li><li>The scale of insect suffering (00:27:01)</li><li>Capacity to suffer (00:35:56)</li><li>The empirical evidence for whether insects can feel pain (00:47:18)</li><li>Nociceptors (01:00:02)</li><li>Integrated nociception (01:08:39)</li><li>Response to analgesia (01:16:17)</li><li>Analgesia preference (01:25:57)</li><li>Flexible self-protective behaviour (01:31:19)</li><li>Motivational tradeoffs and associative learning (01:38:45)</li><li>Results (01:43:31)</li><li>Reasons to be sceptical (01:47:18)</li><li>Meghan’s probability of sentience in insects (02:10:20)</li><li>Views of the broader entomologist community (02:18:18)</li><li>Insect farming (02:26:52)</li><li>How much to worry about insect farming (02:40:56)</li><li>Inhumane slaughter and disease in insect farms (02:44:45)</li><li>Inadequate nutrition, density, and photophobia (02:53:50)</li><li>Most humane ways to kill insects at home (03:01:33)</li><li>Challenges in researching this (03:07:53)</li><li>Most promising reforms (03:18:44)</li><li>Why Meghan is hopeful about working with the industry (03:22:17)</li><li>Careers (03:34:08)</li><li>Insect Welfare Research Society (03:37:16)</li><li>Luisa's outro (03:47:01)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/ab6ce227/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/ab6ce227/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#197 – Nick Joseph on whether Anthropic's AI safety policy is up to the task</title>
      <itunes:title>#197 – Nick Joseph on whether Anthropic's AI safety policy is up to the task</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">48229270-a9eb-4550-bb78-a80af600eef2</guid>
      <link>https://80000hours.org/podcast/episodes/nick-joseph-anthropic-safety-approach-responsible-scaling/?utm_campaign=podcast__nick-joseph&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>The three biggest AI companies — <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Anthropic</a>, <a href="https://openai.com/preparedness/">OpenAI</a>, and <a href="https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/">DeepMind</a> — have now all released policies designed to make their AI models less likely to go rogue or cause catastrophic damage as they approach, and eventually exceed, human capabilities. Are they good enough?</p><p>That’s what host Rob Wiblin tries to hash out in this interview (recorded May 30) with Nick Joseph — one of the original cofounders of Anthropic, its current head of training, and a big fan of Anthropic’s “responsible scaling policy” (or “RSP”). Anthropic is the most safety focused of the AI companies, known for a culture that treats the risks of its work as deadly serious.</p><p><a href="https://80k.info/nj"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>As Nick explains, these scaling policies commit companies to dig into what new dangerous things a model can do — after it’s trained, but before it’s in wide use. The companies then promise to put in place safeguards they think are sufficient to tackle those capabilities before availability is extended further. For instance, if a model could significantly help design a deadly bioweapon, then its <a href="https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/">weights need to be properly secured</a> so they can’t be stolen by terrorists interested in using it that way.</p><p>As capabilities grow further — for example, if testing shows that a model could exfiltrate itself and spread autonomously in the wild — then new measures would need to be put in place to make that impossible, or demonstrate that such a goal can never arise.</p><p>Nick points out what he sees as the biggest virtues of the RSP approach, and then Rob pushes him on some of the best objections he’s found to RSPs being up to the task of keeping AI safe and beneficial. The two also discuss whether it's essential to eventually hand over operation of responsible scaling policies to external auditors or regulatory bodies, if those policies are going to be able to hold up against the intense commercial pressures that might end up arrayed against them.</p><p>In addition to all of that, Nick and Rob talk about:</p><ul><li>What Nick thinks are the current bottlenecks in AI progress: people and time (rather than data or compute).</li><li>What it’s like working in AI safety research at the leading edge, and whether pushing forward capabilities (even in the name of safety) is a good idea.</li><li>What it’s like working at Anthropic, and how to get the skills needed to help with the safe development of AI.</li></ul><p>And as a reminder, if you want to let us know your reaction to this interview, or send any other feedback, our inbox is always open at <a href="mailto:podcast@80000hours.org">podcast@80000hours.org</a>.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:00)</li><li>The interview begins (00:03:44)</li><li>Scaling laws (00:04:12)</li><li>Bottlenecks to further progress in making AIs helpful (00:08:36)</li><li>Anthropic’s responsible scaling policies (00:14:21)</li><li>Pros and cons of the RSP approach for AI safety (00:34:09)</li><li>Alternatives to RSPs (00:46:44)</li><li>Is an internal audit really the best approach? (00:51:56)</li><li>Making promises about things that are currently technically impossible (01:07:54)</li><li>Nick’s biggest reservations about the RSP approach (01:16:05)</li><li>Communicating “acceptable” risk (01:19:27)</li><li>Should Anthropic’s RSP have wider safety buffers? (01:26:13)</li><li>Other impacts on society and future work on RSPs (01:34:01)</li><li>Working at Anthropic (01:36:28)</li><li>Engineering vs research (01:41:04)</li><li>AI safety roles at Anthropic (01:48:31)</li><li>Should concerned people be willing to take capabilities roles? (01:58:20)</li><li>Recent safety work at Anthropic (02:10:05)</li><li>Anthropic culture (02:14:35)</li><li>Overrated and underrated AI applications (02:22:06)</li><li>Rob’s outro (02:26:36)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video engineering: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The three biggest AI companies — <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Anthropic</a>, <a href="https://openai.com/preparedness/">OpenAI</a>, and <a href="https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/">DeepMind</a> — have now all released policies designed to make their AI models less likely to go rogue or cause catastrophic damage as they approach, and eventually exceed, human capabilities. Are they good enough?</p><p>That’s what host Rob Wiblin tries to hash out in this interview (recorded May 30) with Nick Joseph — one of the original cofounders of Anthropic, its current head of training, and a big fan of Anthropic’s “responsible scaling policy” (or “RSP”). Anthropic is the most safety focused of the AI companies, known for a culture that treats the risks of its work as deadly serious.</p><p><a href="https://80k.info/nj"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>As Nick explains, these scaling policies commit companies to dig into what new dangerous things a model can do — after it’s trained, but before it’s in wide use. The companies then promise to put in place safeguards they think are sufficient to tackle those capabilities before availability is extended further. For instance, if a model could significantly help design a deadly bioweapon, then its <a href="https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/">weights need to be properly secured</a> so they can’t be stolen by terrorists interested in using it that way.</p><p>As capabilities grow further — for example, if testing shows that a model could exfiltrate itself and spread autonomously in the wild — then new measures would need to be put in place to make that impossible, or demonstrate that such a goal can never arise.</p><p>Nick points out what he sees as the biggest virtues of the RSP approach, and then Rob pushes him on some of the best objections he’s found to RSPs being up to the task of keeping AI safe and beneficial. The two also discuss whether it's essential to eventually hand over operation of responsible scaling policies to external auditors or regulatory bodies, if those policies are going to be able to hold up against the intense commercial pressures that might end up arrayed against them.</p><p>In addition to all of that, Nick and Rob talk about:</p><ul><li>What Nick thinks are the current bottlenecks in AI progress: people and time (rather than data or compute).</li><li>What it’s like working in AI safety research at the leading edge, and whether pushing forward capabilities (even in the name of safety) is a good idea.</li><li>What it’s like working at Anthropic, and how to get the skills needed to help with the safe development of AI.</li></ul><p>And as a reminder, if you want to let us know your reaction to this interview, or send any other feedback, our inbox is always open at <a href="mailto:podcast@80000hours.org">podcast@80000hours.org</a>.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:00)</li><li>The interview begins (00:03:44)</li><li>Scaling laws (00:04:12)</li><li>Bottlenecks to further progress in making AIs helpful (00:08:36)</li><li>Anthropic’s responsible scaling policies (00:14:21)</li><li>Pros and cons of the RSP approach for AI safety (00:34:09)</li><li>Alternatives to RSPs (00:46:44)</li><li>Is an internal audit really the best approach? (00:51:56)</li><li>Making promises about things that are currently technically impossible (01:07:54)</li><li>Nick’s biggest reservations about the RSP approach (01:16:05)</li><li>Communicating “acceptable” risk (01:19:27)</li><li>Should Anthropic’s RSP have wider safety buffers? (01:26:13)</li><li>Other impacts on society and future work on RSPs (01:34:01)</li><li>Working at Anthropic (01:36:28)</li><li>Engineering vs research (01:41:04)</li><li>AI safety roles at Anthropic (01:48:31)</li><li>Should concerned people be willing to take capabilities roles? (01:58:20)</li><li>Recent safety work at Anthropic (02:10:05)</li><li>Anthropic culture (02:14:35)</li><li>Overrated and underrated AI applications (02:22:06)</li><li>Rob’s outro (02:26:36)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video engineering: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 22 Aug 2024 13:55:28 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/fc9c29ef/974d24f9.mp3" length="143487323" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/FnCzl7WLVf7CNoNGKXlGlq2wkH9hFnCQIWfzwIqNqPA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8yNWJi/YmJhZTIyOTEwZjlm/NjJiYTA5NTVhMDc3/OTdmMi5qcGVn.jpg"/>
      <itunes:duration>8966</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>The three biggest AI companies — <a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">Anthropic</a>, <a href="https://openai.com/preparedness/">OpenAI</a>, and <a href="https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/">DeepMind</a> — have now all released policies designed to make their AI models less likely to go rogue or cause catastrophic damage as they approach, and eventually exceed, human capabilities. Are they good enough?</p><p>That’s what host Rob Wiblin tries to hash out in this interview (recorded May 30) with Nick Joseph — one of the original cofounders of Anthropic, its current head of training, and a big fan of Anthropic’s “responsible scaling policy” (or “RSP”). Anthropic is the most safety focused of the AI companies, known for a culture that treats the risks of its work as deadly serious.</p><p><a href="https://80k.info/nj"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>As Nick explains, these scaling policies commit companies to dig into what new dangerous things a model can do — after it’s trained, but before it’s in wide use. The companies then promise to put in place safeguards they think are sufficient to tackle those capabilities before availability is extended further. For instance, if a model could significantly help design a deadly bioweapon, then its <a href="https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/">weights need to be properly secured</a> so they can’t be stolen by terrorists interested in using it that way.</p><p>As capabilities grow further — for example, if testing shows that a model could exfiltrate itself and spread autonomously in the wild — then new measures would need to be put in place to make that impossible, or demonstrate that such a goal can never arise.</p><p>Nick points out what he sees as the biggest virtues of the RSP approach, and then Rob pushes him on some of the best objections he’s found to RSPs being up to the task of keeping AI safe and beneficial. The two also discuss whether it's essential to eventually hand over operation of responsible scaling policies to external auditors or regulatory bodies, if those policies are going to be able to hold up against the intense commercial pressures that might end up arrayed against them.</p><p>In addition to all of that, Nick and Rob talk about:</p><ul><li>What Nick thinks are the current bottlenecks in AI progress: people and time (rather than data or compute).</li><li>What it’s like working in AI safety research at the leading edge, and whether pushing forward capabilities (even in the name of safety) is a good idea.</li><li>What it’s like working at Anthropic, and how to get the skills needed to help with the safe development of AI.</li></ul><p>And as a reminder, if you want to let us know your reaction to this interview, or send any other feedback, our inbox is always open at <a href="mailto:podcast@80000hours.org">podcast@80000hours.org</a>.</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:00)</li><li>The interview begins (00:03:44)</li><li>Scaling laws (00:04:12)</li><li>Bottlenecks to further progress in making AIs helpful (00:08:36)</li><li>Anthropic’s responsible scaling policies (00:14:21)</li><li>Pros and cons of the RSP approach for AI safety (00:34:09)</li><li>Alternatives to RSPs (00:46:44)</li><li>Is an internal audit really the best approach? (00:51:56)</li><li>Making promises about things that are currently technically impossible (01:07:54)</li><li>Nick’s biggest reservations about the RSP approach (01:16:05)</li><li>Communicating “acceptable” risk (01:19:27)</li><li>Should Anthropic’s RSP have wider safety buffers? (01:26:13)</li><li>Other impacts on society and future work on RSPs (01:34:01)</li><li>Working at Anthropic (01:36:28)</li><li>Engineering vs research (01:41:04)</li><li>AI safety roles at Anthropic (01:48:31)</li><li>Should concerned people be willing to take capabilities roles? (01:58:20)</li><li>Recent safety work at Anthropic (02:10:05)</li><li>Anthropic culture (02:14:35)</li><li>Overrated and underrated AI applications (02:22:06)</li><li>Rob’s outro (02:26:36)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Video engineering: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/fc9c29ef/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/fc9c29ef/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#196 – Jonathan Birch on the edge cases of sentience and why they matter</title>
      <itunes:title>#196 – Jonathan Birch on the edge cases of sentience and why they matter</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3b38ff35-3cb7-4035-9c73-513a7f084cf8</guid>
      <link>https://80000hours.org/podcast/episodes/jonathan-birch-edge-sentience-uncertainty/?utm_campaign=podcast__jonathan-birch&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"In the 1980s, it was still apparently common to perform surgery on newborn babies without anaesthetic on both sides of the Atlantic. This led to appalling cases, and to public outcry, and to campaigns to change clinical practice. And as soon as [some courageous scientists] looked for evidence, it showed that this practice was completely indefensible and then the clinical practice was changed. People don’t need convincing anymore that we should take newborn human babies seriously as sentience candidates. But the tale is a useful cautionary tale, because it shows you how deep that overconfidence can run and how problematic it can be. It just underlines this point that overconfidence about sentience is everywhere and is dangerous." —Jonathan Birch</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://personal.lse.ac.uk/birchj1/">Dr Jonathan Birch</a> — philosophy professor at the London School of Economics — about his new book, <a href="https://www.edgeofsentience.com/"><em>The Edge of Sentience: Risk and Precaution in Humans, Other Animals, and AI</em></a>. (Check out the <a href="https://academic.oup.com/book/57949">free PDF version</a>!)</p><p><a href="https://80k.info/jb"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>Candidates for sentience, such as humans with consciousness disorders, foetuses, neural organoids, invertebrates, and AIs</li><li>Humanity’s history of acting as if we’re sure that such beings are incapable of having subjective experiences — and why Jonathan thinks that that certainty is completely unjustified.</li><li>Chilling tales about overconfident policies that probably caused significant suffering for decades.</li><li>How policymakers can act ethically given real uncertainty.</li><li>Whether simulating the brain of the roundworm C. elegans or Drosophila (aka fruit flies) would create minds equally sentient to the biological versions.</li><li>How new technologies like brain organoids could replace animal testing, and how big the risk is that they could be sentient too.</li><li>Why Jonathan is so excited about citizens’ assemblies.</li><li>Jonathan’s conversation with the Dalai Lama about whether insects are sentient.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:20)</li><li>The interview begins (00:03:04)</li><li>Why does sentience matter? (00:03:31)</li><li>Inescapable uncertainty about other minds (00:05:43)</li><li>The “zone of reasonable disagreement” in sentience research (00:10:31)</li><li>Disorders of consciousness: comas and minimally conscious states (00:17:06)</li><li>Foetuses and the cautionary tale of newborn pain (00:43:23)</li><li>Neural organoids (00:55:49)</li><li>AI sentience and whole brain emulation (01:06:17)</li><li>Policymaking at the edge of sentience (01:28:09)</li><li>Citizens’ assemblies (01:31:13)</li><li>The UK’s Sentience Act (01:39:45)</li><li>Ways Jonathan has changed his mind (01:47:26)</li><li>Careers (01:54:54)</li><li>Discussing animal sentience with the Dalai Lama (01:59:08)</li><li>Luisa’s outro (02:01:04)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"In the 1980s, it was still apparently common to perform surgery on newborn babies without anaesthetic on both sides of the Atlantic. This led to appalling cases, and to public outcry, and to campaigns to change clinical practice. And as soon as [some courageous scientists] looked for evidence, it showed that this practice was completely indefensible and then the clinical practice was changed. People don’t need convincing anymore that we should take newborn human babies seriously as sentience candidates. But the tale is a useful cautionary tale, because it shows you how deep that overconfidence can run and how problematic it can be. It just underlines this point that overconfidence about sentience is everywhere and is dangerous." —Jonathan Birch</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://personal.lse.ac.uk/birchj1/">Dr Jonathan Birch</a> — philosophy professor at the London School of Economics — about his new book, <a href="https://www.edgeofsentience.com/"><em>The Edge of Sentience: Risk and Precaution in Humans, Other Animals, and AI</em></a>. (Check out the <a href="https://academic.oup.com/book/57949">free PDF version</a>!)</p><p><a href="https://80k.info/jb"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>Candidates for sentience, such as humans with consciousness disorders, foetuses, neural organoids, invertebrates, and AIs</li><li>Humanity’s history of acting as if we’re sure that such beings are incapable of having subjective experiences — and why Jonathan thinks that that certainty is completely unjustified.</li><li>Chilling tales about overconfident policies that probably caused significant suffering for decades.</li><li>How policymakers can act ethically given real uncertainty.</li><li>Whether simulating the brain of the roundworm C. elegans or Drosophila (aka fruit flies) would create minds equally sentient to the biological versions.</li><li>How new technologies like brain organoids could replace animal testing, and how big the risk is that they could be sentient too.</li><li>Why Jonathan is so excited about citizens’ assemblies.</li><li>Jonathan’s conversation with the Dalai Lama about whether insects are sentient.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:20)</li><li>The interview begins (00:03:04)</li><li>Why does sentience matter? (00:03:31)</li><li>Inescapable uncertainty about other minds (00:05:43)</li><li>The “zone of reasonable disagreement” in sentience research (00:10:31)</li><li>Disorders of consciousness: comas and minimally conscious states (00:17:06)</li><li>Foetuses and the cautionary tale of newborn pain (00:43:23)</li><li>Neural organoids (00:55:49)</li><li>AI sentience and whole brain emulation (01:06:17)</li><li>Policymaking at the edge of sentience (01:28:09)</li><li>Citizens’ assemblies (01:31:13)</li><li>The UK’s Sentience Act (01:39:45)</li><li>Ways Jonathan has changed his mind (01:47:26)</li><li>Careers (01:54:54)</li><li>Discussing animal sentience with the Dalai Lama (01:59:08)</li><li>Luisa’s outro (02:01:04)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 15 Aug 2024 23:04:03 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/9619d442/cac5e049.mp3" length="117012387" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/79p3i3QqZ4W6oIe65npzm0KLztTheFFe4tUWNlbnZtg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xY2E1/YjNkOTIwYmQxZjA0/NjQ4MzRhYWI0OTI0/YzQyYS5qcGc.jpg"/>
      <itunes:duration>7310</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"In the 1980s, it was still apparently common to perform surgery on newborn babies without anaesthetic on both sides of the Atlantic. This led to appalling cases, and to public outcry, and to campaigns to change clinical practice. And as soon as [some courageous scientists] looked for evidence, it showed that this practice was completely indefensible and then the clinical practice was changed. People don’t need convincing anymore that we should take newborn human babies seriously as sentience candidates. But the tale is a useful cautionary tale, because it shows you how deep that overconfidence can run and how problematic it can be. It just underlines this point that overconfidence about sentience is everywhere and is dangerous." —Jonathan Birch</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://personal.lse.ac.uk/birchj1/">Dr Jonathan Birch</a> — philosophy professor at the London School of Economics — about his new book, <a href="https://www.edgeofsentience.com/"><em>The Edge of Sentience: Risk and Precaution in Humans, Other Animals, and AI</em></a>. (Check out the <a href="https://academic.oup.com/book/57949">free PDF version</a>!)</p><p><a href="https://80k.info/jb"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>Candidates for sentience, such as humans with consciousness disorders, foetuses, neural organoids, invertebrates, and AIs</li><li>Humanity’s history of acting as if we’re sure that such beings are incapable of having subjective experiences — and why Jonathan thinks that that certainty is completely unjustified.</li><li>Chilling tales about overconfident policies that probably caused significant suffering for decades.</li><li>How policymakers can act ethically given real uncertainty.</li><li>Whether simulating the brain of the roundworm C. elegans or Drosophila (aka fruit flies) would create minds equally sentient to the biological versions.</li><li>How new technologies like brain organoids could replace animal testing, and how big the risk is that they could be sentient too.</li><li>Why Jonathan is so excited about citizens’ assemblies.</li><li>Jonathan’s conversation with the Dalai Lama about whether insects are sentient.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:20)</li><li>The interview begins (00:03:04)</li><li>Why does sentience matter? (00:03:31)</li><li>Inescapable uncertainty about other minds (00:05:43)</li><li>The “zone of reasonable disagreement” in sentience research (00:10:31)</li><li>Disorders of consciousness: comas and minimally conscious states (00:17:06)</li><li>Foetuses and the cautionary tale of newborn pain (00:43:23)</li><li>Neural organoids (00:55:49)</li><li>AI sentience and whole brain emulation (01:06:17)</li><li>Policymaking at the edge of sentience (01:28:09)</li><li>Citizens’ assemblies (01:31:13)</li><li>The UK’s Sentience Act (01:39:45)</li><li>Ways Jonathan has changed his mind (01:47:26)</li><li>Careers (01:54:54)</li><li>Discussing animal sentience with the Dalai Lama (01:59:08)</li><li>Luisa’s outro (02:01:04)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering by Ben Cordell, Milo McGuire, Simon Monsour, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/9619d442/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/9619d442/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#195 – Sella Nevo on who's trying to steal frontier AI models, and what they could do with them</title>
      <itunes:title>#195 – Sella Nevo on who's trying to steal frontier AI models, and what they could do with them</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1e3df990-cf8d-4ef2-ab04-697d95be3e83</guid>
      <link>https://80000hours.org/podcast/episodes/sella-nevo-securing-ai-model-weights/?utm_campaign=podcast__sella-nevo&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Computational systems have literally millions of physical and conceptual components, and around 98% of them are embedded into your infrastructure without you ever having heard of them. And an inordinate amount of them can lead to a catastrophic failure of your security assumptions. And because of this, the Iranian secret nuclear programme failed to prevent a breach, most US agencies failed to prevent multiple breaches, most US national security agencies failed to prevent breaches. So ensuring your system is truly secure against highly resourced and dedicated attackers is really, really hard." —Sella Nevo</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://www.sellanevo.com/">Sella Nevo</a> — director of the Meselson Center at RAND — about <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html">his team’s latest report</a> on how to protect the model weights of frontier AI models from actors who might want to steal them.</p><p><a href="https://80k.info/sn"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Real-world examples of sophisticated security breaches, and what we can learn from them.</li><li>Why AI model weights might be such a high-value target for adversaries like hackers, rogue states, and other bad actors.</li><li>The many ways that model weights could be stolen, from using human insiders to sophisticated supply chain hacks.</li><li>The current best practices in cybersecurity, and why they may not be enough to keep bad actors away.</li><li>New security measures that Sella hopes can mitigate with the growing risks.</li><li>Sella’s work using machine learning for flood forecasting, which has significantly reduced injuries and costs from floods across Africa and Asia.</li><li>And plenty more.</li></ul><p>Also, RAND is currently hiring for roles in technical and policy information security — <a href="https://www.rand.org/jobs.html">check them out</a> if you're interested in this field! </p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:56)</li><li>The interview begins (00:02:30)</li><li>The importance of securing the model weights of frontier AI models (00:03:01)</li><li>The most sophisticated and surprising security breaches (00:10:22)</li><li>AI models being leaked (00:25:52)</li><li>Researching for the RAND report (00:30:11)</li><li>Who tries to steal model weights? (00:32:21)</li><li>Malicious code and exploiting zero-days (00:42:06)</li><li>Human insiders (00:53:20)</li><li>Side-channel attacks (01:04:11)</li><li>Getting access to air-gapped networks (01:10:52)</li><li>Model extraction (01:19:47)</li><li>Reducing and hardening authorised access (01:38:52)</li><li>Confidential computing (01:48:05)</li><li>Red-teaming and security testing (01:53:42)</li><li>Careers in information security (01:59:54)</li><li>Sella’s work on flood forecasting systems (02:01:57)</li><li>Luisa’s outro (02:04:51)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Computational systems have literally millions of physical and conceptual components, and around 98% of them are embedded into your infrastructure without you ever having heard of them. And an inordinate amount of them can lead to a catastrophic failure of your security assumptions. And because of this, the Iranian secret nuclear programme failed to prevent a breach, most US agencies failed to prevent multiple breaches, most US national security agencies failed to prevent breaches. So ensuring your system is truly secure against highly resourced and dedicated attackers is really, really hard." —Sella Nevo</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://www.sellanevo.com/">Sella Nevo</a> — director of the Meselson Center at RAND — about <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html">his team’s latest report</a> on how to protect the model weights of frontier AI models from actors who might want to steal them.</p><p><a href="https://80k.info/sn"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Real-world examples of sophisticated security breaches, and what we can learn from them.</li><li>Why AI model weights might be such a high-value target for adversaries like hackers, rogue states, and other bad actors.</li><li>The many ways that model weights could be stolen, from using human insiders to sophisticated supply chain hacks.</li><li>The current best practices in cybersecurity, and why they may not be enough to keep bad actors away.</li><li>New security measures that Sella hopes can mitigate with the growing risks.</li><li>Sella’s work using machine learning for flood forecasting, which has significantly reduced injuries and costs from floods across Africa and Asia.</li><li>And plenty more.</li></ul><p>Also, RAND is currently hiring for roles in technical and policy information security — <a href="https://www.rand.org/jobs.html">check them out</a> if you're interested in this field! </p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:56)</li><li>The interview begins (00:02:30)</li><li>The importance of securing the model weights of frontier AI models (00:03:01)</li><li>The most sophisticated and surprising security breaches (00:10:22)</li><li>AI models being leaked (00:25:52)</li><li>Researching for the RAND report (00:30:11)</li><li>Who tries to steal model weights? (00:32:21)</li><li>Malicious code and exploiting zero-days (00:42:06)</li><li>Human insiders (00:53:20)</li><li>Side-channel attacks (01:04:11)</li><li>Getting access to air-gapped networks (01:10:52)</li><li>Model extraction (01:19:47)</li><li>Reducing and hardening authorised access (01:38:52)</li><li>Confidential computing (01:48:05)</li><li>Red-teaming and security testing (01:53:42)</li><li>Careers in information security (01:59:54)</li><li>Sella’s work on flood forecasting systems (02:01:57)</li><li>Luisa’s outro (02:04:51)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 01 Aug 2024 18:37:55 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c1b476ec/6bd9093b.mp3" length="123388769" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/muhZ9GaqU4Hba0ldBXx7DvcfY19Doc-TQravEiWvoh4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9iMmRh/ZjllMGRhZjlkNzdk/MDBkNmJjMTRmYzVl/MWQ3Yi5wbmc.jpg"/>
      <itunes:duration>7709</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Computational systems have literally millions of physical and conceptual components, and around 98% of them are embedded into your infrastructure without you ever having heard of them. And an inordinate amount of them can lead to a catastrophic failure of your security assumptions. And because of this, the Iranian secret nuclear programme failed to prevent a breach, most US agencies failed to prevent multiple breaches, most US national security agencies failed to prevent breaches. So ensuring your system is truly secure against highly resourced and dedicated attackers is really, really hard." —Sella Nevo</p><p>In today’s episode, host Luisa Rodriguez speaks to <a href="https://www.sellanevo.com/">Sella Nevo</a> — director of the Meselson Center at RAND — about <a href="https://www.rand.org/pubs/research_reports/RRA2849-1.html">his team’s latest report</a> on how to protect the model weights of frontier AI models from actors who might want to steal them.</p><p><a href="https://80k.info/sn"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Real-world examples of sophisticated security breaches, and what we can learn from them.</li><li>Why AI model weights might be such a high-value target for adversaries like hackers, rogue states, and other bad actors.</li><li>The many ways that model weights could be stolen, from using human insiders to sophisticated supply chain hacks.</li><li>The current best practices in cybersecurity, and why they may not be enough to keep bad actors away.</li><li>New security measures that Sella hopes can mitigate with the growing risks.</li><li>Sella’s work using machine learning for flood forecasting, which has significantly reduced injuries and costs from floods across Africa and Asia.</li><li>And plenty more.</li></ul><p>Also, RAND is currently hiring for roles in technical and policy information security — <a href="https://www.rand.org/jobs.html">check them out</a> if you're interested in this field! </p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:56)</li><li>The interview begins (00:02:30)</li><li>The importance of securing the model weights of frontier AI models (00:03:01)</li><li>The most sophisticated and surprising security breaches (00:10:22)</li><li>AI models being leaked (00:25:52)</li><li>Researching for the RAND report (00:30:11)</li><li>Who tries to steal model weights? (00:32:21)</li><li>Malicious code and exploiting zero-days (00:42:06)</li><li>Human insiders (00:53:20)</li><li>Side-channel attacks (01:04:11)</li><li>Getting access to air-gapped networks (01:10:52)</li><li>Model extraction (01:19:47)</li><li>Reducing and hardening authorised access (01:38:52)</li><li>Confidential computing (01:48:05)</li><li>Red-teaming and security testing (01:53:42)</li><li>Careers in information security (01:59:54)</li><li>Sella’s work on flood forecasting systems (02:01:57)</li><li>Luisa’s outro (02:04:51)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/c1b476ec/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/c1b476ec/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#194 – Vitalik Buterin on defensive acceleration and how to regulate AI when you fear government</title>
      <itunes:title>#194 – Vitalik Buterin on defensive acceleration and how to regulate AI when you fear government</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">16321f2a-1563-496a-a196-fb579d4cd43f</guid>
      <link>https://80000hours.org/podcast/episodes/vitalik-buterin-techno-optimism/?utm_campaign=podcast__vitalik-buterin&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"If you’re a power that is an island and that goes by sea, then you’re more likely to do things like valuing freedom, being democratic, being pro-foreigner, being open-minded, being interested in trade. If you are on the Mongolian steppes, then your entire mindset is kill or be killed, conquer or be conquered … the breeding ground for basically everything that all of us consider to be dystopian governance. If you want more utopian governance and less dystopian governance, then find ways to basically change the landscape, to try to make the world look more like mountains and rivers and less like the Mongolian steppes." —Vitalik Buterin</p><p>Can ‘effective accelerationists’ and AI ‘doomers’ agree on a common philosophy of technology? Common sense says no. But programmer and Ethereum cofounder Vitalik Buterin showed otherwise with his essay “<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">My techno-optimism</a>,” which both camps agreed was basically reasonable.</p><p><a href="https://80k.info/vb24"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>Seeing his social circle divided and fighting, Vitalik hoped to write a careful synthesis of the best ideas from both the optimists and the apprehensive.</p><p>Accelerationists are right: most technologies leave us better off, the human cost of delaying further advances can be dreadful, and centralising control in government hands often ends disastrously.</p><p>But the fearful are also right: some technologies are important exceptions, AGI has an unusually high chance of being one of those, and there are options to advance AI in safer directions.</p><p>The upshot? <strong>Defensive acceleration</strong>: humanity should run boldly but also intelligently into the future — speeding up technology to get its benefits, but preferentially developing ‘defensive’ technologies that lower systemic risks, permit safe decentralisation of power, and help both individuals and countries defend themselves against aggression and domination.</p><p><em>Entrepreneur First is running a defensive acceleration incubation programme with $250,000 of investment. If these ideas resonate with you, </em><a href="https://www.joinef.com/posts/introducing-def-acc-at-ef/"><em>learn about the programme</em></a><em> and </em><a href="https://apply.joinef.com/app/80k/"><em>apply by August 2, 2024</em></a><em>. You don’t need a business idea yet — just the hustle to start a technology company.</em></p><p>In addition to all of that, host Rob Wiblin and Vitalik discuss:</p><ul><li>AI regulation disagreements being less about AI in particular, and more whether you’re typically more scared of anarchy or totalitarianism.</li><li>Vitalik’s updated p(doom).</li><li>Whether the social impact of blockchain and crypto has been a disappointment.</li><li>Whether humans can merge with AI, and if that’s even desirable.</li><li>The most valuable defensive technologies to accelerate.</li><li>How to trustlessly identify what everyone will agree is misinformation</li><li>Whether AGI is offence-dominant or defence-dominant.</li><li>Vitalik’s updated take on effective altruism.</li><li>Plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:00:56)</li><li>The interview begins (00:04:47)</li><li>Three different views on technology (00:05:46)</li><li>Vitalik’s updated probability of doom (00:09:25)</li><li>Technology is amazing, and AI is fundamentally different from other tech (00:15:55)</li><li>Fear of totalitarianism and finding middle ground (00:22:44)</li><li>Should AI be more centralised or more decentralised? (00:42:20)</li><li>Humans merging with AIs to remain relevant (01:06:59)</li><li>Vitalik’s “d/acc” alternative (01:18:48)</li><li>Biodefence (01:24:01)</li><li>Pushback on Vitalik’s vision (01:37:09)</li><li>How much do people actually disagree? (01:42:14)</li><li>Cybersecurity (01:47:28)</li><li>Information defence (02:01:44)</li><li>Is AI more offence-dominant or defence-dominant? (02:21:00)</li><li>How Vitalik communicates among different camps (02:25:44)</li><li>Blockchain applications with social impact (02:34:37)</li><li>Rob’s outro (03:01:00)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"If you’re a power that is an island and that goes by sea, then you’re more likely to do things like valuing freedom, being democratic, being pro-foreigner, being open-minded, being interested in trade. If you are on the Mongolian steppes, then your entire mindset is kill or be killed, conquer or be conquered … the breeding ground for basically everything that all of us consider to be dystopian governance. If you want more utopian governance and less dystopian governance, then find ways to basically change the landscape, to try to make the world look more like mountains and rivers and less like the Mongolian steppes." —Vitalik Buterin</p><p>Can ‘effective accelerationists’ and AI ‘doomers’ agree on a common philosophy of technology? Common sense says no. But programmer and Ethereum cofounder Vitalik Buterin showed otherwise with his essay “<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">My techno-optimism</a>,” which both camps agreed was basically reasonable.</p><p><a href="https://80k.info/vb24"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>Seeing his social circle divided and fighting, Vitalik hoped to write a careful synthesis of the best ideas from both the optimists and the apprehensive.</p><p>Accelerationists are right: most technologies leave us better off, the human cost of delaying further advances can be dreadful, and centralising control in government hands often ends disastrously.</p><p>But the fearful are also right: some technologies are important exceptions, AGI has an unusually high chance of being one of those, and there are options to advance AI in safer directions.</p><p>The upshot? <strong>Defensive acceleration</strong>: humanity should run boldly but also intelligently into the future — speeding up technology to get its benefits, but preferentially developing ‘defensive’ technologies that lower systemic risks, permit safe decentralisation of power, and help both individuals and countries defend themselves against aggression and domination.</p><p><em>Entrepreneur First is running a defensive acceleration incubation programme with $250,000 of investment. If these ideas resonate with you, </em><a href="https://www.joinef.com/posts/introducing-def-acc-at-ef/"><em>learn about the programme</em></a><em> and </em><a href="https://apply.joinef.com/app/80k/"><em>apply by August 2, 2024</em></a><em>. You don’t need a business idea yet — just the hustle to start a technology company.</em></p><p>In addition to all of that, host Rob Wiblin and Vitalik discuss:</p><ul><li>AI regulation disagreements being less about AI in particular, and more whether you’re typically more scared of anarchy or totalitarianism.</li><li>Vitalik’s updated p(doom).</li><li>Whether the social impact of blockchain and crypto has been a disappointment.</li><li>Whether humans can merge with AI, and if that’s even desirable.</li><li>The most valuable defensive technologies to accelerate.</li><li>How to trustlessly identify what everyone will agree is misinformation</li><li>Whether AGI is offence-dominant or defence-dominant.</li><li>Vitalik’s updated take on effective altruism.</li><li>Plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:00:56)</li><li>The interview begins (00:04:47)</li><li>Three different views on technology (00:05:46)</li><li>Vitalik’s updated probability of doom (00:09:25)</li><li>Technology is amazing, and AI is fundamentally different from other tech (00:15:55)</li><li>Fear of totalitarianism and finding middle ground (00:22:44)</li><li>Should AI be more centralised or more decentralised? (00:42:20)</li><li>Humans merging with AIs to remain relevant (01:06:59)</li><li>Vitalik’s “d/acc” alternative (01:18:48)</li><li>Biodefence (01:24:01)</li><li>Pushback on Vitalik’s vision (01:37:09)</li><li>How much do people actually disagree? (01:42:14)</li><li>Cybersecurity (01:47:28)</li><li>Information defence (02:01:44)</li><li>Is AI more offence-dominant or defence-dominant? (02:21:00)</li><li>How Vitalik communicates among different camps (02:25:44)</li><li>Blockchain applications with social impact (02:34:37)</li><li>Rob’s outro (03:01:00)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 26 Jul 2024 20:03:10 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/da9a1128/d62612b2.mp3" length="176967620" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/FTw12oMMk6jzCRALGeXc9rqsUqzbM0fceZ0d6PfIX04/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xOWIy/NDY1N2Y0OWQ2NjA0/ZjI0MDY1ZDA4ZWE0/ZGQ0OC5wbmc.jpg"/>
      <itunes:duration>11058</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"If you’re a power that is an island and that goes by sea, then you’re more likely to do things like valuing freedom, being democratic, being pro-foreigner, being open-minded, being interested in trade. If you are on the Mongolian steppes, then your entire mindset is kill or be killed, conquer or be conquered … the breeding ground for basically everything that all of us consider to be dystopian governance. If you want more utopian governance and less dystopian governance, then find ways to basically change the landscape, to try to make the world look more like mountains and rivers and less like the Mongolian steppes." —Vitalik Buterin</p><p>Can ‘effective accelerationists’ and AI ‘doomers’ agree on a common philosophy of technology? Common sense says no. But programmer and Ethereum cofounder Vitalik Buterin showed otherwise with his essay “<a href="https://vitalik.eth.limo/general/2023/11/27/techno_optimism.html">My techno-optimism</a>,” which both camps agreed was basically reasonable.</p><p><a href="https://80k.info/vb24"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>Seeing his social circle divided and fighting, Vitalik hoped to write a careful synthesis of the best ideas from both the optimists and the apprehensive.</p><p>Accelerationists are right: most technologies leave us better off, the human cost of delaying further advances can be dreadful, and centralising control in government hands often ends disastrously.</p><p>But the fearful are also right: some technologies are important exceptions, AGI has an unusually high chance of being one of those, and there are options to advance AI in safer directions.</p><p>The upshot? <strong>Defensive acceleration</strong>: humanity should run boldly but also intelligently into the future — speeding up technology to get its benefits, but preferentially developing ‘defensive’ technologies that lower systemic risks, permit safe decentralisation of power, and help both individuals and countries defend themselves against aggression and domination.</p><p><em>Entrepreneur First is running a defensive acceleration incubation programme with $250,000 of investment. If these ideas resonate with you, </em><a href="https://www.joinef.com/posts/introducing-def-acc-at-ef/"><em>learn about the programme</em></a><em> and </em><a href="https://apply.joinef.com/app/80k/"><em>apply by August 2, 2024</em></a><em>. You don’t need a business idea yet — just the hustle to start a technology company.</em></p><p>In addition to all of that, host Rob Wiblin and Vitalik discuss:</p><ul><li>AI regulation disagreements being less about AI in particular, and more whether you’re typically more scared of anarchy or totalitarianism.</li><li>Vitalik’s updated p(doom).</li><li>Whether the social impact of blockchain and crypto has been a disappointment.</li><li>Whether humans can merge with AI, and if that’s even desirable.</li><li>The most valuable defensive technologies to accelerate.</li><li>How to trustlessly identify what everyone will agree is misinformation</li><li>Whether AGI is offence-dominant or defence-dominant.</li><li>Vitalik’s updated take on effective altruism.</li><li>Plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:00:56)</li><li>The interview begins (00:04:47)</li><li>Three different views on technology (00:05:46)</li><li>Vitalik’s updated probability of doom (00:09:25)</li><li>Technology is amazing, and AI is fundamentally different from other tech (00:15:55)</li><li>Fear of totalitarianism and finding middle ground (00:22:44)</li><li>Should AI be more centralised or more decentralised? (00:42:20)</li><li>Humans merging with AIs to remain relevant (01:06:59)</li><li>Vitalik’s “d/acc” alternative (01:18:48)</li><li>Biodefence (01:24:01)</li><li>Pushback on Vitalik’s vision (01:37:09)</li><li>How much do people actually disagree? (01:42:14)</li><li>Cybersecurity (01:47:28)</li><li>Information defence (02:01:44)</li><li>Is AI more offence-dominant or defence-dominant? (02:21:00)</li><li>How Vitalik communicates among different camps (02:25:44)</li><li>Blockchain applications with social impact (02:34:37)</li><li>Rob’s outro (03:01:00)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/da9a1128/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/da9a1128/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#193 – Sihao Huang on navigating the geopolitics of US–China AI competition</title>
      <itunes:title>#193 – Sihao Huang on navigating the geopolitics of US–China AI competition</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a10cc79d-a57a-4a27-bc0d-ea8f5daf6a5d</guid>
      <link>https://80000hours.org/podcast/episodes/sihao-huang-china-ai-capabilities/?utm_campaign=podcast__sihao-huang&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"You don’t necessarily need world-leading compute to create highly risky AI systems. The biggest biological design tools right now, like AlphaFold’s, are orders of magnitude smaller in terms of compute requirements than the frontier large language models. And China has the compute to train these systems. And if you’re, for instance, building a cyber agent or something that conducts cyberattacks, perhaps you also don’t need the general reasoning or mathematical ability of a large language model. You train on a much smaller subset of data. You fine-tune it on a smaller subset of data. And those systems — one, if China intentionally misuses them, and two, if they get proliferated because China just releases them as open source, or China does not have as comprehensive AI regulations — this could cause a lot of harm in the world." —Sihao Huang</p><p>In today’s episode, host Luisa Rodriguez speaks to Sihao Huang about his work on AI governance and tech policy in China, what’s happening on the ground in China in AI development and regulation, and the importance of US–China cooperation on AI governance.</p><p><a href="https://80k.info/sh24"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Whether the US and China are in an AI race, and the global implications if they are.</li><li>The state of the art of AI in China.</li><li>China’s response to American export controls, and whether China is on track to indigenise its semiconductor supply chain.</li><li>How China’s current AI regulations try to maintain a delicate balance between fostering innovation and keeping strict information control over the Chinese people.</li><li>Whether China’s extensive AI regulations signal real commitment to safety or just censorship — and how AI is already used in China for surveillance and authoritarian control.</li><li>How advancements in AI could reshape global power dynamics, and Sihao’s vision of international cooperation to manage this responsibly.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:02)</li><li>The interview begins (00:02:06)</li><li>Is China in an AI race with the West? (00:03:20)</li><li>How advanced is Chinese AI? (00:15:21)</li><li>Bottlenecks in Chinese AI development (00:22:30)</li><li>China and AI risks (00:27:41)</li><li>Information control and censorship (00:31:32)</li><li>AI safety research in China (00:36:31)</li><li>Could China be a source of catastrophic AI risk? (00:41:58)</li><li>AI enabling human rights abuses and undermining democracy (00:50:10)</li><li>China’s semiconductor industry (00:59:47)</li><li>China’s domestic AI governance landscape (01:29:22)</li><li>China’s international AI governance strategy (01:49:56)</li><li>Coordination (01:53:56)</li><li>Track two dialogues (02:03:04)</li><li>Misunderstandings Western actors have about Chinese approaches (02:07:34)</li><li>Complexity thinking (02:14:40)</li><li>Sihao’s pet bacteria hobby (02:20:34)</li><li>Luisa's outro (02:22:47)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"You don’t necessarily need world-leading compute to create highly risky AI systems. The biggest biological design tools right now, like AlphaFold’s, are orders of magnitude smaller in terms of compute requirements than the frontier large language models. And China has the compute to train these systems. And if you’re, for instance, building a cyber agent or something that conducts cyberattacks, perhaps you also don’t need the general reasoning or mathematical ability of a large language model. You train on a much smaller subset of data. You fine-tune it on a smaller subset of data. And those systems — one, if China intentionally misuses them, and two, if they get proliferated because China just releases them as open source, or China does not have as comprehensive AI regulations — this could cause a lot of harm in the world." —Sihao Huang</p><p>In today’s episode, host Luisa Rodriguez speaks to Sihao Huang about his work on AI governance and tech policy in China, what’s happening on the ground in China in AI development and regulation, and the importance of US–China cooperation on AI governance.</p><p><a href="https://80k.info/sh24"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Whether the US and China are in an AI race, and the global implications if they are.</li><li>The state of the art of AI in China.</li><li>China’s response to American export controls, and whether China is on track to indigenise its semiconductor supply chain.</li><li>How China’s current AI regulations try to maintain a delicate balance between fostering innovation and keeping strict information control over the Chinese people.</li><li>Whether China’s extensive AI regulations signal real commitment to safety or just censorship — and how AI is already used in China for surveillance and authoritarian control.</li><li>How advancements in AI could reshape global power dynamics, and Sihao’s vision of international cooperation to manage this responsibly.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:02)</li><li>The interview begins (00:02:06)</li><li>Is China in an AI race with the West? (00:03:20)</li><li>How advanced is Chinese AI? (00:15:21)</li><li>Bottlenecks in Chinese AI development (00:22:30)</li><li>China and AI risks (00:27:41)</li><li>Information control and censorship (00:31:32)</li><li>AI safety research in China (00:36:31)</li><li>Could China be a source of catastrophic AI risk? (00:41:58)</li><li>AI enabling human rights abuses and undermining democracy (00:50:10)</li><li>China’s semiconductor industry (00:59:47)</li><li>China’s domestic AI governance landscape (01:29:22)</li><li>China’s international AI governance strategy (01:49:56)</li><li>Coordination (01:53:56)</li><li>Track two dialogues (02:03:04)</li><li>Misunderstandings Western actors have about Chinese approaches (02:07:34)</li><li>Complexity thinking (02:14:40)</li><li>Sihao’s pet bacteria hobby (02:20:34)</li><li>Luisa's outro (02:22:47)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 18 Jul 2024 18:34:45 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f21a7ee5/f6ff7dc6.mp3" length="137850810" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/MHR3X1oz1pPnoQhPUvtDIynvlEZuRqtwQKAehihlfFA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9hYmEy/ZjZmNjAwMTQ1ZjVj/OTBlNGRkMTZiZDk1/MDc0MC5qcGc.jpg"/>
      <itunes:duration>8614</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"You don’t necessarily need world-leading compute to create highly risky AI systems. The biggest biological design tools right now, like AlphaFold’s, are orders of magnitude smaller in terms of compute requirements than the frontier large language models. And China has the compute to train these systems. And if you’re, for instance, building a cyber agent or something that conducts cyberattacks, perhaps you also don’t need the general reasoning or mathematical ability of a large language model. You train on a much smaller subset of data. You fine-tune it on a smaller subset of data. And those systems — one, if China intentionally misuses them, and two, if they get proliferated because China just releases them as open source, or China does not have as comprehensive AI regulations — this could cause a lot of harm in the world." —Sihao Huang</p><p>In today’s episode, host Luisa Rodriguez speaks to Sihao Huang about his work on AI governance and tech policy in China, what’s happening on the ground in China in AI development and regulation, and the importance of US–China cooperation on AI governance.</p><p><a href="https://80k.info/sh24"><strong>Links to learn more, highlights, video, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Whether the US and China are in an AI race, and the global implications if they are.</li><li>The state of the art of AI in China.</li><li>China’s response to American export controls, and whether China is on track to indigenise its semiconductor supply chain.</li><li>How China’s current AI regulations try to maintain a delicate balance between fostering innovation and keeping strict information control over the Chinese people.</li><li>Whether China’s extensive AI regulations signal real commitment to safety or just censorship — and how AI is already used in China for surveillance and authoritarian control.</li><li>How advancements in AI could reshape global power dynamics, and Sihao’s vision of international cooperation to manage this responsibly.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:02)</li><li>The interview begins (00:02:06)</li><li>Is China in an AI race with the West? (00:03:20)</li><li>How advanced is Chinese AI? (00:15:21)</li><li>Bottlenecks in Chinese AI development (00:22:30)</li><li>China and AI risks (00:27:41)</li><li>Information control and censorship (00:31:32)</li><li>AI safety research in China (00:36:31)</li><li>Could China be a source of catastrophic AI risk? (00:41:58)</li><li>AI enabling human rights abuses and undermining democracy (00:50:10)</li><li>China’s semiconductor industry (00:59:47)</li><li>China’s domestic AI governance landscape (01:29:22)</li><li>China’s international AI governance strategy (01:49:56)</li><li>Coordination (01:53:56)</li><li>Track two dialogues (02:03:04)</li><li>Misunderstandings Western actors have about Chinese approaches (02:07:34)</li><li>Complexity thinking (02:14:40)</li><li>Sihao’s pet bacteria hobby (02:20:34)</li><li>Luisa's outro (02:22:47)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f21a7ee5/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f21a7ee5/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#192 – Annie Jacobsen on what would happen if North Korea launched a nuclear weapon at the US</title>
      <itunes:title>#192 – Annie Jacobsen on what would happen if North Korea launched a nuclear weapon at the US</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f14bbc1b-e13a-4a33-baa2-7a9958f0e986</guid>
      <link>https://80000hours.org/podcast/episodes/annie-jacobsen-nuclear-catastrophe-escalation/?utm_campaign=podcast__annie-jacobsen&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Ring one: total annihilation; no cellular life remains. Ring two, another three-mile diameter out: everything is ablaze. Ring three, another three or five miles out on every side: third-degree burns among almost everyone. You are talking about people who may have gone down into the secret tunnels beneath Washington, DC, escaped from the Capitol and such: people are now broiling to death; people are dying from carbon monoxide poisoning; people who followed instructions and went into their basement are dying of suffocation. Everywhere there is death, everywhere there is fire.</p><p>"That iconic mushroom stem and cap that represents a nuclear blast — when a nuclear weapon has been exploded on a city — that stem and cap is made up of people. What is left over of people and of human civilisation." —Annie Jacobsen</p><p>In today’s episode, host Luisa Rodriguez speaks to Pulitzer Prize finalist and <em>New York Times</em> bestselling author Annie Jacobsen about her latest book, <a href="https://www.amazon.com/Nuclear-War-Scenario-Annie-Jacobsen/dp/0593476093"><em>Nuclear War: A Scenario</em></a>.</p><p><a href="https://80k.info/aj"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The most harrowing findings from Annie’s hundreds of hours of interviews with nuclear experts.</li><li>What happens during the window that the US president would have to decide about nuclear retaliation after hearing news of a possible nuclear attack.</li><li>The horrific humanitarian impacts on millions of innocent civilians from nuclear strikes.</li><li>The overlooked dangers of a nuclear-triggered electromagnetic pulse (EMP) attack crippling critical infrastructure within seconds.</li><li>How we’re on the razor’s edge between the logic of nuclear deterrence and catastrophe, and urgently need reforms to move away from hair-trigger alert nuclear postures.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:03)</li><li>The interview begins (00:02:28)</li><li>The first 24 minutes (00:02:59)</li><li>The Black Book and presidential advisors (00:13:35)</li><li>False alarms (00:40:43)</li><li>Russian misperception of US counterattack (00:44:50)</li><li>A narcissistic madman with a nuclear arsenal (01:00:13)</li><li>Is escalation inevitable? (01:02:53)</li><li>Firestorms and rings of annihilation (01:12:56)</li><li>Nuclear electromagnetic pulses (01:27:34)</li><li>Continuity of government (01:36:35)</li><li>Rays of hope (01:41:07)</li><li>Where we’re headed (01:43:52)</li><li>Avoiding politics (01:50:34)</li><li>Luisa’s outro (01:52:29)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Ring one: total annihilation; no cellular life remains. Ring two, another three-mile diameter out: everything is ablaze. Ring three, another three or five miles out on every side: third-degree burns among almost everyone. You are talking about people who may have gone down into the secret tunnels beneath Washington, DC, escaped from the Capitol and such: people are now broiling to death; people are dying from carbon monoxide poisoning; people who followed instructions and went into their basement are dying of suffocation. Everywhere there is death, everywhere there is fire.</p><p>"That iconic mushroom stem and cap that represents a nuclear blast — when a nuclear weapon has been exploded on a city — that stem and cap is made up of people. What is left over of people and of human civilisation." —Annie Jacobsen</p><p>In today’s episode, host Luisa Rodriguez speaks to Pulitzer Prize finalist and <em>New York Times</em> bestselling author Annie Jacobsen about her latest book, <a href="https://www.amazon.com/Nuclear-War-Scenario-Annie-Jacobsen/dp/0593476093"><em>Nuclear War: A Scenario</em></a>.</p><p><a href="https://80k.info/aj"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The most harrowing findings from Annie’s hundreds of hours of interviews with nuclear experts.</li><li>What happens during the window that the US president would have to decide about nuclear retaliation after hearing news of a possible nuclear attack.</li><li>The horrific humanitarian impacts on millions of innocent civilians from nuclear strikes.</li><li>The overlooked dangers of a nuclear-triggered electromagnetic pulse (EMP) attack crippling critical infrastructure within seconds.</li><li>How we’re on the razor’s edge between the logic of nuclear deterrence and catastrophe, and urgently need reforms to move away from hair-trigger alert nuclear postures.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:03)</li><li>The interview begins (00:02:28)</li><li>The first 24 minutes (00:02:59)</li><li>The Black Book and presidential advisors (00:13:35)</li><li>False alarms (00:40:43)</li><li>Russian misperception of US counterattack (00:44:50)</li><li>A narcissistic madman with a nuclear arsenal (01:00:13)</li><li>Is escalation inevitable? (01:02:53)</li><li>Firestorms and rings of annihilation (01:12:56)</li><li>Nuclear electromagnetic pulses (01:27:34)</li><li>Continuity of government (01:36:35)</li><li>Rays of hope (01:41:07)</li><li>Where we’re headed (01:43:52)</li><li>Avoiding politics (01:50:34)</li><li>Luisa’s outro (01:52:29)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 12 Jul 2024 17:25:49 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a1479275/5cee5e86.mp3" length="109853650" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/L1T9yq03z9DGmSkLWpptdHWOWVjQpG4B1bC8C40BI_Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mMGZj/Y2IwODQyYWMyYWE0/NzQ3MjJiMDQ4MDli/NjU3Yi5qcGc.jpg"/>
      <itunes:duration>6864</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Ring one: total annihilation; no cellular life remains. Ring two, another three-mile diameter out: everything is ablaze. Ring three, another three or five miles out on every side: third-degree burns among almost everyone. You are talking about people who may have gone down into the secret tunnels beneath Washington, DC, escaped from the Capitol and such: people are now broiling to death; people are dying from carbon monoxide poisoning; people who followed instructions and went into their basement are dying of suffocation. Everywhere there is death, everywhere there is fire.</p><p>"That iconic mushroom stem and cap that represents a nuclear blast — when a nuclear weapon has been exploded on a city — that stem and cap is made up of people. What is left over of people and of human civilisation." —Annie Jacobsen</p><p>In today’s episode, host Luisa Rodriguez speaks to Pulitzer Prize finalist and <em>New York Times</em> bestselling author Annie Jacobsen about her latest book, <a href="https://www.amazon.com/Nuclear-War-Scenario-Annie-Jacobsen/dp/0593476093"><em>Nuclear War: A Scenario</em></a>.</p><p><a href="https://80k.info/aj"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The most harrowing findings from Annie’s hundreds of hours of interviews with nuclear experts.</li><li>What happens during the window that the US president would have to decide about nuclear retaliation after hearing news of a possible nuclear attack.</li><li>The horrific humanitarian impacts on millions of innocent civilians from nuclear strikes.</li><li>The overlooked dangers of a nuclear-triggered electromagnetic pulse (EMP) attack crippling critical infrastructure within seconds.</li><li>How we’re on the razor’s edge between the logic of nuclear deterrence and catastrophe, and urgently need reforms to move away from hair-trigger alert nuclear postures.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:03)</li><li>The interview begins (00:02:28)</li><li>The first 24 minutes (00:02:59)</li><li>The Black Book and presidential advisors (00:13:35)</li><li>False alarms (00:40:43)</li><li>Russian misperception of US counterattack (00:44:50)</li><li>A narcissistic madman with a nuclear arsenal (01:00:13)</li><li>Is escalation inevitable? (01:02:53)</li><li>Firestorms and rings of annihilation (01:12:56)</li><li>Nuclear electromagnetic pulses (01:27:34)</li><li>Continuity of government (01:36:35)</li><li>Rays of hope (01:41:07)</li><li>Where we’re headed (01:43:52)</li><li>Avoiding politics (01:50:34)</li><li>Luisa’s outro (01:52:29)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/a1479275/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/a1479275/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#191 (Part 2) – Carl Shulman on government and society after AGI</title>
      <itunes:title>#191 (Part 2) – Carl Shulman on government and society after AGI</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2da92b1e-2b72-447b-a38c-a7747d001a88</guid>
      <link>https://80000hours.org/podcast/episodes/carl-shulman-society-agi/?utm_campaign=podcast__carl-shulman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p><strong><em>This is the second part of our marathon interview with Carl Shulman. The first episode is on </em></strong><a href="https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/"><strong><em>the economy and national security after AGI</em></strong></a><strong><em>. You can listen to them in either order!</em></strong></p><p>If we develop artificial general intelligence that's reasonably aligned with human goals, it could put a fast and near-free superhuman advisor in everyone's pocket. How would that affect culture, government, and our ability to act sensibly and coordinate together?</p><p>It's common to worry that AI advances will lead to a proliferation of misinformation and further disconnect us from reality. But in today's conversation, AI expert Carl Shulman argues that this underrates the powerful positive applications the technology could have in the public sphere.</p><p><a href="https://80k.info/cs242"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>As Carl explains, today the most important questions we face as a society remain in the "realm of subjective judgement" -- without any "robust, well-founded scientific consensus on how to answer them." But if AI 'evals' and interpretability advance to the point that it's possible to demonstrate which AI models have truly superhuman judgement and give consistently trustworthy advice, society could converge on firm or 'best-guess' answers to far more cases.</p><p>If the answers are publicly visible and confirmable by all, the pressure on officials to act on that advice could be great. </p><p>That's because when it's hard to assess if a line has been crossed or not, we usually give people much more discretion. For instance, a journalist inventing an interview that never happened will get fired because it's an unambiguous violation of honesty norms — but so long as there's no universally agreed-upon standard for selective reporting, that same journalist will have substantial discretion to report information that favours their preferred view more often than that which contradicts it.</p><p>Similarly, today we have no generally agreed-upon way to tell when a decision-maker has behaved irresponsibly. But if experience clearly shows that following AI advice is the wise move, not seeking or ignoring such advice could become more like crossing a red line — less like making an understandable mistake and more like fabricating your balance sheet.</p><p>To illustrate the possible impact, Carl imagines how the COVID pandemic could have played out in the presence of AI advisors that everyone agrees are exceedingly insightful and reliable. But in practice, a significantly superhuman AI might suggest novel approaches better than any we can suggest.</p><p>In the past we've usually found it easier to predict how hard technologies like planes or factories will change than to imagine the social shifts that those technologies will create — and the same is likely happening for AI.</p><p>Carl Shulman and host Rob Wiblin discuss the above, as well as:</p><ul><li>The risk of society using AI to lock in its values.</li><li>The difficulty of preventing coups once AI is key to the military and police.</li><li>What international treaties we need to make this go well.</li><li>How to make AI superhuman at forecasting the future.</li><li>Whether AI will be able to help us with intractable philosophical questions.</li><li>Whether we need dedicated projects to make wise AI advisors, or if it will happen automatically as models scale.</li><li>Why Carl doesn't support AI companies voluntarily pausing AI research, but sees a stronger case for binding international controls once we're closer to 'crunch time.'</li><li>Opportunities for listeners to contribute to making the future go well.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:16)</li><li>The interview begins (00:03:24)</li><li>COVID-19 concrete example (00:11:18)</li><li>Sceptical arguments against the effect of AI advisors (00:24:16)</li><li>Value lock-in (00:33:59)</li><li>How democracies avoid coups (00:48:08)</li><li>Where AI could most easily help (01:00:25)</li><li>AI forecasting (01:04:30)</li><li>Application to the most challenging topics (01:24:03)</li><li>How to make it happen (01:37:50)</li><li>International negotiations and coordination and auditing (01:43:54)</li><li>Opportunities for listeners (02:00:09)</li><li>Why Carl doesn't support enforced pauses on AI research (02:03:58)</li><li>How Carl is feeling about the future (02:15:47)</li><li>Rob’s outro (02:17:37)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris</em></p><p><em>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><strong><em>This is the second part of our marathon interview with Carl Shulman. The first episode is on </em></strong><a href="https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/"><strong><em>the economy and national security after AGI</em></strong></a><strong><em>. You can listen to them in either order!</em></strong></p><p>If we develop artificial general intelligence that's reasonably aligned with human goals, it could put a fast and near-free superhuman advisor in everyone's pocket. How would that affect culture, government, and our ability to act sensibly and coordinate together?</p><p>It's common to worry that AI advances will lead to a proliferation of misinformation and further disconnect us from reality. But in today's conversation, AI expert Carl Shulman argues that this underrates the powerful positive applications the technology could have in the public sphere.</p><p><a href="https://80k.info/cs242"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>As Carl explains, today the most important questions we face as a society remain in the "realm of subjective judgement" -- without any "robust, well-founded scientific consensus on how to answer them." But if AI 'evals' and interpretability advance to the point that it's possible to demonstrate which AI models have truly superhuman judgement and give consistently trustworthy advice, society could converge on firm or 'best-guess' answers to far more cases.</p><p>If the answers are publicly visible and confirmable by all, the pressure on officials to act on that advice could be great. </p><p>That's because when it's hard to assess if a line has been crossed or not, we usually give people much more discretion. For instance, a journalist inventing an interview that never happened will get fired because it's an unambiguous violation of honesty norms — but so long as there's no universally agreed-upon standard for selective reporting, that same journalist will have substantial discretion to report information that favours their preferred view more often than that which contradicts it.</p><p>Similarly, today we have no generally agreed-upon way to tell when a decision-maker has behaved irresponsibly. But if experience clearly shows that following AI advice is the wise move, not seeking or ignoring such advice could become more like crossing a red line — less like making an understandable mistake and more like fabricating your balance sheet.</p><p>To illustrate the possible impact, Carl imagines how the COVID pandemic could have played out in the presence of AI advisors that everyone agrees are exceedingly insightful and reliable. But in practice, a significantly superhuman AI might suggest novel approaches better than any we can suggest.</p><p>In the past we've usually found it easier to predict how hard technologies like planes or factories will change than to imagine the social shifts that those technologies will create — and the same is likely happening for AI.</p><p>Carl Shulman and host Rob Wiblin discuss the above, as well as:</p><ul><li>The risk of society using AI to lock in its values.</li><li>The difficulty of preventing coups once AI is key to the military and police.</li><li>What international treaties we need to make this go well.</li><li>How to make AI superhuman at forecasting the future.</li><li>Whether AI will be able to help us with intractable philosophical questions.</li><li>Whether we need dedicated projects to make wise AI advisors, or if it will happen automatically as models scale.</li><li>Why Carl doesn't support AI companies voluntarily pausing AI research, but sees a stronger case for binding international controls once we're closer to 'crunch time.'</li><li>Opportunities for listeners to contribute to making the future go well.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:16)</li><li>The interview begins (00:03:24)</li><li>COVID-19 concrete example (00:11:18)</li><li>Sceptical arguments against the effect of AI advisors (00:24:16)</li><li>Value lock-in (00:33:59)</li><li>How democracies avoid coups (00:48:08)</li><li>Where AI could most easily help (01:00:25)</li><li>AI forecasting (01:04:30)</li><li>Application to the most challenging topics (01:24:03)</li><li>How to make it happen (01:37:50)</li><li>International negotiations and coordination and auditing (01:43:54)</li><li>Opportunities for listeners (02:00:09)</li><li>Why Carl doesn't support enforced pauses on AI research (02:03:58)</li><li>How Carl is feeling about the future (02:15:47)</li><li>Rob’s outro (02:17:37)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris</em></p><p><em>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 05 Jul 2024 18:19:31 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4f5863e2/8998dce0.mp3" length="134955717" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/E2726kJlHa2BlgJP2Gfaoj9vsN7OSclDoGsM__9RclQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8yMDk5/MTRlOGY2MjA4OWQz/Mjc3M2UyYzAzZmIw/OTQ3MC5qcGc.jpg"/>
      <itunes:duration>8432</itunes:duration>
      <itunes:summary>
        <![CDATA[<p><strong><em>This is the second part of our marathon interview with Carl Shulman. The first episode is on </em></strong><a href="https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/"><strong><em>the economy and national security after AGI</em></strong></a><strong><em>. You can listen to them in either order!</em></strong></p><p>If we develop artificial general intelligence that's reasonably aligned with human goals, it could put a fast and near-free superhuman advisor in everyone's pocket. How would that affect culture, government, and our ability to act sensibly and coordinate together?</p><p>It's common to worry that AI advances will lead to a proliferation of misinformation and further disconnect us from reality. But in today's conversation, AI expert Carl Shulman argues that this underrates the powerful positive applications the technology could have in the public sphere.</p><p><a href="https://80k.info/cs242"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>As Carl explains, today the most important questions we face as a society remain in the "realm of subjective judgement" -- without any "robust, well-founded scientific consensus on how to answer them." But if AI 'evals' and interpretability advance to the point that it's possible to demonstrate which AI models have truly superhuman judgement and give consistently trustworthy advice, society could converge on firm or 'best-guess' answers to far more cases.</p><p>If the answers are publicly visible and confirmable by all, the pressure on officials to act on that advice could be great. </p><p>That's because when it's hard to assess if a line has been crossed or not, we usually give people much more discretion. For instance, a journalist inventing an interview that never happened will get fired because it's an unambiguous violation of honesty norms — but so long as there's no universally agreed-upon standard for selective reporting, that same journalist will have substantial discretion to report information that favours their preferred view more often than that which contradicts it.</p><p>Similarly, today we have no generally agreed-upon way to tell when a decision-maker has behaved irresponsibly. But if experience clearly shows that following AI advice is the wise move, not seeking or ignoring such advice could become more like crossing a red line — less like making an understandable mistake and more like fabricating your balance sheet.</p><p>To illustrate the possible impact, Carl imagines how the COVID pandemic could have played out in the presence of AI advisors that everyone agrees are exceedingly insightful and reliable. But in practice, a significantly superhuman AI might suggest novel approaches better than any we can suggest.</p><p>In the past we've usually found it easier to predict how hard technologies like planes or factories will change than to imagine the social shifts that those technologies will create — and the same is likely happening for AI.</p><p>Carl Shulman and host Rob Wiblin discuss the above, as well as:</p><ul><li>The risk of society using AI to lock in its values.</li><li>The difficulty of preventing coups once AI is key to the military and police.</li><li>What international treaties we need to make this go well.</li><li>How to make AI superhuman at forecasting the future.</li><li>Whether AI will be able to help us with intractable philosophical questions.</li><li>Whether we need dedicated projects to make wise AI advisors, or if it will happen automatically as models scale.</li><li>Why Carl doesn't support AI companies voluntarily pausing AI research, but sees a stronger case for binding international controls once we're closer to 'crunch time.'</li><li>Opportunities for listeners to contribute to making the future go well.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:16)</li><li>The interview begins (00:03:24)</li><li>COVID-19 concrete example (00:11:18)</li><li>Sceptical arguments against the effect of AI advisors (00:24:16)</li><li>Value lock-in (00:33:59)</li><li>How democracies avoid coups (00:48:08)</li><li>Where AI could most easily help (01:00:25)</li><li>AI forecasting (01:04:30)</li><li>Application to the most challenging topics (01:24:03)</li><li>How to make it happen (01:37:50)</li><li>International negotiations and coordination and auditing (01:43:54)</li><li>Opportunities for listeners (02:00:09)</li><li>Why Carl doesn't support enforced pauses on AI research (02:03:58)</li><li>How Carl is feeling about the future (02:15:47)</li><li>Rob’s outro (02:17:37)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris</em></p><p><em>Audio engineering team: Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/4f5863e2/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/4f5863e2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#191 (Part 1) – Carl Shulman on the economy and national security after AGI</title>
      <itunes:title>#191 (Part 1) – Carl Shulman on the economy and national security after AGI</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a65daf82-8b88-4d84-856d-6e421fcc6705</guid>
      <link>https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/?utm_campaign=podcast__carl-shulman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p><strong><em>This is the first part of our marathon interview with Carl Shulman. The second episode is on </em></strong><a href="https://80000hours.org/podcast/episodes/carl-shulman-society-agi/"><strong><em>government and society after AGI</em></strong></a><strong><em>. You can listen to them in either order!</em></strong></p><p>The human brain does what it does with a shockingly low energy supply: just 20 watts — a fraction of a cent worth of electricity per hour. What would happen if AI technology merely matched what evolution has already managed, and could accomplish the work of top human professionals given a 20-watt power supply?</p><p>Many people sort of consider that hypothetical, but maybe nobody has followed through and considered all the implications as much as Carl Shulman. Behind the scenes, his work has greatly influenced how leaders in artificial general intelligence (AGI) picture the world they're creating.</p><p><a href="https://80k.info/cs241"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>Carl simply follows the logic to its natural conclusion. This is a world where 1 cent of electricity can be turned into medical advice, company management, or scientific research that would today cost $100s, resulting in a scramble to manufacture chips and apply them to the most lucrative forms of intellectual labour.</p><p>It's a world where, given their incredible hourly salaries, the supply of outstanding AI researchers quickly goes from 10,000 to 10 million or more, enormously accelerating progress in the field.</p><p>It's a world where companies operated entirely by AIs working together are much faster and more cost-effective than those that lean on humans for decision making, and the latter are progressively driven out of business. </p><p>It's a world where the technical challenges around control of robots are rapidly overcome, leading to robots into strong, fast, precise, and tireless workers able to accomplish any physical work the economy requires, and a rush to build billions of them and cash in.</p><p>As the economy grows, each person could effectively afford the practical equivalent of a team of hundreds of machine 'people' to help them with every aspect of their lives.</p><p>And with growth rates this high, it doesn't take long to run up against Earth's physical limits — in this case, the toughest to engineer your way out of is the Earth's ability to release waste heat. If this machine economy and its insatiable demand for power generates more heat than the Earth radiates into space, then it will rapidly heat up and become uninhabitable for humans and other animals.</p><p>This creates pressure to move economic activity off-planet. So you could develop effective populations of billions of scientific researchers operating on computer chips orbiting in space, sending the results of their work, such as drug designs, back to Earth for use.</p><p>These are just some of the wild implications that could follow naturally from truly embracing the hypothetical: what if we develop AGI that could accomplish everything that the most productive humans can, using the same energy supply?</p><p>In today's episode, Carl explains the above, and then host Rob Wiblin pushes back on whether that’s realistic or just a cool story, asking:</p><ul><li>If we're heading towards the above, how come economic growth is slow now and not really increasing?</li><li>Why have computers and computer chips had so little effect on economic productivity so far?</li><li>Are self-replicating biological systems a good comparison for self-replicating machine systems?</li><li>Isn't this just too crazy and weird to be plausible?</li><li>What bottlenecks would be encountered in supplying energy and natural resources to this growing economy?</li><li>Might there not be severely declining returns to bigger brains and more training?</li><li>Wouldn't humanity get scared and pull the brakes if such a transformation kicked off?</li><li>If this is right, how come economists don't agree?</li></ul><p>Finally, Carl addresses the moral status of machine minds themselves. Would they be conscious or otherwise have a claim to moral or rights? And how might humans and machines coexist with neither side dominating or exploiting the other?</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:00)</li><li>Transitioning to a world where AI systems do almost all the work (00:05:21)</li><li>Economics after an AI explosion (00:14:25)</li><li>Objection: Shouldn’t we be seeing economic growth rates increasing today? (00:59:12)</li><li>Objection: Speed of doubling time (01:07:33)</li><li>Objection: Declining returns to increases in intelligence? (01:11:59)</li><li>Objection: Physical transformation of the environment (01:17:39)</li><li>Objection: Should we expect an increased demand for safety and security? (01:29:14)</li><li>Objection: “This sounds completely whack” (01:36:10)</li><li>Income and wealth distribution (01:48:02)</li><li>Economists and the intelligence explosion (02:13:31)</li><li>Baumol effect arguments (02:19:12)</li><li>Denying that robots can exist (02:27:18)</li><li>Classic economic growth models (02:36:12)</li><li>Robot nannies (02:48:27)</li><li>Slow integration of decision-making and authority power (02:57:39)</li><li>Economists’ mistaken heuristics (03:01:07)</li><li>Moral status of AIs (03:11:45)</li><li>Rob’s outro (04:11:47)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><strong><em>This is the first part of our marathon interview with Carl Shulman. The second episode is on </em></strong><a href="https://80000hours.org/podcast/episodes/carl-shulman-society-agi/"><strong><em>government and society after AGI</em></strong></a><strong><em>. You can listen to them in either order!</em></strong></p><p>The human brain does what it does with a shockingly low energy supply: just 20 watts — a fraction of a cent worth of electricity per hour. What would happen if AI technology merely matched what evolution has already managed, and could accomplish the work of top human professionals given a 20-watt power supply?</p><p>Many people sort of consider that hypothetical, but maybe nobody has followed through and considered all the implications as much as Carl Shulman. Behind the scenes, his work has greatly influenced how leaders in artificial general intelligence (AGI) picture the world they're creating.</p><p><a href="https://80k.info/cs241"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>Carl simply follows the logic to its natural conclusion. This is a world where 1 cent of electricity can be turned into medical advice, company management, or scientific research that would today cost $100s, resulting in a scramble to manufacture chips and apply them to the most lucrative forms of intellectual labour.</p><p>It's a world where, given their incredible hourly salaries, the supply of outstanding AI researchers quickly goes from 10,000 to 10 million or more, enormously accelerating progress in the field.</p><p>It's a world where companies operated entirely by AIs working together are much faster and more cost-effective than those that lean on humans for decision making, and the latter are progressively driven out of business. </p><p>It's a world where the technical challenges around control of robots are rapidly overcome, leading to robots into strong, fast, precise, and tireless workers able to accomplish any physical work the economy requires, and a rush to build billions of them and cash in.</p><p>As the economy grows, each person could effectively afford the practical equivalent of a team of hundreds of machine 'people' to help them with every aspect of their lives.</p><p>And with growth rates this high, it doesn't take long to run up against Earth's physical limits — in this case, the toughest to engineer your way out of is the Earth's ability to release waste heat. If this machine economy and its insatiable demand for power generates more heat than the Earth radiates into space, then it will rapidly heat up and become uninhabitable for humans and other animals.</p><p>This creates pressure to move economic activity off-planet. So you could develop effective populations of billions of scientific researchers operating on computer chips orbiting in space, sending the results of their work, such as drug designs, back to Earth for use.</p><p>These are just some of the wild implications that could follow naturally from truly embracing the hypothetical: what if we develop AGI that could accomplish everything that the most productive humans can, using the same energy supply?</p><p>In today's episode, Carl explains the above, and then host Rob Wiblin pushes back on whether that’s realistic or just a cool story, asking:</p><ul><li>If we're heading towards the above, how come economic growth is slow now and not really increasing?</li><li>Why have computers and computer chips had so little effect on economic productivity so far?</li><li>Are self-replicating biological systems a good comparison for self-replicating machine systems?</li><li>Isn't this just too crazy and weird to be plausible?</li><li>What bottlenecks would be encountered in supplying energy and natural resources to this growing economy?</li><li>Might there not be severely declining returns to bigger brains and more training?</li><li>Wouldn't humanity get scared and pull the brakes if such a transformation kicked off?</li><li>If this is right, how come economists don't agree?</li></ul><p>Finally, Carl addresses the moral status of machine minds themselves. Would they be conscious or otherwise have a claim to moral or rights? And how might humans and machines coexist with neither side dominating or exploiting the other?</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:00)</li><li>Transitioning to a world where AI systems do almost all the work (00:05:21)</li><li>Economics after an AI explosion (00:14:25)</li><li>Objection: Shouldn’t we be seeing economic growth rates increasing today? (00:59:12)</li><li>Objection: Speed of doubling time (01:07:33)</li><li>Objection: Declining returns to increases in intelligence? (01:11:59)</li><li>Objection: Physical transformation of the environment (01:17:39)</li><li>Objection: Should we expect an increased demand for safety and security? (01:29:14)</li><li>Objection: “This sounds completely whack” (01:36:10)</li><li>Income and wealth distribution (01:48:02)</li><li>Economists and the intelligence explosion (02:13:31)</li><li>Baumol effect arguments (02:19:12)</li><li>Denying that robots can exist (02:27:18)</li><li>Classic economic growth models (02:36:12)</li><li>Robot nannies (02:48:27)</li><li>Slow integration of decision-making and authority power (02:57:39)</li><li>Economists’ mistaken heuristics (03:01:07)</li><li>Moral status of AIs (03:11:45)</li><li>Rob’s outro (04:11:47)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 27 Jun 2024 18:44:11 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d287d821/49fe31b8.mp3" length="244795440" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/KrQZUBluEQVECfrduBGQTxCRdZMn6Kjr7krirNXTwAM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8zOGI0/MzdhNDliNTQ5ZjQ2/M2UxY2YzZmY4NjA3/NGQ3Ny5wbmc.jpg"/>
      <itunes:duration>15298</itunes:duration>
      <itunes:summary>
        <![CDATA[<p><strong><em>This is the first part of our marathon interview with Carl Shulman. The second episode is on </em></strong><a href="https://80000hours.org/podcast/episodes/carl-shulman-society-agi/"><strong><em>government and society after AGI</em></strong></a><strong><em>. You can listen to them in either order!</em></strong></p><p>The human brain does what it does with a shockingly low energy supply: just 20 watts — a fraction of a cent worth of electricity per hour. What would happen if AI technology merely matched what evolution has already managed, and could accomplish the work of top human professionals given a 20-watt power supply?</p><p>Many people sort of consider that hypothetical, but maybe nobody has followed through and considered all the implications as much as Carl Shulman. Behind the scenes, his work has greatly influenced how leaders in artificial general intelligence (AGI) picture the world they're creating.</p><p><a href="https://80k.info/cs241"><strong>Links to learn more, highlights, and full transcript.</strong></a><strong></strong></p><p>Carl simply follows the logic to its natural conclusion. This is a world where 1 cent of electricity can be turned into medical advice, company management, or scientific research that would today cost $100s, resulting in a scramble to manufacture chips and apply them to the most lucrative forms of intellectual labour.</p><p>It's a world where, given their incredible hourly salaries, the supply of outstanding AI researchers quickly goes from 10,000 to 10 million or more, enormously accelerating progress in the field.</p><p>It's a world where companies operated entirely by AIs working together are much faster and more cost-effective than those that lean on humans for decision making, and the latter are progressively driven out of business. </p><p>It's a world where the technical challenges around control of robots are rapidly overcome, leading to robots into strong, fast, precise, and tireless workers able to accomplish any physical work the economy requires, and a rush to build billions of them and cash in.</p><p>As the economy grows, each person could effectively afford the practical equivalent of a team of hundreds of machine 'people' to help them with every aspect of their lives.</p><p>And with growth rates this high, it doesn't take long to run up against Earth's physical limits — in this case, the toughest to engineer your way out of is the Earth's ability to release waste heat. If this machine economy and its insatiable demand for power generates more heat than the Earth radiates into space, then it will rapidly heat up and become uninhabitable for humans and other animals.</p><p>This creates pressure to move economic activity off-planet. So you could develop effective populations of billions of scientific researchers operating on computer chips orbiting in space, sending the results of their work, such as drug designs, back to Earth for use.</p><p>These are just some of the wild implications that could follow naturally from truly embracing the hypothetical: what if we develop AGI that could accomplish everything that the most productive humans can, using the same energy supply?</p><p>In today's episode, Carl explains the above, and then host Rob Wiblin pushes back on whether that’s realistic or just a cool story, asking:</p><ul><li>If we're heading towards the above, how come economic growth is slow now and not really increasing?</li><li>Why have computers and computer chips had so little effect on economic productivity so far?</li><li>Are self-replicating biological systems a good comparison for self-replicating machine systems?</li><li>Isn't this just too crazy and weird to be plausible?</li><li>What bottlenecks would be encountered in supplying energy and natural resources to this growing economy?</li><li>Might there not be severely declining returns to bigger brains and more training?</li><li>Wouldn't humanity get scared and pull the brakes if such a transformation kicked off?</li><li>If this is right, how come economists don't agree?</li></ul><p>Finally, Carl addresses the moral status of machine minds themselves. Would they be conscious or otherwise have a claim to moral or rights? And how might humans and machines coexist with neither side dominating or exploiting the other?</p><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:00)</li><li>Transitioning to a world where AI systems do almost all the work (00:05:21)</li><li>Economics after an AI explosion (00:14:25)</li><li>Objection: Shouldn’t we be seeing economic growth rates increasing today? (00:59:12)</li><li>Objection: Speed of doubling time (01:07:33)</li><li>Objection: Declining returns to increases in intelligence? (01:11:59)</li><li>Objection: Physical transformation of the environment (01:17:39)</li><li>Objection: Should we expect an increased demand for safety and security? (01:29:14)</li><li>Objection: “This sounds completely whack” (01:36:10)</li><li>Income and wealth distribution (01:48:02)</li><li>Economists and the intelligence explosion (02:13:31)</li><li>Baumol effect arguments (02:19:12)</li><li>Denying that robots can exist (02:27:18)</li><li>Classic economic growth models (02:36:12)</li><li>Robot nannies (02:48:27)</li><li>Slow integration of decision-making and authority power (02:57:39)</li><li>Economists’ mistaken heuristics (03:01:07)</li><li>Moral status of AIs (03:11:45)</li><li>Rob’s outro (04:11:47)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/d287d821/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/d287d821/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#190 – Eric Schwitzgebel on whether the US is conscious</title>
      <itunes:title>#190 – Eric Schwitzgebel on whether the US is conscious</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">db71d3b6-b0bc-4156-94b7-c34a20a6263c</guid>
      <link>https://80000hours.org/podcast/episodes/eric-schwitzgebel-world-weird-us-consciousness/?utm_campaign=podcast__eric-schwitzgebel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"One of the most amazing things about planet Earth is that there are complex bags of mostly water — you and me – and we can look up at the stars, and look into our brains, and try to grapple with the most complex, difficult questions that there are. And even if we can’t make great progress on them and don’t come to completely satisfying solutions, just the fact of trying to grapple with these things is kind of the universe looking at itself and trying to understand itself. So we’re kind of this bright spot of reflectiveness in the cosmos, and I think we should celebrate that fact for its own intrinsic value and interestingness." —Eric Schwitzgebel</p><p>In today’s episode, host Luisa Rodriguez speaks to Eric Schwitzgebel — professor of philosophy at UC Riverside — about some of the most bizarre and unintuitive claims from his recent book, <a href="https://www.amazon.com/Weirdness-World-Eric-Schwitzgebel-ebook/dp/B0CCGNYM5X"><em>The Weirdness of the World</em></a>.</p><p><a href="https://80k.info/es"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why our intuitions seem so unreliable for answering fundamental questions about reality.</li><li>What the materialist view of consciousness is, and how it might imply some very weird things — like that the United States could be a conscious entity.</li><li>Thought experiments that challenge our intuitions — like supersquids that think and act through detachable tentacles, and intelligent species whose brains are made up of a million bugs.</li><li>Eric’s claim that consciousness and cosmology are universally bizarre and dubious.</li><li>How to think about borderline states of consciousness, and whether consciousness is more like a spectrum or more like a light flicking on.</li><li>The nontrivial possibility that we could be dreaming right now, and the ethical implications if that’s true.</li><li>Why it’s worth it to grapple with the universe’s most complex questions, even if we can’t find completely satisfying solutions.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open |00:00:00|</li><li>Luisa’s intro |00:01:10|</li><li>Bizarre and dubious philosophical theories |00:03:13|</li><li>The materialist view of consciousness |00:13:55|</li><li>What would it mean for the US to be conscious? |00:19:46|</li><li>Supersquids and antheads thought experiments |00:22:37|</li><li>Alternatives to the materialist perspective |00:35:19|</li><li>Are our intuitions useless for thinking about these things? |00:42:55|</li><li>Key ingredients for consciousness |00:46:46|</li><li>Reasons to think the US isn’t conscious |01:01:15|</li><li>Overlapping consciousnesses [01:09:32]</li><li>Borderline cases of consciousness |01:13:22|</li><li>Are we dreaming right now? |01:40:29|</li><li>Will we ever have answers to these dubious and bizarre questions? |01:56:16|</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"One of the most amazing things about planet Earth is that there are complex bags of mostly water — you and me – and we can look up at the stars, and look into our brains, and try to grapple with the most complex, difficult questions that there are. And even if we can’t make great progress on them and don’t come to completely satisfying solutions, just the fact of trying to grapple with these things is kind of the universe looking at itself and trying to understand itself. So we’re kind of this bright spot of reflectiveness in the cosmos, and I think we should celebrate that fact for its own intrinsic value and interestingness." —Eric Schwitzgebel</p><p>In today’s episode, host Luisa Rodriguez speaks to Eric Schwitzgebel — professor of philosophy at UC Riverside — about some of the most bizarre and unintuitive claims from his recent book, <a href="https://www.amazon.com/Weirdness-World-Eric-Schwitzgebel-ebook/dp/B0CCGNYM5X"><em>The Weirdness of the World</em></a>.</p><p><a href="https://80k.info/es"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why our intuitions seem so unreliable for answering fundamental questions about reality.</li><li>What the materialist view of consciousness is, and how it might imply some very weird things — like that the United States could be a conscious entity.</li><li>Thought experiments that challenge our intuitions — like supersquids that think and act through detachable tentacles, and intelligent species whose brains are made up of a million bugs.</li><li>Eric’s claim that consciousness and cosmology are universally bizarre and dubious.</li><li>How to think about borderline states of consciousness, and whether consciousness is more like a spectrum or more like a light flicking on.</li><li>The nontrivial possibility that we could be dreaming right now, and the ethical implications if that’s true.</li><li>Why it’s worth it to grapple with the universe’s most complex questions, even if we can’t find completely satisfying solutions.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open |00:00:00|</li><li>Luisa’s intro |00:01:10|</li><li>Bizarre and dubious philosophical theories |00:03:13|</li><li>The materialist view of consciousness |00:13:55|</li><li>What would it mean for the US to be conscious? |00:19:46|</li><li>Supersquids and antheads thought experiments |00:22:37|</li><li>Alternatives to the materialist perspective |00:35:19|</li><li>Are our intuitions useless for thinking about these things? |00:42:55|</li><li>Key ingredients for consciousness |00:46:46|</li><li>Reasons to think the US isn’t conscious |01:01:15|</li><li>Overlapping consciousnesses [01:09:32]</li><li>Borderline cases of consciousness |01:13:22|</li><li>Are we dreaming right now? |01:40:29|</li><li>Will we ever have answers to these dubious and bizarre questions? |01:56:16|</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 07 Jun 2024 17:48:27 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8c56b090/ea6635a9.mp3" length="116076289" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/hVnokWm83J_gD2eNAlESaYyzS8wuoXZbjTd8crM67dk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8zOTll/MGI2NDYzMTg2ZDc5/NWI0OWE1NzMwYzEy/YmFmMy5qcGc.jpg"/>
      <itunes:duration>7246</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"One of the most amazing things about planet Earth is that there are complex bags of mostly water — you and me – and we can look up at the stars, and look into our brains, and try to grapple with the most complex, difficult questions that there are. And even if we can’t make great progress on them and don’t come to completely satisfying solutions, just the fact of trying to grapple with these things is kind of the universe looking at itself and trying to understand itself. So we’re kind of this bright spot of reflectiveness in the cosmos, and I think we should celebrate that fact for its own intrinsic value and interestingness." —Eric Schwitzgebel</p><p>In today’s episode, host Luisa Rodriguez speaks to Eric Schwitzgebel — professor of philosophy at UC Riverside — about some of the most bizarre and unintuitive claims from his recent book, <a href="https://www.amazon.com/Weirdness-World-Eric-Schwitzgebel-ebook/dp/B0CCGNYM5X"><em>The Weirdness of the World</em></a>.</p><p><a href="https://80k.info/es"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why our intuitions seem so unreliable for answering fundamental questions about reality.</li><li>What the materialist view of consciousness is, and how it might imply some very weird things — like that the United States could be a conscious entity.</li><li>Thought experiments that challenge our intuitions — like supersquids that think and act through detachable tentacles, and intelligent species whose brains are made up of a million bugs.</li><li>Eric’s claim that consciousness and cosmology are universally bizarre and dubious.</li><li>How to think about borderline states of consciousness, and whether consciousness is more like a spectrum or more like a light flicking on.</li><li>The nontrivial possibility that we could be dreaming right now, and the ethical implications if that’s true.</li><li>Why it’s worth it to grapple with the universe’s most complex questions, even if we can’t find completely satisfying solutions.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Cold open |00:00:00|</li><li>Luisa’s intro |00:01:10|</li><li>Bizarre and dubious philosophical theories |00:03:13|</li><li>The materialist view of consciousness |00:13:55|</li><li>What would it mean for the US to be conscious? |00:19:46|</li><li>Supersquids and antheads thought experiments |00:22:37|</li><li>Alternatives to the materialist perspective |00:35:19|</li><li>Are our intuitions useless for thinking about these things? |00:42:55|</li><li>Key ingredients for consciousness |00:46:46|</li><li>Reasons to think the US isn’t conscious |01:01:15|</li><li>Overlapping consciousnesses [01:09:32]</li><li>Borderline cases of consciousness |01:13:22|</li><li>Are we dreaming right now? |01:40:29|</li><li>Will we ever have answers to these dubious and bizarre questions? |01:56:16|</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/8c56b090/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/8c56b090/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#189 – Rachel Glennerster on why we still don’t have vaccines that could save millions</title>
      <itunes:title>#189 – Rachel Glennerster on why we still don’t have vaccines that could save millions</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">09ebd29b-a0b9-4597-83e9-e4ebbd1b7846</guid>
      <link>https://80000hours.org/podcast/episodes/rachel-glennerster-market-shaping-incentives/?utm_campaign=podcast__rachel-glennerster&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"You can’t charge what something is worth during a pandemic. So we estimated that the value of one course of COVID vaccine in January 2021 was over $5,000. They were selling for between $6 and $40. So nothing like their social value. Now, don’t get me wrong. I don’t think that they should have charged $5,000 or $6,000. That’s not ethical. It’s also not economically efficient, because they didn’t cost $5,000 at the marginal cost. So you actually want low price, getting out to lots of people.</p><p>"But it shows you that the market is not going to reward people who do the investment in preparation for a pandemic — because when a pandemic hits, they’re not going to get the reward in line with the social value. They may even have to charge less than they would in a non-pandemic time. So prepping for a pandemic is not an efficient market strategy if I’m a firm, but it’s a very efficient strategy for society, and so we’ve got to bridge that gap." —Rachel Glennerster</p><p>In today’s episode, host Luisa Rodriguez speaks to Rachel Glennerster — associate professor of economics at the University of Chicago and a pioneer in the field of development economics — about how her team’s new <a href="https://marketshaping.uchicago.edu/">Market Shaping Accelerator</a> aims to leverage market forces to drive innovations that can solve pressing world problems.</p><p><a href="https://80k.info/rg"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How market failures and misaligned incentives stifle critical innovations for social goods like pandemic preparedness, climate change interventions, and vaccine development.</li><li>How “pull mechanisms” like advance market commitments (AMCs) can help overcome these challenges — including concrete examples like how one AMC led to speeding up the development of three vaccines which saved around 700,000 lives in low-income countries.</li><li>The challenges in designing effective pull mechanisms, from design to implementation.</li><li>Why it’s important to tie innovation incentives to real-world impact and uptake, not just the invention of a new technology.</li><li>The massive benefits of accelerating vaccine development, in some cases, even if it’s only by a few days or weeks.</li><li>The case for a $6 billion advance market commitment to spur work on a universal COVID-19 vaccine.</li><li>The shortlist of ideas from the Market Shaping Accelerator’s recent Innovation Challenge that use pull mechanisms to address market failures around improving indoor air quality, repurposing generic drugs for alternative uses, and developing eco-friendly air conditioners for a warming planet.</li><li>“Best Buys” and “Bad Buys” for improving education systems in low- and middle-income countries, based on evidence from over 400 studies.</li><li>Lessons from Rachel’s career at the forefront of global development, and how insights from economics can drive transformative change.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>The Market Shaping Accelerator (00:03:33)</li><li>Pull mechanisms for innovation (00:13:10)</li><li>Accelerating the pneumococcal and COVID vaccines (00:19:05)</li><li>Advance market commitments (00:41:46)</li><li>Is this uncertainty hard for funders to plan around? (00:49:17)</li><li>The story of the malaria vaccine that wasn’t (00:57:15)</li><li>Challenges with designing and implementing AMCs and other pull mechanisms (01:01:40)</li><li>Universal COVID vaccine (01:18:14)</li><li>Climate-resilient crops (01:34:09)</li><li>The Market Shaping Accelerator’s Innovation Challenge (01:45:40)</li><li>Indoor air quality to reduce respiratory infections (01:49:09)</li><li>Repurposing generic drugs (01:55:50)</li><li>Clean air conditioning units (02:02:41)</li><li>Broad-spectrum antivirals for pandemic prevention (02:09:11)</li><li>Improving education in low- and middle-income countries (02:15:53)</li><li>What’s still weird for Rachel about living in the US? (02:45:06)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"You can’t charge what something is worth during a pandemic. So we estimated that the value of one course of COVID vaccine in January 2021 was over $5,000. They were selling for between $6 and $40. So nothing like their social value. Now, don’t get me wrong. I don’t think that they should have charged $5,000 or $6,000. That’s not ethical. It’s also not economically efficient, because they didn’t cost $5,000 at the marginal cost. So you actually want low price, getting out to lots of people.</p><p>"But it shows you that the market is not going to reward people who do the investment in preparation for a pandemic — because when a pandemic hits, they’re not going to get the reward in line with the social value. They may even have to charge less than they would in a non-pandemic time. So prepping for a pandemic is not an efficient market strategy if I’m a firm, but it’s a very efficient strategy for society, and so we’ve got to bridge that gap." —Rachel Glennerster</p><p>In today’s episode, host Luisa Rodriguez speaks to Rachel Glennerster — associate professor of economics at the University of Chicago and a pioneer in the field of development economics — about how her team’s new <a href="https://marketshaping.uchicago.edu/">Market Shaping Accelerator</a> aims to leverage market forces to drive innovations that can solve pressing world problems.</p><p><a href="https://80k.info/rg"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How market failures and misaligned incentives stifle critical innovations for social goods like pandemic preparedness, climate change interventions, and vaccine development.</li><li>How “pull mechanisms” like advance market commitments (AMCs) can help overcome these challenges — including concrete examples like how one AMC led to speeding up the development of three vaccines which saved around 700,000 lives in low-income countries.</li><li>The challenges in designing effective pull mechanisms, from design to implementation.</li><li>Why it’s important to tie innovation incentives to real-world impact and uptake, not just the invention of a new technology.</li><li>The massive benefits of accelerating vaccine development, in some cases, even if it’s only by a few days or weeks.</li><li>The case for a $6 billion advance market commitment to spur work on a universal COVID-19 vaccine.</li><li>The shortlist of ideas from the Market Shaping Accelerator’s recent Innovation Challenge that use pull mechanisms to address market failures around improving indoor air quality, repurposing generic drugs for alternative uses, and developing eco-friendly air conditioners for a warming planet.</li><li>“Best Buys” and “Bad Buys” for improving education systems in low- and middle-income countries, based on evidence from over 400 studies.</li><li>Lessons from Rachel’s career at the forefront of global development, and how insights from economics can drive transformative change.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>The Market Shaping Accelerator (00:03:33)</li><li>Pull mechanisms for innovation (00:13:10)</li><li>Accelerating the pneumococcal and COVID vaccines (00:19:05)</li><li>Advance market commitments (00:41:46)</li><li>Is this uncertainty hard for funders to plan around? (00:49:17)</li><li>The story of the malaria vaccine that wasn’t (00:57:15)</li><li>Challenges with designing and implementing AMCs and other pull mechanisms (01:01:40)</li><li>Universal COVID vaccine (01:18:14)</li><li>Climate-resilient crops (01:34:09)</li><li>The Market Shaping Accelerator’s Innovation Challenge (01:45:40)</li><li>Indoor air quality to reduce respiratory infections (01:49:09)</li><li>Repurposing generic drugs (01:55:50)</li><li>Clean air conditioning units (02:02:41)</li><li>Broad-spectrum antivirals for pandemic prevention (02:09:11)</li><li>Improving education in low- and middle-income countries (02:15:53)</li><li>What’s still weird for Rachel about living in the US? (02:45:06)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 29 May 2024 20:13:38 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/08ad3696/f2908afa.mp3" length="162131774" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/l6D5j5dcsdQAr10E0SFYcFZFZ1bUUtb7IaYgCAhVHRA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mNTRk/NTUzMzhlZTI2M2Q1/NGRiYzBjYmUzMzBh/OGM0Yy5qcGVn.jpg"/>
      <itunes:duration>10131</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"You can’t charge what something is worth during a pandemic. So we estimated that the value of one course of COVID vaccine in January 2021 was over $5,000. They were selling for between $6 and $40. So nothing like their social value. Now, don’t get me wrong. I don’t think that they should have charged $5,000 or $6,000. That’s not ethical. It’s also not economically efficient, because they didn’t cost $5,000 at the marginal cost. So you actually want low price, getting out to lots of people.</p><p>"But it shows you that the market is not going to reward people who do the investment in preparation for a pandemic — because when a pandemic hits, they’re not going to get the reward in line with the social value. They may even have to charge less than they would in a non-pandemic time. So prepping for a pandemic is not an efficient market strategy if I’m a firm, but it’s a very efficient strategy for society, and so we’ve got to bridge that gap." —Rachel Glennerster</p><p>In today’s episode, host Luisa Rodriguez speaks to Rachel Glennerster — associate professor of economics at the University of Chicago and a pioneer in the field of development economics — about how her team’s new <a href="https://marketshaping.uchicago.edu/">Market Shaping Accelerator</a> aims to leverage market forces to drive innovations that can solve pressing world problems.</p><p><a href="https://80k.info/rg"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How market failures and misaligned incentives stifle critical innovations for social goods like pandemic preparedness, climate change interventions, and vaccine development.</li><li>How “pull mechanisms” like advance market commitments (AMCs) can help overcome these challenges — including concrete examples like how one AMC led to speeding up the development of three vaccines which saved around 700,000 lives in low-income countries.</li><li>The challenges in designing effective pull mechanisms, from design to implementation.</li><li>Why it’s important to tie innovation incentives to real-world impact and uptake, not just the invention of a new technology.</li><li>The massive benefits of accelerating vaccine development, in some cases, even if it’s only by a few days or weeks.</li><li>The case for a $6 billion advance market commitment to spur work on a universal COVID-19 vaccine.</li><li>The shortlist of ideas from the Market Shaping Accelerator’s recent Innovation Challenge that use pull mechanisms to address market failures around improving indoor air quality, repurposing generic drugs for alternative uses, and developing eco-friendly air conditioners for a warming planet.</li><li>“Best Buys” and “Bad Buys” for improving education systems in low- and middle-income countries, based on evidence from over 400 studies.</li><li>Lessons from Rachel’s career at the forefront of global development, and how insights from economics can drive transformative change.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>The Market Shaping Accelerator (00:03:33)</li><li>Pull mechanisms for innovation (00:13:10)</li><li>Accelerating the pneumococcal and COVID vaccines (00:19:05)</li><li>Advance market commitments (00:41:46)</li><li>Is this uncertainty hard for funders to plan around? (00:49:17)</li><li>The story of the malaria vaccine that wasn’t (00:57:15)</li><li>Challenges with designing and implementing AMCs and other pull mechanisms (01:01:40)</li><li>Universal COVID vaccine (01:18:14)</li><li>Climate-resilient crops (01:34:09)</li><li>The Market Shaping Accelerator’s Innovation Challenge (01:45:40)</li><li>Indoor air quality to reduce respiratory infections (01:49:09)</li><li>Repurposing generic drugs (01:55:50)</li><li>Clean air conditioning units (02:02:41)</li><li>Broad-spectrum antivirals for pandemic prevention (02:09:11)</li><li>Improving education in low- and middle-income countries (02:15:53)</li><li>What’s still weird for Rachel about living in the US? (02:45:06)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/08ad3696/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/08ad3696/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#188 – Matt Clancy on whether science is good</title>
      <itunes:title>#188 – Matt Clancy on whether science is good</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2a2a2fee-11b2-4c45-8b9e-11209b836d32</guid>
      <link>https://80000hours.org/podcast/episodes/matt-clancy-whether-science-is-good/?utm_campaign=podcast__matt-clancy&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Suppose we make these grants, we do some of those experiments I talk about. We discover, for example — I’m just making this up — but we give people superforecasting tests when they’re doing peer review, and we find that you can identify people who are super good at picking science. And then we have this much better targeted science, and we’re making progress at a 10% faster rate than we normally would have. Over time, that aggregates up, and maybe after 10 years, we’re a year ahead of where we would have been if we hadn’t done this kind of stuff.</p><p>"Now, suppose in 10 years we’re going to discover a cheap new genetic engineering technology that anyone can use in the world if they order the right parts off of Amazon. That could be great, but could also allow bad actors to genetically engineer pandemics and basically try to do terrible things with this technology. And if we’ve brought that forward, and that happens at year nine instead of year 10 because of some of these interventions we did, now we start to think that if that’s really bad, if these people using this technology causes huge problems for humanity, it begins to sort of wash out the benefits of getting the science a little bit faster." —Matt Clancy</p><p>In today’s episode, host Luisa Rodriguez speaks to Matt Clancy — who oversees <a href="https://www.openphilanthropy.org/focus/innovation-policy/">Open Philanthropy’s Innovation Policy</a> programme — about his <a href="https://arxiv.org/abs/2312.14289">recent work</a> modelling the risks and benefits of the increasing speed of scientific progress.</p><p><a href="https://80k.info/mc"><strong>Links to learn more, highlights, and full transcript</strong></a><strong>.</strong></p><p>They cover:</p><ul><li>Whether scientific progress is actually net positive for humanity.</li><li>Scenarios where accelerating science could lead to existential risks, such as advanced biotechnology being used by bad actors.</li><li>Why Matt thinks metascience research and targeted funding could improve the scientific process and better incentivise outcomes that are good for humanity.</li><li>Whether Matt trusts domain experts or superforecasters more when estimating how the future will turn out.</li><li>Why Matt is sceptical that AGI could really cause explosive economic growth.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Is scientific progress net positive for humanity? (00:03:00)</li><li>The time of biological perils (00:17:50)</li><li>Modelling the benefits of science (00:25:48)</li><li>Income and health gains from scientific progress (00:32:49)</li><li>Discount rates (00:42:14)</li><li>How big are the returns to science? (00:51:08)</li><li>Forecasting global catastrophic biological risks from scientific progress (01:05:20)</li><li>What’s the value of scientific progress, given the risks? (01:15:09)</li><li>Factoring in extinction risk (01:21:56)</li><li>How science could reduce extinction risk (01:30:18)</li><li>Are we already too late to delay the time of perils? (01:42:38)</li><li>Domain experts vs superforecasters (01:46:03)</li><li>What Open Philanthropy’s Innovation Policy programme settled on (01:53:47)</li><li>Explosive economic growth (02:06:28)</li><li>Matt’s favourite thought experiment (02:34:57)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Suppose we make these grants, we do some of those experiments I talk about. We discover, for example — I’m just making this up — but we give people superforecasting tests when they’re doing peer review, and we find that you can identify people who are super good at picking science. And then we have this much better targeted science, and we’re making progress at a 10% faster rate than we normally would have. Over time, that aggregates up, and maybe after 10 years, we’re a year ahead of where we would have been if we hadn’t done this kind of stuff.</p><p>"Now, suppose in 10 years we’re going to discover a cheap new genetic engineering technology that anyone can use in the world if they order the right parts off of Amazon. That could be great, but could also allow bad actors to genetically engineer pandemics and basically try to do terrible things with this technology. And if we’ve brought that forward, and that happens at year nine instead of year 10 because of some of these interventions we did, now we start to think that if that’s really bad, if these people using this technology causes huge problems for humanity, it begins to sort of wash out the benefits of getting the science a little bit faster." —Matt Clancy</p><p>In today’s episode, host Luisa Rodriguez speaks to Matt Clancy — who oversees <a href="https://www.openphilanthropy.org/focus/innovation-policy/">Open Philanthropy’s Innovation Policy</a> programme — about his <a href="https://arxiv.org/abs/2312.14289">recent work</a> modelling the risks and benefits of the increasing speed of scientific progress.</p><p><a href="https://80k.info/mc"><strong>Links to learn more, highlights, and full transcript</strong></a><strong>.</strong></p><p>They cover:</p><ul><li>Whether scientific progress is actually net positive for humanity.</li><li>Scenarios where accelerating science could lead to existential risks, such as advanced biotechnology being used by bad actors.</li><li>Why Matt thinks metascience research and targeted funding could improve the scientific process and better incentivise outcomes that are good for humanity.</li><li>Whether Matt trusts domain experts or superforecasters more when estimating how the future will turn out.</li><li>Why Matt is sceptical that AGI could really cause explosive economic growth.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Is scientific progress net positive for humanity? (00:03:00)</li><li>The time of biological perils (00:17:50)</li><li>Modelling the benefits of science (00:25:48)</li><li>Income and health gains from scientific progress (00:32:49)</li><li>Discount rates (00:42:14)</li><li>How big are the returns to science? (00:51:08)</li><li>Forecasting global catastrophic biological risks from scientific progress (01:05:20)</li><li>What’s the value of scientific progress, given the risks? (01:15:09)</li><li>Factoring in extinction risk (01:21:56)</li><li>How science could reduce extinction risk (01:30:18)</li><li>Are we already too late to delay the time of perils? (01:42:38)</li><li>Domain experts vs superforecasters (01:46:03)</li><li>What Open Philanthropy’s Innovation Policy programme settled on (01:53:47)</li><li>Explosive economic growth (02:06:28)</li><li>Matt’s favourite thought experiment (02:34:57)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 23 May 2024 21:32:29 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/fea02107/948b10d7.mp3" length="153881438" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/uPA20eObUpMw6wRt8IVZKlIMMGvMZtQKH-o5WfFbiuA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8wY2Zi/YTY3NDE4ODM1NTNl/OGU1OWE3OWEyOTA4/YjE4NS5qcGVn.jpg"/>
      <itunes:duration>9615</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Suppose we make these grants, we do some of those experiments I talk about. We discover, for example — I’m just making this up — but we give people superforecasting tests when they’re doing peer review, and we find that you can identify people who are super good at picking science. And then we have this much better targeted science, and we’re making progress at a 10% faster rate than we normally would have. Over time, that aggregates up, and maybe after 10 years, we’re a year ahead of where we would have been if we hadn’t done this kind of stuff.</p><p>"Now, suppose in 10 years we’re going to discover a cheap new genetic engineering technology that anyone can use in the world if they order the right parts off of Amazon. That could be great, but could also allow bad actors to genetically engineer pandemics and basically try to do terrible things with this technology. And if we’ve brought that forward, and that happens at year nine instead of year 10 because of some of these interventions we did, now we start to think that if that’s really bad, if these people using this technology causes huge problems for humanity, it begins to sort of wash out the benefits of getting the science a little bit faster." —Matt Clancy</p><p>In today’s episode, host Luisa Rodriguez speaks to Matt Clancy — who oversees <a href="https://www.openphilanthropy.org/focus/innovation-policy/">Open Philanthropy’s Innovation Policy</a> programme — about his <a href="https://arxiv.org/abs/2312.14289">recent work</a> modelling the risks and benefits of the increasing speed of scientific progress.</p><p><a href="https://80k.info/mc"><strong>Links to learn more, highlights, and full transcript</strong></a><strong>.</strong></p><p>They cover:</p><ul><li>Whether scientific progress is actually net positive for humanity.</li><li>Scenarios where accelerating science could lead to existential risks, such as advanced biotechnology being used by bad actors.</li><li>Why Matt thinks metascience research and targeted funding could improve the scientific process and better incentivise outcomes that are good for humanity.</li><li>Whether Matt trusts domain experts or superforecasters more when estimating how the future will turn out.</li><li>Why Matt is sceptical that AGI could really cause explosive economic growth.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Is scientific progress net positive for humanity? (00:03:00)</li><li>The time of biological perils (00:17:50)</li><li>Modelling the benefits of science (00:25:48)</li><li>Income and health gains from scientific progress (00:32:49)</li><li>Discount rates (00:42:14)</li><li>How big are the returns to science? (00:51:08)</li><li>Forecasting global catastrophic biological risks from scientific progress (01:05:20)</li><li>What’s the value of scientific progress, given the risks? (01:15:09)</li><li>Factoring in extinction risk (01:21:56)</li><li>How science could reduce extinction risk (01:30:18)</li><li>Are we already too late to delay the time of perils? (01:42:38)</li><li>Domain experts vs superforecasters (01:46:03)</li><li>What Open Philanthropy’s Innovation Policy programme settled on (01:53:47)</li><li>Explosive economic growth (02:06:28)</li><li>Matt’s favourite thought experiment (02:34:57)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/fea02107/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/fea02107/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#187 – Zach Weinersmith on how researching his book turned him from a space optimist into a "space bastard"</title>
      <itunes:title>#187 – Zach Weinersmith on how researching his book turned him from a space optimist into a "space bastard"</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">610c7798-3102-4128-bb28-3b7b067acf5b</guid>
      <link>https://80000hours.org/podcast/episodes/zach-weinersmith-space-settlement/?utm_campaign=podcast__zach-weinersmith&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Earth economists, when they measure how bad the potential for exploitation is, they look at things like, how is labour mobility? How much possibility do labourers have otherwise to go somewhere else? Well, if you are on the one company town on Mars, your labour mobility is zero, which has never existed on Earth. Even in your stereotypical West Virginian company town run by immigrant labour, there’s still, by definition, a train out. On Mars, you might not even be in the launch window. And even if there are five other company towns or five other settlements, they’re not necessarily rated to take more humans. They have their own oxygen budget, right? </p><p>"And so economists use numbers like these, like labour mobility, as a way to put an equation and estimate the ability of a company to set noncompetitive wages or to set noncompetitive work conditions. And essentially, on Mars you’re setting it to infinity." — Zach Weinersmith</p><p>In today’s episode, host Luisa Rodriguez speaks to Zach Weinersmith — the cartoonist behind <a href="https://www.smbc-comics.com/"><em>Saturday Morning Breakfast Cereal</em></a> — about the latest book he wrote with his wife Kelly: <a href="https://www.amazon.com/City-Mars-Settle-Thought-Through/dp/B0BXFM29DW/"><em>A City on Mars: Can We Settle Space, Should We Settle Space, and Have We Really Thought This Through?</em></a></p><p><a href="https://80k.info/zw"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why space travel is suddenly getting a lot cheaper and re-igniting enthusiasm around space settlement.</li><li>What Zach thinks are the best and worst arguments for settling space.</li><li>Zach’s journey from optimistic about space settlement to a self-proclaimed “space bastard” (pessimist).</li><li>How little we know about how microgravity and radiation affects even adults, much less the children potentially born in a space settlement.</li><li>A rundown of where we could settle in the solar system, and the major drawbacks of even the most promising candidates.</li><li>Why digging bunkers or underwater cities on Earth would beat fleeing to Mars in a catastrophe.</li><li>How new space settlements could look a lot like old company towns — and whether or not that’s a bad thing.</li><li>The current state of space law and how it might set us up for international conflict.</li><li>How space cannibalism legal loopholes might work on the International Space Station.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Space optimism and space bastards (00:03:04)</li><li>Bad arguments for why we should settle space (00:14:01)</li><li>Superficially plausible arguments for why we should settle space (00:28:54)</li><li>Is settling space even biologically feasible? (00:32:43)</li><li>Sex, pregnancy, and child development in space (00:41:41)</li><li>Where’s the best space place to settle? (00:55:02)</li><li>Creating self-sustaining habitats (01:15:32)</li><li>What about AI advances? (01:26:23)</li><li>A roadmap for settling space (01:33:45)</li><li>Space law (01:37:22)</li><li>Space signalling and propaganda (01:51:28) </li><li>Space war (02:00:40)</li><li>Mining asteroids (02:06:29)</li><li>Company towns and communes in space (02:10:55)</li><li>Sending digital minds into space (02:26:37)</li><li>The most promising space governance models (02:29:07)</li><li>The tragedy of the commons (02:35:02)</li><li>The tampon bandolier and other bodily functions in space (02:40:14)</li><li>Is space cannibalism legal? (02:47:09)</li><li>The pregnadrome and other bizarre proposals (02:50:02)</li><li>Space sexism (02:58:38)</li><li>What excites Zach about the future (03:02:57)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Earth economists, when they measure how bad the potential for exploitation is, they look at things like, how is labour mobility? How much possibility do labourers have otherwise to go somewhere else? Well, if you are on the one company town on Mars, your labour mobility is zero, which has never existed on Earth. Even in your stereotypical West Virginian company town run by immigrant labour, there’s still, by definition, a train out. On Mars, you might not even be in the launch window. And even if there are five other company towns or five other settlements, they’re not necessarily rated to take more humans. They have their own oxygen budget, right? </p><p>"And so economists use numbers like these, like labour mobility, as a way to put an equation and estimate the ability of a company to set noncompetitive wages or to set noncompetitive work conditions. And essentially, on Mars you’re setting it to infinity." — Zach Weinersmith</p><p>In today’s episode, host Luisa Rodriguez speaks to Zach Weinersmith — the cartoonist behind <a href="https://www.smbc-comics.com/"><em>Saturday Morning Breakfast Cereal</em></a> — about the latest book he wrote with his wife Kelly: <a href="https://www.amazon.com/City-Mars-Settle-Thought-Through/dp/B0BXFM29DW/"><em>A City on Mars: Can We Settle Space, Should We Settle Space, and Have We Really Thought This Through?</em></a></p><p><a href="https://80k.info/zw"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why space travel is suddenly getting a lot cheaper and re-igniting enthusiasm around space settlement.</li><li>What Zach thinks are the best and worst arguments for settling space.</li><li>Zach’s journey from optimistic about space settlement to a self-proclaimed “space bastard” (pessimist).</li><li>How little we know about how microgravity and radiation affects even adults, much less the children potentially born in a space settlement.</li><li>A rundown of where we could settle in the solar system, and the major drawbacks of even the most promising candidates.</li><li>Why digging bunkers or underwater cities on Earth would beat fleeing to Mars in a catastrophe.</li><li>How new space settlements could look a lot like old company towns — and whether or not that’s a bad thing.</li><li>The current state of space law and how it might set us up for international conflict.</li><li>How space cannibalism legal loopholes might work on the International Space Station.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Space optimism and space bastards (00:03:04)</li><li>Bad arguments for why we should settle space (00:14:01)</li><li>Superficially plausible arguments for why we should settle space (00:28:54)</li><li>Is settling space even biologically feasible? (00:32:43)</li><li>Sex, pregnancy, and child development in space (00:41:41)</li><li>Where’s the best space place to settle? (00:55:02)</li><li>Creating self-sustaining habitats (01:15:32)</li><li>What about AI advances? (01:26:23)</li><li>A roadmap for settling space (01:33:45)</li><li>Space law (01:37:22)</li><li>Space signalling and propaganda (01:51:28) </li><li>Space war (02:00:40)</li><li>Mining asteroids (02:06:29)</li><li>Company towns and communes in space (02:10:55)</li><li>Sending digital minds into space (02:26:37)</li><li>The most promising space governance models (02:29:07)</li><li>The tragedy of the commons (02:35:02)</li><li>The tampon bandolier and other bodily functions in space (02:40:14)</li><li>Is space cannibalism legal? (02:47:09)</li><li>The pregnadrome and other bizarre proposals (02:50:02)</li><li>Space sexism (02:58:38)</li><li>What excites Zach about the future (03:02:57)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 14 May 2024 20:17:57 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/601f9369/b3cda57b.mp3" length="179352788" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/IFCHtgRroyPomCJHX078mPq8zLxIgdSdxXCx5vmp1VI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS85OGI0/MDlkMGEwZWQwNmU2/ODFlMjcxY2IwODlj/YmMzYy5qcGVn.jpg"/>
      <itunes:duration>11207</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Earth economists, when they measure how bad the potential for exploitation is, they look at things like, how is labour mobility? How much possibility do labourers have otherwise to go somewhere else? Well, if you are on the one company town on Mars, your labour mobility is zero, which has never existed on Earth. Even in your stereotypical West Virginian company town run by immigrant labour, there’s still, by definition, a train out. On Mars, you might not even be in the launch window. And even if there are five other company towns or five other settlements, they’re not necessarily rated to take more humans. They have their own oxygen budget, right? </p><p>"And so economists use numbers like these, like labour mobility, as a way to put an equation and estimate the ability of a company to set noncompetitive wages or to set noncompetitive work conditions. And essentially, on Mars you’re setting it to infinity." — Zach Weinersmith</p><p>In today’s episode, host Luisa Rodriguez speaks to Zach Weinersmith — the cartoonist behind <a href="https://www.smbc-comics.com/"><em>Saturday Morning Breakfast Cereal</em></a> — about the latest book he wrote with his wife Kelly: <a href="https://www.amazon.com/City-Mars-Settle-Thought-Through/dp/B0BXFM29DW/"><em>A City on Mars: Can We Settle Space, Should We Settle Space, and Have We Really Thought This Through?</em></a></p><p><a href="https://80k.info/zw"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why space travel is suddenly getting a lot cheaper and re-igniting enthusiasm around space settlement.</li><li>What Zach thinks are the best and worst arguments for settling space.</li><li>Zach’s journey from optimistic about space settlement to a self-proclaimed “space bastard” (pessimist).</li><li>How little we know about how microgravity and radiation affects even adults, much less the children potentially born in a space settlement.</li><li>A rundown of where we could settle in the solar system, and the major drawbacks of even the most promising candidates.</li><li>Why digging bunkers or underwater cities on Earth would beat fleeing to Mars in a catastrophe.</li><li>How new space settlements could look a lot like old company towns — and whether or not that’s a bad thing.</li><li>The current state of space law and how it might set us up for international conflict.</li><li>How space cannibalism legal loopholes might work on the International Space Station.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Space optimism and space bastards (00:03:04)</li><li>Bad arguments for why we should settle space (00:14:01)</li><li>Superficially plausible arguments for why we should settle space (00:28:54)</li><li>Is settling space even biologically feasible? (00:32:43)</li><li>Sex, pregnancy, and child development in space (00:41:41)</li><li>Where’s the best space place to settle? (00:55:02)</li><li>Creating self-sustaining habitats (01:15:32)</li><li>What about AI advances? (01:26:23)</li><li>A roadmap for settling space (01:33:45)</li><li>Space law (01:37:22)</li><li>Space signalling and propaganda (01:51:28) </li><li>Space war (02:00:40)</li><li>Mining asteroids (02:06:29)</li><li>Company towns and communes in space (02:10:55)</li><li>Sending digital minds into space (02:26:37)</li><li>The most promising space governance models (02:29:07)</li><li>The tragedy of the commons (02:35:02)</li><li>The tampon bandolier and other bodily functions in space (02:40:14)</li><li>Is space cannibalism legal? (02:47:09)</li><li>The pregnadrome and other bizarre proposals (02:50:02)</li><li>Space sexism (02:58:38)</li><li>What excites Zach about the future (03:02:57)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/601f9369/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#186 – Dean Spears on why babies are born small in Uttar Pradesh, and how to save their lives</title>
      <itunes:title>#186 – Dean Spears on why babies are born small in Uttar Pradesh, and how to save their lives</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c6e080bc-7415-42a0-8d9a-29042281e424</guid>
      <link>https://80000hours.org/podcast/episodes/dean-spears-neonatal-mortality-kangaroo-mother-care/?utm_campaign=podcast__dean-spears&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"I work in a place called Uttar Pradesh, which is a state in India with 240 million people. One in every 33 people in the whole world lives in Uttar Pradesh. It would be the fifth largest country if it were its own country. And if it were its own country, you’d probably know about its human development challenges, because it would have the highest neonatal mortality rate of any country except for South Sudan and Pakistan. Forty percent of children there are stunted. Only two-thirds of women are literate. So Uttar Pradesh is a place where there are lots of health challenges.</p><p>"And then even within that, we’re working in a district called Bahraich, where about 4 million people live. So even that district of Uttar Pradesh is the size of a country, and if it were its own country, it would have a higher neonatal mortality rate than any other country. In other words, babies born in Bahraich district are more likely to die in their first month of life than babies born in any country around the world." — Dean Spears</p><p>In today’s episode, host Luisa Rodriguez speaks to Dean Spears — associate professor of economics at the University of Texas at Austin and founding director of <a href="https://riceinstitute.org/">r.i.c.e.</a> — about his experience implementing a surprisingly low-tech but highly cost-effective kangaroo mother care programme in Uttar Pradesh, India to save the lives of vulnerable newborn infants.</p><p><a href="https://80k.info/ds"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The shockingly high neonatal mortality rates in Uttar Pradesh, India, and how social inequality and gender dynamics contribute to poor health outcomes for both mothers and babies.</li><li>The remarkable benefits for vulnerable newborns that come from skin-to-skin contact and breastfeeding support.</li><li>The challenges and opportunities that come with working with a government hospital to implement new, evidence-based programmes.</li><li>How the currently small programme might be scaled up to save more newborns’ lives in other regions of Uttar Pradesh and beyond.</li><li>How targeted health interventions stack up against direct cash transfers.</li><li>Plus, a sneak peak into Dean’s new book, which explores the looming global population peak that’s expected around 2080, and the consequences of global depopulation.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Why is low birthweight a major problem in Uttar Pradesh? (00:02:45)</li><li>Neonatal mortality and maternal health in Uttar Pradesh (00:06:10)</li><li>Kangaroo mother care (00:12:08)</li><li>What would happen without this intervention? (00:16:07)</li><li>Evidence of KMC’s effectiveness (00:18:15)</li><li>Longer-term outcomes (00:32:14)</li><li>GiveWell’s support and implementation challenges (00:41:13)</li><li>How can KMC be so cost effective? (00:52:38)</li><li>Programme evaluation (00:57:21)</li><li>Is KMC is better than direct cash transfers? (00:59:12)</li><li>Expanding the programme and what skills are needed (01:01:29)</li><li>Fertility and population decline (01:07:28)</li><li>What advice Dean would give his younger self (01:16:09)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"I work in a place called Uttar Pradesh, which is a state in India with 240 million people. One in every 33 people in the whole world lives in Uttar Pradesh. It would be the fifth largest country if it were its own country. And if it were its own country, you’d probably know about its human development challenges, because it would have the highest neonatal mortality rate of any country except for South Sudan and Pakistan. Forty percent of children there are stunted. Only two-thirds of women are literate. So Uttar Pradesh is a place where there are lots of health challenges.</p><p>"And then even within that, we’re working in a district called Bahraich, where about 4 million people live. So even that district of Uttar Pradesh is the size of a country, and if it were its own country, it would have a higher neonatal mortality rate than any other country. In other words, babies born in Bahraich district are more likely to die in their first month of life than babies born in any country around the world." — Dean Spears</p><p>In today’s episode, host Luisa Rodriguez speaks to Dean Spears — associate professor of economics at the University of Texas at Austin and founding director of <a href="https://riceinstitute.org/">r.i.c.e.</a> — about his experience implementing a surprisingly low-tech but highly cost-effective kangaroo mother care programme in Uttar Pradesh, India to save the lives of vulnerable newborn infants.</p><p><a href="https://80k.info/ds"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The shockingly high neonatal mortality rates in Uttar Pradesh, India, and how social inequality and gender dynamics contribute to poor health outcomes for both mothers and babies.</li><li>The remarkable benefits for vulnerable newborns that come from skin-to-skin contact and breastfeeding support.</li><li>The challenges and opportunities that come with working with a government hospital to implement new, evidence-based programmes.</li><li>How the currently small programme might be scaled up to save more newborns’ lives in other regions of Uttar Pradesh and beyond.</li><li>How targeted health interventions stack up against direct cash transfers.</li><li>Plus, a sneak peak into Dean’s new book, which explores the looming global population peak that’s expected around 2080, and the consequences of global depopulation.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Why is low birthweight a major problem in Uttar Pradesh? (00:02:45)</li><li>Neonatal mortality and maternal health in Uttar Pradesh (00:06:10)</li><li>Kangaroo mother care (00:12:08)</li><li>What would happen without this intervention? (00:16:07)</li><li>Evidence of KMC’s effectiveness (00:18:15)</li><li>Longer-term outcomes (00:32:14)</li><li>GiveWell’s support and implementation challenges (00:41:13)</li><li>How can KMC be so cost effective? (00:52:38)</li><li>Programme evaluation (00:57:21)</li><li>Is KMC is better than direct cash transfers? (00:59:12)</li><li>Expanding the programme and what skills are needed (01:01:29)</li><li>Fertility and population decline (01:07:28)</li><li>What advice Dean would give his younger self (01:16:09)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 01 May 2024 18:09:45 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a7e05ef6/0780a13a.mp3" length="75843824" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/tRukuTFGm__VHPVoX0JPxP6iEdkU6Av5T9oX3g7fQv8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8xYzBi/YzMxY2NmMzdkZWUw/MDQyMjRiMTNiYTNk/NmQzZi5qcGc.jpg"/>
      <itunes:duration>4738</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"I work in a place called Uttar Pradesh, which is a state in India with 240 million people. One in every 33 people in the whole world lives in Uttar Pradesh. It would be the fifth largest country if it were its own country. And if it were its own country, you’d probably know about its human development challenges, because it would have the highest neonatal mortality rate of any country except for South Sudan and Pakistan. Forty percent of children there are stunted. Only two-thirds of women are literate. So Uttar Pradesh is a place where there are lots of health challenges.</p><p>"And then even within that, we’re working in a district called Bahraich, where about 4 million people live. So even that district of Uttar Pradesh is the size of a country, and if it were its own country, it would have a higher neonatal mortality rate than any other country. In other words, babies born in Bahraich district are more likely to die in their first month of life than babies born in any country around the world." — Dean Spears</p><p>In today’s episode, host Luisa Rodriguez speaks to Dean Spears — associate professor of economics at the University of Texas at Austin and founding director of <a href="https://riceinstitute.org/">r.i.c.e.</a> — about his experience implementing a surprisingly low-tech but highly cost-effective kangaroo mother care programme in Uttar Pradesh, India to save the lives of vulnerable newborn infants.</p><p><a href="https://80k.info/ds"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The shockingly high neonatal mortality rates in Uttar Pradesh, India, and how social inequality and gender dynamics contribute to poor health outcomes for both mothers and babies.</li><li>The remarkable benefits for vulnerable newborns that come from skin-to-skin contact and breastfeeding support.</li><li>The challenges and opportunities that come with working with a government hospital to implement new, evidence-based programmes.</li><li>How the currently small programme might be scaled up to save more newborns’ lives in other regions of Uttar Pradesh and beyond.</li><li>How targeted health interventions stack up against direct cash transfers.</li><li>Plus, a sneak peak into Dean’s new book, which explores the looming global population peak that’s expected around 2080, and the consequences of global depopulation.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Why is low birthweight a major problem in Uttar Pradesh? (00:02:45)</li><li>Neonatal mortality and maternal health in Uttar Pradesh (00:06:10)</li><li>Kangaroo mother care (00:12:08)</li><li>What would happen without this intervention? (00:16:07)</li><li>Evidence of KMC’s effectiveness (00:18:15)</li><li>Longer-term outcomes (00:32:14)</li><li>GiveWell’s support and implementation challenges (00:41:13)</li><li>How can KMC be so cost effective? (00:52:38)</li><li>Programme evaluation (00:57:21)</li><li>Is KMC is better than direct cash transfers? (00:59:12)</li><li>Expanding the programme and what skills are needed (01:01:29)</li><li>Fertility and population decline (01:07:28)</li><li>What advice Dean would give his younger self (01:16:09)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/a7e05ef6/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#185 – Lewis Bollard on the 7 most promising ways to end factory farming, and whether AI is going to be good or bad for animals</title>
      <itunes:title>#185 – Lewis Bollard on the 7 most promising ways to end factory farming, and whether AI is going to be good or bad for animals</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">0e00e4dd-57c6-43e5-a2d7-670010c1022c</guid>
      <link>https://80000hours.org/podcast/episodes/lewis-bollard-factory-farm-advocacy-gains/?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"The constraint right now on factory farming is how far can you push the biology of these animals? But AI could remove that constraint. It could say, 'Actually, we can push them further in these ways and these ways, and they still stay alive. And we’ve modelled out every possibility and we’ve found that it works.' I think another possibility, which I don’t understand as well, is that AI could lock in current moral values. And I think in particular there’s a risk that if AI is learning from what we do as humans today, the lesson it’s going to learn is that it’s OK to tolerate mass cruelty, so long as it occurs behind closed doors. I think there’s a risk that if it learns that, then it perpetuates that value, and perhaps slows human moral progress on this issue." —Lewis Bollard</p><p>In today’s episode, host Luisa Rodriguez speaks to Lewis Bollard — <a href="https://www.openphilanthropy.org/about/team/lewis-bollard/">director of the Farm Animal Welfare programme at Open Philanthropy</a> — about the promising progress and future interventions to end the worst factory farming practices still around today.</p><p><a href="https://80k.info/lb24"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The staggering scale of animal suffering in factory farms, and how it will only get worse without intervention.</li><li>Work to improve farmed animal welfare that Open Philanthropy is excited about funding.</li><li>The amazing recent progress made in farm animal welfare — including regulatory attention in the EU and a big win at the US Supreme Court — and the work that still needs to be done.</li><li>The occasional tension between ending factory farming and curbing climate change</li><li>How AI could transform factory farming for better or worse — and Lewis’s fears that the technology will just help us maximise cruelty in the name of profit.</li><li>How Lewis has updated his opinions or grantmaking as a result of <a href="https://80000hours.org/podcast/episodes/bob-fischer-comparing-animal-welfare-moral-weight/">new research on the “moral weights” of different species</a>.</li><li>Lewis’s personal journey working on farm animal welfare, and how he copes with the emotional toll of confronting the scale of animal suffering.</li><li>How listeners can get involved in the growing movement to end factory farming — from career and volunteer opportunities to impactful donations.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Common objections to ending factory farming (00:13:21)</li><li>Potential solutions (00:30:55)</li><li>Cage-free reforms (00:34:25)</li><li>Broiler chicken welfare (00:46:48)</li><li>Do companies follow through on these commitments? (01:00:21)</li><li>Fish welfare (01:05:02)</li><li>Alternatives to animal proteins (01:16:36)</li><li>Farm animal welfare in Asia (01:26:00)</li><li>Farm animal welfare in Europe (01:30:45)</li><li>Animal welfare science (01:42:09)</li><li>Approaches Lewis is less excited about (01:52:10)</li><li>Will we end factory farming in our lifetimes? (01:56:36)</li><li>Effect of AI (01:57:59)</li><li>Recent big wins for farm animals (02:07:38)</li><li>How animal advocacy has changed since Lewis first got involved (02:15:57)</li><li>Response to the Moral Weight Project (02:19:52)</li><li>How to help (02:28:14)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore<br></em><br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"The constraint right now on factory farming is how far can you push the biology of these animals? But AI could remove that constraint. It could say, 'Actually, we can push them further in these ways and these ways, and they still stay alive. And we’ve modelled out every possibility and we’ve found that it works.' I think another possibility, which I don’t understand as well, is that AI could lock in current moral values. And I think in particular there’s a risk that if AI is learning from what we do as humans today, the lesson it’s going to learn is that it’s OK to tolerate mass cruelty, so long as it occurs behind closed doors. I think there’s a risk that if it learns that, then it perpetuates that value, and perhaps slows human moral progress on this issue." —Lewis Bollard</p><p>In today’s episode, host Luisa Rodriguez speaks to Lewis Bollard — <a href="https://www.openphilanthropy.org/about/team/lewis-bollard/">director of the Farm Animal Welfare programme at Open Philanthropy</a> — about the promising progress and future interventions to end the worst factory farming practices still around today.</p><p><a href="https://80k.info/lb24"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The staggering scale of animal suffering in factory farms, and how it will only get worse without intervention.</li><li>Work to improve farmed animal welfare that Open Philanthropy is excited about funding.</li><li>The amazing recent progress made in farm animal welfare — including regulatory attention in the EU and a big win at the US Supreme Court — and the work that still needs to be done.</li><li>The occasional tension between ending factory farming and curbing climate change</li><li>How AI could transform factory farming for better or worse — and Lewis’s fears that the technology will just help us maximise cruelty in the name of profit.</li><li>How Lewis has updated his opinions or grantmaking as a result of <a href="https://80000hours.org/podcast/episodes/bob-fischer-comparing-animal-welfare-moral-weight/">new research on the “moral weights” of different species</a>.</li><li>Lewis’s personal journey working on farm animal welfare, and how he copes with the emotional toll of confronting the scale of animal suffering.</li><li>How listeners can get involved in the growing movement to end factory farming — from career and volunteer opportunities to impactful donations.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Common objections to ending factory farming (00:13:21)</li><li>Potential solutions (00:30:55)</li><li>Cage-free reforms (00:34:25)</li><li>Broiler chicken welfare (00:46:48)</li><li>Do companies follow through on these commitments? (01:00:21)</li><li>Fish welfare (01:05:02)</li><li>Alternatives to animal proteins (01:16:36)</li><li>Farm animal welfare in Asia (01:26:00)</li><li>Farm animal welfare in Europe (01:30:45)</li><li>Animal welfare science (01:42:09)</li><li>Approaches Lewis is less excited about (01:52:10)</li><li>Will we end factory farming in our lifetimes? (01:56:36)</li><li>Effect of AI (01:57:59)</li><li>Recent big wins for farm animals (02:07:38)</li><li>How animal advocacy has changed since Lewis first got involved (02:15:57)</li><li>Response to the Moral Weight Project (02:19:52)</li><li>How to help (02:28:14)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore<br></em><br></p>]]>
      </content:encoded>
      <pubDate>Thu, 18 Apr 2024 19:27:07 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/65d9dd76/8cfa08ab.mp3" length="147083790" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/imXpEIobTf1Tu3ykfOFveqnOnmT6tJ7ZVAsqYbtu0h4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8wZjA5/NTU2ZmMxMzcwM2U4/NDZjZWVjZGVmZTJl/YWZmZS5qcGc.jpg"/>
      <itunes:duration>9192</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"The constraint right now on factory farming is how far can you push the biology of these animals? But AI could remove that constraint. It could say, 'Actually, we can push them further in these ways and these ways, and they still stay alive. And we’ve modelled out every possibility and we’ve found that it works.' I think another possibility, which I don’t understand as well, is that AI could lock in current moral values. And I think in particular there’s a risk that if AI is learning from what we do as humans today, the lesson it’s going to learn is that it’s OK to tolerate mass cruelty, so long as it occurs behind closed doors. I think there’s a risk that if it learns that, then it perpetuates that value, and perhaps slows human moral progress on this issue." —Lewis Bollard</p><p>In today’s episode, host Luisa Rodriguez speaks to Lewis Bollard — <a href="https://www.openphilanthropy.org/about/team/lewis-bollard/">director of the Farm Animal Welfare programme at Open Philanthropy</a> — about the promising progress and future interventions to end the worst factory farming practices still around today.</p><p><a href="https://80k.info/lb24"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The staggering scale of animal suffering in factory farms, and how it will only get worse without intervention.</li><li>Work to improve farmed animal welfare that Open Philanthropy is excited about funding.</li><li>The amazing recent progress made in farm animal welfare — including regulatory attention in the EU and a big win at the US Supreme Court — and the work that still needs to be done.</li><li>The occasional tension between ending factory farming and curbing climate change</li><li>How AI could transform factory farming for better or worse — and Lewis’s fears that the technology will just help us maximise cruelty in the name of profit.</li><li>How Lewis has updated his opinions or grantmaking as a result of <a href="https://80000hours.org/podcast/episodes/bob-fischer-comparing-animal-welfare-moral-weight/">new research on the “moral weights” of different species</a>.</li><li>Lewis’s personal journey working on farm animal welfare, and how he copes with the emotional toll of confronting the scale of animal suffering.</li><li>How listeners can get involved in the growing movement to end factory farming — from career and volunteer opportunities to impactful donations.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Common objections to ending factory farming (00:13:21)</li><li>Potential solutions (00:30:55)</li><li>Cage-free reforms (00:34:25)</li><li>Broiler chicken welfare (00:46:48)</li><li>Do companies follow through on these commitments? (01:00:21)</li><li>Fish welfare (01:05:02)</li><li>Alternatives to animal proteins (01:16:36)</li><li>Farm animal welfare in Asia (01:26:00)</li><li>Farm animal welfare in Europe (01:30:45)</li><li>Animal welfare science (01:42:09)</li><li>Approaches Lewis is less excited about (01:52:10)</li><li>Will we end factory farming in our lifetimes? (01:56:36)</li><li>Effect of AI (01:57:59)</li><li>Recent big wins for farm animals (02:07:38)</li><li>How animal advocacy has changed since Lewis first got involved (02:15:57)</li><li>Response to the Moral Weight Project (02:19:52)</li><li>How to help (02:28:14)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio engineering lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore<br></em><br></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/65d9dd76/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#184 – Zvi Mowshowitz on sleeping on sleeper agents, and the biggest AI updates since ChatGPT</title>
      <itunes:title>#184 – Zvi Mowshowitz on sleeping on sleeper agents, and the biggest AI updates since ChatGPT</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f39dad01-b081-4502-82e8-5e76d9ac6dff</guid>
      <link>https://80000hours.org/podcast/episodes/zvi-mowshowitz-sleeper-agents-ai-updates/?utm_campaign=podcast__zvi-mowshowitz&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Many of you will have heard of Zvi Mowshowitz as a superhuman information-absorbing-and-processing machine — which he definitely is. As the author of the Substack <a href="https://thezvi.substack.com/">Don’t Worry About the Vase</a>, Zvi has spent as much time as literally anyone in the world over the last two years tracking in detail how the explosion of AI has been playing out — and he has strong opinions about almost every aspect of it. </p><p><a href="https://80k.info/zm"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In today’s episode, host Rob Wiblin asks Zvi for his takes on:</p><ul><li>US-China negotiations</li><li>Whether AI progress has stalled</li><li>The biggest wins and losses for alignment in 2023</li><li>EU and White House AI regulations</li><li>Which major AI lab has the best safety strategy</li><li>The pros and cons of the Pause AI movement</li><li>Recent breakthroughs in capabilities</li><li>In what situations it’s morally acceptable to work at AI labs</li></ul><p>Whether you agree or disagree with his views, Zvi is super informed and brimming with concrete details.</p><p><br>Zvi and Rob also talk about:</p><ul><li>The risk of AI labs fooling themselves into believing their alignment plans are working when they may not be.</li><li>The “<a href="https://thezvi.substack.com/p/on-anthropics-sleeper-agents-paper">sleeper agent</a>” issue uncovered in a recent Anthropic paper, and how it shows us how hard alignment actually is.</li><li>Why Zvi disagrees with 80,000 Hours’ advice about gaining career capital to have a positive impact.</li><li>Zvi’s project to identify the most strikingly horrible and neglected policy failures in the US, and how Zvi founded a new think tank (<a href="https://www.balsaresearch.com/">Balsa Research</a>) to identify innovative solutions to overthrow the horrible status quo in areas like domestic shipping, environmental reviews, and housing supply.</li><li>Why Zvi thinks that improving people’s prosperity and housing can make them care more about existential risks like AI.</li><li>An idea from the online rationality community that Zvi thinks is really underrated and more people should have heard of: <a href="https://thezvi.substack.com/p/simulacra-levels-summary">simulacra levels</a>.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Zvi’s AI-related worldview (00:03:41)</li><li>Sleeper agents (00:05:55)</li><li>Safety plans of the three major labs (00:21:47)</li><li>Misalignment vs misuse vs structural issues (00:50:00)</li><li>Should concerned people work at AI labs? (00:55:45)</li><li>Pause AI campaign (01:30:16)</li><li>Has progress on useful AI products stalled? (01:38:03)</li><li>White House executive order and US politics (01:42:09)</li><li>Reasons for AI policy optimism (01:56:38)</li><li>Zvi’s day-to-day (02:09:47)</li><li>Big wins and losses on safety and alignment in 2023 (02:12:29)</li><li>Other unappreciated technical breakthroughs (02:17:54)</li><li>Concrete things we can do to mitigate risks (02:31:19)</li><li>Balsa Research and the Jones Act (02:34:40)</li><li>The National Environmental Policy Act (02:50:36)</li><li>Housing policy (02:59:59)</li><li>Underrated rationalist worldviews (03:16:22)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions and additional content editing: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Many of you will have heard of Zvi Mowshowitz as a superhuman information-absorbing-and-processing machine — which he definitely is. As the author of the Substack <a href="https://thezvi.substack.com/">Don’t Worry About the Vase</a>, Zvi has spent as much time as literally anyone in the world over the last two years tracking in detail how the explosion of AI has been playing out — and he has strong opinions about almost every aspect of it. </p><p><a href="https://80k.info/zm"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In today’s episode, host Rob Wiblin asks Zvi for his takes on:</p><ul><li>US-China negotiations</li><li>Whether AI progress has stalled</li><li>The biggest wins and losses for alignment in 2023</li><li>EU and White House AI regulations</li><li>Which major AI lab has the best safety strategy</li><li>The pros and cons of the Pause AI movement</li><li>Recent breakthroughs in capabilities</li><li>In what situations it’s morally acceptable to work at AI labs</li></ul><p>Whether you agree or disagree with his views, Zvi is super informed and brimming with concrete details.</p><p><br>Zvi and Rob also talk about:</p><ul><li>The risk of AI labs fooling themselves into believing their alignment plans are working when they may not be.</li><li>The “<a href="https://thezvi.substack.com/p/on-anthropics-sleeper-agents-paper">sleeper agent</a>” issue uncovered in a recent Anthropic paper, and how it shows us how hard alignment actually is.</li><li>Why Zvi disagrees with 80,000 Hours’ advice about gaining career capital to have a positive impact.</li><li>Zvi’s project to identify the most strikingly horrible and neglected policy failures in the US, and how Zvi founded a new think tank (<a href="https://www.balsaresearch.com/">Balsa Research</a>) to identify innovative solutions to overthrow the horrible status quo in areas like domestic shipping, environmental reviews, and housing supply.</li><li>Why Zvi thinks that improving people’s prosperity and housing can make them care more about existential risks like AI.</li><li>An idea from the online rationality community that Zvi thinks is really underrated and more people should have heard of: <a href="https://thezvi.substack.com/p/simulacra-levels-summary">simulacra levels</a>.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Zvi’s AI-related worldview (00:03:41)</li><li>Sleeper agents (00:05:55)</li><li>Safety plans of the three major labs (00:21:47)</li><li>Misalignment vs misuse vs structural issues (00:50:00)</li><li>Should concerned people work at AI labs? (00:55:45)</li><li>Pause AI campaign (01:30:16)</li><li>Has progress on useful AI products stalled? (01:38:03)</li><li>White House executive order and US politics (01:42:09)</li><li>Reasons for AI policy optimism (01:56:38)</li><li>Zvi’s day-to-day (02:09:47)</li><li>Big wins and losses on safety and alignment in 2023 (02:12:29)</li><li>Other unappreciated technical breakthroughs (02:17:54)</li><li>Concrete things we can do to mitigate risks (02:31:19)</li><li>Balsa Research and the Jones Act (02:34:40)</li><li>The National Environmental Policy Act (02:50:36)</li><li>Housing policy (02:59:59)</li><li>Underrated rationalist worldviews (03:16:22)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions and additional content editing: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 11 Apr 2024 21:07:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/69395874/c14eca4d.mp3" length="202950557" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/UE_g3Opo-olwSMpztYkzIBqbYMNjAYYx_uSfzGbD3WM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lMjk4/MmQ0ZmI5MWE0ZmJi/MGVkZTE1ZjlmYjNi/NTBkYy5qcGc.jpg"/>
      <itunes:duration>12682</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Many of you will have heard of Zvi Mowshowitz as a superhuman information-absorbing-and-processing machine — which he definitely is. As the author of the Substack <a href="https://thezvi.substack.com/">Don’t Worry About the Vase</a>, Zvi has spent as much time as literally anyone in the world over the last two years tracking in detail how the explosion of AI has been playing out — and he has strong opinions about almost every aspect of it. </p><p><a href="https://80k.info/zm"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In today’s episode, host Rob Wiblin asks Zvi for his takes on:</p><ul><li>US-China negotiations</li><li>Whether AI progress has stalled</li><li>The biggest wins and losses for alignment in 2023</li><li>EU and White House AI regulations</li><li>Which major AI lab has the best safety strategy</li><li>The pros and cons of the Pause AI movement</li><li>Recent breakthroughs in capabilities</li><li>In what situations it’s morally acceptable to work at AI labs</li></ul><p>Whether you agree or disagree with his views, Zvi is super informed and brimming with concrete details.</p><p><br>Zvi and Rob also talk about:</p><ul><li>The risk of AI labs fooling themselves into believing their alignment plans are working when they may not be.</li><li>The “<a href="https://thezvi.substack.com/p/on-anthropics-sleeper-agents-paper">sleeper agent</a>” issue uncovered in a recent Anthropic paper, and how it shows us how hard alignment actually is.</li><li>Why Zvi disagrees with 80,000 Hours’ advice about gaining career capital to have a positive impact.</li><li>Zvi’s project to identify the most strikingly horrible and neglected policy failures in the US, and how Zvi founded a new think tank (<a href="https://www.balsaresearch.com/">Balsa Research</a>) to identify innovative solutions to overthrow the horrible status quo in areas like domestic shipping, environmental reviews, and housing supply.</li><li>Why Zvi thinks that improving people’s prosperity and housing can make them care more about existential risks like AI.</li><li>An idea from the online rationality community that Zvi thinks is really underrated and more people should have heard of: <a href="https://thezvi.substack.com/p/simulacra-levels-summary">simulacra levels</a>.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Zvi’s AI-related worldview (00:03:41)</li><li>Sleeper agents (00:05:55)</li><li>Safety plans of the three major labs (00:21:47)</li><li>Misalignment vs misuse vs structural issues (00:50:00)</li><li>Should concerned people work at AI labs? (00:55:45)</li><li>Pause AI campaign (01:30:16)</li><li>Has progress on useful AI products stalled? (01:38:03)</li><li>White House executive order and US politics (01:42:09)</li><li>Reasons for AI policy optimism (01:56:38)</li><li>Zvi’s day-to-day (02:09:47)</li><li>Big wins and losses on safety and alignment in 2023 (02:12:29)</li><li>Other unappreciated technical breakthroughs (02:17:54)</li><li>Concrete things we can do to mitigate risks (02:31:19)</li><li>Balsa Research and the Jones Act (02:34:40)</li><li>The National Environmental Policy Act (02:50:36)</li><li>Housing policy (02:59:59)</li><li>Underrated rationalist worldviews (03:16:22)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions and additional content editing: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/69395874/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>AI governance and policy (Article)</title>
      <itunes:title>AI governance and policy (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">af7c3eb1-1673-45db-8dca-553c00901fc5</guid>
      <link>https://80000hours.org/career-reviews/ai-policy-and-strategy/?utm_campaign=podcast__ai-governance&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Today’s release is a reading of our career review of <a href="https://80k.info/AIG"><strong>AI governance and policy</strong></a>, written and narrated by Cody Fenwick.</p><p>Advanced AI systems could have massive impacts on humanity and potentially pose global catastrophic risks, and there are opportunities in the broad field of AI governance to positively shape how society responds to and prepares for the challenges posed by the technology.</p><p>Given the high stakes, pursuing this career path could be many people’s highest-impact option. But they should be very careful not to accidentally exacerbate the threats rather than mitigate them.</p><p>If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.info/AIG"><strong>here.</strong></a><strong></strong></p><p><em>Editing and audio proofing: Ben Cordell and Simon Monsour<br>Narration: Cody Fenwick</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s release is a reading of our career review of <a href="https://80k.info/AIG"><strong>AI governance and policy</strong></a>, written and narrated by Cody Fenwick.</p><p>Advanced AI systems could have massive impacts on humanity and potentially pose global catastrophic risks, and there are opportunities in the broad field of AI governance to positively shape how society responds to and prepares for the challenges posed by the technology.</p><p>Given the high stakes, pursuing this career path could be many people’s highest-impact option. But they should be very careful not to accidentally exacerbate the threats rather than mitigate them.</p><p>If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.info/AIG"><strong>here.</strong></a><strong></strong></p><p><em>Editing and audio proofing: Ben Cordell and Simon Monsour<br>Narration: Cody Fenwick</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 28 Mar 2024 20:12:31 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/77155494/6404c792.mp3" length="49125609" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/CTyJwb_IFPdZGzJDzoEIhnTDBLrMa9NxVIU2-f6D_dE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE4MTczOTEv/MTcxMTY1NTk1Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3066</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Today’s release is a reading of our career review of <a href="https://80k.info/AIG"><strong>AI governance and policy</strong></a>, written and narrated by Cody Fenwick.</p><p>Advanced AI systems could have massive impacts on humanity and potentially pose global catastrophic risks, and there are opportunities in the broad field of AI governance to positively shape how society responds to and prepares for the challenges posed by the technology.</p><p>Given the high stakes, pursuing this career path could be many people’s highest-impact option. But they should be very careful not to accidentally exacerbate the threats rather than mitigate them.</p><p>If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.info/AIG"><strong>here.</strong></a><strong></strong></p><p><em>Editing and audio proofing: Ben Cordell and Simon Monsour<br>Narration: Cody Fenwick</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/77155494/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#183 – Spencer Greenberg on causation without correlation, money and happiness, lightgassing, hype vs value, and more</title>
      <itunes:title>#183 – Spencer Greenberg on causation without correlation, money and happiness, lightgassing, hype vs value, and more</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">de6f6d95-96c6-487d-aa07-c18bad40b6fd</guid>
      <link>https://80000hours.org/podcast/episodes/spencer-greenberg-money-happiness-hype-value/?utm_campaign=podcast__spencer-greenberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"When a friend comes to me with a decision, and they want my thoughts on it, very rarely am I trying to give them a really specific answer, like, 'I solved your problem.' What I’m trying to do often is give them other ways of thinking about what they’re doing, or giving different framings. A classic example of this would be someone who’s been working on a project for a long time and they feel really trapped by it. And someone says, 'Let’s suppose you currently weren’t working on the project, but you could join it. And if you joined, it would be exactly the state it is now. Would you join?' And they’d be like, 'Hell no!' It’s a reframe. It doesn’t mean you definitely shouldn’t join, but it’s a reframe that gives you a new way of looking at it." —Spencer Greenberg</p><p>In today’s episode, host Rob Wiblin speaks for a fourth time with listener favourite Spencer Greenberg — serial entrepreneur and host of the <a href="https://podcast.clearerthinking.org/"><em>Clearer Thinking</em> podcast</a> — about a grab-bag of topics that Spencer has explored since his <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-stopping-valueless-papers/">last appearance on the show</a> a year ago.</p><p><a href="https://80k.info/sg24"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How much money makes you happy — and the tricky methodological issues that come up trying to answer that question.</li><li>The importance of hype in making valuable things happen.</li><li>How to recognise warning signs that someone is untrustworthy or likely to hurt you.</li><li>Whether Registered Reports are successfully solving reproducibility issues in science.</li><li>The personal principles Spencer lives by, and whether or not we should all establish our own list of life principles.</li><li>The biggest and most harmful systemic mistakes we commit when making decisions, both individually and as groups.</li><li>The potential harms of lightgassing, which is the opposite of gaslighting.</li><li>How Spencer’s team used non-statistical methods to test whether astrology works.</li><li>Whether there’s any social value in retaliation.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Does money make you happy? (00:05:54)</li><li>Hype vs value (00:31:27)</li><li>Warning signs that someone is bad news (00:41:25)</li><li>Integrity and reproducibility in social science research (00:57:54)</li><li>Personal principles (01:16:22)</li><li>Decision-making errors (01:25:56)</li><li>Lightgassing (01:49:23)</li><li>Astrology (02:02:26)</li><li>Game theory, tit for tat, and retaliation (02:20:51)</li><li>Parenting (02:30:00)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"When a friend comes to me with a decision, and they want my thoughts on it, very rarely am I trying to give them a really specific answer, like, 'I solved your problem.' What I’m trying to do often is give them other ways of thinking about what they’re doing, or giving different framings. A classic example of this would be someone who’s been working on a project for a long time and they feel really trapped by it. And someone says, 'Let’s suppose you currently weren’t working on the project, but you could join it. And if you joined, it would be exactly the state it is now. Would you join?' And they’d be like, 'Hell no!' It’s a reframe. It doesn’t mean you definitely shouldn’t join, but it’s a reframe that gives you a new way of looking at it." —Spencer Greenberg</p><p>In today’s episode, host Rob Wiblin speaks for a fourth time with listener favourite Spencer Greenberg — serial entrepreneur and host of the <a href="https://podcast.clearerthinking.org/"><em>Clearer Thinking</em> podcast</a> — about a grab-bag of topics that Spencer has explored since his <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-stopping-valueless-papers/">last appearance on the show</a> a year ago.</p><p><a href="https://80k.info/sg24"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How much money makes you happy — and the tricky methodological issues that come up trying to answer that question.</li><li>The importance of hype in making valuable things happen.</li><li>How to recognise warning signs that someone is untrustworthy or likely to hurt you.</li><li>Whether Registered Reports are successfully solving reproducibility issues in science.</li><li>The personal principles Spencer lives by, and whether or not we should all establish our own list of life principles.</li><li>The biggest and most harmful systemic mistakes we commit when making decisions, both individually and as groups.</li><li>The potential harms of lightgassing, which is the opposite of gaslighting.</li><li>How Spencer’s team used non-statistical methods to test whether astrology works.</li><li>Whether there’s any social value in retaliation.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Does money make you happy? (00:05:54)</li><li>Hype vs value (00:31:27)</li><li>Warning signs that someone is bad news (00:41:25)</li><li>Integrity and reproducibility in social science research (00:57:54)</li><li>Personal principles (01:16:22)</li><li>Decision-making errors (01:25:56)</li><li>Lightgassing (01:49:23)</li><li>Astrology (02:02:26)</li><li>Game theory, tit for tat, and retaliation (02:20:51)</li><li>Parenting (02:30:00)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 14 Mar 2024 17:40:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/1b71cea2/4a21c0d8.mp3" length="150447329" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/X67xl0-pz9tWfioU2Z31_s7fPnig-jYmdvo34CnsgDI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3OTAwMjMv/MTcxMDQyNDc2Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9398</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"When a friend comes to me with a decision, and they want my thoughts on it, very rarely am I trying to give them a really specific answer, like, 'I solved your problem.' What I’m trying to do often is give them other ways of thinking about what they’re doing, or giving different framings. A classic example of this would be someone who’s been working on a project for a long time and they feel really trapped by it. And someone says, 'Let’s suppose you currently weren’t working on the project, but you could join it. And if you joined, it would be exactly the state it is now. Would you join?' And they’d be like, 'Hell no!' It’s a reframe. It doesn’t mean you definitely shouldn’t join, but it’s a reframe that gives you a new way of looking at it." —Spencer Greenberg</p><p>In today’s episode, host Rob Wiblin speaks for a fourth time with listener favourite Spencer Greenberg — serial entrepreneur and host of the <a href="https://podcast.clearerthinking.org/"><em>Clearer Thinking</em> podcast</a> — about a grab-bag of topics that Spencer has explored since his <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-stopping-valueless-papers/">last appearance on the show</a> a year ago.</p><p><a href="https://80k.info/sg24"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How much money makes you happy — and the tricky methodological issues that come up trying to answer that question.</li><li>The importance of hype in making valuable things happen.</li><li>How to recognise warning signs that someone is untrustworthy or likely to hurt you.</li><li>Whether Registered Reports are successfully solving reproducibility issues in science.</li><li>The personal principles Spencer lives by, and whether or not we should all establish our own list of life principles.</li><li>The biggest and most harmful systemic mistakes we commit when making decisions, both individually and as groups.</li><li>The potential harms of lightgassing, which is the opposite of gaslighting.</li><li>How Spencer’s team used non-statistical methods to test whether astrology works.</li><li>Whether there’s any social value in retaliation.</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Does money make you happy? (00:05:54)</li><li>Hype vs value (00:31:27)</li><li>Warning signs that someone is bad news (00:41:25)</li><li>Integrity and reproducibility in social science research (00:57:54)</li><li>Personal principles (01:16:22)</li><li>Decision-making errors (01:25:56)</li><li>Lightgassing (01:49:23)</li><li>Astrology (02:02:26)</li><li>Game theory, tit for tat, and retaliation (02:20:51)</li><li>Parenting (02:30:00)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour, Milo McGuire, and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/1b71cea2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#182 – Bob Fischer on comparing the welfare of humans, chickens, pigs, octopuses, bees, and more</title>
      <itunes:title>#182 – Bob Fischer on comparing the welfare of humans, chickens, pigs, octopuses, bees, and more</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e0cd68c8-d0bb-4447-96d5-eb2cf11f2c2c</guid>
      <link>https://80000hours.org/podcast/episodes/bob-fischer-comparing-animal-welfare-moral-weight/?utm_campaign=podcast__bob-fischer&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"[One] thing is just to spend time thinking about the kinds of things animals can do and what their lives are like. Just how hard a chicken will work to get to a nest box before she lays an egg, the amount of labour she’s willing to go through to do that, to think about how important that is to her. And to realise that we can quantify that, and see how much they care, or to see that they get stressed out when fellow chickens are threatened and that they seem to have some sympathy for conspecifics.</p><p>"Those kinds of things make me say there is something in there that is recognisable to me as another individual, with desires and preferences and a vantage point on the world, who wants things to go a certain way and is frustrated and upset when they don’t. And recognising the individuality, the perspective of nonhuman animals, for me, really challenges my tendency to not take them as seriously as I think I ought to, all things considered." — Bob Fischer</p><p>In today’s episode, host Luisa Rodriguez speaks to Bob Fischer — senior research manager at <a href="https://rethinkpriorities.org/">Rethink Priorities</a> and the director of the <a href="https://www.ethicsandanimals.org/">Society for the Study of Ethics and Animals</a> — about Rethink Priorities’s <a href="https://rethinkpriorities.org/publications/an-introduction-to-the-moral-weight-project">Moral Weight Project</a>.</p><p><a href="https://80k.info/bf"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The methods used to assess the welfare ranges and capacities for pleasure and pain of chickens, pigs, octopuses, bees, and other animals — and the limitations of that approach.</li><li>Concrete examples of how someone might use the estimated moral weights to compare the benefits of animal vs human interventions.</li><li>The results that most surprised Bob.</li><li>Why the team used a hedonic theory of welfare to inform the project, and what non-hedonic theories of welfare might bring to the table.</li><li>Thought experiments like Tortured Tim that test different philosophical assumptions about welfare.</li><li>Confronting our own biases when estimating animal mental capacities and moral worth.</li><li>The limitations of using neuron counts as a proxy for moral weights.</li><li>How different types of risk aversion, like avoiding worst-case scenarios, could impact cause prioritisation.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Welfare ranges (00:10:19)</li><li>Historical assessments (00:16:47)</li><li>Method (00:24:02)</li><li>The present / absent approach (00:27:39)</li><li>Results (00:31:42)</li><li>Chickens (00:32:42)</li><li>Bees (00:50:00)</li><li>Salmon and limits of methodology (00:56:18)</li><li>Octopuses (01:00:31)</li><li>Pigs (01:27:50)</li><li>Surprises about the project (01:30:19)</li><li>Objections to the project (01:34:25)</li><li>Alternative decision theories and risk aversion (01:39:14)</li><li>Hedonism assumption (02:00:54)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"[One] thing is just to spend time thinking about the kinds of things animals can do and what their lives are like. Just how hard a chicken will work to get to a nest box before she lays an egg, the amount of labour she’s willing to go through to do that, to think about how important that is to her. And to realise that we can quantify that, and see how much they care, or to see that they get stressed out when fellow chickens are threatened and that they seem to have some sympathy for conspecifics.</p><p>"Those kinds of things make me say there is something in there that is recognisable to me as another individual, with desires and preferences and a vantage point on the world, who wants things to go a certain way and is frustrated and upset when they don’t. And recognising the individuality, the perspective of nonhuman animals, for me, really challenges my tendency to not take them as seriously as I think I ought to, all things considered." — Bob Fischer</p><p>In today’s episode, host Luisa Rodriguez speaks to Bob Fischer — senior research manager at <a href="https://rethinkpriorities.org/">Rethink Priorities</a> and the director of the <a href="https://www.ethicsandanimals.org/">Society for the Study of Ethics and Animals</a> — about Rethink Priorities’s <a href="https://rethinkpriorities.org/publications/an-introduction-to-the-moral-weight-project">Moral Weight Project</a>.</p><p><a href="https://80k.info/bf"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The methods used to assess the welfare ranges and capacities for pleasure and pain of chickens, pigs, octopuses, bees, and other animals — and the limitations of that approach.</li><li>Concrete examples of how someone might use the estimated moral weights to compare the benefits of animal vs human interventions.</li><li>The results that most surprised Bob.</li><li>Why the team used a hedonic theory of welfare to inform the project, and what non-hedonic theories of welfare might bring to the table.</li><li>Thought experiments like Tortured Tim that test different philosophical assumptions about welfare.</li><li>Confronting our own biases when estimating animal mental capacities and moral worth.</li><li>The limitations of using neuron counts as a proxy for moral weights.</li><li>How different types of risk aversion, like avoiding worst-case scenarios, could impact cause prioritisation.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Welfare ranges (00:10:19)</li><li>Historical assessments (00:16:47)</li><li>Method (00:24:02)</li><li>The present / absent approach (00:27:39)</li><li>Results (00:31:42)</li><li>Chickens (00:32:42)</li><li>Bees (00:50:00)</li><li>Salmon and limits of methodology (00:56:18)</li><li>Octopuses (01:00:31)</li><li>Pigs (01:27:50)</li><li>Surprises about the project (01:30:19)</li><li>Objections to the project (01:34:25)</li><li>Alternative decision theories and risk aversion (01:39:14)</li><li>Hedonism assumption (02:00:54)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 08 Mar 2024 19:45:16 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8ba23cec/073a8a6f.mp3" length="135879030" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/YXz84cJRcGFE0ZSmR_og6HTJGDq40rLv0BTpQ7zt9VQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3NzgzODIv/MTcwOTc0MDk4MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8491</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"[One] thing is just to spend time thinking about the kinds of things animals can do and what their lives are like. Just how hard a chicken will work to get to a nest box before she lays an egg, the amount of labour she’s willing to go through to do that, to think about how important that is to her. And to realise that we can quantify that, and see how much they care, or to see that they get stressed out when fellow chickens are threatened and that they seem to have some sympathy for conspecifics.</p><p>"Those kinds of things make me say there is something in there that is recognisable to me as another individual, with desires and preferences and a vantage point on the world, who wants things to go a certain way and is frustrated and upset when they don’t. And recognising the individuality, the perspective of nonhuman animals, for me, really challenges my tendency to not take them as seriously as I think I ought to, all things considered." — Bob Fischer</p><p>In today’s episode, host Luisa Rodriguez speaks to Bob Fischer — senior research manager at <a href="https://rethinkpriorities.org/">Rethink Priorities</a> and the director of the <a href="https://www.ethicsandanimals.org/">Society for the Study of Ethics and Animals</a> — about Rethink Priorities’s <a href="https://rethinkpriorities.org/publications/an-introduction-to-the-moral-weight-project">Moral Weight Project</a>.</p><p><a href="https://80k.info/bf"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The methods used to assess the welfare ranges and capacities for pleasure and pain of chickens, pigs, octopuses, bees, and other animals — and the limitations of that approach.</li><li>Concrete examples of how someone might use the estimated moral weights to compare the benefits of animal vs human interventions.</li><li>The results that most surprised Bob.</li><li>Why the team used a hedonic theory of welfare to inform the project, and what non-hedonic theories of welfare might bring to the table.</li><li>Thought experiments like Tortured Tim that test different philosophical assumptions about welfare.</li><li>Confronting our own biases when estimating animal mental capacities and moral worth.</li><li>The limitations of using neuron counts as a proxy for moral weights.</li><li>How different types of risk aversion, like avoiding worst-case scenarios, could impact cause prioritisation.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Welfare ranges (00:10:19)</li><li>Historical assessments (00:16:47)</li><li>Method (00:24:02)</li><li>The present / absent approach (00:27:39)</li><li>Results (00:31:42)</li><li>Chickens (00:32:42)</li><li>Bees (00:50:00)</li><li>Salmon and limits of methodology (00:56:18)</li><li>Octopuses (01:00:31)</li><li>Pigs (01:27:50)</li><li>Surprises about the project (01:30:19)</li><li>Objections to the project (01:34:25)</li><li>Alternative decision theories and risk aversion (01:39:14)</li><li>Hedonism assumption (02:00:54)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/8ba23cec/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#181 – Laura Deming on the science that could keep us healthy in our 80s and beyond</title>
      <itunes:title>#181 – Laura Deming on the science that could keep us healthy in our 80s and beyond</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">75c8525e-755a-45e4-9370-4d7bea3d19ac</guid>
      <link>https://80000hours.org/podcast/episodes/laura-deming-ending-ageing/?utm_campaign=podcast__laura-deming&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"The question I care about is: What do I want to do? Like, when I'm 80, how strong do I want to be? OK, and then if I want to be that strong, how well do my muscles have to work? OK, and then if that's true, what would they have to look like at the cellular level for that to be true? Then what do we have to do to make that happen? In my head, it's much more about agency and what choice do I have over my health. And even if I live the same number of years, can I live as an 80-year-old running every day happily with my grandkids?" — Laura Deming</p><p>In today’s episode, host Luisa Rodriguez speaks to Laura Deming — founder of <a href="https://www.longevity.vc/">The Longevity Fund</a> — about the challenge of ending ageing.</p><p><a href="https://80k.info/ld"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How lifespan is surprisingly easy to manipulate in animals, which suggests human longevity could be increased too.</li><li>Why we irrationally accept age-related health decline as inevitable.</li><li>The engineering mindset Laura takes to solving the problem of ageing.</li><li>Laura’s thoughts on how ending ageing is primarily a social challenge, not a scientific one.</li><li>The recent exciting regulatory breakthrough for an anti-ageing drug for dogs.</li><li>Laura’s vision for how increased longevity could positively transform society by giving humans agency over when and how they age.</li><li>Why this decade may be the most important decade ever for making progress on anti-ageing research.</li><li>The beauty and fascination of biology, which makes it such a compelling field to work in.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>The case for ending ageing (00:04:00)</li><li>What might the world look like if this all goes well? (00:21:57)</li><li>Reasons not to work on ageing research (00:27:25)</li><li>Things that make mice live longer (00:44:12)</li><li>Parabiosis, changing the brain, and organ replacement can increase lifespan (00:54:25)</li><li>Big wins the field of ageing research (01:11:40)</li><li>Talent shortages and other bottlenecks for ageing research (01:17:36)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"The question I care about is: What do I want to do? Like, when I'm 80, how strong do I want to be? OK, and then if I want to be that strong, how well do my muscles have to work? OK, and then if that's true, what would they have to look like at the cellular level for that to be true? Then what do we have to do to make that happen? In my head, it's much more about agency and what choice do I have over my health. And even if I live the same number of years, can I live as an 80-year-old running every day happily with my grandkids?" — Laura Deming</p><p>In today’s episode, host Luisa Rodriguez speaks to Laura Deming — founder of <a href="https://www.longevity.vc/">The Longevity Fund</a> — about the challenge of ending ageing.</p><p><a href="https://80k.info/ld"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How lifespan is surprisingly easy to manipulate in animals, which suggests human longevity could be increased too.</li><li>Why we irrationally accept age-related health decline as inevitable.</li><li>The engineering mindset Laura takes to solving the problem of ageing.</li><li>Laura’s thoughts on how ending ageing is primarily a social challenge, not a scientific one.</li><li>The recent exciting regulatory breakthrough for an anti-ageing drug for dogs.</li><li>Laura’s vision for how increased longevity could positively transform society by giving humans agency over when and how they age.</li><li>Why this decade may be the most important decade ever for making progress on anti-ageing research.</li><li>The beauty and fascination of biology, which makes it such a compelling field to work in.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>The case for ending ageing (00:04:00)</li><li>What might the world look like if this all goes well? (00:21:57)</li><li>Reasons not to work on ageing research (00:27:25)</li><li>Things that make mice live longer (00:44:12)</li><li>Parabiosis, changing the brain, and organ replacement can increase lifespan (00:54:25)</li><li>Big wins the field of ageing research (01:11:40)</li><li>Talent shortages and other bottlenecks for ageing research (01:17:36)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 01 Mar 2024 21:02:05 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ea87fe37/05b44a2f.mp3" length="93498278" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HlW8B7KtQryjMd9UPcMr3PkDsDkpDC4aac5nYzBxhKw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3NjkxMTIv/MTcwOTMxNjk4OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5841</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"The question I care about is: What do I want to do? Like, when I'm 80, how strong do I want to be? OK, and then if I want to be that strong, how well do my muscles have to work? OK, and then if that's true, what would they have to look like at the cellular level for that to be true? Then what do we have to do to make that happen? In my head, it's much more about agency and what choice do I have over my health. And even if I live the same number of years, can I live as an 80-year-old running every day happily with my grandkids?" — Laura Deming</p><p>In today’s episode, host Luisa Rodriguez speaks to Laura Deming — founder of <a href="https://www.longevity.vc/">The Longevity Fund</a> — about the challenge of ending ageing.</p><p><a href="https://80k.info/ld"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How lifespan is surprisingly easy to manipulate in animals, which suggests human longevity could be increased too.</li><li>Why we irrationally accept age-related health decline as inevitable.</li><li>The engineering mindset Laura takes to solving the problem of ageing.</li><li>Laura’s thoughts on how ending ageing is primarily a social challenge, not a scientific one.</li><li>The recent exciting regulatory breakthrough for an anti-ageing drug for dogs.</li><li>Laura’s vision for how increased longevity could positively transform society by giving humans agency over when and how they age.</li><li>Why this decade may be the most important decade ever for making progress on anti-ageing research.</li><li>The beauty and fascination of biology, which makes it such a compelling field to work in.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>The case for ending ageing (00:04:00)</li><li>What might the world look like if this all goes well? (00:21:57)</li><li>Reasons not to work on ageing research (00:27:25)</li><li>Things that make mice live longer (00:44:12)</li><li>Parabiosis, changing the brain, and organ replacement can increase lifespan (00:54:25)</li><li>Big wins the field of ageing research (01:11:40)</li><li>Talent shortages and other bottlenecks for ageing research (01:17:36)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/ea87fe37/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#180 – Hugo Mercier on why gullibility and misinformation are overrated</title>
      <itunes:title>#180 – Hugo Mercier on why gullibility and misinformation are overrated</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1ecaf2e2-199a-4ba5-b5a7-e7d4d20916f4</guid>
      <link>https://80000hours.org/podcast/episodes/hugo-mercier-misinformation-mass-persuasion/?utm_campaign=podcast__hugo-mercier&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>The World Economic Forum’s global risks survey of 1,400 experts, policymakers, and industry leaders ranked <a href="https://www.weforum.org/publications/global-risks-report-2024/digest/">misinformation and disinformation as the number one global risk over the next two years</a> — ranking it ahead of war, environmental problems, and other threats from AI.</p><p>And the discussion around misinformation and disinformation has shifted to focus on how generative AI or a future super-persuasive AI might change the game and make it extremely hard to figure out what was going on in the world — or alternatively, extremely easy to mislead people into believing convenient lies.</p><p>But this week’s guest, cognitive scientist Hugo Mercier, has a very different view on how people form beliefs and figure out who to trust — one in which misinformation really is barely a problem today, and is unlikely to be a problem anytime soon. As he explains in his book <a href="https://sites.google.com/site/hugomercier/not-born-yesterday?authuser=0"><em>Not Born Yesterday</em></a>, Hugo believes we seriously underrate the perceptiveness and judgement of ordinary people.</p><p><a href="https://80k.info/hm"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In this interview, host Rob Wiblin and Hugo discuss:</p><ul><li>How our reasoning mechanisms evolved to facilitate beneficial communication, not blind gullibility.</li><li>How Hugo makes sense of our apparent gullibility in many cases — like falling for financial scams, astrology, or bogus medical treatments, and voting for policies that aren’t actually beneficial for us.</li><li>Rob and Hugo’s ideas about whether AI might make misinformation radically worse, and which mass persuasion approaches we should be most worried about.</li><li>Why Hugo thinks our intuitions about who to trust are generally quite sound, even in today’s complex information environment.</li><li>The distinction between intuitive beliefs that guide our actions versus reflective beliefs that don’t.</li><li>Why fake news and conspiracy theories actually have less impact than most people assume.</li><li>False beliefs that have persisted across cultures and generations — like bloodletting and vaccine hesitancy — and theories about why.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>The view that humans are really gullible (00:04:26)</li><li>The evolutionary argument against humans being gullible (00:07:46) </li><li>Open vigilance (00:18:56)</li><li>Intuitive and reflective beliefs (00:32:25)</li><li>How people decide who to trust (00:41:15)</li><li>Redefining beliefs (00:51:57)</li><li>Bloodletting (01:00:38)</li><li>Vaccine hesitancy and creationism (01:06:38)</li><li>False beliefs without skin in the game (01:12:36)</li><li>One consistent weakness in human judgement (01:22:57)</li><li>Trying to explain harmful financial decisions (01:27:15)</li><li>Astrology (01:40:40)</li><li>Medical treatments that don’t work (01:45:47)</li><li>Generative AI, LLMs, and persuasion (01:54:50)</li><li>Ways AI could improve the information environment (02:29:59)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The World Economic Forum’s global risks survey of 1,400 experts, policymakers, and industry leaders ranked <a href="https://www.weforum.org/publications/global-risks-report-2024/digest/">misinformation and disinformation as the number one global risk over the next two years</a> — ranking it ahead of war, environmental problems, and other threats from AI.</p><p>And the discussion around misinformation and disinformation has shifted to focus on how generative AI or a future super-persuasive AI might change the game and make it extremely hard to figure out what was going on in the world — or alternatively, extremely easy to mislead people into believing convenient lies.</p><p>But this week’s guest, cognitive scientist Hugo Mercier, has a very different view on how people form beliefs and figure out who to trust — one in which misinformation really is barely a problem today, and is unlikely to be a problem anytime soon. As he explains in his book <a href="https://sites.google.com/site/hugomercier/not-born-yesterday?authuser=0"><em>Not Born Yesterday</em></a>, Hugo believes we seriously underrate the perceptiveness and judgement of ordinary people.</p><p><a href="https://80k.info/hm"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In this interview, host Rob Wiblin and Hugo discuss:</p><ul><li>How our reasoning mechanisms evolved to facilitate beneficial communication, not blind gullibility.</li><li>How Hugo makes sense of our apparent gullibility in many cases — like falling for financial scams, astrology, or bogus medical treatments, and voting for policies that aren’t actually beneficial for us.</li><li>Rob and Hugo’s ideas about whether AI might make misinformation radically worse, and which mass persuasion approaches we should be most worried about.</li><li>Why Hugo thinks our intuitions about who to trust are generally quite sound, even in today’s complex information environment.</li><li>The distinction between intuitive beliefs that guide our actions versus reflective beliefs that don’t.</li><li>Why fake news and conspiracy theories actually have less impact than most people assume.</li><li>False beliefs that have persisted across cultures and generations — like bloodletting and vaccine hesitancy — and theories about why.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>The view that humans are really gullible (00:04:26)</li><li>The evolutionary argument against humans being gullible (00:07:46) </li><li>Open vigilance (00:18:56)</li><li>Intuitive and reflective beliefs (00:32:25)</li><li>How people decide who to trust (00:41:15)</li><li>Redefining beliefs (00:51:57)</li><li>Bloodletting (01:00:38)</li><li>Vaccine hesitancy and creationism (01:06:38)</li><li>False beliefs without skin in the game (01:12:36)</li><li>One consistent weakness in human judgement (01:22:57)</li><li>Trying to explain harmful financial decisions (01:27:15)</li><li>Astrology (01:40:40)</li><li>Medical treatments that don’t work (01:45:47)</li><li>Generative AI, LLMs, and persuasion (01:54:50)</li><li>Ways AI could improve the information environment (02:29:59)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 21 Feb 2024 22:36:26 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a9cc2aa1/956425a1.mp3" length="150701086" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/6TLZRyghudzVbKxSqBjZsFgh67Fcb24G4JSS9y7VhWs/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3NDY4Mjkv/MTcwODUzMDk3NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9415</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>The World Economic Forum’s global risks survey of 1,400 experts, policymakers, and industry leaders ranked <a href="https://www.weforum.org/publications/global-risks-report-2024/digest/">misinformation and disinformation as the number one global risk over the next two years</a> — ranking it ahead of war, environmental problems, and other threats from AI.</p><p>And the discussion around misinformation and disinformation has shifted to focus on how generative AI or a future super-persuasive AI might change the game and make it extremely hard to figure out what was going on in the world — or alternatively, extremely easy to mislead people into believing convenient lies.</p><p>But this week’s guest, cognitive scientist Hugo Mercier, has a very different view on how people form beliefs and figure out who to trust — one in which misinformation really is barely a problem today, and is unlikely to be a problem anytime soon. As he explains in his book <a href="https://sites.google.com/site/hugomercier/not-born-yesterday?authuser=0"><em>Not Born Yesterday</em></a>, Hugo believes we seriously underrate the perceptiveness and judgement of ordinary people.</p><p><a href="https://80k.info/hm"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In this interview, host Rob Wiblin and Hugo discuss:</p><ul><li>How our reasoning mechanisms evolved to facilitate beneficial communication, not blind gullibility.</li><li>How Hugo makes sense of our apparent gullibility in many cases — like falling for financial scams, astrology, or bogus medical treatments, and voting for policies that aren’t actually beneficial for us.</li><li>Rob and Hugo’s ideas about whether AI might make misinformation radically worse, and which mass persuasion approaches we should be most worried about.</li><li>Why Hugo thinks our intuitions about who to trust are generally quite sound, even in today’s complex information environment.</li><li>The distinction between intuitive beliefs that guide our actions versus reflective beliefs that don’t.</li><li>Why fake news and conspiracy theories actually have less impact than most people assume.</li><li>False beliefs that have persisted across cultures and generations — like bloodletting and vaccine hesitancy — and theories about why.</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>The view that humans are really gullible (00:04:26)</li><li>The evolutionary argument against humans being gullible (00:07:46) </li><li>Open vigilance (00:18:56)</li><li>Intuitive and reflective beliefs (00:32:25)</li><li>How people decide who to trust (00:41:15)</li><li>Redefining beliefs (00:51:57)</li><li>Bloodletting (01:00:38)</li><li>Vaccine hesitancy and creationism (01:06:38)</li><li>False beliefs without skin in the game (01:12:36)</li><li>One consistent weakness in human judgement (01:22:57)</li><li>Trying to explain harmful financial decisions (01:27:15)</li><li>Astrology (01:40:40)</li><li>Medical treatments that don’t work (01:45:47)</li><li>Generative AI, LLMs, and persuasion (01:54:50)</li><li>Ways AI could improve the information environment (02:29:59)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/a9cc2aa1/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#179 – Randy Nesse on why evolution left us so vulnerable to depression and anxiety</title>
      <itunes:title>#179 – Randy Nesse on why evolution left us so vulnerable to depression and anxiety</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2b9dc1d4-103e-4b3a-afcb-ad35c1b11f54</guid>
      <link>https://80000hours.org/podcast/episodes/randy-nesse-evolutionary-medicine-psychiatry/?utm_campaign=podcast__randy-nesse&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Mental health problems like depression and anxiety affect enormous numbers of people and severely interfere with their lives. By contrast, we don’t see similar levels of physical ill health in young people. At any point in time, something like 20% of young people are working through anxiety or depression that’s seriously interfering with their lives — but nowhere near 20% of people in their 20s have severe heart disease or cancer or a similar failure in a key organ of the body other than the brain.</p><p>From an evolutionary perspective, that’s to be expected, right? If your heart or lungs or legs or skin stop working properly while you’re a teenager, you’re less likely to reproduce, and the genes that cause that malfunction get weeded out of the gene pool.</p><p>So why is it that these evolutionary selective pressures seemingly fixed our bodies so that they work pretty smoothly for young people most of the time, but it feels like evolution fell asleep on the job when it comes to the brain? Why did evolution never get around to patching the most basic problems, like social anxiety, panic attacks, debilitating pessimism, or inappropriate mood swings? For that matter, why did evolution go out of its way to give us the capacity for low mood or chronic anxiety or extreme mood swings <em>at all</em>?</p><p>Today’s guest, Randy Nesse — a leader in the field of evolutionary psychiatry — wrote the book <a href="https://www.amazon.com/Good-Reasons-for-Bad-Feelings-audiobook/dp/B07N7PR34Q"><em>Good Reasons for Bad Feelings</em></a>, in which he sets out to try to resolve this paradox.</p><p><a href="https://80k.info/rn"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p>In the interview, host Rob Wiblin and Randy discuss the key points of the book, as well as:</p><ul><li>How the evolutionary psychiatry perspective can help people appreciate that their mental health problems are often the result of a useful and important system.</li><li>How evolutionary pressures and dynamics lead to a wide range of different personalities, behaviours, strategies, and tradeoffs.</li><li>The missing intellectual foundations of psychiatry, and how an evolutionary lens could revolutionise the field.</li><li>How working as both an academic and a practicing psychiatrist shaped Randy’s understanding of treating mental health problems.</li><li>The “smoke detector principle” of why we experience so many false alarms along with true threats.</li><li>The origins of morality and capacity for genuine love, and why Randy thinks it’s a mistake to try to explain these from a selfish gene perspective.</li><li>Evolutionary theories on why we age and die.</li><li>And much more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Mental health problems like depression and anxiety affect enormous numbers of people and severely interfere with their lives. By contrast, we don’t see similar levels of physical ill health in young people. At any point in time, something like 20% of young people are working through anxiety or depression that’s seriously interfering with their lives — but nowhere near 20% of people in their 20s have severe heart disease or cancer or a similar failure in a key organ of the body other than the brain.</p><p>From an evolutionary perspective, that’s to be expected, right? If your heart or lungs or legs or skin stop working properly while you’re a teenager, you’re less likely to reproduce, and the genes that cause that malfunction get weeded out of the gene pool.</p><p>So why is it that these evolutionary selective pressures seemingly fixed our bodies so that they work pretty smoothly for young people most of the time, but it feels like evolution fell asleep on the job when it comes to the brain? Why did evolution never get around to patching the most basic problems, like social anxiety, panic attacks, debilitating pessimism, or inappropriate mood swings? For that matter, why did evolution go out of its way to give us the capacity for low mood or chronic anxiety or extreme mood swings <em>at all</em>?</p><p>Today’s guest, Randy Nesse — a leader in the field of evolutionary psychiatry — wrote the book <a href="https://www.amazon.com/Good-Reasons-for-Bad-Feelings-audiobook/dp/B07N7PR34Q"><em>Good Reasons for Bad Feelings</em></a>, in which he sets out to try to resolve this paradox.</p><p><a href="https://80k.info/rn"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p>In the interview, host Rob Wiblin and Randy discuss the key points of the book, as well as:</p><ul><li>How the evolutionary psychiatry perspective can help people appreciate that their mental health problems are often the result of a useful and important system.</li><li>How evolutionary pressures and dynamics lead to a wide range of different personalities, behaviours, strategies, and tradeoffs.</li><li>The missing intellectual foundations of psychiatry, and how an evolutionary lens could revolutionise the field.</li><li>How working as both an academic and a practicing psychiatrist shaped Randy’s understanding of treating mental health problems.</li><li>The “smoke detector principle” of why we experience so many false alarms along with true threats.</li><li>The origins of morality and capacity for genuine love, and why Randy thinks it’s a mistake to try to explain these from a selfish gene perspective.</li><li>Evolutionary theories on why we age and die.</li><li>And much more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 12 Feb 2024 23:26:09 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f93ce3e8/b9ed5ed6.mp3" length="169767551" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/3rE63uwkP9xBUeyBvk2POTMQ4yhDxTK0KVUTG5LtuqI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3MjU1OTQv/MTcwNzQ5ODcwNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10608</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Mental health problems like depression and anxiety affect enormous numbers of people and severely interfere with their lives. By contrast, we don’t see similar levels of physical ill health in young people. At any point in time, something like 20% of young people are working through anxiety or depression that’s seriously interfering with their lives — but nowhere near 20% of people in their 20s have severe heart disease or cancer or a similar failure in a key organ of the body other than the brain.</p><p>From an evolutionary perspective, that’s to be expected, right? If your heart or lungs or legs or skin stop working properly while you’re a teenager, you’re less likely to reproduce, and the genes that cause that malfunction get weeded out of the gene pool.</p><p>So why is it that these evolutionary selective pressures seemingly fixed our bodies so that they work pretty smoothly for young people most of the time, but it feels like evolution fell asleep on the job when it comes to the brain? Why did evolution never get around to patching the most basic problems, like social anxiety, panic attacks, debilitating pessimism, or inappropriate mood swings? For that matter, why did evolution go out of its way to give us the capacity for low mood or chronic anxiety or extreme mood swings <em>at all</em>?</p><p>Today’s guest, Randy Nesse — a leader in the field of evolutionary psychiatry — wrote the book <a href="https://www.amazon.com/Good-Reasons-for-Bad-Feelings-audiobook/dp/B07N7PR34Q"><em>Good Reasons for Bad Feelings</em></a>, in which he sets out to try to resolve this paradox.</p><p><a href="https://80k.info/rn"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p>In the interview, host Rob Wiblin and Randy discuss the key points of the book, as well as:</p><ul><li>How the evolutionary psychiatry perspective can help people appreciate that their mental health problems are often the result of a useful and important system.</li><li>How evolutionary pressures and dynamics lead to a wide range of different personalities, behaviours, strategies, and tradeoffs.</li><li>The missing intellectual foundations of psychiatry, and how an evolutionary lens could revolutionise the field.</li><li>How working as both an academic and a practicing psychiatrist shaped Randy’s understanding of treating mental health problems.</li><li>The “smoke detector principle” of why we experience so many false alarms along with true threats.</li><li>The origins of morality and capacity for genuine love, and why Randy thinks it’s a mistake to try to explain these from a selfish gene perspective.</li><li>Evolutionary theories on why we age and die.</li><li>And much more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/f93ce3e8/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#178 – Emily Oster on what the evidence actually says about pregnancy and parenting</title>
      <itunes:title>#178 – Emily Oster on what the evidence actually says about pregnancy and parenting</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2a217be0-de45-4164-9499-f585cde7feea</guid>
      <link>https://80000hours.org/podcast/episodes/emily-oster-pregnancy-parenting-careers/?utm_campaign=podcast__emily-oster&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"I think at various times — before you have the kid, after you have the kid — it's useful to sit down and think about: What do I want the shape of this to look like? What time do I want to be spending? Which hours? How do I want the weekends to look? The things that are going to shape the way your day-to-day goes, and the time you spend with your kids, and what you're doing in that time with your kids, and all of those things: you have an opportunity to deliberately plan them. And you can then feel like, 'I've thought about this, and this is a life that I want. This is a life that we're trying to craft for our family, for our kids.' And that is distinct from thinking you're doing a good job in every moment — which you can't achieve. But you can achieve, 'I'm doing this the way that I think works for my family.'" — Emily Oster</p><p>In today’s episode, host Luisa Rodriguez speaks to Emily Oster — economist at Brown University, host of the <em>ParentData</em> podcast, and the author of three hugely popular books that provide evidence-based insights into pregnancy and early childhood.</p><p><a href="https://80k.info/eo"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Common pregnancy myths and advice that Emily disagrees with — and why you should probably get a doula.</li><li>Whether it’s fine to continue with antidepressants and coffee during pregnancy.</li><li>What the data says — and doesn’t say — about outcomes from parenting decisions around breastfeeding, sleep training, childcare, and more.</li><li>Which factors really matter for kids to thrive — and why that means parents shouldn’t sweat the small stuff.</li><li>How to reduce parental guilt and anxiety with facts, and reject judgemental “Mommy Wars” attitudes when making decisions that are best for your family.</li><li>The effects of having kids on career ambitions, pay, and productivity — and how the effects are different for men and women.</li><li>Practical advice around managing the tradeoffs between career and family.</li><li>What to consider when deciding whether and when to have kids.</li><li>Relationship challenges after having kids, and the protective factors that help.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"I think at various times — before you have the kid, after you have the kid — it's useful to sit down and think about: What do I want the shape of this to look like? What time do I want to be spending? Which hours? How do I want the weekends to look? The things that are going to shape the way your day-to-day goes, and the time you spend with your kids, and what you're doing in that time with your kids, and all of those things: you have an opportunity to deliberately plan them. And you can then feel like, 'I've thought about this, and this is a life that I want. This is a life that we're trying to craft for our family, for our kids.' And that is distinct from thinking you're doing a good job in every moment — which you can't achieve. But you can achieve, 'I'm doing this the way that I think works for my family.'" — Emily Oster</p><p>In today’s episode, host Luisa Rodriguez speaks to Emily Oster — economist at Brown University, host of the <em>ParentData</em> podcast, and the author of three hugely popular books that provide evidence-based insights into pregnancy and early childhood.</p><p><a href="https://80k.info/eo"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Common pregnancy myths and advice that Emily disagrees with — and why you should probably get a doula.</li><li>Whether it’s fine to continue with antidepressants and coffee during pregnancy.</li><li>What the data says — and doesn’t say — about outcomes from parenting decisions around breastfeeding, sleep training, childcare, and more.</li><li>Which factors really matter for kids to thrive — and why that means parents shouldn’t sweat the small stuff.</li><li>How to reduce parental guilt and anxiety with facts, and reject judgemental “Mommy Wars” attitudes when making decisions that are best for your family.</li><li>The effects of having kids on career ambitions, pay, and productivity — and how the effects are different for men and women.</li><li>Practical advice around managing the tradeoffs between career and family.</li><li>What to consider when deciding whether and when to have kids.</li><li>Relationship challenges after having kids, and the protective factors that help.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 01 Feb 2024 21:56:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/95314312/f03f5b75.mp3" length="136935950" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/iu6cJIKEKOBAEeKfsSiVzzA2Dr00FTAtwbaJc9wCNNg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3MTAyMzMv/MTcwNjY1NzIzNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8556</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"I think at various times — before you have the kid, after you have the kid — it's useful to sit down and think about: What do I want the shape of this to look like? What time do I want to be spending? Which hours? How do I want the weekends to look? The things that are going to shape the way your day-to-day goes, and the time you spend with your kids, and what you're doing in that time with your kids, and all of those things: you have an opportunity to deliberately plan them. And you can then feel like, 'I've thought about this, and this is a life that I want. This is a life that we're trying to craft for our family, for our kids.' And that is distinct from thinking you're doing a good job in every moment — which you can't achieve. But you can achieve, 'I'm doing this the way that I think works for my family.'" — Emily Oster</p><p>In today’s episode, host Luisa Rodriguez speaks to Emily Oster — economist at Brown University, host of the <em>ParentData</em> podcast, and the author of three hugely popular books that provide evidence-based insights into pregnancy and early childhood.</p><p><a href="https://80k.info/eo"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>Common pregnancy myths and advice that Emily disagrees with — and why you should probably get a doula.</li><li>Whether it’s fine to continue with antidepressants and coffee during pregnancy.</li><li>What the data says — and doesn’t say — about outcomes from parenting decisions around breastfeeding, sleep training, childcare, and more.</li><li>Which factors really matter for kids to thrive — and why that means parents shouldn’t sweat the small stuff.</li><li>How to reduce parental guilt and anxiety with facts, and reject judgemental “Mommy Wars” attitudes when making decisions that are best for your family.</li><li>The effects of having kids on career ambitions, pay, and productivity — and how the effects are different for men and women.</li><li>Practical advice around managing the tradeoffs between career and family.</li><li>What to consider when deciding whether and when to have kids.</li><li>Relationship challenges after having kids, and the protective factors that help.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/95314312/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#177 – Nathan Labenz on recent AI breakthroughs and navigating the growing rift between AI safety and accelerationist camps</title>
      <itunes:title>#177 – Nathan Labenz on recent AI breakthroughs and navigating the growing rift between AI safety and accelerationist camps</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a7456c0a-ff03-4961-8980-eac11af9d8b7</guid>
      <link>https://80000hours.org/podcast/episodes/nathan-labenz-ai-breakthroughs-controversies/?utm_campaign=podcast__nathan-labenz&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Back in December we spoke with Nathan Labenz — AI entrepreneur and host of <a href="https://www.cognitiverevolution.ai/"><em>The Cognitive Revolution Podcast</em></a> — about <a href="https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/">the speed of progress towards AGI and OpenAI's leadership drama</a>, drawing on Nathan's alarming experience red-teaming an early version of GPT-4 and resulting conversations with OpenAI staff and board members.</p><p><a href="https://80k.info/nl24"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p>Today we go deeper, diving into:</p><ul><li>What AI now actually can and can’t do, across language and visual models, medicine, scientific research, self-driving cars, robotics, weapons — and what the next big breakthrough might be.</li><li>Why most people, including most listeners, probably don’t know and can’t keep up with the new capabilities and wild results coming out across so many AI applications — and what we should do about that.</li><li>How we need to learn to talk about AI more productively, particularly addressing the growing chasm between those concerned about AI risks and those who want to see progress accelerate, which may be counterproductive for everyone.</li><li>Where Nathan agrees with and departs from the views of ‘AI scaling accelerationists.’</li><li>The chances that anti-regulation rhetoric from some AI entrepreneurs backfires.</li><li>How governments could (and already do) abuse AI tools like facial recognition, and how militarisation of AI is progressing.</li><li>Preparing for coming societal impacts and potential disruption from AI.</li><li>Practical ways that curious listeners can try to stay abreast of everything that’s going on.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Back in December we spoke with Nathan Labenz — AI entrepreneur and host of <a href="https://www.cognitiverevolution.ai/"><em>The Cognitive Revolution Podcast</em></a> — about <a href="https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/">the speed of progress towards AGI and OpenAI's leadership drama</a>, drawing on Nathan's alarming experience red-teaming an early version of GPT-4 and resulting conversations with OpenAI staff and board members.</p><p><a href="https://80k.info/nl24"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p>Today we go deeper, diving into:</p><ul><li>What AI now actually can and can’t do, across language and visual models, medicine, scientific research, self-driving cars, robotics, weapons — and what the next big breakthrough might be.</li><li>Why most people, including most listeners, probably don’t know and can’t keep up with the new capabilities and wild results coming out across so many AI applications — and what we should do about that.</li><li>How we need to learn to talk about AI more productively, particularly addressing the growing chasm between those concerned about AI risks and those who want to see progress accelerate, which may be counterproductive for everyone.</li><li>Where Nathan agrees with and departs from the views of ‘AI scaling accelerationists.’</li><li>The chances that anti-regulation rhetoric from some AI entrepreneurs backfires.</li><li>How governments could (and already do) abuse AI tools like facial recognition, and how militarisation of AI is progressing.</li><li>Preparing for coming societal impacts and potential disruption from AI.</li><li>Practical ways that curious listeners can try to stay abreast of everything that’s going on.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 24 Jan 2024 22:08:17 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/162f1a18/d7bef8af.mp3" length="160529854" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/rmIqy6Bub4n_t2CphUqBb1YQvGg3nUfYv99A4TfckYY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE3MDIyMzkv/MTcwNjExMzg3Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10029</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Back in December we spoke with Nathan Labenz — AI entrepreneur and host of <a href="https://www.cognitiverevolution.ai/"><em>The Cognitive Revolution Podcast</em></a> — about <a href="https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/">the speed of progress towards AGI and OpenAI's leadership drama</a>, drawing on Nathan's alarming experience red-teaming an early version of GPT-4 and resulting conversations with OpenAI staff and board members.</p><p><a href="https://80k.info/nl24"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p>Today we go deeper, diving into:</p><ul><li>What AI now actually can and can’t do, across language and visual models, medicine, scientific research, self-driving cars, robotics, weapons — and what the next big breakthrough might be.</li><li>Why most people, including most listeners, probably don’t know and can’t keep up with the new capabilities and wild results coming out across so many AI applications — and what we should do about that.</li><li>How we need to learn to talk about AI more productively, particularly addressing the growing chasm between those concerned about AI risks and those who want to see progress accelerate, which may be counterproductive for everyone.</li><li>Where Nathan agrees with and departs from the views of ‘AI scaling accelerationists.’</li><li>The chances that anti-regulation rhetoric from some AI entrepreneurs backfires.</li><li>How governments could (and already do) abuse AI tools like facial recognition, and how militarisation of AI is progressing.</li><li>Preparing for coming societal impacts and potential disruption from AI.</li><li>Practical ways that curious listeners can try to stay abreast of everything that’s going on.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/162f1a18/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#90 Classic episode – Ajeya Cotra on worldview diversification and how big the future could be</title>
      <itunes:title>#90 Classic episode – Ajeya Cotra on worldview diversification and how big the future could be</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b8fbfddc-d04b-4668-9059-65e82cfbc98d</guid>
      <link>https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/?utm_campaign=podcast__classic-ajeya-cotra&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>You wake up in a mysterious box, and hear the booming voice of God: “I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it. If it came up tails, I made ten billion boxes, labeled 1 through 10 billion — also with one human in each box. To get into heaven, you have to answer this correctly: Which way did the coin land?”</p><p>You think briefly, and decide you should bet your eternal soul on tails. The fact that you woke up at all seems like pretty good evidence that you’re in the big world — if the coin landed tails, way more people should be having an experience just like yours.</p><p>But then you get up, walk outside, and look at the number on your box.</p><p>‘3’. Huh. Now you don’t know what to believe.</p><p>If God made 10 billion boxes, surely it’s much more likely that you would have seen a number like 7,346,678,928?</p><p>In today’s interview, Ajeya Cotra — a senior research analyst at Open Philanthropy — explains why this thought experiment from the niche of philosophy known as ‘anthropic reasoning’ could be relevant for figuring out where we should direct our charitable giving.</p><p><strong>Rebroadcast: this episode was originally released in January 2021.</strong></p><p><a href="https://80k.info/acc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>Some thinkers both inside and outside Open Philanthropy believe that philanthropic giving should be guided by ‘<a href="https://80000hours.org/articles/future-generations/">longtermism</a>’ — the idea that we can do the most good if we focus primarily on the impact our actions will have on the long-term future.</p><p>Ajeya thinks that for that notion to make sense, there needs to be a good chance we can settle other planets and solar systems and build a society that’s both very large relative to what’s possible on Earth and, by virtue of being so spread out, able to protect itself from extinction for a very long time.</p><p>But imagine that humanity has two possible futures ahead of it: Either we’re going to have a huge future like that, in which trillions of people ultimately exist, or we’re going to wipe ourselves out quite soon, thereby ensuring that only around 100 billion people ever get to live.</p><p>If there are eventually going to be 1,000 trillion humans, what should we think of the fact that we seemingly find ourselves so early in history? Being among the first 100 billion humans, as we are, is equivalent to walking outside and seeing a three on your box. Suspicious! If the future will have many trillions of people, the odds of us appearing so strangely early are very low indeed.</p><p>If we accept the analogy, maybe we can be confident that humanity is at a high risk of extinction based on this so-called ‘<a href="https://en.wikipedia.org/wiki/Doomsday_argument">doomsday argument</a>‘ alone.</p><p>If that’s true, maybe we should put more of our resources into avoiding apparent extinction threats like nuclear war and pandemics. But on the other hand, maybe the argument shows we’re incredibly unlikely to achieve a long and stable future no matter what we do, and we should forget the long term and just focus on the here and now instead.</p><p>There are many critics of this theoretical ‘doomsday argument’, and it may be the case that it logically doesn’t work. This is why Ajeya spent time investigating it, with the goal of ultimately making better philanthropic grants.</p><p>In this conversation, Ajeya and Rob discuss both the doomsday argument and the challenge Open Phil faces striking a balance between taking big ideas seriously, and not going all in on philosophical arguments that may turn out to be barking up the wrong tree entirely.</p><p>They also discuss:</p><ul><li>Which worldviews Open Phil finds most plausible, and how it balances them</li><li>Which worldviews Ajeya doesn’t embrace but almost does</li><li>How hard it is to get to other solar systems</li><li>The famous ‘simulation argument’</li><li>When transformative AI might actually arrive</li><li>The biggest challenges involved in working on big research reports</li><li>What it’s like working at Open Phil</li><li>And much more</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>You wake up in a mysterious box, and hear the booming voice of God: “I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it. If it came up tails, I made ten billion boxes, labeled 1 through 10 billion — also with one human in each box. To get into heaven, you have to answer this correctly: Which way did the coin land?”</p><p>You think briefly, and decide you should bet your eternal soul on tails. The fact that you woke up at all seems like pretty good evidence that you’re in the big world — if the coin landed tails, way more people should be having an experience just like yours.</p><p>But then you get up, walk outside, and look at the number on your box.</p><p>‘3’. Huh. Now you don’t know what to believe.</p><p>If God made 10 billion boxes, surely it’s much more likely that you would have seen a number like 7,346,678,928?</p><p>In today’s interview, Ajeya Cotra — a senior research analyst at Open Philanthropy — explains why this thought experiment from the niche of philosophy known as ‘anthropic reasoning’ could be relevant for figuring out where we should direct our charitable giving.</p><p><strong>Rebroadcast: this episode was originally released in January 2021.</strong></p><p><a href="https://80k.info/acc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>Some thinkers both inside and outside Open Philanthropy believe that philanthropic giving should be guided by ‘<a href="https://80000hours.org/articles/future-generations/">longtermism</a>’ — the idea that we can do the most good if we focus primarily on the impact our actions will have on the long-term future.</p><p>Ajeya thinks that for that notion to make sense, there needs to be a good chance we can settle other planets and solar systems and build a society that’s both very large relative to what’s possible on Earth and, by virtue of being so spread out, able to protect itself from extinction for a very long time.</p><p>But imagine that humanity has two possible futures ahead of it: Either we’re going to have a huge future like that, in which trillions of people ultimately exist, or we’re going to wipe ourselves out quite soon, thereby ensuring that only around 100 billion people ever get to live.</p><p>If there are eventually going to be 1,000 trillion humans, what should we think of the fact that we seemingly find ourselves so early in history? Being among the first 100 billion humans, as we are, is equivalent to walking outside and seeing a three on your box. Suspicious! If the future will have many trillions of people, the odds of us appearing so strangely early are very low indeed.</p><p>If we accept the analogy, maybe we can be confident that humanity is at a high risk of extinction based on this so-called ‘<a href="https://en.wikipedia.org/wiki/Doomsday_argument">doomsday argument</a>‘ alone.</p><p>If that’s true, maybe we should put more of our resources into avoiding apparent extinction threats like nuclear war and pandemics. But on the other hand, maybe the argument shows we’re incredibly unlikely to achieve a long and stable future no matter what we do, and we should forget the long term and just focus on the here and now instead.</p><p>There are many critics of this theoretical ‘doomsday argument’, and it may be the case that it logically doesn’t work. This is why Ajeya spent time investigating it, with the goal of ultimately making better philanthropic grants.</p><p>In this conversation, Ajeya and Rob discuss both the doomsday argument and the challenge Open Phil faces striking a balance between taking big ideas seriously, and not going all in on philosophical arguments that may turn out to be barking up the wrong tree entirely.</p><p>They also discuss:</p><ul><li>Which worldviews Open Phil finds most plausible, and how it balances them</li><li>Which worldviews Ajeya doesn’t embrace but almost does</li><li>How hard it is to get to other solar systems</li><li>The famous ‘simulation argument’</li><li>When transformative AI might actually arrive</li><li>The biggest challenges involved in working on big research reports</li><li>What it’s like working at Open Phil</li><li>And much more</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 12 Jan 2024 20:02:47 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4b40a888/92c59f4f.mp3" length="172169687" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/5TDWU3Ip2Cfu3YgBcwOS5cpUAFCwPNUW3hw9Tvmo8Sk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NjI4NTUv/MTcwMzc3NjMyNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10757</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>You wake up in a mysterious box, and hear the booming voice of God: “I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it. If it came up tails, I made ten billion boxes, labeled 1 through 10 billion — also with one human in each box. To get into heaven, you have to answer this correctly: Which way did the coin land?”</p><p>You think briefly, and decide you should bet your eternal soul on tails. The fact that you woke up at all seems like pretty good evidence that you’re in the big world — if the coin landed tails, way more people should be having an experience just like yours.</p><p>But then you get up, walk outside, and look at the number on your box.</p><p>‘3’. Huh. Now you don’t know what to believe.</p><p>If God made 10 billion boxes, surely it’s much more likely that you would have seen a number like 7,346,678,928?</p><p>In today’s interview, Ajeya Cotra — a senior research analyst at Open Philanthropy — explains why this thought experiment from the niche of philosophy known as ‘anthropic reasoning’ could be relevant for figuring out where we should direct our charitable giving.</p><p><strong>Rebroadcast: this episode was originally released in January 2021.</strong></p><p><a href="https://80k.info/acc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>Some thinkers both inside and outside Open Philanthropy believe that philanthropic giving should be guided by ‘<a href="https://80000hours.org/articles/future-generations/">longtermism</a>’ — the idea that we can do the most good if we focus primarily on the impact our actions will have on the long-term future.</p><p>Ajeya thinks that for that notion to make sense, there needs to be a good chance we can settle other planets and solar systems and build a society that’s both very large relative to what’s possible on Earth and, by virtue of being so spread out, able to protect itself from extinction for a very long time.</p><p>But imagine that humanity has two possible futures ahead of it: Either we’re going to have a huge future like that, in which trillions of people ultimately exist, or we’re going to wipe ourselves out quite soon, thereby ensuring that only around 100 billion people ever get to live.</p><p>If there are eventually going to be 1,000 trillion humans, what should we think of the fact that we seemingly find ourselves so early in history? Being among the first 100 billion humans, as we are, is equivalent to walking outside and seeing a three on your box. Suspicious! If the future will have many trillions of people, the odds of us appearing so strangely early are very low indeed.</p><p>If we accept the analogy, maybe we can be confident that humanity is at a high risk of extinction based on this so-called ‘<a href="https://en.wikipedia.org/wiki/Doomsday_argument">doomsday argument</a>‘ alone.</p><p>If that’s true, maybe we should put more of our resources into avoiding apparent extinction threats like nuclear war and pandemics. But on the other hand, maybe the argument shows we’re incredibly unlikely to achieve a long and stable future no matter what we do, and we should forget the long term and just focus on the here and now instead.</p><p>There are many critics of this theoretical ‘doomsday argument’, and it may be the case that it logically doesn’t work. This is why Ajeya spent time investigating it, with the goal of ultimately making better philanthropic grants.</p><p>In this conversation, Ajeya and Rob discuss both the doomsday argument and the challenge Open Phil faces striking a balance between taking big ideas seriously, and not going all in on philosophical arguments that may turn out to be barking up the wrong tree entirely.</p><p>They also discuss:</p><ul><li>Which worldviews Open Phil finds most plausible, and how it balances them</li><li>Which worldviews Ajeya doesn’t embrace but almost does</li><li>How hard it is to get to other solar systems</li><li>The famous ‘simulation argument’</li><li>When transformative AI might actually arrive</li><li>The biggest challenges involved in working on big research reports</li><li>What it’s like working at Open Phil</li><li>And much more</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/4b40a888/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#112 Classic episode – Carl Shulman on the common-sense case for existential risk work and its practical implications</title>
      <itunes:title>#112 Classic episode – Carl Shulman on the common-sense case for existential risk work and its practical implications</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">53dd4269-0410-4445-b38a-423cf0aaf32b</guid>
      <link>https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/?utm_campaign=podcast__classic-carl-shulman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation.</p><p>But the policy of US government agencies is already to spend up to $4 million to save the life of a citizen, making the death of all Americans a $1,300,000,000,000,000 disaster.</p><p>According to Carl Shulman, research associate at Oxford University’s <a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a>, that means you don’t need any fancy philosophical arguments about the value or size of the future to justify working to reduce existential risk — it passes a mundane cost-benefit analysis whether or not you place any value on the long-term future.</p><p><strong>Rebroadcast: this episode was originally released in October 2021.</strong></p><p><a href="https://80k.info/csc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>The key reason to make it a top priority is factual, not philosophical. That is, the risk of a disaster that kills billions of people alive today is alarmingly high, and it can be reduced at a reasonable cost. A back-of-the-envelope version of the argument runs:</p><ul><li>The US government is willing to pay up to $4 million (depending on the agency) to save the life of an American.</li><li>So saving all US citizens at any given point in time would be worth $1,300 trillion.</li><li>If you believe that the risk of human extinction over the next century is something like one in six (as Toby Ord suggests is a reasonable figure in his book <a href="https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/"><em>The Precipice</em></a>), then it would be worth the US government spending up to $2.2 trillion to reduce that risk by just 1%, in terms of American lives saved alone.</li><li>Carl thinks it would cost a <em>lot </em>less than that to achieve a 1% risk reduction if the money were spent intelligently. So it easily passes a government cost-benefit test, with a very big benefit-to-cost ratio — likely over 1000:1 today.</li></ul><p>This argument helped NASA <a href="https://en.wikipedia.org/wiki/Center_for_Near-Earth_Object_Studies">get funding</a> to scan the sky for any asteroids that might be on a collision course with Earth, and it was directly promoted by famous economists like <a href="https://en.wikipedia.org/wiki/Catastrophe:_Risk_and_Response">Richard Posner</a>, <a href="https://www.nber.org/system/files/working_papers/w22137/w22137.pdf">Larry Summers</a>, and <a href="https://www.amazon.com/dp/B001GS6ZMW/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">Cass Sunstein</a>.</p><p>If the case is clear enough, why hasn’t it already motivated a lot more spending or regulations to limit existential risks — enough to drive down what any additional efforts would achieve?</p><p>Carl thinks that one key barrier is that infrequent disasters are rarely politically salient. <a href="https://press.princeton.edu/books/paperback/9780691178240/democracy-for-realists">Research indicates</a> that extra money is spent on flood defences in the years immediately following a massive flood — but as memories fade, that spending quickly dries up. Of course the annual probability of a disaster was the same the whole time; all that changed is what voters had on their minds.</p><p>Carl suspects another reason is that it’s difficult for the average voter to estimate and understand how large these respective risks are, and what responses would be appropriate rather than self-serving. If the public doesn’t know what good performance looks like, politicians can’t be given incentives to do the right thing.</p><p>It’s reasonable to assume that if we found out a giant asteroid were going to crash into the Earth one year from now, most of our resources would be quickly diverted into figuring out how to avert catastrophe.</p><p>But even in the case of COVID-19, an event that massively disrupted the lives of everyone on Earth, we’ve still seen a substantial lack of investment in vaccine manufacturing capacity and other ways of controlling the spread of the virus, relative to what economists recommended.</p><p>Carl expects that all the reasons we didn’t adequately prepare for or respond to COVID-19 — with <a href="https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates">excess mortality</a> over 15 million and costs well over <a href="https://jamanetwork.com/journals/jama/fullarticle/2771764">$10 trillion</a> — bite even harder when it comes to threats we’ve never faced before, such as engineered pandemics, risks from advanced artificial intelligence, and so on.</p><p>Today’s episode is in part our way of trying to improve this situation. In today’s wide-ranging conversation, Carl and Rob also cover:</p><ul><li>A few reasons Carl isn’t excited by ‘strong longtermism’</li><li>How x-risk reduction compares to GiveWell recommendations</li><li>Solutions for asteroids, comets, supervolcanoes, nuclear war, pandemics, and climate change</li><li>The history of bioweapons</li><li>Whether gain-of-function research is justifiable</li><li>Successes and failures around COVID-19</li><li>The history of existential risk</li><li>And much more</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation.</p><p>But the policy of US government agencies is already to spend up to $4 million to save the life of a citizen, making the death of all Americans a $1,300,000,000,000,000 disaster.</p><p>According to Carl Shulman, research associate at Oxford University’s <a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a>, that means you don’t need any fancy philosophical arguments about the value or size of the future to justify working to reduce existential risk — it passes a mundane cost-benefit analysis whether or not you place any value on the long-term future.</p><p><strong>Rebroadcast: this episode was originally released in October 2021.</strong></p><p><a href="https://80k.info/csc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>The key reason to make it a top priority is factual, not philosophical. That is, the risk of a disaster that kills billions of people alive today is alarmingly high, and it can be reduced at a reasonable cost. A back-of-the-envelope version of the argument runs:</p><ul><li>The US government is willing to pay up to $4 million (depending on the agency) to save the life of an American.</li><li>So saving all US citizens at any given point in time would be worth $1,300 trillion.</li><li>If you believe that the risk of human extinction over the next century is something like one in six (as Toby Ord suggests is a reasonable figure in his book <a href="https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/"><em>The Precipice</em></a>), then it would be worth the US government spending up to $2.2 trillion to reduce that risk by just 1%, in terms of American lives saved alone.</li><li>Carl thinks it would cost a <em>lot </em>less than that to achieve a 1% risk reduction if the money were spent intelligently. So it easily passes a government cost-benefit test, with a very big benefit-to-cost ratio — likely over 1000:1 today.</li></ul><p>This argument helped NASA <a href="https://en.wikipedia.org/wiki/Center_for_Near-Earth_Object_Studies">get funding</a> to scan the sky for any asteroids that might be on a collision course with Earth, and it was directly promoted by famous economists like <a href="https://en.wikipedia.org/wiki/Catastrophe:_Risk_and_Response">Richard Posner</a>, <a href="https://www.nber.org/system/files/working_papers/w22137/w22137.pdf">Larry Summers</a>, and <a href="https://www.amazon.com/dp/B001GS6ZMW/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">Cass Sunstein</a>.</p><p>If the case is clear enough, why hasn’t it already motivated a lot more spending or regulations to limit existential risks — enough to drive down what any additional efforts would achieve?</p><p>Carl thinks that one key barrier is that infrequent disasters are rarely politically salient. <a href="https://press.princeton.edu/books/paperback/9780691178240/democracy-for-realists">Research indicates</a> that extra money is spent on flood defences in the years immediately following a massive flood — but as memories fade, that spending quickly dries up. Of course the annual probability of a disaster was the same the whole time; all that changed is what voters had on their minds.</p><p>Carl suspects another reason is that it’s difficult for the average voter to estimate and understand how large these respective risks are, and what responses would be appropriate rather than self-serving. If the public doesn’t know what good performance looks like, politicians can’t be given incentives to do the right thing.</p><p>It’s reasonable to assume that if we found out a giant asteroid were going to crash into the Earth one year from now, most of our resources would be quickly diverted into figuring out how to avert catastrophe.</p><p>But even in the case of COVID-19, an event that massively disrupted the lives of everyone on Earth, we’ve still seen a substantial lack of investment in vaccine manufacturing capacity and other ways of controlling the spread of the virus, relative to what economists recommended.</p><p>Carl expects that all the reasons we didn’t adequately prepare for or respond to COVID-19 — with <a href="https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates">excess mortality</a> over 15 million and costs well over <a href="https://jamanetwork.com/journals/jama/fullarticle/2771764">$10 trillion</a> — bite even harder when it comes to threats we’ve never faced before, such as engineered pandemics, risks from advanced artificial intelligence, and so on.</p><p>Today’s episode is in part our way of trying to improve this situation. In today’s wide-ranging conversation, Carl and Rob also cover:</p><ul><li>A few reasons Carl isn’t excited by ‘strong longtermism’</li><li>How x-risk reduction compares to GiveWell recommendations</li><li>Solutions for asteroids, comets, supervolcanoes, nuclear war, pandemics, and climate change</li><li>The history of bioweapons</li><li>Whether gain-of-function research is justifiable</li><li>Successes and failures around COVID-19</li><li>The history of existential risk</li><li>And much more</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 08 Jan 2024 20:55:33 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ce8f2dc8/7982967e.mp3" length="221316535" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/byFGhJF6prXOwtwD0gHJ8TNwlOsZ3arcLqG7qXUdpMI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NjI4NDgv/MTcwMzc3NjIyNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13830</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation.</p><p>But the policy of US government agencies is already to spend up to $4 million to save the life of a citizen, making the death of all Americans a $1,300,000,000,000,000 disaster.</p><p>According to Carl Shulman, research associate at Oxford University’s <a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a>, that means you don’t need any fancy philosophical arguments about the value or size of the future to justify working to reduce existential risk — it passes a mundane cost-benefit analysis whether or not you place any value on the long-term future.</p><p><strong>Rebroadcast: this episode was originally released in October 2021.</strong></p><p><a href="https://80k.info/csc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>The key reason to make it a top priority is factual, not philosophical. That is, the risk of a disaster that kills billions of people alive today is alarmingly high, and it can be reduced at a reasonable cost. A back-of-the-envelope version of the argument runs:</p><ul><li>The US government is willing to pay up to $4 million (depending on the agency) to save the life of an American.</li><li>So saving all US citizens at any given point in time would be worth $1,300 trillion.</li><li>If you believe that the risk of human extinction over the next century is something like one in six (as Toby Ord suggests is a reasonable figure in his book <a href="https://80000hours.org/podcast/episodes/toby-ord-the-precipice-existential-risk-future-humanity/"><em>The Precipice</em></a>), then it would be worth the US government spending up to $2.2 trillion to reduce that risk by just 1%, in terms of American lives saved alone.</li><li>Carl thinks it would cost a <em>lot </em>less than that to achieve a 1% risk reduction if the money were spent intelligently. So it easily passes a government cost-benefit test, with a very big benefit-to-cost ratio — likely over 1000:1 today.</li></ul><p>This argument helped NASA <a href="https://en.wikipedia.org/wiki/Center_for_Near-Earth_Object_Studies">get funding</a> to scan the sky for any asteroids that might be on a collision course with Earth, and it was directly promoted by famous economists like <a href="https://en.wikipedia.org/wiki/Catastrophe:_Risk_and_Response">Richard Posner</a>, <a href="https://www.nber.org/system/files/working_papers/w22137/w22137.pdf">Larry Summers</a>, and <a href="https://www.amazon.com/dp/B001GS6ZMW/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">Cass Sunstein</a>.</p><p>If the case is clear enough, why hasn’t it already motivated a lot more spending or regulations to limit existential risks — enough to drive down what any additional efforts would achieve?</p><p>Carl thinks that one key barrier is that infrequent disasters are rarely politically salient. <a href="https://press.princeton.edu/books/paperback/9780691178240/democracy-for-realists">Research indicates</a> that extra money is spent on flood defences in the years immediately following a massive flood — but as memories fade, that spending quickly dries up. Of course the annual probability of a disaster was the same the whole time; all that changed is what voters had on their minds.</p><p>Carl suspects another reason is that it’s difficult for the average voter to estimate and understand how large these respective risks are, and what responses would be appropriate rather than self-serving. If the public doesn’t know what good performance looks like, politicians can’t be given incentives to do the right thing.</p><p>It’s reasonable to assume that if we found out a giant asteroid were going to crash into the Earth one year from now, most of our resources would be quickly diverted into figuring out how to avert catastrophe.</p><p>But even in the case of COVID-19, an event that massively disrupted the lives of everyone on Earth, we’ve still seen a substantial lack of investment in vaccine manufacturing capacity and other ways of controlling the spread of the virus, relative to what economists recommended.</p><p>Carl expects that all the reasons we didn’t adequately prepare for or respond to COVID-19 — with <a href="https://www.economist.com/graphic-detail/coronavirus-excess-deaths-estimates">excess mortality</a> over 15 million and costs well over <a href="https://jamanetwork.com/journals/jama/fullarticle/2771764">$10 trillion</a> — bite even harder when it comes to threats we’ve never faced before, such as engineered pandemics, risks from advanced artificial intelligence, and so on.</p><p>Today’s episode is in part our way of trying to improve this situation. In today’s wide-ranging conversation, Carl and Rob also cover:</p><ul><li>A few reasons Carl isn’t excited by ‘strong longtermism’</li><li>How x-risk reduction compares to GiveWell recommendations</li><li>Solutions for asteroids, comets, supervolcanoes, nuclear war, pandemics, and climate change</li><li>The history of bioweapons</li><li>Whether gain-of-function research is justifiable</li><li>Successes and failures around COVID-19</li><li>The history of existential risk</li><li>And much more</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/ce8f2dc8/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#111 Classic episode – Mushtaq Khan on using institutional economics to predict effective government reforms</title>
      <itunes:title>#111 Classic episode – Mushtaq Khan on using institutional economics to predict effective government reforms</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">5fb4bda8-15d5-4ab5-9035-720cd14a0079</guid>
      <link>https://80000hours.org/podcast/episodes/mushtaq-khan-institutional-economics/?utm_campaign=podcast__classic-mushtaq-khan&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines.</p><p>The resulting oil spills damage the environment and cause severe health problems, but the Nigerian government has continually failed in their attempts to stop this theft.</p><p>They send in the army, and the army gets corrupted. They send in enforcement agencies, and the enforcement agencies get corrupted. What’s happening here?</p><p>According to Mushtaq Khan, economics professor at SOAS University of London, this is a classic example of ‘networked corruption’. Everyone in the community is benefiting from the criminal enterprise — so much so that the locals would prefer civil war to following the law. It pays vastly better than other local jobs, hotels and restaurants have formed around it, and houses are even powered by the electricity generated from the oil.</p><p><strong>Rebroadcast: this episode was originally released in September 2021.</strong></p><p><a href="https://80k.info/mkc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In today’s episode, Mushtaq elaborates on the models he uses to understand these problems and make predictions he can test in the real world.</p><p>Some of the most important factors shaping the fate of nations are their structures of power: who is powerful, how they are organized, which interest groups can pull in favours with the government, and the constant push and pull between the country’s rulers and its ruled. While traditional economic theory has relatively little to say about these topics, institutional economists like Mushtaq have a lot to say, and participate in lively debates about which of their competing ideas best explain the world around us.</p><p>The issues at stake are nothing less than why some countries are rich and others are poor, why some countries are mostly law abiding while others are not, and why some government programmes improve public welfare while others just enrich the well connected.</p><p>Mushtaq’s specialties are anti-corruption and industrial policy, where he believes mainstream theory and practice are largely misguided. To root out fraud, aid agencies try to impose institutions and laws that work in countries like the U.K. today. Everyone nods their heads and appears to go along, but years later they find nothing has changed, or worse — the new anti-corruption laws are mostly just used to persecute anyone who challenges the country’s rulers.</p><p>As Mushtaq explains, to people who specialise in understanding why corruption is ubiquitous in some countries but not others, this is entirely predictable. Western agencies imagine a situation where most people are law abiding, but a handful of selfish fat cats are engaging in large-scale graft. In fact in the countries they’re trying to change everyone is breaking some rule or other, or participating in so-called ‘corruption’, because it’s the only way to get things done and always has been.</p><p>Mushtaq’s rule of thumb is that when the locals most concerned with a specific issue are invested in preserving a status quo they’re participating in, they almost always win out.</p><p>To actually reduce corruption, countries like his native Bangladesh have to follow the same gradual path the U.K. once did: find organizations that benefit from rule-abiding behaviour and are selfishly motivated to promote it, and help them police their peers.</p><p>Trying to impose a new way of doing things from the top down wasn’t how Europe modernised, and it won’t work elsewhere either.</p><p>In cases like oil theft in Nigeria, where no one wants to follow the rules, Mushtaq says corruption may be impossible to solve directly. Instead you have to play a long game, bringing in other employment opportunities, improving health services, and deploying alternative forms of energy — in the hope that one day this will give people a viable alternative to corruption.</p><p>In this extensive interview Rob and Mushtaq cover this and much more, including:</p><ul><li>How does one test theories like this?</li><li>Why are companies in some poor countries so much less productive than their peers in rich countries?</li><li>Have rich countries just legalized the corruption in their societies?</li><li>What are the big live debates in institutional economics?</li><li>Should poor countries protect their industries from foreign competition?</li><li>Where has industrial policy worked, and why?</li><li>How can listeners use these theories to predict which policies will work in their own countries?</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines.</p><p>The resulting oil spills damage the environment and cause severe health problems, but the Nigerian government has continually failed in their attempts to stop this theft.</p><p>They send in the army, and the army gets corrupted. They send in enforcement agencies, and the enforcement agencies get corrupted. What’s happening here?</p><p>According to Mushtaq Khan, economics professor at SOAS University of London, this is a classic example of ‘networked corruption’. Everyone in the community is benefiting from the criminal enterprise — so much so that the locals would prefer civil war to following the law. It pays vastly better than other local jobs, hotels and restaurants have formed around it, and houses are even powered by the electricity generated from the oil.</p><p><strong>Rebroadcast: this episode was originally released in September 2021.</strong></p><p><a href="https://80k.info/mkc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In today’s episode, Mushtaq elaborates on the models he uses to understand these problems and make predictions he can test in the real world.</p><p>Some of the most important factors shaping the fate of nations are their structures of power: who is powerful, how they are organized, which interest groups can pull in favours with the government, and the constant push and pull between the country’s rulers and its ruled. While traditional economic theory has relatively little to say about these topics, institutional economists like Mushtaq have a lot to say, and participate in lively debates about which of their competing ideas best explain the world around us.</p><p>The issues at stake are nothing less than why some countries are rich and others are poor, why some countries are mostly law abiding while others are not, and why some government programmes improve public welfare while others just enrich the well connected.</p><p>Mushtaq’s specialties are anti-corruption and industrial policy, where he believes mainstream theory and practice are largely misguided. To root out fraud, aid agencies try to impose institutions and laws that work in countries like the U.K. today. Everyone nods their heads and appears to go along, but years later they find nothing has changed, or worse — the new anti-corruption laws are mostly just used to persecute anyone who challenges the country’s rulers.</p><p>As Mushtaq explains, to people who specialise in understanding why corruption is ubiquitous in some countries but not others, this is entirely predictable. Western agencies imagine a situation where most people are law abiding, but a handful of selfish fat cats are engaging in large-scale graft. In fact in the countries they’re trying to change everyone is breaking some rule or other, or participating in so-called ‘corruption’, because it’s the only way to get things done and always has been.</p><p>Mushtaq’s rule of thumb is that when the locals most concerned with a specific issue are invested in preserving a status quo they’re participating in, they almost always win out.</p><p>To actually reduce corruption, countries like his native Bangladesh have to follow the same gradual path the U.K. once did: find organizations that benefit from rule-abiding behaviour and are selfishly motivated to promote it, and help them police their peers.</p><p>Trying to impose a new way of doing things from the top down wasn’t how Europe modernised, and it won’t work elsewhere either.</p><p>In cases like oil theft in Nigeria, where no one wants to follow the rules, Mushtaq says corruption may be impossible to solve directly. Instead you have to play a long game, bringing in other employment opportunities, improving health services, and deploying alternative forms of energy — in the hope that one day this will give people a viable alternative to corruption.</p><p>In this extensive interview Rob and Mushtaq cover this and much more, including:</p><ul><li>How does one test theories like this?</li><li>Why are companies in some poor countries so much less productive than their peers in rich countries?</li><li>Have rich countries just legalized the corruption in their societies?</li><li>What are the big live debates in institutional economics?</li><li>Should poor countries protect their industries from foreign competition?</li><li>Where has industrial policy worked, and why?</li><li>How can listeners use these theories to predict which policies will work in their own countries?</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 04 Jan 2024 22:56:04 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/82ddb1d1/f5fd604e.mp3" length="194215609" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/kEi_8VC031RKTrU-JmiUUOt1nbxZsj54Aqfp2qmfMAo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NjI4MzYv/MTcwMzc3NjE4MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>12137</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines.</p><p>The resulting oil spills damage the environment and cause severe health problems, but the Nigerian government has continually failed in their attempts to stop this theft.</p><p>They send in the army, and the army gets corrupted. They send in enforcement agencies, and the enforcement agencies get corrupted. What’s happening here?</p><p>According to Mushtaq Khan, economics professor at SOAS University of London, this is a classic example of ‘networked corruption’. Everyone in the community is benefiting from the criminal enterprise — so much so that the locals would prefer civil war to following the law. It pays vastly better than other local jobs, hotels and restaurants have formed around it, and houses are even powered by the electricity generated from the oil.</p><p><strong>Rebroadcast: this episode was originally released in September 2021.</strong></p><p><a href="https://80k.info/mkc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>In today’s episode, Mushtaq elaborates on the models he uses to understand these problems and make predictions he can test in the real world.</p><p>Some of the most important factors shaping the fate of nations are their structures of power: who is powerful, how they are organized, which interest groups can pull in favours with the government, and the constant push and pull between the country’s rulers and its ruled. While traditional economic theory has relatively little to say about these topics, institutional economists like Mushtaq have a lot to say, and participate in lively debates about which of their competing ideas best explain the world around us.</p><p>The issues at stake are nothing less than why some countries are rich and others are poor, why some countries are mostly law abiding while others are not, and why some government programmes improve public welfare while others just enrich the well connected.</p><p>Mushtaq’s specialties are anti-corruption and industrial policy, where he believes mainstream theory and practice are largely misguided. To root out fraud, aid agencies try to impose institutions and laws that work in countries like the U.K. today. Everyone nods their heads and appears to go along, but years later they find nothing has changed, or worse — the new anti-corruption laws are mostly just used to persecute anyone who challenges the country’s rulers.</p><p>As Mushtaq explains, to people who specialise in understanding why corruption is ubiquitous in some countries but not others, this is entirely predictable. Western agencies imagine a situation where most people are law abiding, but a handful of selfish fat cats are engaging in large-scale graft. In fact in the countries they’re trying to change everyone is breaking some rule or other, or participating in so-called ‘corruption’, because it’s the only way to get things done and always has been.</p><p>Mushtaq’s rule of thumb is that when the locals most concerned with a specific issue are invested in preserving a status quo they’re participating in, they almost always win out.</p><p>To actually reduce corruption, countries like his native Bangladesh have to follow the same gradual path the U.K. once did: find organizations that benefit from rule-abiding behaviour and are selfishly motivated to promote it, and help them police their peers.</p><p>Trying to impose a new way of doing things from the top down wasn’t how Europe modernised, and it won’t work elsewhere either.</p><p>In cases like oil theft in Nigeria, where no one wants to follow the rules, Mushtaq says corruption may be impossible to solve directly. Instead you have to play a long game, bringing in other employment opportunities, improving health services, and deploying alternative forms of energy — in the hope that one day this will give people a viable alternative to corruption.</p><p>In this extensive interview Rob and Mushtaq cover this and much more, including:</p><ul><li>How does one test theories like this?</li><li>Why are companies in some poor countries so much less productive than their peers in rich countries?</li><li>Have rich countries just legalized the corruption in their societies?</li><li>What are the big live debates in institutional economics?</li><li>Should poor countries protect their industries from foreign competition?</li><li>Where has industrial policy worked, and why?</li><li>How can listeners use these theories to predict which policies will work in their own countries?</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/82ddb1d1/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>2023 Mega-highlights Extravaganza</title>
      <itunes:title>2023 Mega-highlights Extravaganza</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">51fca1b2-557c-459d-9cfe-aa1f9d995f57</guid>
      <link>https://share.transistor.fm/s/b82e31d0</link>
      <description>
        <![CDATA[<p>Happy new year! We've got a different kind of holiday release for you today. Rather than a 'classic episode,' we've put together one of our favourite highlights from <a href="https://80000hours.org/podcast/#list-by-date">each episode of the show that came out in 2023</a>. </p><p>That's 32 of our favourite ideas packed into one episode that's so bursting with substance it might be more than the human mind can safely handle.</p><p>There's something for everyone here:</p><ul><li>Ezra Klein on punctuated equilibrium</li><li>Tom Davidson on why AI takeoff might be shockingly fast</li><li>Johannes Ackva on political action versus lifestyle changes</li><li>Hannah Ritchie on how buying environmentally friendly technology helps low-income countries </li><li>Bryan Caplan on rational irrationality on the part of voters</li><li>Jan Leike on whether the release of ChatGPT increased or reduced AI extinction risks</li><li>Athena Aktipis on why elephants get deadly cancers less often than humans</li><li>Anders Sandberg on the lifespan of civilisations</li><li>Nita Farahany on hacking neural interfaces</li></ul><p>...plus another 23 such gems. </p><p>And they're in an order that our audio engineer Simon Monsour described as having an "eight-dimensional-tetris-like rationale."</p><p>I don't know what the hell that means either, but I'm curious to find out.</p><p>And remember: if you like these highlights, note that we release 20-minute highlights reels for every new episode over on our sister feed, which is called <a href="https://80000hours.org/after-hours-podcast/"><strong><em>80k After Hours</em></strong></a>. So even if you're struggling to make time to listen to every single one, you can always get some of the best bits of our episodes.</p><p>We hope for all the best things to happen for you in 2024, and we'll be back with a traditional classic episode soon.</p><p><em>This Mega-highlights Extravaganza was brought to you by Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Happy new year! We've got a different kind of holiday release for you today. Rather than a 'classic episode,' we've put together one of our favourite highlights from <a href="https://80000hours.org/podcast/#list-by-date">each episode of the show that came out in 2023</a>. </p><p>That's 32 of our favourite ideas packed into one episode that's so bursting with substance it might be more than the human mind can safely handle.</p><p>There's something for everyone here:</p><ul><li>Ezra Klein on punctuated equilibrium</li><li>Tom Davidson on why AI takeoff might be shockingly fast</li><li>Johannes Ackva on political action versus lifestyle changes</li><li>Hannah Ritchie on how buying environmentally friendly technology helps low-income countries </li><li>Bryan Caplan on rational irrationality on the part of voters</li><li>Jan Leike on whether the release of ChatGPT increased or reduced AI extinction risks</li><li>Athena Aktipis on why elephants get deadly cancers less often than humans</li><li>Anders Sandberg on the lifespan of civilisations</li><li>Nita Farahany on hacking neural interfaces</li></ul><p>...plus another 23 such gems. </p><p>And they're in an order that our audio engineer Simon Monsour described as having an "eight-dimensional-tetris-like rationale."</p><p>I don't know what the hell that means either, but I'm curious to find out.</p><p>And remember: if you like these highlights, note that we release 20-minute highlights reels for every new episode over on our sister feed, which is called <a href="https://80000hours.org/after-hours-podcast/"><strong><em>80k After Hours</em></strong></a>. So even if you're struggling to make time to listen to every single one, you can always get some of the best bits of our episodes.</p><p>We hope for all the best things to happen for you in 2024, and we'll be back with a traditional classic episode soon.</p><p><em>This Mega-highlights Extravaganza was brought to you by Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong</em></p>]]>
      </content:encoded>
      <pubDate>Sun, 31 Dec 2023 20:52:59 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b82e31d0/252ea4cb.mp3" length="109201095" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/kwRsJZ53MV5lljszyrjKb9FoJV6SRzH9MMI1ODSnsqs/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NjI4NjAv/MTcwNDMwNjgzOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6823</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Happy new year! We've got a different kind of holiday release for you today. Rather than a 'classic episode,' we've put together one of our favourite highlights from <a href="https://80000hours.org/podcast/#list-by-date">each episode of the show that came out in 2023</a>. </p><p>That's 32 of our favourite ideas packed into one episode that's so bursting with substance it might be more than the human mind can safely handle.</p><p>There's something for everyone here:</p><ul><li>Ezra Klein on punctuated equilibrium</li><li>Tom Davidson on why AI takeoff might be shockingly fast</li><li>Johannes Ackva on political action versus lifestyle changes</li><li>Hannah Ritchie on how buying environmentally friendly technology helps low-income countries </li><li>Bryan Caplan on rational irrationality on the part of voters</li><li>Jan Leike on whether the release of ChatGPT increased or reduced AI extinction risks</li><li>Athena Aktipis on why elephants get deadly cancers less often than humans</li><li>Anders Sandberg on the lifespan of civilisations</li><li>Nita Farahany on hacking neural interfaces</li></ul><p>...plus another 23 such gems. </p><p>And they're in an order that our audio engineer Simon Monsour described as having an "eight-dimensional-tetris-like rationale."</p><p>I don't know what the hell that means either, but I'm curious to find out.</p><p>And remember: if you like these highlights, note that we release 20-minute highlights reels for every new episode over on our sister feed, which is called <a href="https://80000hours.org/after-hours-podcast/"><strong><em>80k After Hours</em></strong></a>. So even if you're struggling to make time to listen to every single one, you can always get some of the best bits of our episodes.</p><p>We hope for all the best things to happen for you in 2024, and we'll be back with a traditional classic episode soon.</p><p><em>This Mega-highlights Extravaganza was brought to you by Ben Cordell, Simon Monsour, Milo McGuire, and Dominic Armstrong</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/b82e31d0/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#100 Classic episode – Having a successful career with depression, anxiety, and imposter syndrome</title>
      <itunes:title>#100 Classic episode – Having a successful career with depression, anxiety, and imposter syndrome</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e264ffbe-0c79-4f13-b9ce-5854b0ac3582</guid>
      <link>https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/?utm_campaign=podcast__classic-howie&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Today’s episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!).</p><p>The producer of this show, Keiran Harris, interviewed our mutual colleague Howie about the major ways that mental illness has affected his life and career. While depression, anxiety, ADHD and other problems are extremely common, it’s rare for people to offer detailed insight into their thoughts and struggles — and even rarer for someone as perceptive as Howie to do so.</p><p><strong>Rebroadcast: this episode was originally released in May 2021.</strong></p><p><a href="https://80k.info/hl"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>The first half of this conversation is a searingly honest account of Howie’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today.</p><p>The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort.</p><p>Howie tries his best to be a corrective to this misguided attitude and pragmatically focus on what actually matters — doing whatever will help you get better.</p><p>Mental illness is one of the things that most often trips up people who could otherwise enjoy flourishing careers and have a large social impact, so we think this could plausibly be one of our more valuable episodes. If you’re in a hurry, we’ve extracted the key advice that Howie has to share in a <a href="https://80k.info/hl">section below</a>.</p><p>Howie and Keiran basically treated it like a private conversation, with the understanding that it may be too sensitive to release. But, after getting some really positive feedback, they’ve decided to share it with the world.</p><p>Here are a few quotes from early reviewers:</p><p>"I think there’s a big difference between admitting you have depression/seeing a psych and giving a warts-and-all account of a major depressive episode like Howie does in this episode… His description was relatable and really inspiring."</p><p>Someone who works on mental health issues said:</p><p>"This episode is perhaps the most vivid and tangible example of what it is like to experience psychological distress that I’ve ever encountered. Even though the content of Howie and Keiran’s discussion was serious, I thought they both managed to converse about it in an approachable and not-overly-somber way."</p><p>And another reviewer said:</p><p>"I found Howie’s reflections on what is actually going on in his head when he engages in negative self-talk to be considerably more illuminating than anything I’ve heard from my therapist."</p><p>We also hope that the episode will:</p><ol><li>Help people realise that they have a shot at making a difference in the future, even if they’re experiencing (or have experienced in the past) mental illness, self doubt, imposter syndrome, or other personal obstacles.</li><li>Give insight into what it’s like in the head of one person with depression, anxiety, and imposter syndrome, including the specific thought patterns they experience on typical days and more extreme days. In addition to being interesting for its own sake, this might make it easier for people to understand the experiences of family members, friends, and colleagues — and know how to react more helpfully.</li></ol><p>Several early listeners have even made specific behavioral changes due to listening to the episode — including people who generally have good mental health but were convinced it’s well worth the low cost of setting up a plan in case they have problems in the future.</p><p>So we think this episode will be valuable for:</p><ul><li>People who have experienced mental health problems or might in future;</li><li>People who have had troubles with stress, anxiety, low mood, low self esteem, imposter syndrome and similar issues, even if their experience isn’t well described as ‘mental illness’;</li><li>People who have never experienced these problems but want to learn about what it’s like, so they can better relate to and assist family, friends or colleagues who do.</li><li>In other words, we think this episode could be worthwhile for almost everybody.</li></ul><p>Just a heads up that this conversation gets pretty intense at times, and includes references to self-harm and suicidal thoughts.</p><p>If you don’t want to hear or read the most intense section, you can skip the chapter called ‘Disaster’. And if you’d rather avoid almost all of these references, you could skip straight to the chapter called ‘80,000 Hours’.</p><p>We’ve collected a large list of high quality resources for overcoming mental health problems in <a href="https://80k.info/hl">our links section</a>.</p><p><em>If you’re feeling suicidal or have thoughts of harming yourself right now, there are suicide hotlines at </em><a href="https://suicidepreventionlifeline.org/"><em>National Suicide Prevention Lifeline</em></a><em> in the US (800-273-8255) and </em><a href="https://www.samaritans.org/"><em>Samaritans</em></a><em> in the UK (116 123). You may also want to find and save a number for a local service where possible.</em></p><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!).</p><p>The producer of this show, Keiran Harris, interviewed our mutual colleague Howie about the major ways that mental illness has affected his life and career. While depression, anxiety, ADHD and other problems are extremely common, it’s rare for people to offer detailed insight into their thoughts and struggles — and even rarer for someone as perceptive as Howie to do so.</p><p><strong>Rebroadcast: this episode was originally released in May 2021.</strong></p><p><a href="https://80k.info/hl"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>The first half of this conversation is a searingly honest account of Howie’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today.</p><p>The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort.</p><p>Howie tries his best to be a corrective to this misguided attitude and pragmatically focus on what actually matters — doing whatever will help you get better.</p><p>Mental illness is one of the things that most often trips up people who could otherwise enjoy flourishing careers and have a large social impact, so we think this could plausibly be one of our more valuable episodes. If you’re in a hurry, we’ve extracted the key advice that Howie has to share in a <a href="https://80k.info/hl">section below</a>.</p><p>Howie and Keiran basically treated it like a private conversation, with the understanding that it may be too sensitive to release. But, after getting some really positive feedback, they’ve decided to share it with the world.</p><p>Here are a few quotes from early reviewers:</p><p>"I think there’s a big difference between admitting you have depression/seeing a psych and giving a warts-and-all account of a major depressive episode like Howie does in this episode… His description was relatable and really inspiring."</p><p>Someone who works on mental health issues said:</p><p>"This episode is perhaps the most vivid and tangible example of what it is like to experience psychological distress that I’ve ever encountered. Even though the content of Howie and Keiran’s discussion was serious, I thought they both managed to converse about it in an approachable and not-overly-somber way."</p><p>And another reviewer said:</p><p>"I found Howie’s reflections on what is actually going on in his head when he engages in negative self-talk to be considerably more illuminating than anything I’ve heard from my therapist."</p><p>We also hope that the episode will:</p><ol><li>Help people realise that they have a shot at making a difference in the future, even if they’re experiencing (or have experienced in the past) mental illness, self doubt, imposter syndrome, or other personal obstacles.</li><li>Give insight into what it’s like in the head of one person with depression, anxiety, and imposter syndrome, including the specific thought patterns they experience on typical days and more extreme days. In addition to being interesting for its own sake, this might make it easier for people to understand the experiences of family members, friends, and colleagues — and know how to react more helpfully.</li></ol><p>Several early listeners have even made specific behavioral changes due to listening to the episode — including people who generally have good mental health but were convinced it’s well worth the low cost of setting up a plan in case they have problems in the future.</p><p>So we think this episode will be valuable for:</p><ul><li>People who have experienced mental health problems or might in future;</li><li>People who have had troubles with stress, anxiety, low mood, low self esteem, imposter syndrome and similar issues, even if their experience isn’t well described as ‘mental illness’;</li><li>People who have never experienced these problems but want to learn about what it’s like, so they can better relate to and assist family, friends or colleagues who do.</li><li>In other words, we think this episode could be worthwhile for almost everybody.</li></ul><p>Just a heads up that this conversation gets pretty intense at times, and includes references to self-harm and suicidal thoughts.</p><p>If you don’t want to hear or read the most intense section, you can skip the chapter called ‘Disaster’. And if you’d rather avoid almost all of these references, you could skip straight to the chapter called ‘80,000 Hours’.</p><p>We’ve collected a large list of high quality resources for overcoming mental health problems in <a href="https://80k.info/hl">our links section</a>.</p><p><em>If you’re feeling suicidal or have thoughts of harming yourself right now, there are suicide hotlines at </em><a href="https://suicidepreventionlifeline.org/"><em>National Suicide Prevention Lifeline</em></a><em> in the US (800-273-8255) and </em><a href="https://www.samaritans.org/"><em>Samaritans</em></a><em> in the UK (116 123). You may also want to find and save a number for a local service where possible.</em></p><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 27 Dec 2023 21:31:27 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c71ef54d/74af48c3.mp3" length="164711080" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Xf6FDPSJcVJEQCDwspe4nyRHkHWKoXRJSOR4PlEoJUg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NjI2MzYv/MTcwMzcwMjI3My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10292</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Today’s episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!).</p><p>The producer of this show, Keiran Harris, interviewed our mutual colleague Howie about the major ways that mental illness has affected his life and career. While depression, anxiety, ADHD and other problems are extremely common, it’s rare for people to offer detailed insight into their thoughts and struggles — and even rarer for someone as perceptive as Howie to do so.</p><p><strong>Rebroadcast: this episode was originally released in May 2021.</strong></p><p><a href="https://80k.info/hl"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>The first half of this conversation is a searingly honest account of Howie’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today.</p><p>The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort.</p><p>Howie tries his best to be a corrective to this misguided attitude and pragmatically focus on what actually matters — doing whatever will help you get better.</p><p>Mental illness is one of the things that most often trips up people who could otherwise enjoy flourishing careers and have a large social impact, so we think this could plausibly be one of our more valuable episodes. If you’re in a hurry, we’ve extracted the key advice that Howie has to share in a <a href="https://80k.info/hl">section below</a>.</p><p>Howie and Keiran basically treated it like a private conversation, with the understanding that it may be too sensitive to release. But, after getting some really positive feedback, they’ve decided to share it with the world.</p><p>Here are a few quotes from early reviewers:</p><p>"I think there’s a big difference between admitting you have depression/seeing a psych and giving a warts-and-all account of a major depressive episode like Howie does in this episode… His description was relatable and really inspiring."</p><p>Someone who works on mental health issues said:</p><p>"This episode is perhaps the most vivid and tangible example of what it is like to experience psychological distress that I’ve ever encountered. Even though the content of Howie and Keiran’s discussion was serious, I thought they both managed to converse about it in an approachable and not-overly-somber way."</p><p>And another reviewer said:</p><p>"I found Howie’s reflections on what is actually going on in his head when he engages in negative self-talk to be considerably more illuminating than anything I’ve heard from my therapist."</p><p>We also hope that the episode will:</p><ol><li>Help people realise that they have a shot at making a difference in the future, even if they’re experiencing (or have experienced in the past) mental illness, self doubt, imposter syndrome, or other personal obstacles.</li><li>Give insight into what it’s like in the head of one person with depression, anxiety, and imposter syndrome, including the specific thought patterns they experience on typical days and more extreme days. In addition to being interesting for its own sake, this might make it easier for people to understand the experiences of family members, friends, and colleagues — and know how to react more helpfully.</li></ol><p>Several early listeners have even made specific behavioral changes due to listening to the episode — including people who generally have good mental health but were convinced it’s well worth the low cost of setting up a plan in case they have problems in the future.</p><p>So we think this episode will be valuable for:</p><ul><li>People who have experienced mental health problems or might in future;</li><li>People who have had troubles with stress, anxiety, low mood, low self esteem, imposter syndrome and similar issues, even if their experience isn’t well described as ‘mental illness’;</li><li>People who have never experienced these problems but want to learn about what it’s like, so they can better relate to and assist family, friends or colleagues who do.</li><li>In other words, we think this episode could be worthwhile for almost everybody.</li></ul><p>Just a heads up that this conversation gets pretty intense at times, and includes references to self-harm and suicidal thoughts.</p><p>If you don’t want to hear or read the most intense section, you can skip the chapter called ‘Disaster’. And if you’d rather avoid almost all of these references, you could skip straight to the chapter called ‘80,000 Hours’.</p><p>We’ve collected a large list of high quality resources for overcoming mental health problems in <a href="https://80k.info/hl">our links section</a>.</p><p><em>If you’re feeling suicidal or have thoughts of harming yourself right now, there are suicide hotlines at </em><a href="https://suicidepreventionlifeline.org/"><em>National Suicide Prevention Lifeline</em></a><em> in the US (800-273-8255) and </em><a href="https://www.samaritans.org/"><em>Samaritans</em></a><em> in the UK (116 123). You may also want to find and save a number for a local service where possible.</em></p><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/c71ef54d/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#176 – Nathan Labenz on the final push for AGI, understanding OpenAI's leadership drama, and red-teaming frontier models</title>
      <itunes:title>#176 – Nathan Labenz on the final push for AGI, understanding OpenAI's leadership drama, and red-teaming frontier models</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f0600fe7-0a2f-4274-a32e-f6f2e53c0bea</guid>
      <link>https://80000hours.org/podcast/episodes/nathan-labenz-openai-red-team-safety/?utm_campaign=podcast__nathan-labenz&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>OpenAI says its mission is to build AGI — an AI system that is better than human beings at everything. Should the world trust them to do that safely?</p><p>That’s the central theme of today’s episode with Nathan Labenz — entrepreneur, AI scout, and host of <em>The Cognitive Revolution</em> podcast.</p><p><a href="https://80k.info/nl"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p> Nathan saw the AI revolution coming years ago, and, astonished by the research he was seeing, set aside his role as CEO of Waymark and made it his full-time job to understand AI capabilities across every domain. He has been obsessively tracking the AI world since — including joining OpenAI’s “red team” that probed GPT-4 to find ways it could be abused, long before it was public.</p><p>Whether OpenAI was taking AI safety seriously enough became a topic of dinner table conversation around the world after the shocking firing and reinstatement of Sam Altman as CEO last month.</p><p>Nathan’s view: it’s complicated. Discussion of this topic has often been heated, polarising, and personal. But Nathan wants to avoid that and simply lay out, in a way that is impartial and fair to everyone involved, what OpenAI has done right and how it could do better in his view.</p><p>When he started on the GPT-4 red team, the model would do anything from diagnose a skin condition to plan a terrorist attack without the slightest reservation or objection. When later shown a “Safety” version of GPT-4 that was almost the same, he approached a member of OpenAI’s board to share his concerns and tell them they really needed to try out GPT-4 for themselves and form an opinion.</p><p>In today’s episode, we share this story as Nathan told it on his own show, <em>The Cognitive Revolution</em>, which he did in the hope that it would provide useful background to understanding the OpenAI board’s reservations about Sam Altman, which to this day have not been laid out in any detail.</p><p>But while he feared throughout 2022 that OpenAI and Sam Altman didn’t understand the power and risk of their own system, he has since been repeatedly impressed, and came to think of OpenAI as among the better companies that could hypothetically be working to build AGI.</p><p>Their efforts to make GPT-4 safe turned out to be much larger and more successful than Nathan was seeing. Sam Altman and other leaders at OpenAI seem to sincerely believe they’re playing with fire, and take the threat posed by their work very seriously. With the benefit of hindsight, Nathan suspects OpenAI’s decision to release GPT-4 when it did was for the best.</p><p>On top of that, OpenAI has been among the most sane and sophisticated voices advocating for AI regulations that would target just the most powerful AI systems — the type they themselves are building — and that could make a real difference. They’ve also invested major resources into new ‘Superalignment’ and ‘Preparedness’ teams, while avoiding using competition with China as an excuse for recklessness.</p><p>At the same time, it’s very hard to know whether it’s all enough. The challenge of making an AGI safe and beneficial may require much more than they hope or have bargained for. Given that, Nathan poses the question of whether it makes sense to try to build a fully general AGI that can outclass humans in every domain at the first opportunity. Maybe in the short term, we should focus on harvesting the enormous possible economic and humanitarian benefits of narrow applied AI models, and wait until we not only have <em>a</em> way to build AGI, but <em>a good way</em> to build AGI — an AGI that we’re confident we want, which we can prove will remain safe as its capabilities get ever greater.</p><p>By threatening to follow Sam Altman to Microsoft before his reinstatement as OpenAI CEO, OpenAI’s research team has proven they have enormous influence over the direction of the company. If they put their minds to it, they’re also better placed than maybe anyone in the world to assess if the company’s strategy is on the right track and serving the interests of humanity as a whole. Nathan concludes that this power and insight only adds to the enormous weight of responsibility already resting on their shoulders.</p><p>In today’s extensive conversation, Nathan and host Rob Wiblin discuss not only all of the above, but also:</p><ul><li>Speculation about the OpenAI boardroom drama with Sam Altman, given Nathan’s interactions with the board when he raised concerns from his red teaming efforts.</li><li>Which AI applications we should be urgently rolling out, with less worry about safety.</li><li>Whether governance issues at OpenAI demonstrate AI research can only be slowed by governments.</li><li>Whether AI capabilities are advancing faster than safety efforts and controls.</li><li>The costs and benefits of releasing powerful models like GPT-4.</li><li>Nathan’s view on the game theory of AI arms races and China.</li><li>Whether it’s worth taking some risk with AI for huge potential upside.</li><li>The need for more “AI scouts” to understand and communicate AI progress.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>OpenAI says its mission is to build AGI — an AI system that is better than human beings at everything. Should the world trust them to do that safely?</p><p>That’s the central theme of today’s episode with Nathan Labenz — entrepreneur, AI scout, and host of <em>The Cognitive Revolution</em> podcast.</p><p><a href="https://80k.info/nl"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p> Nathan saw the AI revolution coming years ago, and, astonished by the research he was seeing, set aside his role as CEO of Waymark and made it his full-time job to understand AI capabilities across every domain. He has been obsessively tracking the AI world since — including joining OpenAI’s “red team” that probed GPT-4 to find ways it could be abused, long before it was public.</p><p>Whether OpenAI was taking AI safety seriously enough became a topic of dinner table conversation around the world after the shocking firing and reinstatement of Sam Altman as CEO last month.</p><p>Nathan’s view: it’s complicated. Discussion of this topic has often been heated, polarising, and personal. But Nathan wants to avoid that and simply lay out, in a way that is impartial and fair to everyone involved, what OpenAI has done right and how it could do better in his view.</p><p>When he started on the GPT-4 red team, the model would do anything from diagnose a skin condition to plan a terrorist attack without the slightest reservation or objection. When later shown a “Safety” version of GPT-4 that was almost the same, he approached a member of OpenAI’s board to share his concerns and tell them they really needed to try out GPT-4 for themselves and form an opinion.</p><p>In today’s episode, we share this story as Nathan told it on his own show, <em>The Cognitive Revolution</em>, which he did in the hope that it would provide useful background to understanding the OpenAI board’s reservations about Sam Altman, which to this day have not been laid out in any detail.</p><p>But while he feared throughout 2022 that OpenAI and Sam Altman didn’t understand the power and risk of their own system, he has since been repeatedly impressed, and came to think of OpenAI as among the better companies that could hypothetically be working to build AGI.</p><p>Their efforts to make GPT-4 safe turned out to be much larger and more successful than Nathan was seeing. Sam Altman and other leaders at OpenAI seem to sincerely believe they’re playing with fire, and take the threat posed by their work very seriously. With the benefit of hindsight, Nathan suspects OpenAI’s decision to release GPT-4 when it did was for the best.</p><p>On top of that, OpenAI has been among the most sane and sophisticated voices advocating for AI regulations that would target just the most powerful AI systems — the type they themselves are building — and that could make a real difference. They’ve also invested major resources into new ‘Superalignment’ and ‘Preparedness’ teams, while avoiding using competition with China as an excuse for recklessness.</p><p>At the same time, it’s very hard to know whether it’s all enough. The challenge of making an AGI safe and beneficial may require much more than they hope or have bargained for. Given that, Nathan poses the question of whether it makes sense to try to build a fully general AGI that can outclass humans in every domain at the first opportunity. Maybe in the short term, we should focus on harvesting the enormous possible economic and humanitarian benefits of narrow applied AI models, and wait until we not only have <em>a</em> way to build AGI, but <em>a good way</em> to build AGI — an AGI that we’re confident we want, which we can prove will remain safe as its capabilities get ever greater.</p><p>By threatening to follow Sam Altman to Microsoft before his reinstatement as OpenAI CEO, OpenAI’s research team has proven they have enormous influence over the direction of the company. If they put their minds to it, they’re also better placed than maybe anyone in the world to assess if the company’s strategy is on the right track and serving the interests of humanity as a whole. Nathan concludes that this power and insight only adds to the enormous weight of responsibility already resting on their shoulders.</p><p>In today’s extensive conversation, Nathan and host Rob Wiblin discuss not only all of the above, but also:</p><ul><li>Speculation about the OpenAI boardroom drama with Sam Altman, given Nathan’s interactions with the board when he raised concerns from his red teaming efforts.</li><li>Which AI applications we should be urgently rolling out, with less worry about safety.</li><li>Whether governance issues at OpenAI demonstrate AI research can only be slowed by governments.</li><li>Whether AI capabilities are advancing faster than safety efforts and controls.</li><li>The costs and benefits of releasing powerful models like GPT-4.</li><li>Nathan’s view on the game theory of AI arms races and China.</li><li>Whether it’s worth taking some risk with AI for huge potential upside.</li><li>The need for more “AI scouts” to understand and communicate AI progress.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 22 Dec 2023 21:31:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/32a5599f/e66edbd0.mp3" length="217832300" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/RwiaEM67LHx-gm5cONgRKnGBzYIa5rjwg4ztSDg8Ds0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NTA5NTgv/MTcwMjk5ODYyNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13612</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>OpenAI says its mission is to build AGI — an AI system that is better than human beings at everything. Should the world trust them to do that safely?</p><p>That’s the central theme of today’s episode with Nathan Labenz — entrepreneur, AI scout, and host of <em>The Cognitive Revolution</em> podcast.</p><p><a href="https://80k.info/nl"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p> Nathan saw the AI revolution coming years ago, and, astonished by the research he was seeing, set aside his role as CEO of Waymark and made it his full-time job to understand AI capabilities across every domain. He has been obsessively tracking the AI world since — including joining OpenAI’s “red team” that probed GPT-4 to find ways it could be abused, long before it was public.</p><p>Whether OpenAI was taking AI safety seriously enough became a topic of dinner table conversation around the world after the shocking firing and reinstatement of Sam Altman as CEO last month.</p><p>Nathan’s view: it’s complicated. Discussion of this topic has often been heated, polarising, and personal. But Nathan wants to avoid that and simply lay out, in a way that is impartial and fair to everyone involved, what OpenAI has done right and how it could do better in his view.</p><p>When he started on the GPT-4 red team, the model would do anything from diagnose a skin condition to plan a terrorist attack without the slightest reservation or objection. When later shown a “Safety” version of GPT-4 that was almost the same, he approached a member of OpenAI’s board to share his concerns and tell them they really needed to try out GPT-4 for themselves and form an opinion.</p><p>In today’s episode, we share this story as Nathan told it on his own show, <em>The Cognitive Revolution</em>, which he did in the hope that it would provide useful background to understanding the OpenAI board’s reservations about Sam Altman, which to this day have not been laid out in any detail.</p><p>But while he feared throughout 2022 that OpenAI and Sam Altman didn’t understand the power and risk of their own system, he has since been repeatedly impressed, and came to think of OpenAI as among the better companies that could hypothetically be working to build AGI.</p><p>Their efforts to make GPT-4 safe turned out to be much larger and more successful than Nathan was seeing. Sam Altman and other leaders at OpenAI seem to sincerely believe they’re playing with fire, and take the threat posed by their work very seriously. With the benefit of hindsight, Nathan suspects OpenAI’s decision to release GPT-4 when it did was for the best.</p><p>On top of that, OpenAI has been among the most sane and sophisticated voices advocating for AI regulations that would target just the most powerful AI systems — the type they themselves are building — and that could make a real difference. They’ve also invested major resources into new ‘Superalignment’ and ‘Preparedness’ teams, while avoiding using competition with China as an excuse for recklessness.</p><p>At the same time, it’s very hard to know whether it’s all enough. The challenge of making an AGI safe and beneficial may require much more than they hope or have bargained for. Given that, Nathan poses the question of whether it makes sense to try to build a fully general AGI that can outclass humans in every domain at the first opportunity. Maybe in the short term, we should focus on harvesting the enormous possible economic and humanitarian benefits of narrow applied AI models, and wait until we not only have <em>a</em> way to build AGI, but <em>a good way</em> to build AGI — an AGI that we’re confident we want, which we can prove will remain safe as its capabilities get ever greater.</p><p>By threatening to follow Sam Altman to Microsoft before his reinstatement as OpenAI CEO, OpenAI’s research team has proven they have enormous influence over the direction of the company. If they put their minds to it, they’re also better placed than maybe anyone in the world to assess if the company’s strategy is on the right track and serving the interests of humanity as a whole. Nathan concludes that this power and insight only adds to the enormous weight of responsibility already resting on their shoulders.</p><p>In today’s extensive conversation, Nathan and host Rob Wiblin discuss not only all of the above, but also:</p><ul><li>Speculation about the OpenAI boardroom drama with Sam Altman, given Nathan’s interactions with the board when he raised concerns from his red teaming efforts.</li><li>Which AI applications we should be urgently rolling out, with less worry about safety.</li><li>Whether governance issues at OpenAI demonstrate AI research can only be slowed by governments.</li><li>Whether AI capabilities are advancing faster than safety efforts and controls.</li><li>The costs and benefits of releasing powerful models like GPT-4.</li><li>Nathan’s view on the game theory of AI arms races and China.</li><li>Whether it’s worth taking some risk with AI for huge potential upside.</li><li>The need for more “AI scouts” to understand and communicate AI progress.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/32a5599f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#175 – Lucia Coulter on preventing lead poisoning for $1.66 per child</title>
      <itunes:title>#175 – Lucia Coulter on preventing lead poisoning for $1.66 per child</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b4ca53a4-a827-414c-afe3-aca0f85e0101</guid>
      <link>https://80000hours.org/podcast/episodes/lucia-coulter-lead-exposure-elimination-project/?utm_campaign=podcast__lucia-coulter&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Lead is one of the most poisonous things going. A single sugar sachet of lead, spread over a park the size of an American football field, is enough to give a child that regularly plays there lead poisoning. For life they’ll be condemned to a ~3-point-lower IQ; a 50% higher risk of heart attacks; and elevated risk of kidney disease, anaemia, and ADHD, among other effects.</p><p>We’ve known lead is a health nightmare for at least 50 years, and that got lead out of car fuel everywhere. So is the situation under control? Not even close.</p><p>Around half the kids in poor and middle-income countries have blood lead levels above 5 micrograms per decilitre; the US declared a national emergency when just 5% of the children in Flint, Michigan exceeded that level. The collective damage this is doing to children’s intellectual potential, health, and life expectancy is vast — the health damage involved is around that caused by malaria, tuberculosis, and HIV combined.</p><p>This week’s guest, Lucia Coulter — cofounder of the incredibly successful <a href="https://leadelimination.org/">Lead Exposure Elimination Project</a> (LEEP) — speaks about how LEEP has been reducing childhood lead exposure in poor countries by getting bans on lead in paint enforced.</p><p><a href="https://80k.info/lc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>Various estimates suggest the work is absurdly cost effective. LEEP is in expectation <a href="https://www.founderspledge.com/research/lead-exposure-elimination-project-leep">preventing kids from getting lead poisoning for under $2 per child</a> (explore the analysis <a href="https://docs.google.com/spreadsheets/d/1Q3YpnHrg9hkax5r6YaZxThEvGSSy8--N-eiQdcXgNI0/edit#gid=679643831">here</a>). Or, looking at it differently, LEEP is <a href="https://leadelimination.org/malawi_cost-effectiveness_intro/">saving a year of healthy life for $14</a>, and in the long run is increasing people’s lifetime income anywhere from $300–1,200 for each $1 it spends, by preventing intellectual stunting.</p><p>Which raises the question: why hasn’t this happened already? How is lead still in paint in most poor countries, even when that’s oftentimes already illegal? And how is LEEP able to get bans on leaded paint enforced in a country while spending barely tens of thousands of dollars? When leaded paint is gone, what should they target next?</p><p>With host Robert Wiblin, Lucia answers all those questions and more:</p><ul><li>Why LEEP isn’t fully funded, and what it would do with extra money (you can <a href="https://leadelimination.org/">donate here</a>).</li><li>How bad lead poisoning is in rich countries.</li><li>Why lead is still in aeroplane fuel.</li><li>How lead got put straight in food in Bangladesh, and a handful of people got it removed.</li><li>Why the enormous damage done by lead mostly goes unnoticed.</li><li>The other major sources of lead exposure aside from paint.</li><li>Lucia’s story of founding a highly effective nonprofit, despite having no prior entrepreneurship experience, through <a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship’s Incubation Program</a>.</li><li>Why Lucia pledges 10% of her income to cost-effective charities.</li><li>Lucia’s take on why GiveWell didn’t support LEEP earlier on.</li><li>How the <a href="https://www.leaddetectprize.com/">invention of cheap, accessible lead testing</a> for blood and consumer products would be a game changer.</li><li>Generalisable lessons LEEP has learned from coordinating with governments in poor countries.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Lead is one of the most poisonous things going. A single sugar sachet of lead, spread over a park the size of an American football field, is enough to give a child that regularly plays there lead poisoning. For life they’ll be condemned to a ~3-point-lower IQ; a 50% higher risk of heart attacks; and elevated risk of kidney disease, anaemia, and ADHD, among other effects.</p><p>We’ve known lead is a health nightmare for at least 50 years, and that got lead out of car fuel everywhere. So is the situation under control? Not even close.</p><p>Around half the kids in poor and middle-income countries have blood lead levels above 5 micrograms per decilitre; the US declared a national emergency when just 5% of the children in Flint, Michigan exceeded that level. The collective damage this is doing to children’s intellectual potential, health, and life expectancy is vast — the health damage involved is around that caused by malaria, tuberculosis, and HIV combined.</p><p>This week’s guest, Lucia Coulter — cofounder of the incredibly successful <a href="https://leadelimination.org/">Lead Exposure Elimination Project</a> (LEEP) — speaks about how LEEP has been reducing childhood lead exposure in poor countries by getting bans on lead in paint enforced.</p><p><a href="https://80k.info/lc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>Various estimates suggest the work is absurdly cost effective. LEEP is in expectation <a href="https://www.founderspledge.com/research/lead-exposure-elimination-project-leep">preventing kids from getting lead poisoning for under $2 per child</a> (explore the analysis <a href="https://docs.google.com/spreadsheets/d/1Q3YpnHrg9hkax5r6YaZxThEvGSSy8--N-eiQdcXgNI0/edit#gid=679643831">here</a>). Or, looking at it differently, LEEP is <a href="https://leadelimination.org/malawi_cost-effectiveness_intro/">saving a year of healthy life for $14</a>, and in the long run is increasing people’s lifetime income anywhere from $300–1,200 for each $1 it spends, by preventing intellectual stunting.</p><p>Which raises the question: why hasn’t this happened already? How is lead still in paint in most poor countries, even when that’s oftentimes already illegal? And how is LEEP able to get bans on leaded paint enforced in a country while spending barely tens of thousands of dollars? When leaded paint is gone, what should they target next?</p><p>With host Robert Wiblin, Lucia answers all those questions and more:</p><ul><li>Why LEEP isn’t fully funded, and what it would do with extra money (you can <a href="https://leadelimination.org/">donate here</a>).</li><li>How bad lead poisoning is in rich countries.</li><li>Why lead is still in aeroplane fuel.</li><li>How lead got put straight in food in Bangladesh, and a handful of people got it removed.</li><li>Why the enormous damage done by lead mostly goes unnoticed.</li><li>The other major sources of lead exposure aside from paint.</li><li>Lucia’s story of founding a highly effective nonprofit, despite having no prior entrepreneurship experience, through <a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship’s Incubation Program</a>.</li><li>Why Lucia pledges 10% of her income to cost-effective charities.</li><li>Lucia’s take on why GiveWell didn’t support LEEP earlier on.</li><li>How the <a href="https://www.leaddetectprize.com/">invention of cheap, accessible lead testing</a> for blood and consumer products would be a game changer.</li><li>Generalisable lessons LEEP has learned from coordinating with governments in poor countries.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 14 Dec 2023 21:38:35 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b0899679/eac9f0fe.mp3" length="128809225" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/pRLTGecOuljAxmkp8VbkxdM11miufWPVV-WQn6ytrCI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2NDM0Mzcv/MTcwMjQ4NTI0MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8048</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Lead is one of the most poisonous things going. A single sugar sachet of lead, spread over a park the size of an American football field, is enough to give a child that regularly plays there lead poisoning. For life they’ll be condemned to a ~3-point-lower IQ; a 50% higher risk of heart attacks; and elevated risk of kidney disease, anaemia, and ADHD, among other effects.</p><p>We’ve known lead is a health nightmare for at least 50 years, and that got lead out of car fuel everywhere. So is the situation under control? Not even close.</p><p>Around half the kids in poor and middle-income countries have blood lead levels above 5 micrograms per decilitre; the US declared a national emergency when just 5% of the children in Flint, Michigan exceeded that level. The collective damage this is doing to children’s intellectual potential, health, and life expectancy is vast — the health damage involved is around that caused by malaria, tuberculosis, and HIV combined.</p><p>This week’s guest, Lucia Coulter — cofounder of the incredibly successful <a href="https://leadelimination.org/">Lead Exposure Elimination Project</a> (LEEP) — speaks about how LEEP has been reducing childhood lead exposure in poor countries by getting bans on lead in paint enforced.</p><p><a href="https://80k.info/lc"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>Various estimates suggest the work is absurdly cost effective. LEEP is in expectation <a href="https://www.founderspledge.com/research/lead-exposure-elimination-project-leep">preventing kids from getting lead poisoning for under $2 per child</a> (explore the analysis <a href="https://docs.google.com/spreadsheets/d/1Q3YpnHrg9hkax5r6YaZxThEvGSSy8--N-eiQdcXgNI0/edit#gid=679643831">here</a>). Or, looking at it differently, LEEP is <a href="https://leadelimination.org/malawi_cost-effectiveness_intro/">saving a year of healthy life for $14</a>, and in the long run is increasing people’s lifetime income anywhere from $300–1,200 for each $1 it spends, by preventing intellectual stunting.</p><p>Which raises the question: why hasn’t this happened already? How is lead still in paint in most poor countries, even when that’s oftentimes already illegal? And how is LEEP able to get bans on leaded paint enforced in a country while spending barely tens of thousands of dollars? When leaded paint is gone, what should they target next?</p><p>With host Robert Wiblin, Lucia answers all those questions and more:</p><ul><li>Why LEEP isn’t fully funded, and what it would do with extra money (you can <a href="https://leadelimination.org/">donate here</a>).</li><li>How bad lead poisoning is in rich countries.</li><li>Why lead is still in aeroplane fuel.</li><li>How lead got put straight in food in Bangladesh, and a handful of people got it removed.</li><li>Why the enormous damage done by lead mostly goes unnoticed.</li><li>The other major sources of lead exposure aside from paint.</li><li>Lucia’s story of founding a highly effective nonprofit, despite having no prior entrepreneurship experience, through <a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship’s Incubation Program</a>.</li><li>Why Lucia pledges 10% of her income to cost-effective charities.</li><li>Lucia’s take on why GiveWell didn’t support LEEP earlier on.</li><li>How the <a href="https://www.leaddetectprize.com/">invention of cheap, accessible lead testing</a> for blood and consumer products would be a game changer.</li><li>Generalisable lessons LEEP has learned from coordinating with governments in poor countries.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/b0899679/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#174 – Nita Farahany on the neurotechnology already being used to convict criminals and manipulate workers</title>
      <itunes:title>#174 – Nita Farahany on the neurotechnology already being used to convict criminals and manipulate workers</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">73d4b890-54c1-473e-a0c5-e56b1d55e26e</guid>
      <link>https://80000hours.org/podcast/episodes/nita-farahany-neurotechnology/?utm_campaign=podcast__nita-farahany&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"It will change everything: it will change our workplaces, it will change our interactions with the government, it will change our interactions with each other. It will make all of us unwitting neuromarketing subjects at all times, because at every moment in time, when you’re interacting on any platform that also has issued you a multifunctional device where they’re looking at your brainwave activity, they are marketing to you, they’re cognitively shaping you.</p><p>"So I wrote the book as both a wake-up call, but also as an agenda-setting: to say, what do we need to do, given that this is coming? And there’s a lot of hope, and we should be able to reap the benefits of the technology, but how do we do that without actually ending up in this world of like, 'Oh my god, mind reading is here. Now what?'" — Nita Farahany</p><p>In today’s episode, host Luisa Rodriguez speaks to Nita Farahany — professor of law and philosophy at Duke Law School — about applications of cutting-edge neurotechnology.</p><p><a href="https://80k.info/nf"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How close we are to actual mind reading.</li><li>How hacking neural interfaces could cure depression.</li><li>How companies might use neural data in the workplace — like tracking how productive you are, or using your emotional states against you in negotiations.</li><li>How close we are to being able to unlock our phones by singing a song in our heads.</li><li>How neurodata has been used for interrogations, and even criminal prosecutions.</li><li>The possibility of linking brains to the point where you could experience exactly the same thing as another person.</li><li>Military applications of this tech, including the possibility of one soldier controlling swarms of drones with their mind.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"It will change everything: it will change our workplaces, it will change our interactions with the government, it will change our interactions with each other. It will make all of us unwitting neuromarketing subjects at all times, because at every moment in time, when you’re interacting on any platform that also has issued you a multifunctional device where they’re looking at your brainwave activity, they are marketing to you, they’re cognitively shaping you.</p><p>"So I wrote the book as both a wake-up call, but also as an agenda-setting: to say, what do we need to do, given that this is coming? And there’s a lot of hope, and we should be able to reap the benefits of the technology, but how do we do that without actually ending up in this world of like, 'Oh my god, mind reading is here. Now what?'" — Nita Farahany</p><p>In today’s episode, host Luisa Rodriguez speaks to Nita Farahany — professor of law and philosophy at Duke Law School — about applications of cutting-edge neurotechnology.</p><p><a href="https://80k.info/nf"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How close we are to actual mind reading.</li><li>How hacking neural interfaces could cure depression.</li><li>How companies might use neural data in the workplace — like tracking how productive you are, or using your emotional states against you in negotiations.</li><li>How close we are to being able to unlock our phones by singing a song in our heads.</li><li>How neurodata has been used for interrogations, and even criminal prosecutions.</li><li>The possibility of linking brains to the point where you could experience exactly the same thing as another person.</li><li>Military applications of this tech, including the possibility of one soldier controlling swarms of drones with their mind.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 07 Dec 2023 22:20:37 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cb5beb2b/1dfadbae.mp3" length="115743558" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Jv9GE4qJAbN-oSLj5mGiSsxKgzfaHY0nOVYSAxIffMM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2MzM1Njgv/MTcwMTg4NTQ1My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7231</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"It will change everything: it will change our workplaces, it will change our interactions with the government, it will change our interactions with each other. It will make all of us unwitting neuromarketing subjects at all times, because at every moment in time, when you’re interacting on any platform that also has issued you a multifunctional device where they’re looking at your brainwave activity, they are marketing to you, they’re cognitively shaping you.</p><p>"So I wrote the book as both a wake-up call, but also as an agenda-setting: to say, what do we need to do, given that this is coming? And there’s a lot of hope, and we should be able to reap the benefits of the technology, but how do we do that without actually ending up in this world of like, 'Oh my god, mind reading is here. Now what?'" — Nita Farahany</p><p>In today’s episode, host Luisa Rodriguez speaks to Nita Farahany — professor of law and philosophy at Duke Law School — about applications of cutting-edge neurotechnology.</p><p><a href="https://80k.info/nf"><strong>Links to learn more, summary, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How close we are to actual mind reading.</li><li>How hacking neural interfaces could cure depression.</li><li>How companies might use neural data in the workplace — like tracking how productive you are, or using your emotional states against you in negotiations.</li><li>How close we are to being able to unlock our phones by singing a song in our heads.</li><li>How neurodata has been used for interrogations, and even criminal prosecutions.</li><li>The possibility of linking brains to the point where you could experience exactly the same thing as another person.</li><li>Military applications of this tech, including the possibility of one soldier controlling swarms of drones with their mind.</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/cb5beb2b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#173 – Jeff Sebo on digital minds, and how to avoid sleepwalking into a major moral catastrophe</title>
      <itunes:title>#173 – Jeff Sebo on digital minds, and how to avoid sleepwalking into a major moral catastrophe</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">92830af3-e00c-41d7-ba1a-0a95420bac79</guid>
      <link>https://80000hours.org/podcast/episodes/jeff-sebo-ethics-digital-minds/?utm_campaign=podcast__jeff-sebo&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"We do have a tendency to anthropomorphise nonhumans — which means attributing human characteristics to them, even when they lack those characteristics. But we also have a tendency towards anthropodenial — which involves denying that nonhumans have human characteristics, even when they have them. And those tendencies are both strong, and they can both be triggered by different types of systems. So which one is stronger, which one is more probable, is again going to be contextual. </p><p>"But when we then consider that we, right now, are building societies and governments and economies that depend on the objectification, exploitation, and extermination of nonhumans, that — plus our speciesism, plus a lot of other biases and forms of ignorance that we have — gives us a strong incentive to err on the side of anthropodenial instead of anthropomorphism." — Jeff Sebo</p><p>In today’s episode, host Luisa Rodriguez interviews Jeff Sebo — director of the <a href="https://sites.google.com/nyu.edu/mindethicspolicy">Mind, Ethics, and Policy Program</a> at NYU — about preparing for a world with digital minds.</p><p><a href="https://80k.info/js"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The non-negligible chance that AI systems will be sentient by 2030</li><li>What AI systems might want and need, and how that might affect our moral concepts</li><li>What happens when beings can copy themselves? Are they one person or multiple people? Does the original own the copy or does the copy have its own rights? Do copies get the right to vote?</li><li>What kind of legal and political status should AI systems have? Legal personhood? Political citizenship?</li><li>What happens when minds can be connected? If two minds are connected, and one does something illegal, is it possible to punish one but not the other?</li><li>The repugnant conclusion and the rebugnant conclusion</li><li>The experience of trying to build the field of AI welfare</li><li>What improv comedy can teach us about doing good in the world</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:00)</li><li>The interview begins (00:02:45)</li><li>We should extend moral consideration to some AI systems by 2030 (00:06:41)</li><li>A one-in-1,000 threshold (00:15:23)</li><li>What does moral consideration mean? (00:24:36)</li><li>Hitting the threshold by 2030 (00:27:38)</li><li>Is the threshold too permissive? (00:38:24)</li><li>The Rebugnant Conclusion (00:41:00)</li><li>A world where AI experiences could matter more than human experiences (00:52:33)</li><li>Should we just accept this argument? (00:55:13)</li><li>Searching for positive-sum solutions (01:05:41)</li><li>Are we going to sleepwalk into causing massive amounts of harm to AI systems? (01:13:48)</li><li>Discourse and messaging (01:27:17)</li><li>What will AI systems want and need? (01:31:17)</li><li>Copies of digital minds (01:33:20)</li><li>Connected minds (01:40:26)</li><li>Psychological connectedness and continuity (01:49:58)</li><li>Assigning responsibility to connected minds (01:58:41)</li><li>Counting the wellbeing of connected minds (02:02:36)</li><li>Legal personhood and political citizenship (02:09:49)</li><li>Building the field of AI welfare (02:24:03)</li><li>What we can learn from improv comedy (02:29:29)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"We do have a tendency to anthropomorphise nonhumans — which means attributing human characteristics to them, even when they lack those characteristics. But we also have a tendency towards anthropodenial — which involves denying that nonhumans have human characteristics, even when they have them. And those tendencies are both strong, and they can both be triggered by different types of systems. So which one is stronger, which one is more probable, is again going to be contextual. </p><p>"But when we then consider that we, right now, are building societies and governments and economies that depend on the objectification, exploitation, and extermination of nonhumans, that — plus our speciesism, plus a lot of other biases and forms of ignorance that we have — gives us a strong incentive to err on the side of anthropodenial instead of anthropomorphism." — Jeff Sebo</p><p>In today’s episode, host Luisa Rodriguez interviews Jeff Sebo — director of the <a href="https://sites.google.com/nyu.edu/mindethicspolicy">Mind, Ethics, and Policy Program</a> at NYU — about preparing for a world with digital minds.</p><p><a href="https://80k.info/js"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The non-negligible chance that AI systems will be sentient by 2030</li><li>What AI systems might want and need, and how that might affect our moral concepts</li><li>What happens when beings can copy themselves? Are they one person or multiple people? Does the original own the copy or does the copy have its own rights? Do copies get the right to vote?</li><li>What kind of legal and political status should AI systems have? Legal personhood? Political citizenship?</li><li>What happens when minds can be connected? If two minds are connected, and one does something illegal, is it possible to punish one but not the other?</li><li>The repugnant conclusion and the rebugnant conclusion</li><li>The experience of trying to build the field of AI welfare</li><li>What improv comedy can teach us about doing good in the world</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:00)</li><li>The interview begins (00:02:45)</li><li>We should extend moral consideration to some AI systems by 2030 (00:06:41)</li><li>A one-in-1,000 threshold (00:15:23)</li><li>What does moral consideration mean? (00:24:36)</li><li>Hitting the threshold by 2030 (00:27:38)</li><li>Is the threshold too permissive? (00:38:24)</li><li>The Rebugnant Conclusion (00:41:00)</li><li>A world where AI experiences could matter more than human experiences (00:52:33)</li><li>Should we just accept this argument? (00:55:13)</li><li>Searching for positive-sum solutions (01:05:41)</li><li>Are we going to sleepwalk into causing massive amounts of harm to AI systems? (01:13:48)</li><li>Discourse and messaging (01:27:17)</li><li>What will AI systems want and need? (01:31:17)</li><li>Copies of digital minds (01:33:20)</li><li>Connected minds (01:40:26)</li><li>Psychological connectedness and continuity (01:49:58)</li><li>Assigning responsibility to connected minds (01:58:41)</li><li>Counting the wellbeing of connected minds (02:02:36)</li><li>Legal personhood and political citizenship (02:09:49)</li><li>Building the field of AI welfare (02:24:03)</li><li>What we can learn from improv comedy (02:29:29)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 22 Nov 2023 21:01:37 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/243d3b95/6a1c9c15.mp3" length="152037327" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HSxnAsi6vnVl06y7rDv6_t_DanzyOsINwGqRKrPNUf8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2MDkzNDcv/MTcwMDY2NTk4MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9500</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"We do have a tendency to anthropomorphise nonhumans — which means attributing human characteristics to them, even when they lack those characteristics. But we also have a tendency towards anthropodenial — which involves denying that nonhumans have human characteristics, even when they have them. And those tendencies are both strong, and they can both be triggered by different types of systems. So which one is stronger, which one is more probable, is again going to be contextual. </p><p>"But when we then consider that we, right now, are building societies and governments and economies that depend on the objectification, exploitation, and extermination of nonhumans, that — plus our speciesism, plus a lot of other biases and forms of ignorance that we have — gives us a strong incentive to err on the side of anthropodenial instead of anthropomorphism." — Jeff Sebo</p><p>In today’s episode, host Luisa Rodriguez interviews Jeff Sebo — director of the <a href="https://sites.google.com/nyu.edu/mindethicspolicy">Mind, Ethics, and Policy Program</a> at NYU — about preparing for a world with digital minds.</p><p><a href="https://80k.info/js"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>The non-negligible chance that AI systems will be sentient by 2030</li><li>What AI systems might want and need, and how that might affect our moral concepts</li><li>What happens when beings can copy themselves? Are they one person or multiple people? Does the original own the copy or does the copy have its own rights? Do copies get the right to vote?</li><li>What kind of legal and political status should AI systems have? Legal personhood? Political citizenship?</li><li>What happens when minds can be connected? If two minds are connected, and one does something illegal, is it possible to punish one but not the other?</li><li>The repugnant conclusion and the rebugnant conclusion</li><li>The experience of trying to build the field of AI welfare</li><li>What improv comedy can teach us about doing good in the world</li><li>And plenty more.</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:00)</li><li>The interview begins (00:02:45)</li><li>We should extend moral consideration to some AI systems by 2030 (00:06:41)</li><li>A one-in-1,000 threshold (00:15:23)</li><li>What does moral consideration mean? (00:24:36)</li><li>Hitting the threshold by 2030 (00:27:38)</li><li>Is the threshold too permissive? (00:38:24)</li><li>The Rebugnant Conclusion (00:41:00)</li><li>A world where AI experiences could matter more than human experiences (00:52:33)</li><li>Should we just accept this argument? (00:55:13)</li><li>Searching for positive-sum solutions (01:05:41)</li><li>Are we going to sleepwalk into causing massive amounts of harm to AI systems? (01:13:48)</li><li>Discourse and messaging (01:27:17)</li><li>What will AI systems want and need? (01:31:17)</li><li>Copies of digital minds (01:33:20)</li><li>Connected minds (01:40:26)</li><li>Psychological connectedness and continuity (01:49:58)</li><li>Assigning responsibility to connected minds (01:58:41)</li><li>Counting the wellbeing of connected minds (02:02:36)</li><li>Legal personhood and political citizenship (02:09:49)</li><li>Building the field of AI welfare (02:24:03)</li><li>What we can learn from improv comedy (02:29:29)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/243d3b95/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/243d3b95/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#172 – Bryan Caplan on why you should stop reading the news</title>
      <itunes:title>#172 – Bryan Caplan on why you should stop reading the news</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">88514143-6012-472a-9b03-ce7de0c93170</guid>
      <link>https://80000hours.org/podcast/episodes/bryan-caplan-stop-reading-the-news/?utm_campaign=podcast__bryan-caplan&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Is following important political and international news a civic duty — or is it our civic duty to avoid it?</p><p>It's common to think that 'staying informed' and checking the headlines every day is just what responsible adults do. </p><p>But in today's episode, host Rob Wiblin is joined by economist Bryan Caplan to discuss the book <a href="https://www.amazon.com/Stop-Reading-News-Manifesto-Happier/dp/1529342724/"><em>Stop Reading the News: A Manifesto for a Happier, Calmer and Wiser Life</em></a> — which argues that reading the news both makes us miserable and distorts our understanding of the world. Far from informing us and enabling us to improve the world, consuming the news distracts us, confuses us, and leaves us feeling powerless.</p><p><a href="https://80k.info/bc23"><strong>Links to learn more, summary, and full transcript.</strong></a><strong></strong></p><p>In the first half of the episode, Bryan and Rob discuss various alleged problems with the news, including:</p><ul><li>That it overwhelmingly provides us with information we can't usefully act on.</li><li>That it's very non-representative in what it covers, in particular favouring the negative over the positive and the new over the significant.</li><li>That it obscures the big picture, falling into the trap of thinking 'something important happens every day.'</li><li>That it's highly addictive, for many people chewing up 10% or more of their waking hours.</li><li>That regularly checking the news leaves us in a state of constant distraction and less able to engage in deep thought.</li><li>And plenty more.</li></ul><p>Bryan and Rob conclude that if you want to understand the world, you're better off blocking news websites and spending your time on Wikipedia, Our World in Data, or reading a textbook. And if you want to generate political change, stop reading about problems you already know exist and instead write your political representative a physical letter — or better yet, go meet them in person.</p><p>In the second half of the episode, Bryan and Rob cover: </p><ul><li>Why Bryan is pretty sceptical that AI is going to lead to extreme, rapid changes, or that there's a meaningful chance of it going terribly.</li><li>Bryan’s case that rational irrationality on the part of voters leads to many very harmful policy decisions.</li><li>How to allocate resources in space.</li><li>Bryan's experience homeschooling his kids.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Is following important political and international news a civic duty — or is it our civic duty to avoid it?</p><p>It's common to think that 'staying informed' and checking the headlines every day is just what responsible adults do. </p><p>But in today's episode, host Rob Wiblin is joined by economist Bryan Caplan to discuss the book <a href="https://www.amazon.com/Stop-Reading-News-Manifesto-Happier/dp/1529342724/"><em>Stop Reading the News: A Manifesto for a Happier, Calmer and Wiser Life</em></a> — which argues that reading the news both makes us miserable and distorts our understanding of the world. Far from informing us and enabling us to improve the world, consuming the news distracts us, confuses us, and leaves us feeling powerless.</p><p><a href="https://80k.info/bc23"><strong>Links to learn more, summary, and full transcript.</strong></a><strong></strong></p><p>In the first half of the episode, Bryan and Rob discuss various alleged problems with the news, including:</p><ul><li>That it overwhelmingly provides us with information we can't usefully act on.</li><li>That it's very non-representative in what it covers, in particular favouring the negative over the positive and the new over the significant.</li><li>That it obscures the big picture, falling into the trap of thinking 'something important happens every day.'</li><li>That it's highly addictive, for many people chewing up 10% or more of their waking hours.</li><li>That regularly checking the news leaves us in a state of constant distraction and less able to engage in deep thought.</li><li>And plenty more.</li></ul><p>Bryan and Rob conclude that if you want to understand the world, you're better off blocking news websites and spending your time on Wikipedia, Our World in Data, or reading a textbook. And if you want to generate political change, stop reading about problems you already know exist and instead write your political representative a physical letter — or better yet, go meet them in person.</p><p>In the second half of the episode, Bryan and Rob cover: </p><ul><li>Why Bryan is pretty sceptical that AI is going to lead to extreme, rapid changes, or that there's a meaningful chance of it going terribly.</li><li>Bryan’s case that rational irrationality on the part of voters leads to many very harmful policy decisions.</li><li>How to allocate resources in space.</li><li>Bryan's experience homeschooling his kids.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 17 Nov 2023 21:13:21 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ba9c0bad/8afb0b0e.mp3" length="137668397" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Z1VNUZhiGIjsZzZCHZ7jhsTzkkvFayqAbn1uRYktNq4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE2MDI5MTcv/MTcwMDI1NDMxMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8602</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Is following important political and international news a civic duty — or is it our civic duty to avoid it?</p><p>It's common to think that 'staying informed' and checking the headlines every day is just what responsible adults do. </p><p>But in today's episode, host Rob Wiblin is joined by economist Bryan Caplan to discuss the book <a href="https://www.amazon.com/Stop-Reading-News-Manifesto-Happier/dp/1529342724/"><em>Stop Reading the News: A Manifesto for a Happier, Calmer and Wiser Life</em></a> — which argues that reading the news both makes us miserable and distorts our understanding of the world. Far from informing us and enabling us to improve the world, consuming the news distracts us, confuses us, and leaves us feeling powerless.</p><p><a href="https://80k.info/bc23"><strong>Links to learn more, summary, and full transcript.</strong></a><strong></strong></p><p>In the first half of the episode, Bryan and Rob discuss various alleged problems with the news, including:</p><ul><li>That it overwhelmingly provides us with information we can't usefully act on.</li><li>That it's very non-representative in what it covers, in particular favouring the negative over the positive and the new over the significant.</li><li>That it obscures the big picture, falling into the trap of thinking 'something important happens every day.'</li><li>That it's highly addictive, for many people chewing up 10% or more of their waking hours.</li><li>That regularly checking the news leaves us in a state of constant distraction and less able to engage in deep thought.</li><li>And plenty more.</li></ul><p>Bryan and Rob conclude that if you want to understand the world, you're better off blocking news websites and spending your time on Wikipedia, Our World in Data, or reading a textbook. And if you want to generate political change, stop reading about problems you already know exist and instead write your political representative a physical letter — or better yet, go meet them in person.</p><p>In the second half of the episode, Bryan and Rob cover: </p><ul><li>Why Bryan is pretty sceptical that AI is going to lead to extreme, rapid changes, or that there's a meaningful chance of it going terribly.</li><li>Bryan’s case that rational irrationality on the part of voters leads to many very harmful policy decisions.</li><li>How to allocate resources in space.</li><li>Bryan's experience homeschooling his kids.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/ba9c0bad/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#171 – Alison Young on how top labs have jeopardised public health with repeated biosafety failures</title>
      <itunes:title>#171 – Alison Young on how top labs have jeopardised public health with repeated biosafety failures</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e7cbb41b-05c0-4580-9527-4ef4391218bd</guid>
      <link>https://80000hours.org/podcast/episodes/alison-young-biosafety-lab-leaks/?utm_campaign=podcast__alison-young&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Rare events can still cause catastrophic accidents. The concern that has been raised by experts going back over time, is that really, the more of these experiments, the more labs, the more opportunities there are for a rare event to occur — that the right pathogen is involved and infects somebody in one of these labs, or is released in some way from these labs. And what I chronicle in <em>Pandora's Gamble</em> is that there have been these previous outbreaks that have been associated with various kinds of lab accidents. So this is not a theoretical thing that can happen: it <em>has</em> happened in the past." — Alison Young</p><p>In today’s episode, host Luisa Rodriguez interviews award-winning investigative journalist Alison Young on the surprising frequency of lab leaks and what needs to be done to prevent them in the future.</p><p><a href="https://80k.info/ay"><strong>Links to learn more, summary, and full transcript</strong></a><strong>.</strong></p><p>They cover:</p><ul><li>The most egregious biosafety mistakes made by the CDC, and how Alison uncovered them through her investigative reporting</li><li>The Dugway life science test facility case, where live anthrax was accidentally sent to labs across the US and several other countries over a period of many years</li><li>The time the Soviets had a major anthrax leak, and then hid it for over a decade</li><li>The 1977 influenza pandemic caused by vaccine trial gone wrong in China</li><li>The last death from smallpox, caused not by the virus spreading in the wild, but by a lab leak in the UK </li><li>Ways we could get more reliable oversight and accountability for these labs</li><li>And the investigative work Alison’s most proud of</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:13)</li><li>Investigating leaks at the CDC (00:05:16)</li><li>High-profile CDC accidents (00:16:13)</li><li>Dugway live anthrax accidents (00:32:08)</li><li>Soviet anthrax leak (00:44:41)</li><li>The 1977 influenza pandemic (00:53:43)</li><li>The last death from smallpox (00:59:27)</li><li>How common are lab leaks? (01:09:05)</li><li>Improving the regulation of dangerous biological research (01:18:36)</li><li>Potential solutions (01:34:55)</li><li>The investigative work Alison’s most proud of (01:40:33)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Rare events can still cause catastrophic accidents. The concern that has been raised by experts going back over time, is that really, the more of these experiments, the more labs, the more opportunities there are for a rare event to occur — that the right pathogen is involved and infects somebody in one of these labs, or is released in some way from these labs. And what I chronicle in <em>Pandora's Gamble</em> is that there have been these previous outbreaks that have been associated with various kinds of lab accidents. So this is not a theoretical thing that can happen: it <em>has</em> happened in the past." — Alison Young</p><p>In today’s episode, host Luisa Rodriguez interviews award-winning investigative journalist Alison Young on the surprising frequency of lab leaks and what needs to be done to prevent them in the future.</p><p><a href="https://80k.info/ay"><strong>Links to learn more, summary, and full transcript</strong></a><strong>.</strong></p><p>They cover:</p><ul><li>The most egregious biosafety mistakes made by the CDC, and how Alison uncovered them through her investigative reporting</li><li>The Dugway life science test facility case, where live anthrax was accidentally sent to labs across the US and several other countries over a period of many years</li><li>The time the Soviets had a major anthrax leak, and then hid it for over a decade</li><li>The 1977 influenza pandemic caused by vaccine trial gone wrong in China</li><li>The last death from smallpox, caused not by the virus spreading in the wild, but by a lab leak in the UK </li><li>Ways we could get more reliable oversight and accountability for these labs</li><li>And the investigative work Alison’s most proud of</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:13)</li><li>Investigating leaks at the CDC (00:05:16)</li><li>High-profile CDC accidents (00:16:13)</li><li>Dugway live anthrax accidents (00:32:08)</li><li>Soviet anthrax leak (00:44:41)</li><li>The 1977 influenza pandemic (00:53:43)</li><li>The last death from smallpox (00:59:27)</li><li>How common are lab leaks? (01:09:05)</li><li>Improving the regulation of dangerous biological research (01:18:36)</li><li>Potential solutions (01:34:55)</li><li>The investigative work Alison’s most proud of (01:40:33)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 09 Nov 2023 21:39:54 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/269bdb45/fbefe09d.mp3" length="76529387" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/OmqsRomKJt2bYqpQ1mpXgweQ7QUzyNLnpXNfr-cDsCI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1OTA2MTUv/MTY5OTQ3NzE3Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6374</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Rare events can still cause catastrophic accidents. The concern that has been raised by experts going back over time, is that really, the more of these experiments, the more labs, the more opportunities there are for a rare event to occur — that the right pathogen is involved and infects somebody in one of these labs, or is released in some way from these labs. And what I chronicle in <em>Pandora's Gamble</em> is that there have been these previous outbreaks that have been associated with various kinds of lab accidents. So this is not a theoretical thing that can happen: it <em>has</em> happened in the past." — Alison Young</p><p>In today’s episode, host Luisa Rodriguez interviews award-winning investigative journalist Alison Young on the surprising frequency of lab leaks and what needs to be done to prevent them in the future.</p><p><a href="https://80k.info/ay"><strong>Links to learn more, summary, and full transcript</strong></a><strong>.</strong></p><p>They cover:</p><ul><li>The most egregious biosafety mistakes made by the CDC, and how Alison uncovered them through her investigative reporting</li><li>The Dugway life science test facility case, where live anthrax was accidentally sent to labs across the US and several other countries over a period of many years</li><li>The time the Soviets had a major anthrax leak, and then hid it for over a decade</li><li>The 1977 influenza pandemic caused by vaccine trial gone wrong in China</li><li>The last death from smallpox, caused not by the virus spreading in the wild, but by a lab leak in the UK </li><li>Ways we could get more reliable oversight and accountability for these labs</li><li>And the investigative work Alison’s most proud of</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa's intro (00:01:13)</li><li>Investigating leaks at the CDC (00:05:16)</li><li>High-profile CDC accidents (00:16:13)</li><li>Dugway live anthrax accidents (00:32:08)</li><li>Soviet anthrax leak (00:44:41)</li><li>The 1977 influenza pandemic (00:53:43)</li><li>The last death from smallpox (00:59:27)</li><li>How common are lab leaks? (01:09:05)</li><li>Improving the regulation of dangerous biological research (01:18:36)</li><li>Potential solutions (01:34:55)</li><li>The investigative work Alison’s most proud of (01:40:33)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/269bdb45/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/269bdb45/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#170 – Santosh Harish on how air pollution is responsible for ~12% of global deaths — and how to get that number down</title>
      <itunes:title>#170 – Santosh Harish on how air pollution is responsible for ~12% of global deaths — and how to get that number down</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">294aa34a-c745-4514-a7be-66762d155c56</guid>
      <link>https://80000hours.org/podcast/episodes/santosh-harish-air-pollution/?utm_campaign=podcast__santosh-harish&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"One [outrageous example of air pollution] is municipal waste burning that happens in many cities in the Global South. Basically, this is waste that gets collected from people's homes, and instead of being transported to a waste management facility or a landfill or something, gets burned at some point, because that's the fastest way to dispose of it — which really points to poor delivery of public services. But this is ubiquitous in virtually every small- or even medium-sized city. It happens in larger cities too, in this part of the world. </p><p>"That's something that truly annoys me, because it feels like the kind of thing that ought to be fairly easily managed, but it happens a lot. It happens because people presumably don't think that it's particularly harmful. I don't think it saves a tonne of money for the municipal corporations and other local government that are meant to manage it. I find it particularly annoying simply because it happens so often; it's something that you're able to smell in so many different parts of these cities." — Santosh Harish</p><p>In today’s episode, host Rob Wiblin interviews Santosh Harish — leader of <a href="https://www.openphilanthropy.org/research/south-asian-air-quality/">Open Philanthropy’s grantmaking in South Asian air quality</a> — about the scale of the harm caused by air pollution.</p><p><a href="https://80k.info/sh"><strong>Links to learn more, summary, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>How bad air pollution is for our health and life expectancy</li><li>The different kinds of harm that particulate pollution causes</li><li>The strength of the evidence that it damages our brain function and reduces our productivity</li><li>Whether it was a mistake to switch our attention to climate change and away from air pollution</li><li>Whether most listeners to this show should have an air purifier running in their house right now</li><li>Where air pollution in India is worst and why, and whether it's going up or down</li><li>Where most air pollution comes from</li><li>The policy blunders that led to many sources of air pollution in India being effectively unregulated</li><li>Why indoor air pollution packs an enormous punch</li><li>The politics of air pollution in India</li><li>How India ended up spending a lot of money on outdoor air purifiers</li><li>The challenges faced by foreign philanthropists in India</li><li>Why Santosh has made the grants he has so far</li><li>And plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:07)</li><li>How bad is air pollution? (00:03:41)</li><li>Quantifying the scale of the damage (00:15:47)</li><li>Effects on cognitive performance and mood (00:24:19)</li><li>How do we really know the harms are as big as is claimed? (00:27:05)</li><li>Misconceptions about air pollution (00:36:56)</li><li>Why don’t environmental advocacy groups focus on air pollution? (00:42:22)</li><li>How listeners should approach air pollution in their own lives (00:46:58)</li><li>How bad is air pollution in India in particular (00:54:23)</li><li>The trend in India over the last few decades (01:12:33)</li><li>Why aren’t people able to fix these problems? (01:24:17)</li><li>Household waste burning (01:35:06)</li><li>Vehicle emissions (01:42:10)</li><li>The role that courts have played in air pollution regulation in India (01:50:09)</li><li>Industrial emissions (01:57:10)</li><li>The political economy of air pollution in northern India (02:02:14)</li><li>Can philanthropists drive policy change? (02:13:42)</li><li>Santosh’s grants (02:29:45)</li><li>Examples of other countries that have managed to greatly reduce air pollution (02:45:44)</li><li>Career advice for listeners in India (02:51:11)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"One [outrageous example of air pollution] is municipal waste burning that happens in many cities in the Global South. Basically, this is waste that gets collected from people's homes, and instead of being transported to a waste management facility or a landfill or something, gets burned at some point, because that's the fastest way to dispose of it — which really points to poor delivery of public services. But this is ubiquitous in virtually every small- or even medium-sized city. It happens in larger cities too, in this part of the world. </p><p>"That's something that truly annoys me, because it feels like the kind of thing that ought to be fairly easily managed, but it happens a lot. It happens because people presumably don't think that it's particularly harmful. I don't think it saves a tonne of money for the municipal corporations and other local government that are meant to manage it. I find it particularly annoying simply because it happens so often; it's something that you're able to smell in so many different parts of these cities." — Santosh Harish</p><p>In today’s episode, host Rob Wiblin interviews Santosh Harish — leader of <a href="https://www.openphilanthropy.org/research/south-asian-air-quality/">Open Philanthropy’s grantmaking in South Asian air quality</a> — about the scale of the harm caused by air pollution.</p><p><a href="https://80k.info/sh"><strong>Links to learn more, summary, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>How bad air pollution is for our health and life expectancy</li><li>The different kinds of harm that particulate pollution causes</li><li>The strength of the evidence that it damages our brain function and reduces our productivity</li><li>Whether it was a mistake to switch our attention to climate change and away from air pollution</li><li>Whether most listeners to this show should have an air purifier running in their house right now</li><li>Where air pollution in India is worst and why, and whether it's going up or down</li><li>Where most air pollution comes from</li><li>The policy blunders that led to many sources of air pollution in India being effectively unregulated</li><li>Why indoor air pollution packs an enormous punch</li><li>The politics of air pollution in India</li><li>How India ended up spending a lot of money on outdoor air purifiers</li><li>The challenges faced by foreign philanthropists in India</li><li>Why Santosh has made the grants he has so far</li><li>And plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:07)</li><li>How bad is air pollution? (00:03:41)</li><li>Quantifying the scale of the damage (00:15:47)</li><li>Effects on cognitive performance and mood (00:24:19)</li><li>How do we really know the harms are as big as is claimed? (00:27:05)</li><li>Misconceptions about air pollution (00:36:56)</li><li>Why don’t environmental advocacy groups focus on air pollution? (00:42:22)</li><li>How listeners should approach air pollution in their own lives (00:46:58)</li><li>How bad is air pollution in India in particular (00:54:23)</li><li>The trend in India over the last few decades (01:12:33)</li><li>Why aren’t people able to fix these problems? (01:24:17)</li><li>Household waste burning (01:35:06)</li><li>Vehicle emissions (01:42:10)</li><li>The role that courts have played in air pollution regulation in India (01:50:09)</li><li>Industrial emissions (01:57:10)</li><li>The political economy of air pollution in northern India (02:02:14)</li><li>Can philanthropists drive policy change? (02:13:42)</li><li>Santosh’s grants (02:29:45)</li><li>Examples of other countries that have managed to greatly reduce air pollution (02:45:44)</li><li>Career advice for listeners in India (02:51:11)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 01 Nov 2023 22:15:25 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/abe15537/a919c9a1.mp3" length="128061485" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-et0920e1nJWWAZoKNvES30lWf5FuYRyngFnby8-sPk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1NzY1MzYv/MTY5ODg1NzgyOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10666</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"One [outrageous example of air pollution] is municipal waste burning that happens in many cities in the Global South. Basically, this is waste that gets collected from people's homes, and instead of being transported to a waste management facility or a landfill or something, gets burned at some point, because that's the fastest way to dispose of it — which really points to poor delivery of public services. But this is ubiquitous in virtually every small- or even medium-sized city. It happens in larger cities too, in this part of the world. </p><p>"That's something that truly annoys me, because it feels like the kind of thing that ought to be fairly easily managed, but it happens a lot. It happens because people presumably don't think that it's particularly harmful. I don't think it saves a tonne of money for the municipal corporations and other local government that are meant to manage it. I find it particularly annoying simply because it happens so often; it's something that you're able to smell in so many different parts of these cities." — Santosh Harish</p><p>In today’s episode, host Rob Wiblin interviews Santosh Harish — leader of <a href="https://www.openphilanthropy.org/research/south-asian-air-quality/">Open Philanthropy’s grantmaking in South Asian air quality</a> — about the scale of the harm caused by air pollution.</p><p><a href="https://80k.info/sh"><strong>Links to learn more, summary, and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>How bad air pollution is for our health and life expectancy</li><li>The different kinds of harm that particulate pollution causes</li><li>The strength of the evidence that it damages our brain function and reduces our productivity</li><li>Whether it was a mistake to switch our attention to climate change and away from air pollution</li><li>Whether most listeners to this show should have an air purifier running in their house right now</li><li>Where air pollution in India is worst and why, and whether it's going up or down</li><li>Where most air pollution comes from</li><li>The policy blunders that led to many sources of air pollution in India being effectively unregulated</li><li>Why indoor air pollution packs an enormous punch</li><li>The politics of air pollution in India</li><li>How India ended up spending a lot of money on outdoor air purifiers</li><li>The challenges faced by foreign philanthropists in India</li><li>Why Santosh has made the grants he has so far</li><li>And plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:07)</li><li>How bad is air pollution? (00:03:41)</li><li>Quantifying the scale of the damage (00:15:47)</li><li>Effects on cognitive performance and mood (00:24:19)</li><li>How do we really know the harms are as big as is claimed? (00:27:05)</li><li>Misconceptions about air pollution (00:36:56)</li><li>Why don’t environmental advocacy groups focus on air pollution? (00:42:22)</li><li>How listeners should approach air pollution in their own lives (00:46:58)</li><li>How bad is air pollution in India in particular (00:54:23)</li><li>The trend in India over the last few decades (01:12:33)</li><li>Why aren’t people able to fix these problems? (01:24:17)</li><li>Household waste burning (01:35:06)</li><li>Vehicle emissions (01:42:10)</li><li>The role that courts have played in air pollution regulation in India (01:50:09)</li><li>Industrial emissions (01:57:10)</li><li>The political economy of air pollution in northern India (02:02:14)</li><li>Can philanthropists drive policy change? (02:13:42)</li><li>Santosh’s grants (02:29:45)</li><li>Examples of other countries that have managed to greatly reduce air pollution (02:45:44)</li><li>Career advice for listeners in India (02:51:11)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/abe15537/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/abe15537/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#169 – Paul Niehaus on whether cash transfers cause economic growth, and keeping theft to acceptable levels</title>
      <itunes:title>#169 – Paul Niehaus on whether cash transfers cause economic growth, and keeping theft to acceptable levels</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">cd487c93-a5cb-4b06-b3c0-14d420d96d71</guid>
      <link>https://80000hours.org/podcast/episodes/paul-niehaus-cash-transfers/?utm_campaign=podcast__paul-niehaus&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"One of our earliest supporters and a dear friend of mine, Mark Lampert, once said to me, “The way I think about it is, imagine that this money were already in the hands of people living in poverty. If I could, would I want to tax it and then use it to finance other projects that I think would benefit them?” </p><p>I think that's an interesting thought experiment -- and a good one -- to say, “Are there cases in which I think that's justifiable?” — Paul Niehaus</p><p>In today’s episode, host Luisa Rodriguez interviews Paul Niehaus — co-founder of <a href="https://www.givedirectly.org/">GiveDirectly</a> — on the case for giving unconditional cash to the world's poorest households.</p><p><a href="https://80k.info/pn"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>The empirical evidence on whether giving cash directly can drive meaningful economic growth</li><li>How the impacts of GiveDirectly compare to USAID employment programmes</li><li>GiveDirectly vs GiveWell’s top-recommended charities</li><li>How long-term guaranteed income affects people's risk-taking and investments</li><li>Whether recipients prefer getting lump sums or monthly instalments</li><li>How GiveDirectly tackles cases of fraud and theft</li><li>The case for universal basic income, and GiveDirectly’s UBI studies in Kenya, Malawi, and Liberia</li><li>The political viability of UBI</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:58)</li><li>The basic case for giving cash directly to the poor (00:03:28)</li><li>Comparing GiveDirectly to USAID programmes (00:15:42)</li><li>GiveDirectly vs GiveWell’s top-recommended charities (00:35:16)</li><li>Cash might be able to drive economic growth (00:41:59)</li><li>Fraud and theft of GiveDirectly funds (01:09:48)</li><li>Universal basic income studies (01:22:33)</li><li>Skyjo (01:44:43)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Luisa Rodriguez and Katy Moore<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"One of our earliest supporters and a dear friend of mine, Mark Lampert, once said to me, “The way I think about it is, imagine that this money were already in the hands of people living in poverty. If I could, would I want to tax it and then use it to finance other projects that I think would benefit them?” </p><p>I think that's an interesting thought experiment -- and a good one -- to say, “Are there cases in which I think that's justifiable?” — Paul Niehaus</p><p>In today’s episode, host Luisa Rodriguez interviews Paul Niehaus — co-founder of <a href="https://www.givedirectly.org/">GiveDirectly</a> — on the case for giving unconditional cash to the world's poorest households.</p><p><a href="https://80k.info/pn"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>The empirical evidence on whether giving cash directly can drive meaningful economic growth</li><li>How the impacts of GiveDirectly compare to USAID employment programmes</li><li>GiveDirectly vs GiveWell’s top-recommended charities</li><li>How long-term guaranteed income affects people's risk-taking and investments</li><li>Whether recipients prefer getting lump sums or monthly instalments</li><li>How GiveDirectly tackles cases of fraud and theft</li><li>The case for universal basic income, and GiveDirectly’s UBI studies in Kenya, Malawi, and Liberia</li><li>The political viability of UBI</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:58)</li><li>The basic case for giving cash directly to the poor (00:03:28)</li><li>Comparing GiveDirectly to USAID programmes (00:15:42)</li><li>GiveDirectly vs GiveWell’s top-recommended charities (00:35:16)</li><li>Cash might be able to drive economic growth (00:41:59)</li><li>Fraud and theft of GiveDirectly funds (01:09:48)</li><li>Universal basic income studies (01:22:33)</li><li>Skyjo (01:44:43)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Luisa Rodriguez and Katy Moore<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 26 Oct 2023 20:42:51 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cea6466a/ac7813a8.mp3" length="77755489" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2JfDCn466l15pNynKsQ2gQ2CODN3IPpkt47tOiA8qjc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1Njg4ODkv/MTY5ODM1MDQyMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6476</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"One of our earliest supporters and a dear friend of mine, Mark Lampert, once said to me, “The way I think about it is, imagine that this money were already in the hands of people living in poverty. If I could, would I want to tax it and then use it to finance other projects that I think would benefit them?” </p><p>I think that's an interesting thought experiment -- and a good one -- to say, “Are there cases in which I think that's justifiable?” — Paul Niehaus</p><p>In today’s episode, host Luisa Rodriguez interviews Paul Niehaus — co-founder of <a href="https://www.givedirectly.org/">GiveDirectly</a> — on the case for giving unconditional cash to the world's poorest households.</p><p><a href="https://80k.info/pn"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>The empirical evidence on whether giving cash directly can drive meaningful economic growth</li><li>How the impacts of GiveDirectly compare to USAID employment programmes</li><li>GiveDirectly vs GiveWell’s top-recommended charities</li><li>How long-term guaranteed income affects people's risk-taking and investments</li><li>Whether recipients prefer getting lump sums or monthly instalments</li><li>How GiveDirectly tackles cases of fraud and theft</li><li>The case for universal basic income, and GiveDirectly’s UBI studies in Kenya, Malawi, and Liberia</li><li>The political viability of UBI</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:00:58)</li><li>The basic case for giving cash directly to the poor (00:03:28)</li><li>Comparing GiveDirectly to USAID programmes (00:15:42)</li><li>GiveDirectly vs GiveWell’s top-recommended charities (00:35:16)</li><li>Cash might be able to drive economic growth (00:41:59)</li><li>Fraud and theft of GiveDirectly funds (01:09:48)</li><li>Universal basic income studies (01:22:33)</li><li>Skyjo (01:44:43)</li></ul><p><br><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Luisa Rodriguez and Katy Moore<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/cea6466a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/cea6466a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#168 – Ian Morris on whether deep history says we're heading for an intelligence explosion</title>
      <itunes:title>#168 – Ian Morris on whether deep history says we're heading for an intelligence explosion</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1a5f4fa4-4702-4382-b0c6-e7371579c28d</guid>
      <link>https://80000hours.org/podcast/episodes/ian-morris-deep-history-intelligence-explosion/?utm_campaign=podcast__ian-morris&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"If we carry on looking at these industrialised economies, not thinking about what it is they're actually doing and what the potential of this is, you can make an argument that, yes, rates of growth are slowing, the rate of innovation is slowing. But it isn't. </p><p>What we're doing is creating wildly new technologies: basically producing what is nothing less than an evolutionary change in what it means to be a human being. But this has not yet spilled over into the kind of growth that we have accustomed ourselves to in the fossil-fuel industrial era. That is about to hit us in a big way." — Ian Morris</p><p>In today’s episode, host Rob Wiblin speaks with repeat guest Ian Morris about what big-picture history says about the likely impact of machine intelligence. </p><p><a href="https://80k.info/im23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>Some crazy anomalies in the historical record of civilisational progress</li><li>Whether we should think about technology from an evolutionary perspective</li><li>Whether we ought to expect war to make a resurgence or continue dying out</li><li>Why we can't end up living like <em>The Jetsons</em></li><li>Whether stagnation or cyclical recurring futures seem very plausible</li><li>What it means that the rate of increase in the economy has been increasing</li><li>Whether violence is likely between humans and powerful AI systems</li><li>The most likely reasons for Rob and Ian to be really wrong about all of this</li><li>How professional historians react to this sort of talk</li><li>The future of Ian’s work</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:27)</li><li>Why we should expect the future to be wild (00:04:08)</li><li>How historians have reacted to the idea of radically different futures (00:21:20)</li><li>Why we won’t end up in The Jetsons (00:26:20)</li><li>The rise of machine intelligence (00:31:28)</li><li>AI from an evolutionary point of view (00:46:32)</li><li>Is violence likely between humans and powerful AI systems? (00:59:53)</li><li>Most troubling objections to this approach in Ian’s view (01:28:20)</li><li>Confronting anomalies in the historical record (01:33:10)</li><li>The cyclical view of history (01:56:11)</li><li>Is stagnation plausible? (02:01:38)</li><li>The limit on how long this growth trend can continue (02:20:57)</li><li>The future of Ian’s work (02:37:17)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"If we carry on looking at these industrialised economies, not thinking about what it is they're actually doing and what the potential of this is, you can make an argument that, yes, rates of growth are slowing, the rate of innovation is slowing. But it isn't. </p><p>What we're doing is creating wildly new technologies: basically producing what is nothing less than an evolutionary change in what it means to be a human being. But this has not yet spilled over into the kind of growth that we have accustomed ourselves to in the fossil-fuel industrial era. That is about to hit us in a big way." — Ian Morris</p><p>In today’s episode, host Rob Wiblin speaks with repeat guest Ian Morris about what big-picture history says about the likely impact of machine intelligence. </p><p><a href="https://80k.info/im23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>Some crazy anomalies in the historical record of civilisational progress</li><li>Whether we should think about technology from an evolutionary perspective</li><li>Whether we ought to expect war to make a resurgence or continue dying out</li><li>Why we can't end up living like <em>The Jetsons</em></li><li>Whether stagnation or cyclical recurring futures seem very plausible</li><li>What it means that the rate of increase in the economy has been increasing</li><li>Whether violence is likely between humans and powerful AI systems</li><li>The most likely reasons for Rob and Ian to be really wrong about all of this</li><li>How professional historians react to this sort of talk</li><li>The future of Ian’s work</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:27)</li><li>Why we should expect the future to be wild (00:04:08)</li><li>How historians have reacted to the idea of radically different futures (00:21:20)</li><li>Why we won’t end up in The Jetsons (00:26:20)</li><li>The rise of machine intelligence (00:31:28)</li><li>AI from an evolutionary point of view (00:46:32)</li><li>Is violence likely between humans and powerful AI systems? (00:59:53)</li><li>Most troubling objections to this approach in Ian’s view (01:28:20)</li><li>Confronting anomalies in the historical record (01:33:10)</li><li>The cyclical view of history (01:56:11)</li><li>Is stagnation plausible? (02:01:38)</li><li>The limit on how long this growth trend can continue (02:20:57)</li><li>The future of Ian’s work (02:37:17)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 23 Oct 2023 23:42:43 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b681879d/27ccbc59.mp3" length="118073418" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/3zWKYtDlZ-X4qQ32Ar4dWL2J5ogM887eQIxiRGXpoI0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1NjA2ODAv/MTY5ODA5MzQ0NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9835</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"If we carry on looking at these industrialised economies, not thinking about what it is they're actually doing and what the potential of this is, you can make an argument that, yes, rates of growth are slowing, the rate of innovation is slowing. But it isn't. </p><p>What we're doing is creating wildly new technologies: basically producing what is nothing less than an evolutionary change in what it means to be a human being. But this has not yet spilled over into the kind of growth that we have accustomed ourselves to in the fossil-fuel industrial era. That is about to hit us in a big way." — Ian Morris</p><p>In today’s episode, host Rob Wiblin speaks with repeat guest Ian Morris about what big-picture history says about the likely impact of machine intelligence. </p><p><a href="https://80k.info/im23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>Some crazy anomalies in the historical record of civilisational progress</li><li>Whether we should think about technology from an evolutionary perspective</li><li>Whether we ought to expect war to make a resurgence or continue dying out</li><li>Why we can't end up living like <em>The Jetsons</em></li><li>Whether stagnation or cyclical recurring futures seem very plausible</li><li>What it means that the rate of increase in the economy has been increasing</li><li>Whether violence is likely between humans and powerful AI systems</li><li>The most likely reasons for Rob and Ian to be really wrong about all of this</li><li>How professional historians react to this sort of talk</li><li>The future of Ian’s work</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob’s intro (00:01:27)</li><li>Why we should expect the future to be wild (00:04:08)</li><li>How historians have reacted to the idea of radically different futures (00:21:20)</li><li>Why we won’t end up in The Jetsons (00:26:20)</li><li>The rise of machine intelligence (00:31:28)</li><li>AI from an evolutionary point of view (00:46:32)</li><li>Is violence likely between humans and powerful AI systems? (00:59:53)</li><li>Most troubling objections to this approach in Ian’s view (01:28:20)</li><li>Confronting anomalies in the historical record (01:33:10)</li><li>The cyclical view of history (01:56:11)</li><li>Is stagnation plausible? (02:01:38)</li><li>The limit on how long this growth trend can continue (02:20:57)</li><li>The future of Ian’s work (02:37:17)</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/b681879d/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/b681879d/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#167 – Seren Kell on the research gaps holding back alternative proteins from mass adoption</title>
      <itunes:title>#167 – Seren Kell on the research gaps holding back alternative proteins from mass adoption</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2e043d21-753d-4636-b88e-9ded1dd0a182</guid>
      <link>https://80000hours.org/podcast/episodes/seren-kell-alternative-proteins/?utm_campaign=podcast__seren-kell&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"There have been literally thousands of years of breeding and living with animals to optimise these kinds of problems. But because we're just so early on with alternative proteins and there's so much white space, it's actually just really exciting to know that we can keep on innovating and being far more efficient than this existing technology — which, fundamentally, is just quite inefficient. You're feeding animals a bunch of food to then extract a small fraction of their biomass to then eat that.</p><p>Animal agriculture takes up 83% of farmland, but produces just 18% of food calories. So the current system just is so wasteful. And the limiting factor is that you're just growing a bunch of food to then feed a third of the world's crops directly to animals, where the vast majority of those calories going in are lost to animals existing." — Seren Kell</p><p><a href="https://80k.info/sk"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>In today’s episode, host Luisa Rodriguez interviews Seren Kell — Senior Science and Technology Manager at the Good Food Institute Europe — about making alternative proteins as tasty, cheap, and convenient as traditional meat, dairy, and egg products.</p><p>They cover:</p><ul><li>The basic case for alternative proteins, and why they’re so hard to make</li><li>Why fermentation is a surprisingly promising technology for creating delicious alternative proteins </li><li>The main scientific challenges that need to be solved to make fermentation even more useful</li><li>The progress that’s been made on the cultivated meat front, and what it will take to make cultivated meat affordable</li><li>How GFI Europe is helping with some of these challenges</li><li>How people can use their careers to contribute to replacing factory farming with alternative proteins</li><li>The best part of Seren’s job</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:08)</li><li>The interview begins (00:02:22)</li><li>Why alternative proteins? (00:02:36)</li><li>What makes alternative proteins so hard to make? (00:11:30)</li><li>Why fermentation is so exciting (00:24:23)</li><li>The technical challenges involved in scaling fermentation (00:44:38)</li><li>Progress in cultivated meat (01:06:04)</li><li>GFI Europe’s work (01:32:47)</li><li>Careers (01:45:10)</li><li>The best part of Seren’s job (01:50:07)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Luisa Rodriguez and Katy Moore<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"There have been literally thousands of years of breeding and living with animals to optimise these kinds of problems. But because we're just so early on with alternative proteins and there's so much white space, it's actually just really exciting to know that we can keep on innovating and being far more efficient than this existing technology — which, fundamentally, is just quite inefficient. You're feeding animals a bunch of food to then extract a small fraction of their biomass to then eat that.</p><p>Animal agriculture takes up 83% of farmland, but produces just 18% of food calories. So the current system just is so wasteful. And the limiting factor is that you're just growing a bunch of food to then feed a third of the world's crops directly to animals, where the vast majority of those calories going in are lost to animals existing." — Seren Kell</p><p><a href="https://80k.info/sk"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>In today’s episode, host Luisa Rodriguez interviews Seren Kell — Senior Science and Technology Manager at the Good Food Institute Europe — about making alternative proteins as tasty, cheap, and convenient as traditional meat, dairy, and egg products.</p><p>They cover:</p><ul><li>The basic case for alternative proteins, and why they’re so hard to make</li><li>Why fermentation is a surprisingly promising technology for creating delicious alternative proteins </li><li>The main scientific challenges that need to be solved to make fermentation even more useful</li><li>The progress that’s been made on the cultivated meat front, and what it will take to make cultivated meat affordable</li><li>How GFI Europe is helping with some of these challenges</li><li>How people can use their careers to contribute to replacing factory farming with alternative proteins</li><li>The best part of Seren’s job</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:08)</li><li>The interview begins (00:02:22)</li><li>Why alternative proteins? (00:02:36)</li><li>What makes alternative proteins so hard to make? (00:11:30)</li><li>Why fermentation is so exciting (00:24:23)</li><li>The technical challenges involved in scaling fermentation (00:44:38)</li><li>Progress in cultivated meat (01:06:04)</li><li>GFI Europe’s work (01:32:47)</li><li>Careers (01:45:10)</li><li>The best part of Seren’s job (01:50:07)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Luisa Rodriguez and Katy Moore<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 18 Oct 2023 20:32:33 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cbcc6bbe/278552ba.mp3" length="82705857" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/KzSrt78Mo4QtIdk3oFl2TGp3fg_lfJNh7D-E3je7WTM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1NTI0NjQv/MTY5NzY2MDc0My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6889</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"There have been literally thousands of years of breeding and living with animals to optimise these kinds of problems. But because we're just so early on with alternative proteins and there's so much white space, it's actually just really exciting to know that we can keep on innovating and being far more efficient than this existing technology — which, fundamentally, is just quite inefficient. You're feeding animals a bunch of food to then extract a small fraction of their biomass to then eat that.</p><p>Animal agriculture takes up 83% of farmland, but produces just 18% of food calories. So the current system just is so wasteful. And the limiting factor is that you're just growing a bunch of food to then feed a third of the world's crops directly to animals, where the vast majority of those calories going in are lost to animals existing." — Seren Kell</p><p><a href="https://80k.info/sk"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>In today’s episode, host Luisa Rodriguez interviews Seren Kell — Senior Science and Technology Manager at the Good Food Institute Europe — about making alternative proteins as tasty, cheap, and convenient as traditional meat, dairy, and egg products.</p><p>They cover:</p><ul><li>The basic case for alternative proteins, and why they’re so hard to make</li><li>Why fermentation is a surprisingly promising technology for creating delicious alternative proteins </li><li>The main scientific challenges that need to be solved to make fermentation even more useful</li><li>The progress that’s been made on the cultivated meat front, and what it will take to make cultivated meat affordable</li><li>How GFI Europe is helping with some of these challenges</li><li>How people can use their careers to contribute to replacing factory farming with alternative proteins</li><li>The best part of Seren’s job</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Luisa’s intro (00:01:08)</li><li>The interview begins (00:02:22)</li><li>Why alternative proteins? (00:02:36)</li><li>What makes alternative proteins so hard to make? (00:11:30)</li><li>Why fermentation is so exciting (00:24:23)</li><li>The technical challenges involved in scaling fermentation (00:44:38)</li><li>Progress in cultivated meat (01:06:04)</li><li>GFI Europe’s work (01:32:47)</li><li>Careers (01:45:10)</li><li>The best part of Seren’s job (01:50:07)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Dominic Armstrong and Milo McGuire<br>Additional content editing: Luisa Rodriguez and Katy Moore<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/cbcc6bbe/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/cbcc6bbe/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#166 – Tantum Collins on what he’s learned as an AI policy insider at the White House, DeepMind and elsewhere</title>
      <itunes:title>#166 – Tantum Collins on what he’s learned as an AI policy insider at the White House, DeepMind and elsewhere</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1875fc4e-3a55-4586-95db-4c9c5464ea10</guid>
      <link>https://80000hours.org/podcast/episodes/tantum-collins-ai-policy-insider/?utm_campaign=podcast__tantum-collins&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"If you and I and 100 other people were on the first ship that was going to go settle Mars, and were going to build a human civilisation, and we have to decide what that government looks like, and we have all of the technology available today, how do we think about choosing a subset of that design space? </p><p>That space is huge and it includes absolutely awful things, and mixed-bag things, and maybe some things that almost everyone would agree are really wonderful, or at least an improvement on the way that things work today. But that raises all kinds of tricky questions. </p><p>My concern is that if we don't approach the evolution of collective decision making and government in a deliberate way, we may end up inadvertently backing ourselves into a corner, where we have ended up on some slippery slope -- and all of a sudden we have, let's say, autocracies on the global stage are strengthened relative to democracies." — Tantum Collins</p><p>In today’s episode, host Rob Wiblin gets the rare chance to interview someone with insider AI policy experience at the White House and DeepMind who’s willing to speak openly — Tantum Collins.</p><p><a href="https://80k.info/tc"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How AI could strengthen government capacity, and how that's a double-edged sword</li><li>How new technologies force us to confront tradeoffs in political philosophy that we were previously able to pretend weren't there</li><li>To what extent policymakers take different threats from AI seriously</li><li>Whether the US and China are in an AI arms race or not</li><li>Whether it's OK to transform the world without much of the world agreeing to it</li><li>The tyranny of small differences in AI policy</li><li>Disagreements between different schools of thought in AI policy, and proposals that could unite them</li><li>How the US AI Bill of Rights could be improved</li><li>Whether AI will transform the labour market, and whether it will become a partisan political issue</li><li>The tensions between the cultures of San Francisco and DC, and how to bridge the divide between them</li><li>What listeners might be able to do to help with this whole mess</li><li>Panpsychism</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:00)</li><li>The interview begins (00:04:01)</li><li>The risk of autocratic lock-in due to AI (00:10:02)</li><li>The state of play in AI policymaking (00:13:40)</li><li>China and AI (00:32:12)</li><li>The most promising regulatory approaches (00:57:51)</li><li>Transforming the world without the world agreeing (01:04:44)</li><li>AI Bill of Rights (01:17:32)</li><li>Who’s ultimately responsible for the consequences of AI? (01:20:39)</li><li>Policy ideas that could appeal to many different groups (01:29:08)</li><li>Tension between those focused on x-risk and those focused on AI ethics (01:38:56)</li><li>Communicating with policymakers (01:54:22)</li><li>Is AI going to transform the labour market in the next few years? (01:58:51)</li><li>Is AI policy going to become a partisan political issue? (02:08:10)</li><li>The value of political philosophy (02:10:53)</li><li>Tantum’s work at DeepMind (02:21:20)</li><li>CSET (02:32:48)</li><li>Career advice (02:35:21)</li><li>Panpsychism (02:55:24)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"If you and I and 100 other people were on the first ship that was going to go settle Mars, and were going to build a human civilisation, and we have to decide what that government looks like, and we have all of the technology available today, how do we think about choosing a subset of that design space? </p><p>That space is huge and it includes absolutely awful things, and mixed-bag things, and maybe some things that almost everyone would agree are really wonderful, or at least an improvement on the way that things work today. But that raises all kinds of tricky questions. </p><p>My concern is that if we don't approach the evolution of collective decision making and government in a deliberate way, we may end up inadvertently backing ourselves into a corner, where we have ended up on some slippery slope -- and all of a sudden we have, let's say, autocracies on the global stage are strengthened relative to democracies." — Tantum Collins</p><p>In today’s episode, host Rob Wiblin gets the rare chance to interview someone with insider AI policy experience at the White House and DeepMind who’s willing to speak openly — Tantum Collins.</p><p><a href="https://80k.info/tc"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How AI could strengthen government capacity, and how that's a double-edged sword</li><li>How new technologies force us to confront tradeoffs in political philosophy that we were previously able to pretend weren't there</li><li>To what extent policymakers take different threats from AI seriously</li><li>Whether the US and China are in an AI arms race or not</li><li>Whether it's OK to transform the world without much of the world agreeing to it</li><li>The tyranny of small differences in AI policy</li><li>Disagreements between different schools of thought in AI policy, and proposals that could unite them</li><li>How the US AI Bill of Rights could be improved</li><li>Whether AI will transform the labour market, and whether it will become a partisan political issue</li><li>The tensions between the cultures of San Francisco and DC, and how to bridge the divide between them</li><li>What listeners might be able to do to help with this whole mess</li><li>Panpsychism</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:00)</li><li>The interview begins (00:04:01)</li><li>The risk of autocratic lock-in due to AI (00:10:02)</li><li>The state of play in AI policymaking (00:13:40)</li><li>China and AI (00:32:12)</li><li>The most promising regulatory approaches (00:57:51)</li><li>Transforming the world without the world agreeing (01:04:44)</li><li>AI Bill of Rights (01:17:32)</li><li>Who’s ultimately responsible for the consequences of AI? (01:20:39)</li><li>Policy ideas that could appeal to many different groups (01:29:08)</li><li>Tension between those focused on x-risk and those focused on AI ethics (01:38:56)</li><li>Communicating with policymakers (01:54:22)</li><li>Is AI going to transform the labour market in the next few years? (01:58:51)</li><li>Is AI policy going to become a partisan political issue? (02:08:10)</li><li>The value of political philosophy (02:10:53)</li><li>Tantum’s work at DeepMind (02:21:20)</li><li>CSET (02:32:48)</li><li>Career advice (02:35:21)</li><li>Panpsychism (02:55:24)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 12 Oct 2023 21:12:36 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6bcc8daf/4db4659c.mp3" length="135979453" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/t7UV3fOTwyT9RMhBzrLI5QwOMQMD-FO5V7Qy6dgIh78/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1NDQ1MzYv/MTY5NzEzODY3MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11329</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"If you and I and 100 other people were on the first ship that was going to go settle Mars, and were going to build a human civilisation, and we have to decide what that government looks like, and we have all of the technology available today, how do we think about choosing a subset of that design space? </p><p>That space is huge and it includes absolutely awful things, and mixed-bag things, and maybe some things that almost everyone would agree are really wonderful, or at least an improvement on the way that things work today. But that raises all kinds of tricky questions. </p><p>My concern is that if we don't approach the evolution of collective decision making and government in a deliberate way, we may end up inadvertently backing ourselves into a corner, where we have ended up on some slippery slope -- and all of a sudden we have, let's say, autocracies on the global stage are strengthened relative to democracies." — Tantum Collins</p><p>In today’s episode, host Rob Wiblin gets the rare chance to interview someone with insider AI policy experience at the White House and DeepMind who’s willing to speak openly — Tantum Collins.</p><p><a href="https://80k.info/tc"><strong>Links to learn more, highlights, and full transcript.</strong></a></p><p>They cover:</p><ul><li>How AI could strengthen government capacity, and how that's a double-edged sword</li><li>How new technologies force us to confront tradeoffs in political philosophy that we were previously able to pretend weren't there</li><li>To what extent policymakers take different threats from AI seriously</li><li>Whether the US and China are in an AI arms race or not</li><li>Whether it's OK to transform the world without much of the world agreeing to it</li><li>The tyranny of small differences in AI policy</li><li>Disagreements between different schools of thought in AI policy, and proposals that could unite them</li><li>How the US AI Bill of Rights could be improved</li><li>Whether AI will transform the labour market, and whether it will become a partisan political issue</li><li>The tensions between the cultures of San Francisco and DC, and how to bridge the divide between them</li><li>What listeners might be able to do to help with this whole mess</li><li>Panpsychism</li><li>Plenty more</li></ul><p>Chapters:</p><ul><li>Cold open (00:00:00)</li><li>Rob's intro (00:01:00)</li><li>The interview begins (00:04:01)</li><li>The risk of autocratic lock-in due to AI (00:10:02)</li><li>The state of play in AI policymaking (00:13:40)</li><li>China and AI (00:32:12)</li><li>The most promising regulatory approaches (00:57:51)</li><li>Transforming the world without the world agreeing (01:04:44)</li><li>AI Bill of Rights (01:17:32)</li><li>Who’s ultimately responsible for the consequences of AI? (01:20:39)</li><li>Policy ideas that could appeal to many different groups (01:29:08)</li><li>Tension between those focused on x-risk and those focused on AI ethics (01:38:56)</li><li>Communicating with policymakers (01:54:22)</li><li>Is AI going to transform the labour market in the next few years? (01:58:51)</li><li>Is AI policy going to become a partisan political issue? (02:08:10)</li><li>The value of political philosophy (02:10:53)</li><li>Tantum’s work at DeepMind (02:21:20)</li><li>CSET (02:32:48)</li><li>Career advice (02:35:21)</li><li>Panpsychism (02:55:24)</li></ul><p><br></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/6bcc8daf/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/6bcc8daf/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#165 – Anders Sandberg on war in space, whether civilisations age, and the best things possible in our universe</title>
      <itunes:title>#165 – Anders Sandberg on war in space, whether civilisations age, and the best things possible in our universe</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">81846db4-5a75-416b-a9ec-ae6b24f55d39</guid>
      <link>https://80000hours.org/podcast/episodes/anders-sandberg-best-things-possible-in-our-universe/?utm_campaign=podcast__anders-sandberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Now, the really interesting question is: How much is there an attacker-versus-defender advantage in this kind of advanced future? </p><p>Right now, if somebody's sitting on Mars and you're going to war against them, it's very hard to hit them. You don't have a weapon that can hit them very well. But in theory, if you fire a missile, after a few months, it's going to arrive and maybe hit them, but they have a few months to move away. Distance actually makes you safer: if you spread out in space, it's actually very hard to hit you. </p><p>So it seems like you get a defence-dominant situation if you spread out sufficiently far. But if you're in Earth orbit, everything is close, and the lasers and missiles and the debris are a terrible danger, and everything is moving very fast. </p><p>So my general conclusion has been that war looks unlikely on some size scales but not on others." — Anders Sandberg</p><p>In today’s episode, host Rob Wiblin speaks with repeat guest and audience favourite Anders Sandberg about the most impressive things that could be achieved in our universe given the laws of physics.</p><p><a href="https://80k.info/a23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>The epic new book Anders is working on, and whether he’ll ever finish it</li><li>Whether there's a best possible world or we can just keep improving forever</li><li>What wars might look like if the galaxy is mostly settled</li><li>The impediments to AI or humans making it to other stars</li><li>How the universe will end a million trillion years in the future</li><li>Whether it’s useful to wonder about whether we’re living in a simulation</li><li>The grabby aliens theory</li><li>Whether civilizations get more likely to fail the older they get</li><li>The best way to generate energy that could ever exist</li><li>Black hole bombs</li><li>Whether superintelligence is necessary to get a lot of value</li><li>The likelihood that life from elsewhere has already visited Earth</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Now, the really interesting question is: How much is there an attacker-versus-defender advantage in this kind of advanced future? </p><p>Right now, if somebody's sitting on Mars and you're going to war against them, it's very hard to hit them. You don't have a weapon that can hit them very well. But in theory, if you fire a missile, after a few months, it's going to arrive and maybe hit them, but they have a few months to move away. Distance actually makes you safer: if you spread out in space, it's actually very hard to hit you. </p><p>So it seems like you get a defence-dominant situation if you spread out sufficiently far. But if you're in Earth orbit, everything is close, and the lasers and missiles and the debris are a terrible danger, and everything is moving very fast. </p><p>So my general conclusion has been that war looks unlikely on some size scales but not on others." — Anders Sandberg</p><p>In today’s episode, host Rob Wiblin speaks with repeat guest and audience favourite Anders Sandberg about the most impressive things that could be achieved in our universe given the laws of physics.</p><p><a href="https://80k.info/a23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>The epic new book Anders is working on, and whether he’ll ever finish it</li><li>Whether there's a best possible world or we can just keep improving forever</li><li>What wars might look like if the galaxy is mostly settled</li><li>The impediments to AI or humans making it to other stars</li><li>How the universe will end a million trillion years in the future</li><li>Whether it’s useful to wonder about whether we’re living in a simulation</li><li>The grabby aliens theory</li><li>Whether civilizations get more likely to fail the older they get</li><li>The best way to generate energy that could ever exist</li><li>Black hole bombs</li><li>Whether superintelligence is necessary to get a lot of value</li><li>The likelihood that life from elsewhere has already visited Earth</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 06 Oct 2023 20:22:20 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/9decbdbc/42393a1e.mp3" length="121413475" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2ZF_nBy-wizkg8eP98wDXWNXXwwNkPsxbzC-jrsAU-s/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1MzY4Njkv/MTY5NjYyMzM2MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10113</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Now, the really interesting question is: How much is there an attacker-versus-defender advantage in this kind of advanced future? </p><p>Right now, if somebody's sitting on Mars and you're going to war against them, it's very hard to hit them. You don't have a weapon that can hit them very well. But in theory, if you fire a missile, after a few months, it's going to arrive and maybe hit them, but they have a few months to move away. Distance actually makes you safer: if you spread out in space, it's actually very hard to hit you. </p><p>So it seems like you get a defence-dominant situation if you spread out sufficiently far. But if you're in Earth orbit, everything is close, and the lasers and missiles and the debris are a terrible danger, and everything is moving very fast. </p><p>So my general conclusion has been that war looks unlikely on some size scales but not on others." — Anders Sandberg</p><p>In today’s episode, host Rob Wiblin speaks with repeat guest and audience favourite Anders Sandberg about the most impressive things that could be achieved in our universe given the laws of physics.</p><p><a href="https://80k.info/a23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>The epic new book Anders is working on, and whether he’ll ever finish it</li><li>Whether there's a best possible world or we can just keep improving forever</li><li>What wars might look like if the galaxy is mostly settled</li><li>The impediments to AI or humans making it to other stars</li><li>How the universe will end a million trillion years in the future</li><li>Whether it’s useful to wonder about whether we’re living in a simulation</li><li>The grabby aliens theory</li><li>Whether civilizations get more likely to fail the older they get</li><li>The best way to generate energy that could ever exist</li><li>Black hole bombs</li><li>Whether superintelligence is necessary to get a lot of value</li><li>The likelihood that life from elsewhere has already visited Earth</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/9decbdbc/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#164 – Kevin Esvelt on cults that want to kill everyone, stealth vs wildfire pandemics, and how he felt inventing gene drives</title>
      <itunes:title>#164 – Kevin Esvelt on cults that want to kill everyone, stealth vs wildfire pandemics, and how he felt inventing gene drives</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4464da84-25c8-443f-9e27-30ac947bfecb</guid>
      <link>https://80000hours.org/podcast/episodes/kevin-esvelt-stealth-wildfire-pandemics/?utm_campaign=podcast__kevin-esvelt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Imagine a fast-spreading respiratory HIV. It sweeps around the world. Almost nobody has symptoms. Nobody notices until years later, when the first people who are infected begin to succumb. They might die, something else debilitating might happen to them, but by that point, just about everyone on the planet would have been infected already. </p><p>And then it would be a race. Can we come up with some way of defusing the thing? Can we come up with the equivalent of HIV antiretrovirals before it's too late?" — Kevin Esvelt</p><p>In today’s episode, host Luisa Rodriguez interviews Kevin Esvelt — a biologist at the MIT Media Lab and the inventor of CRISPR-based gene drive — about the threat posed by engineered bioweapons.</p><p><a href="https://80k.info/ke"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why it makes sense to focus on deliberately released pandemics</li><li>Case studies of people who actually wanted to kill billions of humans</li><li>How many people have the technical ability to produce dangerous viruses</li><li>The different threats of stealth and wildfire pandemics that could crash civilisation</li><li>The potential for AI models to increase access to dangerous pathogens</li><li>Why scientists try to identify new pandemic-capable pathogens, and the case against that research</li><li>Technological solutions, including UV lights and advanced PPE</li><li>Using CRISPR-based gene drive to fight diseases and reduce animal suffering</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Imagine a fast-spreading respiratory HIV. It sweeps around the world. Almost nobody has symptoms. Nobody notices until years later, when the first people who are infected begin to succumb. They might die, something else debilitating might happen to them, but by that point, just about everyone on the planet would have been infected already. </p><p>And then it would be a race. Can we come up with some way of defusing the thing? Can we come up with the equivalent of HIV antiretrovirals before it's too late?" — Kevin Esvelt</p><p>In today’s episode, host Luisa Rodriguez interviews Kevin Esvelt — a biologist at the MIT Media Lab and the inventor of CRISPR-based gene drive — about the threat posed by engineered bioweapons.</p><p><a href="https://80k.info/ke"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why it makes sense to focus on deliberately released pandemics</li><li>Case studies of people who actually wanted to kill billions of humans</li><li>How many people have the technical ability to produce dangerous viruses</li><li>The different threats of stealth and wildfire pandemics that could crash civilisation</li><li>The potential for AI models to increase access to dangerous pathogens</li><li>Why scientists try to identify new pandemic-capable pathogens, and the case against that research</li><li>Technological solutions, including UV lights and advanced PPE</li><li>Using CRISPR-based gene drive to fight diseases and reduce animal suffering</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 02 Oct 2023 18:14:54 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4cbd38c0/0a81d2fc.mp3" length="132302213" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/y-fi8ygJ7MCNlG22hEBCpqEnXJC5j0u8jj8XPElXTZM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1MzAyNjEv/MTY5NjI2NTgwOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11022</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Imagine a fast-spreading respiratory HIV. It sweeps around the world. Almost nobody has symptoms. Nobody notices until years later, when the first people who are infected begin to succumb. They might die, something else debilitating might happen to them, but by that point, just about everyone on the planet would have been infected already. </p><p>And then it would be a race. Can we come up with some way of defusing the thing? Can we come up with the equivalent of HIV antiretrovirals before it's too late?" — Kevin Esvelt</p><p>In today’s episode, host Luisa Rodriguez interviews Kevin Esvelt — a biologist at the MIT Media Lab and the inventor of CRISPR-based gene drive — about the threat posed by engineered bioweapons.</p><p><a href="https://80k.info/ke"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>They cover:</p><ul><li>Why it makes sense to focus on deliberately released pandemics</li><li>Case studies of people who actually wanted to kill billions of humans</li><li>How many people have the technical ability to produce dangerous viruses</li><li>The different threats of stealth and wildfire pandemics that could crash civilisation</li><li>The potential for AI models to increase access to dangerous pathogens</li><li>Why scientists try to identify new pandemic-capable pathogens, and the case against that research</li><li>Technological solutions, including UV lights and advanced PPE</li><li>Using CRISPR-based gene drive to fight diseases and reduce animal suffering</li><li>And plenty more.</li></ul><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/4cbd38c0/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Great power conflict (Article)</title>
      <itunes:title>Great power conflict (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e7aa6b41-ceae-4f6d-acf0-e86cacd735be</guid>
      <link>https://80000hours.org/problem-profiles/great-power-conflict/?utm_campaign=podcast__great-power-conflict&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Today’s release is a reading of our <strong>Great power conflict</strong> problem profile, written and narrated by Stephen Clare.</p><p>If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.info/GPCp"><strong>here</strong></a><strong>.</strong></p><p>And if you like this article, you might enjoy a couple of related episodes of this podcast:</p><ul><li><a href="https://80000hours.org/podcast/episodes/chris-blattman-five-reasons-wars-happen/"><strong>#128 – Chris Blattman on the five reasons wars happen</strong></a></li><li><a href="https://80000hours.org/podcast/episodes/bear-braumoeller-decline-of-war/"><strong>#140 – Bear Braumoeller on the case that war isn’t in decline</strong></a></li></ul><p><em>Audio mastering and editing for this episode: Dominic Armstrong<br>Audio Engineering Lead: Ben Cordell<br>Producer: Keiran Harris</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s release is a reading of our <strong>Great power conflict</strong> problem profile, written and narrated by Stephen Clare.</p><p>If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.info/GPCp"><strong>here</strong></a><strong>.</strong></p><p>And if you like this article, you might enjoy a couple of related episodes of this podcast:</p><ul><li><a href="https://80000hours.org/podcast/episodes/chris-blattman-five-reasons-wars-happen/"><strong>#128 – Chris Blattman on the five reasons wars happen</strong></a></li><li><a href="https://80000hours.org/podcast/episodes/bear-braumoeller-decline-of-war/"><strong>#140 – Bear Braumoeller on the case that war isn’t in decline</strong></a></li></ul><p><em>Audio mastering and editing for this episode: Dominic Armstrong<br>Audio Engineering Lead: Ben Cordell<br>Producer: Keiran Harris</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 22 Sep 2023 18:36:23 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b88b2877/69c97c7b.mp3" length="57464173" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/B8raBUGUoTuU2N6_sxL8PY4_9CwBFZ6v9TbPGLThFjY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE1MTYxOTQv/MTY5NTQwNzUyNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4786</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Today’s release is a reading of our <strong>Great power conflict</strong> problem profile, written and narrated by Stephen Clare.</p><p>If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.info/GPCp"><strong>here</strong></a><strong>.</strong></p><p>And if you like this article, you might enjoy a couple of related episodes of this podcast:</p><ul><li><a href="https://80000hours.org/podcast/episodes/chris-blattman-five-reasons-wars-happen/"><strong>#128 – Chris Blattman on the five reasons wars happen</strong></a></li><li><a href="https://80000hours.org/podcast/episodes/bear-braumoeller-decline-of-war/"><strong>#140 – Bear Braumoeller on the case that war isn’t in decline</strong></a></li></ul><p><em>Audio mastering and editing for this episode: Dominic Armstrong<br>Audio Engineering Lead: Ben Cordell<br>Producer: Keiran Harris</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/b88b2877/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#163 – Toby Ord on the perils of maximising the good that you do</title>
      <itunes:title>#163 – Toby Ord on the perils of maximising the good that you do</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4eea2230-2cc7-41e3-b33c-91f88d8b0f5e</guid>
      <link>https://80000hours.org/podcast/episodes/toby-ord-perils-of-maximising-good/?utm_campaign=podcast__toby-ord-23&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Effective altruism is associated with the slogan "do the most good." On one level, this has to be unobjectionable: What could be bad about helping people more and more?</p><p>But in today's interview, Toby Ord — moral philosopher at the University of Oxford and one of the founding figures of effective altruism — lays out three reasons to be cautious about the idea of maximising the good that you do. He suggests that rather than “doing the most good that we can,” perhaps we should be happy with a more modest and manageable goal: “doing most of the good that we can.”</p><p><a href="https://80k.info/to3"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>Toby was inspired to revisit these ideas by the possibility that Sam Bankman-Fried, who stands accused of committing severe fraud as CEO of the cryptocurrency exchange FTX, was motivated to break the law by a desire to give away as much money as possible to worthy causes.</p><p>Toby's top reason not to fully maximise is the following: if the goal you're aiming at is subtly wrong or incomplete, then going all the way towards maximising it will usually cause you to start doing some very harmful things.</p><p>This result can be shown mathematically, but can also be made intuitive, and may explain why we feel instinctively wary of going “all-in” on any idea, or goal, or way of living — even something as benign as helping other people as much as possible.</p><p>Toby gives the example of someone pursuing a career as a professional swimmer. Initially, as our swimmer takes their training and performance more seriously, they adjust their diet, hire a better trainer, and pay more attention to their technique. While swimming is the main focus of their life, they feel fit and healthy and also enjoy other aspects of their life as well — family, friends, and personal projects.</p><p>But if they decide to increase their commitment further and really go all-in on their swimming career, holding back nothing back, then this picture can radically change. Their effort was already substantial, so how can they shave those final few seconds off their racing time? The only remaining options are those which were so costly they were loath to consider them before.</p><p>To eke out those final gains — and go from 80% effort to 100% — our swimmer must sacrifice other hobbies, deprioritise their relationships, neglect their career, ignore food preferences, accept a higher risk of injury, and maybe even consider using steroids.</p><p>Now, if maximising one's speed at swimming really were the only goal they ought to be pursuing, there'd be no problem with this. But if it's the wrong goal, or only one of many things they should be aiming for, then the outcome is disastrous. In going from 80% to 100% effort, their swimming speed was only increased by a tiny amount, while everything else they were accomplishing dropped off a cliff.</p><p>The bottom line is simple: a dash of moderation makes you much more robust to uncertainty and error.</p><p>As Toby notes, this is similar to the observation that a sufficiently capable superintelligent AI, given any one goal, would ruin the world if it maximised it to the exclusion of everything else. And it follows a similar pattern to performance falling off a cliff when a statistical model is 'overfit' to its data.</p><p>In the full interview, Toby also explains the “moral trade” argument against pursuing narrow goals at the expense of everything else, and how consequentialism changes if you judge not just outcomes or acts, but everything according to its impacts on the world.</p><p>Toby and Rob also discuss:</p><ul><li>The rise and fall of FTX and some of its impacts</li><li>What Toby hoped effective altruism would and wouldn't become when he helped to get it off the ground</li><li>What utilitarianism has going for it, and what's wrong with it in Toby's view</li><li>How to mathematically model the importance of personal integrity</li><li>Which AI labs Toby thinks have been acting more responsibly than others</li><li>How having a young child affects Toby’s feelings about AI risk</li><li>Whether infinities present a fundamental problem for any theory of ethics that aspire to be fully impartial</li><li>How Toby ended up being the source of the highest quality images of the Earth from space</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Effective altruism is associated with the slogan "do the most good." On one level, this has to be unobjectionable: What could be bad about helping people more and more?</p><p>But in today's interview, Toby Ord — moral philosopher at the University of Oxford and one of the founding figures of effective altruism — lays out three reasons to be cautious about the idea of maximising the good that you do. He suggests that rather than “doing the most good that we can,” perhaps we should be happy with a more modest and manageable goal: “doing most of the good that we can.”</p><p><a href="https://80k.info/to3"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>Toby was inspired to revisit these ideas by the possibility that Sam Bankman-Fried, who stands accused of committing severe fraud as CEO of the cryptocurrency exchange FTX, was motivated to break the law by a desire to give away as much money as possible to worthy causes.</p><p>Toby's top reason not to fully maximise is the following: if the goal you're aiming at is subtly wrong or incomplete, then going all the way towards maximising it will usually cause you to start doing some very harmful things.</p><p>This result can be shown mathematically, but can also be made intuitive, and may explain why we feel instinctively wary of going “all-in” on any idea, or goal, or way of living — even something as benign as helping other people as much as possible.</p><p>Toby gives the example of someone pursuing a career as a professional swimmer. Initially, as our swimmer takes their training and performance more seriously, they adjust their diet, hire a better trainer, and pay more attention to their technique. While swimming is the main focus of their life, they feel fit and healthy and also enjoy other aspects of their life as well — family, friends, and personal projects.</p><p>But if they decide to increase their commitment further and really go all-in on their swimming career, holding back nothing back, then this picture can radically change. Their effort was already substantial, so how can they shave those final few seconds off their racing time? The only remaining options are those which were so costly they were loath to consider them before.</p><p>To eke out those final gains — and go from 80% effort to 100% — our swimmer must sacrifice other hobbies, deprioritise their relationships, neglect their career, ignore food preferences, accept a higher risk of injury, and maybe even consider using steroids.</p><p>Now, if maximising one's speed at swimming really were the only goal they ought to be pursuing, there'd be no problem with this. But if it's the wrong goal, or only one of many things they should be aiming for, then the outcome is disastrous. In going from 80% to 100% effort, their swimming speed was only increased by a tiny amount, while everything else they were accomplishing dropped off a cliff.</p><p>The bottom line is simple: a dash of moderation makes you much more robust to uncertainty and error.</p><p>As Toby notes, this is similar to the observation that a sufficiently capable superintelligent AI, given any one goal, would ruin the world if it maximised it to the exclusion of everything else. And it follows a similar pattern to performance falling off a cliff when a statistical model is 'overfit' to its data.</p><p>In the full interview, Toby also explains the “moral trade” argument against pursuing narrow goals at the expense of everything else, and how consequentialism changes if you judge not just outcomes or acts, but everything according to its impacts on the world.</p><p>Toby and Rob also discuss:</p><ul><li>The rise and fall of FTX and some of its impacts</li><li>What Toby hoped effective altruism would and wouldn't become when he helped to get it off the ground</li><li>What utilitarianism has going for it, and what's wrong with it in Toby's view</li><li>How to mathematically model the importance of personal integrity</li><li>Which AI labs Toby thinks have been acting more responsibly than others</li><li>How having a young child affects Toby’s feelings about AI risk</li><li>Whether infinities present a fundamental problem for any theory of ethics that aspire to be fully impartial</li><li>How Toby ended up being the source of the highest quality images of the Earth from space</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 08 Sep 2023 20:28:23 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e004dd78/b2101ecd.mp3" length="134774435" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/9OcHFGiF-XHdHuRBWjXhPua3n5dlS18zsEglHCfKJPg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0OTcyMzAv/MTY5NDIwNDQ2Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11228</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Effective altruism is associated with the slogan "do the most good." On one level, this has to be unobjectionable: What could be bad about helping people more and more?</p><p>But in today's interview, Toby Ord — moral philosopher at the University of Oxford and one of the founding figures of effective altruism — lays out three reasons to be cautious about the idea of maximising the good that you do. He suggests that rather than “doing the most good that we can,” perhaps we should be happy with a more modest and manageable goal: “doing most of the good that we can.”</p><p><a href="https://80k.info/to3"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>Toby was inspired to revisit these ideas by the possibility that Sam Bankman-Fried, who stands accused of committing severe fraud as CEO of the cryptocurrency exchange FTX, was motivated to break the law by a desire to give away as much money as possible to worthy causes.</p><p>Toby's top reason not to fully maximise is the following: if the goal you're aiming at is subtly wrong or incomplete, then going all the way towards maximising it will usually cause you to start doing some very harmful things.</p><p>This result can be shown mathematically, but can also be made intuitive, and may explain why we feel instinctively wary of going “all-in” on any idea, or goal, or way of living — even something as benign as helping other people as much as possible.</p><p>Toby gives the example of someone pursuing a career as a professional swimmer. Initially, as our swimmer takes their training and performance more seriously, they adjust their diet, hire a better trainer, and pay more attention to their technique. While swimming is the main focus of their life, they feel fit and healthy and also enjoy other aspects of their life as well — family, friends, and personal projects.</p><p>But if they decide to increase their commitment further and really go all-in on their swimming career, holding back nothing back, then this picture can radically change. Their effort was already substantial, so how can they shave those final few seconds off their racing time? The only remaining options are those which were so costly they were loath to consider them before.</p><p>To eke out those final gains — and go from 80% effort to 100% — our swimmer must sacrifice other hobbies, deprioritise their relationships, neglect their career, ignore food preferences, accept a higher risk of injury, and maybe even consider using steroids.</p><p>Now, if maximising one's speed at swimming really were the only goal they ought to be pursuing, there'd be no problem with this. But if it's the wrong goal, or only one of many things they should be aiming for, then the outcome is disastrous. In going from 80% to 100% effort, their swimming speed was only increased by a tiny amount, while everything else they were accomplishing dropped off a cliff.</p><p>The bottom line is simple: a dash of moderation makes you much more robust to uncertainty and error.</p><p>As Toby notes, this is similar to the observation that a sufficiently capable superintelligent AI, given any one goal, would ruin the world if it maximised it to the exclusion of everything else. And it follows a similar pattern to performance falling off a cliff when a statistical model is 'overfit' to its data.</p><p>In the full interview, Toby also explains the “moral trade” argument against pursuing narrow goals at the expense of everything else, and how consequentialism changes if you judge not just outcomes or acts, but everything according to its impacts on the world.</p><p>Toby and Rob also discuss:</p><ul><li>The rise and fall of FTX and some of its impacts</li><li>What Toby hoped effective altruism would and wouldn't become when he helped to get it off the ground</li><li>What utilitarianism has going for it, and what's wrong with it in Toby's view</li><li>How to mathematically model the importance of personal integrity</li><li>Which AI labs Toby thinks have been acting more responsibly than others</li><li>How having a young child affects Toby’s feelings about AI risk</li><li>Whether infinities present a fundamental problem for any theory of ethics that aspire to be fully impartial</li><li>How Toby ended up being the source of the highest quality images of the Earth from space</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/e004dd78/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>The 80,000 Hours Career Guide (2023)</title>
      <itunes:title>The 80,000 Hours Career Guide (2023)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f0a9e53d-d5e6-4c52-b576-72e1df2d7491</guid>
      <link>https://80000hours.org/career-guide/</link>
      <description>
        <![CDATA[<p>An audio version of the 2023 80,000 Hours career guide, also available on our <a href="https://80000hours.org/career-guide/">website</a>, on <a href="https://www.amazon.com/80-000-Hours-fulfilling-career/dp/1399957090/">Amazon</a>, and on <a href="https://www.audible.com/pd/80000-Hours-Audiobook/B0CGL49G63">Audible</a>.</p><p>If you know someone who might find our career guide helpful, you can get a free copy sent to them by going to <a href="http://80000hours.org/gift">80000hours.org/gift</a>.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Introduction (00:04:08)</li><li>Chapter 1: What Makes for a Dream Job? (00:09:09)</li><li>Chapter 2: Can One Person Make a Difference? What the Evidence Says. (00:33:02)</li><li>Chapter 3: Three Ways Anyone Can Make a Difference, No Matter Their Job (00:43:33)</li><li>Chapter 4: Want to Do Good? Here's How to Choose an Area to Focus on (00:58:50)</li><li>Chapter 5: The World's Biggest Problems and Why They're Not What First Comes to Mind (01:12:03)</li><li>Chapter 6: Which Jobs Help People the Most? (01:42:15)</li><li>Chapter 7: Which Jobs Put You in the Best Long-Term Position? (02:19:11)</li><li>Chapter 8: How to Find the Right Career for You (02:59:26)</li><li>Chapter 9: How to Make Your Career Plan (03:32:30)</li><li>Chapter 10: All the Best Advice We Could Find on How to Get a Job (03:55:34)</li><li>Chapter 11: One of the Most Powerful Ways to Improve Your Career - Join a Community (04:24:21)</li><li>The End: The Entire Guide, in One Minute (04:35:49)</li><li>Rob's outro (04:40:05)</li></ul>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>An audio version of the 2023 80,000 Hours career guide, also available on our <a href="https://80000hours.org/career-guide/">website</a>, on <a href="https://www.amazon.com/80-000-Hours-fulfilling-career/dp/1399957090/">Amazon</a>, and on <a href="https://www.audible.com/pd/80000-Hours-Audiobook/B0CGL49G63">Audible</a>.</p><p>If you know someone who might find our career guide helpful, you can get a free copy sent to them by going to <a href="http://80000hours.org/gift">80000hours.org/gift</a>.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Introduction (00:04:08)</li><li>Chapter 1: What Makes for a Dream Job? (00:09:09)</li><li>Chapter 2: Can One Person Make a Difference? What the Evidence Says. (00:33:02)</li><li>Chapter 3: Three Ways Anyone Can Make a Difference, No Matter Their Job (00:43:33)</li><li>Chapter 4: Want to Do Good? Here's How to Choose an Area to Focus on (00:58:50)</li><li>Chapter 5: The World's Biggest Problems and Why They're Not What First Comes to Mind (01:12:03)</li><li>Chapter 6: Which Jobs Help People the Most? (01:42:15)</li><li>Chapter 7: Which Jobs Put You in the Best Long-Term Position? (02:19:11)</li><li>Chapter 8: How to Find the Right Career for You (02:59:26)</li><li>Chapter 9: How to Make Your Career Plan (03:32:30)</li><li>Chapter 10: All the Best Advice We Could Find on How to Get a Job (03:55:34)</li><li>Chapter 11: One of the Most Powerful Ways to Improve Your Career - Join a Community (04:24:21)</li><li>The End: The Entire Guide, in One Minute (04:35:49)</li><li>Rob's outro (04:40:05)</li></ul>]]>
      </content:encoded>
      <pubDate>Mon, 04 Sep 2023 07:09:53 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f8138017/07e18d03.mp3" length="135043572" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/fHJtMUnrsjaLwbZ7XLjnBLGApO5EUy23dQYmIyC40mI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0ODY1ODEv/MTY5MzYwMjg2Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>16873</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>An audio version of the 2023 80,000 Hours career guide, also available on our <a href="https://80000hours.org/career-guide/">website</a>, on <a href="https://www.amazon.com/80-000-Hours-fulfilling-career/dp/1399957090/">Amazon</a>, and on <a href="https://www.audible.com/pd/80000-Hours-Audiobook/B0CGL49G63">Audible</a>.</p><p>If you know someone who might find our career guide helpful, you can get a free copy sent to them by going to <a href="http://80000hours.org/gift">80000hours.org/gift</a>.</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>Introduction (00:04:08)</li><li>Chapter 1: What Makes for a Dream Job? (00:09:09)</li><li>Chapter 2: Can One Person Make a Difference? What the Evidence Says. (00:33:02)</li><li>Chapter 3: Three Ways Anyone Can Make a Difference, No Matter Their Job (00:43:33)</li><li>Chapter 4: Want to Do Good? Here's How to Choose an Area to Focus on (00:58:50)</li><li>Chapter 5: The World's Biggest Problems and Why They're Not What First Comes to Mind (01:12:03)</li><li>Chapter 6: Which Jobs Help People the Most? (01:42:15)</li><li>Chapter 7: Which Jobs Put You in the Best Long-Term Position? (02:19:11)</li><li>Chapter 8: How to Find the Right Career for You (02:59:26)</li><li>Chapter 9: How to Make Your Career Plan (03:32:30)</li><li>Chapter 10: All the Best Advice We Could Find on How to Get a Job (03:55:34)</li><li>Chapter 11: One of the Most Powerful Ways to Improve Your Career - Join a Community (04:24:21)</li><li>The End: The Entire Guide, in One Minute (04:35:49)</li><li>Rob's outro (04:40:05)</li></ul>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/f8138017/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#162 – Mustafa Suleyman on getting Washington and Silicon Valley to tame AI</title>
      <itunes:title>#162 – Mustafa Suleyman on getting Washington and Silicon Valley to tame AI</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4a34f1f1-55e4-49e4-97df-15ba4b83bc92</guid>
      <link>https://80000hours.org/podcast/episodes/mustafa-suleyman-getting-washington-and-silicon-valley-to-tame-ai/?utm_campaign=podcast__mustafa-suleyman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Mustafa Suleyman was part of the trio that founded DeepMind, and his new AI project is building one of the world's largest supercomputers to train a large language model on 10–100x the compute used to train ChatGPT.</p><p>But far from the stereotype of the incorrigibly optimistic tech founder, Mustafa is deeply worried about the future, for reasons he lays out in his new book <a href="https://www.amazon.com/Coming-Wave-Technology-Twenty-first-Centurys/dp/0593593952"><em>The Coming Wave: Technology, Power, and the 21st Century's Greatest Dilemma</em></a><em> </em>(coauthored with Michael Bhaskar). The future could be really good, but only if we grab the bull by the horns and solve the new problems technology is throwing at us.</p><p><a href="https://80k.info/ms"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>On Mustafa's telling, AI and biotechnology will soon be a huge aid to criminals and terrorists, empowering small groups to cause harm on previously unimaginable scales. Democratic countries have learned to walk a 'narrow path' between chaos on the one hand and authoritarianism on the other, avoiding the downsides that come from both extreme openness and extreme closure. AI could easily destabilise that present equilibrium, throwing us off dangerously in either direction. And ultimately, within our lifetimes humans may not need to work to live any more -- or indeed, even have the option to do so.</p><p>And those are just three of the challenges confronting us. In Mustafa's view, 'misaligned' AI that goes rogue and pursues its own agenda won't be an issue for the next few years, and it isn't a problem for the current style of large language models. But he thinks that at some point -- in eight, ten, or twelve years -- it will become an entirely legitimate concern, and says that we need to be planning ahead.</p><p>In <em>The Coming Wave</em>, Mustafa lays out a 10-part agenda for 'containment' -- that is to say, for limiting the negative and unforeseen consequences of emerging technologies:</p><p>1. Developing an Apollo programme for technical AI safety<br>2. Instituting capability audits for AI models<br>3. Buying time by exploiting hardware choke points<br>4. Getting critics involved in directly engineering AI models<br>5. Getting AI labs to be guided by motives other than profit<br>6. Radically increasing governments’ understanding of AI and their capabilities to sensibly regulate it<br>7. Creating international treaties to prevent proliferation of the most dangerous AI capabilities<br>8. Building a self-critical culture in AI labs of openly accepting when the status quo isn't working<br>9. Creating a mass public movement that understands AI and can demand the necessary controls<br>10. Not relying too much on delay, but instead seeking to move into a new somewhat-stable equilibria</p><p>As Mustafa put it, "AI is a technology with almost every use case imaginable" and that will demand that, in time, we rethink everything. </p><p>Rob and Mustafa discuss the above, as well as:</p><ul><li>Whether we should be open sourcing AI models</li><li>Whether Mustafa's policy views are consistent with his timelines for transformative AI</li><li>How people with very different views on these issues get along at AI labs</li><li>The failed efforts (so far) to get a wider range of people involved in these decisions</li><li>Whether it's dangerous for Mustafa's new company to be training far larger models than GPT-4</li><li>Whether we'll be blown away by AI progress over the next year</li><li>What mandatory regulations government should be imposing on AI labs right now</li><li>Appropriate priorities for the UK's upcoming AI safety summit</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Mustafa Suleyman was part of the trio that founded DeepMind, and his new AI project is building one of the world's largest supercomputers to train a large language model on 10–100x the compute used to train ChatGPT.</p><p>But far from the stereotype of the incorrigibly optimistic tech founder, Mustafa is deeply worried about the future, for reasons he lays out in his new book <a href="https://www.amazon.com/Coming-Wave-Technology-Twenty-first-Centurys/dp/0593593952"><em>The Coming Wave: Technology, Power, and the 21st Century's Greatest Dilemma</em></a><em> </em>(coauthored with Michael Bhaskar). The future could be really good, but only if we grab the bull by the horns and solve the new problems technology is throwing at us.</p><p><a href="https://80k.info/ms"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>On Mustafa's telling, AI and biotechnology will soon be a huge aid to criminals and terrorists, empowering small groups to cause harm on previously unimaginable scales. Democratic countries have learned to walk a 'narrow path' between chaos on the one hand and authoritarianism on the other, avoiding the downsides that come from both extreme openness and extreme closure. AI could easily destabilise that present equilibrium, throwing us off dangerously in either direction. And ultimately, within our lifetimes humans may not need to work to live any more -- or indeed, even have the option to do so.</p><p>And those are just three of the challenges confronting us. In Mustafa's view, 'misaligned' AI that goes rogue and pursues its own agenda won't be an issue for the next few years, and it isn't a problem for the current style of large language models. But he thinks that at some point -- in eight, ten, or twelve years -- it will become an entirely legitimate concern, and says that we need to be planning ahead.</p><p>In <em>The Coming Wave</em>, Mustafa lays out a 10-part agenda for 'containment' -- that is to say, for limiting the negative and unforeseen consequences of emerging technologies:</p><p>1. Developing an Apollo programme for technical AI safety<br>2. Instituting capability audits for AI models<br>3. Buying time by exploiting hardware choke points<br>4. Getting critics involved in directly engineering AI models<br>5. Getting AI labs to be guided by motives other than profit<br>6. Radically increasing governments’ understanding of AI and their capabilities to sensibly regulate it<br>7. Creating international treaties to prevent proliferation of the most dangerous AI capabilities<br>8. Building a self-critical culture in AI labs of openly accepting when the status quo isn't working<br>9. Creating a mass public movement that understands AI and can demand the necessary controls<br>10. Not relying too much on delay, but instead seeking to move into a new somewhat-stable equilibria</p><p>As Mustafa put it, "AI is a technology with almost every use case imaginable" and that will demand that, in time, we rethink everything. </p><p>Rob and Mustafa discuss the above, as well as:</p><ul><li>Whether we should be open sourcing AI models</li><li>Whether Mustafa's policy views are consistent with his timelines for transformative AI</li><li>How people with very different views on these issues get along at AI labs</li><li>The failed efforts (so far) to get a wider range of people involved in these decisions</li><li>Whether it's dangerous for Mustafa's new company to be training far larger models than GPT-4</li><li>Whether we'll be blown away by AI progress over the next year</li><li>What mandatory regulations government should be imposing on AI labs right now</li><li>Appropriate priorities for the UK's upcoming AI safety summit</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 01 Sep 2023 19:51:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/873eba49/79a6755f.mp3" length="28653249" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/QN4hOch28loeW5YvokYvmRKmll3wSU2h7HspWhqJo9s/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0ODY1MDMv/MTY5MzU5NzUyOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3574</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Mustafa Suleyman was part of the trio that founded DeepMind, and his new AI project is building one of the world's largest supercomputers to train a large language model on 10–100x the compute used to train ChatGPT.</p><p>But far from the stereotype of the incorrigibly optimistic tech founder, Mustafa is deeply worried about the future, for reasons he lays out in his new book <a href="https://www.amazon.com/Coming-Wave-Technology-Twenty-first-Centurys/dp/0593593952"><em>The Coming Wave: Technology, Power, and the 21st Century's Greatest Dilemma</em></a><em> </em>(coauthored with Michael Bhaskar). The future could be really good, but only if we grab the bull by the horns and solve the new problems technology is throwing at us.</p><p><a href="https://80k.info/ms"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>On Mustafa's telling, AI and biotechnology will soon be a huge aid to criminals and terrorists, empowering small groups to cause harm on previously unimaginable scales. Democratic countries have learned to walk a 'narrow path' between chaos on the one hand and authoritarianism on the other, avoiding the downsides that come from both extreme openness and extreme closure. AI could easily destabilise that present equilibrium, throwing us off dangerously in either direction. And ultimately, within our lifetimes humans may not need to work to live any more -- or indeed, even have the option to do so.</p><p>And those are just three of the challenges confronting us. In Mustafa's view, 'misaligned' AI that goes rogue and pursues its own agenda won't be an issue for the next few years, and it isn't a problem for the current style of large language models. But he thinks that at some point -- in eight, ten, or twelve years -- it will become an entirely legitimate concern, and says that we need to be planning ahead.</p><p>In <em>The Coming Wave</em>, Mustafa lays out a 10-part agenda for 'containment' -- that is to say, for limiting the negative and unforeseen consequences of emerging technologies:</p><p>1. Developing an Apollo programme for technical AI safety<br>2. Instituting capability audits for AI models<br>3. Buying time by exploiting hardware choke points<br>4. Getting critics involved in directly engineering AI models<br>5. Getting AI labs to be guided by motives other than profit<br>6. Radically increasing governments’ understanding of AI and their capabilities to sensibly regulate it<br>7. Creating international treaties to prevent proliferation of the most dangerous AI capabilities<br>8. Building a self-critical culture in AI labs of openly accepting when the status quo isn't working<br>9. Creating a mass public movement that understands AI and can demand the necessary controls<br>10. Not relying too much on delay, but instead seeking to move into a new somewhat-stable equilibria</p><p>As Mustafa put it, "AI is a technology with almost every use case imaginable" and that will demand that, in time, we rethink everything. </p><p>Rob and Mustafa discuss the above, as well as:</p><ul><li>Whether we should be open sourcing AI models</li><li>Whether Mustafa's policy views are consistent with his timelines for transformative AI</li><li>How people with very different views on these issues get along at AI labs</li><li>The failed efforts (so far) to get a wider range of people involved in these decisions</li><li>Whether it's dangerous for Mustafa's new company to be training far larger models than GPT-4</li><li>Whether we'll be blown away by AI progress over the next year</li><li>What mandatory regulations government should be imposing on AI labs right now</li><li>Appropriate priorities for the UK's upcoming AI safety summit</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/873eba49/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#161 – Michael Webb on whether AI will soon cause job loss, lower incomes, and higher inequality — or the opposite</title>
      <itunes:title>#161 – Michael Webb on whether AI will soon cause job loss, lower incomes, and higher inequality — or the opposite</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e6c7f33b-0920-4840-a510-04a5ff862816</guid>
      <link>https://80000hours.org/podcast/episodes/michael-webb-ai-jobs-labour-market/?utm_campaign=podcast__michael-webb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"Do you remember seeing these photographs of generally women sitting in front of these huge panels and connecting calls, plugging different calls between different numbers? The automated version of that was invented in 1892. </p><p>However, the number of human manual operators peaked in 1920 -- 30 years after this. At which point, AT&amp;T is the monopoly provider of this, and they are the largest single employer in America, 30 years after they've invented the complete automation of this thing that they're employing people to do. And the last person who is a manual switcher does not lose their job, as it were: that job doesn't stop existing until I think like 1980.</p><p>So it takes 90 years from the invention of full automation to the full adoption of it in a single company that's a monopoly provider. It can do what it wants, basically. And so the question perhaps you might have is why?" — Michael Webb</p><p>In today’s episode, host Luisa Rodriguez interviews economist Michael Webb of DeepMind, the British Government, and Stanford about how AI progress is going to affect people's jobs and the labour market.</p><p><a href="https://80k.info/mw"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The jobs most and least exposed to AI</li><li>Whether we’ll we see mass unemployment in the short term </li><li>How long it took other technologies like electricity and computers to have economy-wide effects</li><li>Whether AI will increase or decrease inequality</li><li>Whether AI will lead to explosive economic growth</li><li>What we can we learn from history, and reasons to think this time is different</li><li>Career advice for a world of LLMs</li><li>Why Michael is starting a new org to relieve talent bottlenecks through accelerated learning, and how you can get involved</li><li>Michael's take as a musician on AI-generated music</li><li>And plenty more</li></ul><p>If you'd like to work with Michael on his new org to radically accelerate how quickly people acquire expertise in critical cause areas, he's now hiring! Check out <a href="http://quantumleap.education/">Quantum Leap's website</a>.</p><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"Do you remember seeing these photographs of generally women sitting in front of these huge panels and connecting calls, plugging different calls between different numbers? The automated version of that was invented in 1892. </p><p>However, the number of human manual operators peaked in 1920 -- 30 years after this. At which point, AT&amp;T is the monopoly provider of this, and they are the largest single employer in America, 30 years after they've invented the complete automation of this thing that they're employing people to do. And the last person who is a manual switcher does not lose their job, as it were: that job doesn't stop existing until I think like 1980.</p><p>So it takes 90 years from the invention of full automation to the full adoption of it in a single company that's a monopoly provider. It can do what it wants, basically. And so the question perhaps you might have is why?" — Michael Webb</p><p>In today’s episode, host Luisa Rodriguez interviews economist Michael Webb of DeepMind, the British Government, and Stanford about how AI progress is going to affect people's jobs and the labour market.</p><p><a href="https://80k.info/mw"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The jobs most and least exposed to AI</li><li>Whether we’ll we see mass unemployment in the short term </li><li>How long it took other technologies like electricity and computers to have economy-wide effects</li><li>Whether AI will increase or decrease inequality</li><li>Whether AI will lead to explosive economic growth</li><li>What we can we learn from history, and reasons to think this time is different</li><li>Career advice for a world of LLMs</li><li>Why Michael is starting a new org to relieve talent bottlenecks through accelerated learning, and how you can get involved</li><li>Michael's take as a musician on AI-generated music</li><li>And plenty more</li></ul><p>If you'd like to work with Michael on his new org to radically accelerate how quickly people acquire expertise in critical cause areas, he's now hiring! Check out <a href="http://quantumleap.education/">Quantum Leap's website</a>.</p><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 23 Aug 2023 21:27:49 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5229d5ce/89bfad09.mp3" length="101092159" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/wKFea73u6KDriBdQS3IEAwV2qouJlF80Zq05wRdZGhI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0NzE1MjUv/MTY5MjgyNTYzOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>12632</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"Do you remember seeing these photographs of generally women sitting in front of these huge panels and connecting calls, plugging different calls between different numbers? The automated version of that was invented in 1892. </p><p>However, the number of human manual operators peaked in 1920 -- 30 years after this. At which point, AT&amp;T is the monopoly provider of this, and they are the largest single employer in America, 30 years after they've invented the complete automation of this thing that they're employing people to do. And the last person who is a manual switcher does not lose their job, as it were: that job doesn't stop existing until I think like 1980.</p><p>So it takes 90 years from the invention of full automation to the full adoption of it in a single company that's a monopoly provider. It can do what it wants, basically. And so the question perhaps you might have is why?" — Michael Webb</p><p>In today’s episode, host Luisa Rodriguez interviews economist Michael Webb of DeepMind, the British Government, and Stanford about how AI progress is going to affect people's jobs and the labour market.</p><p><a href="https://80k.info/mw"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The jobs most and least exposed to AI</li><li>Whether we’ll we see mass unemployment in the short term </li><li>How long it took other technologies like electricity and computers to have economy-wide effects</li><li>Whether AI will increase or decrease inequality</li><li>Whether AI will lead to explosive economic growth</li><li>What we can we learn from history, and reasons to think this time is different</li><li>Career advice for a world of LLMs</li><li>Why Michael is starting a new org to relieve talent bottlenecks through accelerated learning, and how you can get involved</li><li>Michael's take as a musician on AI-generated music</li><li>And plenty more</li></ul><p>If you'd like to work with Michael on his new org to radically accelerate how quickly people acquire expertise in critical cause areas, he's now hiring! Check out <a href="http://quantumleap.education/">Quantum Leap's website</a>.</p><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/5229d5ce/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#160 – Hannah Ritchie on why it makes sense to be optimistic about the environment</title>
      <itunes:title>#160 – Hannah Ritchie on why it makes sense to be optimistic about the environment</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">08d67a43-000d-46f2-ab1c-8c0f79c95721</guid>
      <link>https://80000hours.org/podcast/episodes/hannah-ritchie-environmental-optimism/?utm_campaign=podcast__hannah-ritchie&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"There's no money to invest in education elsewhere, so they almost get trapped in the cycle where they don't get a lot from crop production, but everyone in the family has to work there to just stay afloat. Basically, you get locked in. There's almost no opportunities externally to go elsewhere. So one of my core arguments is that if you're going to address global poverty, you have to increase agricultural productivity in sub-Saharan Africa. There's almost no way of avoiding that." — Hannah Ritchie</p><p>In today’s episode, host Luisa Rodriguez interviews the head of research at <a href="https://ourworldindata.org/">Our World in Data</a> — Hannah Ritchie — on the case for environmental optimism.</p><p><a href="https://80k.info/hr"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>Why agricultural productivity in sub-Saharan Africa could be so important, and how much better things could get</li><li>Her new book about how we could be the first generation to build a sustainable planet</li><li>Whether climate change is the most worrying environmental issue</li><li>How we reduced outdoor air pollution</li><li>Why Hannah is worried about the state of ​​biodiversity</li><li>Solutions that address multiple environmental issues at once</li><li>How the world coordinated to address the hole in the ozone layer</li><li>Surprises from Our World in Data’s research</li><li>Psychological challenges that come up in Hannah’s work</li><li>And plenty more</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"There's no money to invest in education elsewhere, so they almost get trapped in the cycle where they don't get a lot from crop production, but everyone in the family has to work there to just stay afloat. Basically, you get locked in. There's almost no opportunities externally to go elsewhere. So one of my core arguments is that if you're going to address global poverty, you have to increase agricultural productivity in sub-Saharan Africa. There's almost no way of avoiding that." — Hannah Ritchie</p><p>In today’s episode, host Luisa Rodriguez interviews the head of research at <a href="https://ourworldindata.org/">Our World in Data</a> — Hannah Ritchie — on the case for environmental optimism.</p><p><a href="https://80k.info/hr"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>Why agricultural productivity in sub-Saharan Africa could be so important, and how much better things could get</li><li>Her new book about how we could be the first generation to build a sustainable planet</li><li>Whether climate change is the most worrying environmental issue</li><li>How we reduced outdoor air pollution</li><li>Why Hannah is worried about the state of ​​biodiversity</li><li>Solutions that address multiple environmental issues at once</li><li>How the world coordinated to address the hole in the ozone layer</li><li>Surprises from Our World in Data’s research</li><li>Psychological challenges that come up in Hannah’s work</li><li>And plenty more</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 14 Aug 2023 21:18:28 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3cad2cf7/f951870a.mp3" length="75258411" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/sbHzuW9JvsngQTbGavVdfIJ4yA0lBLB-1H5i5AKRWPY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0NjA4MjEv/MTY5MjAzNTg1My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9402</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"There's no money to invest in education elsewhere, so they almost get trapped in the cycle where they don't get a lot from crop production, but everyone in the family has to work there to just stay afloat. Basically, you get locked in. There's almost no opportunities externally to go elsewhere. So one of my core arguments is that if you're going to address global poverty, you have to increase agricultural productivity in sub-Saharan Africa. There's almost no way of avoiding that." — Hannah Ritchie</p><p>In today’s episode, host Luisa Rodriguez interviews the head of research at <a href="https://ourworldindata.org/">Our World in Data</a> — Hannah Ritchie — on the case for environmental optimism.</p><p><a href="https://80k.info/hr"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>Why agricultural productivity in sub-Saharan Africa could be so important, and how much better things could get</li><li>Her new book about how we could be the first generation to build a sustainable planet</li><li>Whether climate change is the most worrying environmental issue</li><li>How we reduced outdoor air pollution</li><li>Why Hannah is worried about the state of ​​biodiversity</li><li>Solutions that address multiple environmental issues at once</li><li>How the world coordinated to address the hole in the ozone layer</li><li>Surprises from Our World in Data’s research</li><li>Psychological challenges that come up in Hannah’s work</li><li>And plenty more</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Milo McGuire and Dominic Armstrong<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/3cad2cf7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#159 – Jan Leike on OpenAI's massive push to make superintelligence safe in 4 years or less</title>
      <itunes:title>#159 – Jan Leike on OpenAI's massive push to make superintelligence safe in 4 years or less</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">36aa2290-7232-489d-80d6-f13971c6c12a</guid>
      <link>https://80000hours.org/podcast/episodes/jan-leike-superalignment/?utm_campaign=podcast__jan-leike&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>In July, OpenAI announced a new team and project: <a href="https://openai.com/blog/introducing-superalignment">Superalignment</a>. The goal is to figure out how to make superintelligent AI systems aligned and safe to use within four years, and the lab is putting a massive 20% of its computational resources behind the effort.</p><p>Today's guest, Jan Leike, is Head of Alignment at OpenAI and will be co-leading the project. As OpenAI puts it, "...the vast power of superintelligence could be very dangerous, and lead to the disempowerment of humanity or even human extinction. ... Currently, we don't have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue."</p><p><a href="https://80k.info/jl23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Given that OpenAI is in the business of developing superintelligent AI, it sees that as a scary problem that urgently has to be fixed. So it’s not just throwing compute at the problem -- it’s also hiring dozens of scientists and engineers to build out the Superalignment team.</p><p>Plenty of people are pessimistic that this can be done at all, let alone in four years. But Jan is guardedly optimistic. As he explains: </p>Honestly, it really feels like we have a real angle of attack on the problem that we can actually iterate on... and I think it's pretty likely going to work, actually. And that's really, really wild, and it's really exciting. It's like we have this hard problem that we've been talking about for years and years and years, and now we have a real shot at actually solving it. And that'd be so good if we did.<p><br>Jan thinks that this work is actually the most scientifically interesting part of machine learning. Rather than just throwing more chips and more data at a training run, this work requires actually understanding how these models work and how they think. The answers are likely to be breakthroughs on the level of solving the mysteries of the human brain.</p><p>The plan, in a nutshell, is to get AI to help us solve alignment. That might sound a bit crazy -- as one person described it, “like using one fire to put out another fire.”</p><p>But Jan’s thinking is this: the core problem is that AI capabilities will keep getting better and the challenge of monitoring cutting-edge models will keep getting harder, while human intelligence stays more or less the same. To have any hope of ensuring safety, we need our ability to monitor, understand, and design ML models to advance at the same pace as the complexity of the models themselves. </p><p>And there's an obvious way to do that: get AI to do most of the work, such that the sophistication of the AIs that need aligning, and the sophistication of the AIs doing the aligning, advance in lockstep.</p><p>Jan doesn't want to produce machine learning models capable of doing ML research. But such models are coming, whether we like it or not. And at that point Jan wants to make sure we turn them towards useful alignment and safety work, as much or more than we use them to advance AI capabilities.</p><p>Jan thinks it's so crazy it just might work. But some critics think it's simply crazy. They ask a wide range of difficult questions, including:</p><ul><li>If you don't know how to solve alignment, how can you tell that your alignment assistant AIs are actually acting in your interest rather than working against you? Especially as they could just be pretending to care about what you care about.</li><li>How do you know that these technical problems can be solved at all, even in principle?</li><li>At the point that models are able to help with alignment, won't they also be so good at improving capabilities that we're in the middle of an explosion in what AI can do?</li></ul><p><br>In today's interview host Rob Wiblin puts these doubts to Jan to hear how he responds to each, and they also cover:</p><ul><li>OpenAI's current plans to achieve 'superalignment' and the reasoning behind them</li><li>Why alignment work is the most fundamental and scientifically interesting research in ML</li><li>The kinds of people he’s excited to hire to join his team and maybe save the world</li><li>What most readers misunderstood about the OpenAI announcement</li><li>The three ways Jan expects AI to help solve alignment: mechanistic interpretability, generalization, and scalable oversight</li><li>What the standard should be for confirming whether Jan's team has succeeded</li><li>Whether OpenAI should (or will) commit to stop training more powerful general models if they don't think the alignment problem has been solved</li><li>Whether Jan thinks OpenAI has deployed models too quickly or too slowly</li><li>The many other actors who also have to do their jobs really well if we're going to have a good AI future</li><li>Plenty more</li></ul><p><br><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In July, OpenAI announced a new team and project: <a href="https://openai.com/blog/introducing-superalignment">Superalignment</a>. The goal is to figure out how to make superintelligent AI systems aligned and safe to use within four years, and the lab is putting a massive 20% of its computational resources behind the effort.</p><p>Today's guest, Jan Leike, is Head of Alignment at OpenAI and will be co-leading the project. As OpenAI puts it, "...the vast power of superintelligence could be very dangerous, and lead to the disempowerment of humanity or even human extinction. ... Currently, we don't have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue."</p><p><a href="https://80k.info/jl23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Given that OpenAI is in the business of developing superintelligent AI, it sees that as a scary problem that urgently has to be fixed. So it’s not just throwing compute at the problem -- it’s also hiring dozens of scientists and engineers to build out the Superalignment team.</p><p>Plenty of people are pessimistic that this can be done at all, let alone in four years. But Jan is guardedly optimistic. As he explains: </p>Honestly, it really feels like we have a real angle of attack on the problem that we can actually iterate on... and I think it's pretty likely going to work, actually. And that's really, really wild, and it's really exciting. It's like we have this hard problem that we've been talking about for years and years and years, and now we have a real shot at actually solving it. And that'd be so good if we did.<p><br>Jan thinks that this work is actually the most scientifically interesting part of machine learning. Rather than just throwing more chips and more data at a training run, this work requires actually understanding how these models work and how they think. The answers are likely to be breakthroughs on the level of solving the mysteries of the human brain.</p><p>The plan, in a nutshell, is to get AI to help us solve alignment. That might sound a bit crazy -- as one person described it, “like using one fire to put out another fire.”</p><p>But Jan’s thinking is this: the core problem is that AI capabilities will keep getting better and the challenge of monitoring cutting-edge models will keep getting harder, while human intelligence stays more or less the same. To have any hope of ensuring safety, we need our ability to monitor, understand, and design ML models to advance at the same pace as the complexity of the models themselves. </p><p>And there's an obvious way to do that: get AI to do most of the work, such that the sophistication of the AIs that need aligning, and the sophistication of the AIs doing the aligning, advance in lockstep.</p><p>Jan doesn't want to produce machine learning models capable of doing ML research. But such models are coming, whether we like it or not. And at that point Jan wants to make sure we turn them towards useful alignment and safety work, as much or more than we use them to advance AI capabilities.</p><p>Jan thinks it's so crazy it just might work. But some critics think it's simply crazy. They ask a wide range of difficult questions, including:</p><ul><li>If you don't know how to solve alignment, how can you tell that your alignment assistant AIs are actually acting in your interest rather than working against you? Especially as they could just be pretending to care about what you care about.</li><li>How do you know that these technical problems can be solved at all, even in principle?</li><li>At the point that models are able to help with alignment, won't they also be so good at improving capabilities that we're in the middle of an explosion in what AI can do?</li></ul><p><br>In today's interview host Rob Wiblin puts these doubts to Jan to hear how he responds to each, and they also cover:</p><ul><li>OpenAI's current plans to achieve 'superalignment' and the reasoning behind them</li><li>Why alignment work is the most fundamental and scientifically interesting research in ML</li><li>The kinds of people he’s excited to hire to join his team and maybe save the world</li><li>What most readers misunderstood about the OpenAI announcement</li><li>The three ways Jan expects AI to help solve alignment: mechanistic interpretability, generalization, and scalable oversight</li><li>What the standard should be for confirming whether Jan's team has succeeded</li><li>Whether OpenAI should (or will) commit to stop training more powerful general models if they don't think the alignment problem has been solved</li><li>Whether Jan thinks OpenAI has deployed models too quickly or too slowly</li><li>The many other actors who also have to do their jobs really well if we're going to have a good AI future</li><li>Plenty more</li></ul><p><br><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 07 Aug 2023 22:08:47 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3cf9e142/d7c35d12.mp3" length="82320499" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/H_yBUv9V7sBhkUouRVCWM5w-WcHiL-NxColxLG9ekr8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0NTIxNTYv/MTY5MTQ0NDczOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10280</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>In July, OpenAI announced a new team and project: <a href="https://openai.com/blog/introducing-superalignment">Superalignment</a>. The goal is to figure out how to make superintelligent AI systems aligned and safe to use within four years, and the lab is putting a massive 20% of its computational resources behind the effort.</p><p>Today's guest, Jan Leike, is Head of Alignment at OpenAI and will be co-leading the project. As OpenAI puts it, "...the vast power of superintelligence could be very dangerous, and lead to the disempowerment of humanity or even human extinction. ... Currently, we don't have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue."</p><p><a href="https://80k.info/jl23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Given that OpenAI is in the business of developing superintelligent AI, it sees that as a scary problem that urgently has to be fixed. So it’s not just throwing compute at the problem -- it’s also hiring dozens of scientists and engineers to build out the Superalignment team.</p><p>Plenty of people are pessimistic that this can be done at all, let alone in four years. But Jan is guardedly optimistic. As he explains: </p>Honestly, it really feels like we have a real angle of attack on the problem that we can actually iterate on... and I think it's pretty likely going to work, actually. And that's really, really wild, and it's really exciting. It's like we have this hard problem that we've been talking about for years and years and years, and now we have a real shot at actually solving it. And that'd be so good if we did.<p><br>Jan thinks that this work is actually the most scientifically interesting part of machine learning. Rather than just throwing more chips and more data at a training run, this work requires actually understanding how these models work and how they think. The answers are likely to be breakthroughs on the level of solving the mysteries of the human brain.</p><p>The plan, in a nutshell, is to get AI to help us solve alignment. That might sound a bit crazy -- as one person described it, “like using one fire to put out another fire.”</p><p>But Jan’s thinking is this: the core problem is that AI capabilities will keep getting better and the challenge of monitoring cutting-edge models will keep getting harder, while human intelligence stays more or less the same. To have any hope of ensuring safety, we need our ability to monitor, understand, and design ML models to advance at the same pace as the complexity of the models themselves. </p><p>And there's an obvious way to do that: get AI to do most of the work, such that the sophistication of the AIs that need aligning, and the sophistication of the AIs doing the aligning, advance in lockstep.</p><p>Jan doesn't want to produce machine learning models capable of doing ML research. But such models are coming, whether we like it or not. And at that point Jan wants to make sure we turn them towards useful alignment and safety work, as much or more than we use them to advance AI capabilities.</p><p>Jan thinks it's so crazy it just might work. But some critics think it's simply crazy. They ask a wide range of difficult questions, including:</p><ul><li>If you don't know how to solve alignment, how can you tell that your alignment assistant AIs are actually acting in your interest rather than working against you? Especially as they could just be pretending to care about what you care about.</li><li>How do you know that these technical problems can be solved at all, even in principle?</li><li>At the point that models are able to help with alignment, won't they also be so good at improving capabilities that we're in the middle of an explosion in what AI can do?</li></ul><p><br>In today's interview host Rob Wiblin puts these doubts to Jan to hear how he responds to each, and they also cover:</p><ul><li>OpenAI's current plans to achieve 'superalignment' and the reasoning behind them</li><li>Why alignment work is the most fundamental and scientifically interesting research in ML</li><li>The kinds of people he’s excited to hire to join his team and maybe save the world</li><li>What most readers misunderstood about the OpenAI announcement</li><li>The three ways Jan expects AI to help solve alignment: mechanistic interpretability, generalization, and scalable oversight</li><li>What the standard should be for confirming whether Jan's team has succeeded</li><li>Whether OpenAI should (or will) commit to stop training more powerful general models if they don't think the alignment problem has been solved</li><li>Whether Jan thinks OpenAI has deployed models too quickly or too slowly</li><li>The many other actors who also have to do their jobs really well if we're going to have a good AI future</li><li>Plenty more</li></ul><p><br><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer and editor: Keiran Harris<br>Audio Engineering Lead: Ben Cordell<br>Technical editing: Simon Monsour and Milo McGuire<br>Additional content editing: Katy Moore and Luisa Rodriguez<br>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/3cf9e142/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>We now offer shorter 'interview highlights' episodes</title>
      <itunes:title>We now offer shorter 'interview highlights' episodes</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6ba8c6a4-eba9-4646-ae38-aea8bda41b2c</guid>
      <link>https://share.transistor.fm/s/76bdf292</link>
      <description>
        <![CDATA[<p>Over on our other feed, <a href="https://80000hours.org/after-hours-podcast/">80k After Hours</a>, you can now find 20-30 minute highlights episodes of our 80,000 Hours Podcast interviews. These aren’t necessarily the most important parts of the interview, and if a topic matters to you we do recommend listening to the full episode — but we think these will be a nice upgrade on skipping episodes entirely.</p><p><strong>Get these highlight episodes by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type 80k After Hours into your podcasting app.</strong></p><p><em>Highlights put together by Simon Monsour and Milo McGuire</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Over on our other feed, <a href="https://80000hours.org/after-hours-podcast/">80k After Hours</a>, you can now find 20-30 minute highlights episodes of our 80,000 Hours Podcast interviews. These aren’t necessarily the most important parts of the interview, and if a topic matters to you we do recommend listening to the full episode — but we think these will be a nice upgrade on skipping episodes entirely.</p><p><strong>Get these highlight episodes by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type 80k After Hours into your podcasting app.</strong></p><p><em>Highlights put together by Simon Monsour and Milo McGuire</em></p>]]>
      </content:encoded>
      <pubDate>Sat, 05 Aug 2023 07:44:29 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/76bdf292/496bcc57.mp3" length="4496623" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/4TGgS6yFlxiILIlONBwOB25QfOKjj6bkqzdwXiDiAjU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0NDg1MTIv/MTY5MTIyMTMyOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>370</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Over on our other feed, <a href="https://80000hours.org/after-hours-podcast/">80k After Hours</a>, you can now find 20-30 minute highlights episodes of our 80,000 Hours Podcast interviews. These aren’t necessarily the most important parts of the interview, and if a topic matters to you we do recommend listening to the full episode — but we think these will be a nice upgrade on skipping episodes entirely.</p><p><strong>Get these highlight episodes by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type 80k After Hours into your podcasting app.</strong></p><p><em>Highlights put together by Simon Monsour and Milo McGuire</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#158 – Holden Karnofsky on how AIs might take over even if they're no smarter than humans, and his 4-part playbook for AI risk</title>
      <itunes:title>#158 – Holden Karnofsky on how AIs might take over even if they're no smarter than humans, and his 4-part playbook for AI risk</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c6c3cd4f-c409-4776-9ac6-93e9a69ca376</guid>
      <link>https://80000hours.org/podcast/episodes/holden-karnofsky-how-ai-could-take-over-the-world/?utm_campaign=podcast__holden-karnofsky&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Back in 2007, Holden Karnofsky cofounded <a href="https://www.givewell.org/">GiveWell</a>, where he sought out the charities that most cost-effectively helped save lives. He then cofounded <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>, where he oversaw a team making billions of dollars’ worth of grants across a range of areas: pandemic control, criminal justice reform, farmed animal welfare, and making AI safe, among others. This year, having learned about AI for years and observed recent events, he's narrowing his focus once again, this time on making the transition to advanced AI go well.</p><p>In today's conversation, Holden returns to the show to share his overall understanding of the promise and the risks posed by machine intelligence, and what to do about it. That understanding has accumulated over around 14 years, during which he went from being sceptical that AI was important or risky, to making AI risks the focus of his work.</p><p><a href="https://80k.info/hk23"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>(As Holden reminds us, his wife is also the president of one of the world's top AI labs, <a href="https://www.anthropic.com/">Anthropic</a>, giving him both conflicts of interest and a front-row seat to recent events. For our part, Open Philanthropy is 80,000 Hours' largest financial supporter.)</p><p>One point he makes is that people are too narrowly focused on AI becoming 'superintelligent.' While that could happen and would be important, it's not necessary for AI to be transformative or perilous. Rather, machines with human levels of intelligence could end up being enormously influential simply if the amount of computer hardware globally were able to operate tens or hundreds of billions of them, in a sense making machine intelligences a majority of the global population, or at least a majority of global thought.</p><p>As Holden explains, he sees four key parts to the playbook humanity should use to guide the transition to very advanced AI in a positive direction: alignment research, standards and monitoring, creating a successful and careful AI lab, and finally, information security.</p><p>In today’s episode, host Rob Wiblin interviews return guest Holden Karnofsky about that playbook, as well as:</p><ul><li>Why we can’t rely on just gradually solving those problems as they come up, the way we usually do with new technologies.</li><li>What multiple different groups can do to improve our chances of a good outcome — including listeners to this show, governments, computer security experts, and journalists.</li><li>Holden’s case against 'hardcore utilitarianism' and what actually motivates him to work hard for a better world.</li><li>What the ML and AI safety communities get wrong in Holden's view.</li><li>Ways we might succeed with AI just by dumb luck.</li><li>The value of laying out imaginable success stories.</li><li>Why information security is so important and underrated.</li><li>Whether it's good to work at an AI lab that you think is particularly careful.</li><li>The track record of futurists’ predictions.</li><li>And much more.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Simon Monsour and Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Back in 2007, Holden Karnofsky cofounded <a href="https://www.givewell.org/">GiveWell</a>, where he sought out the charities that most cost-effectively helped save lives. He then cofounded <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>, where he oversaw a team making billions of dollars’ worth of grants across a range of areas: pandemic control, criminal justice reform, farmed animal welfare, and making AI safe, among others. This year, having learned about AI for years and observed recent events, he's narrowing his focus once again, this time on making the transition to advanced AI go well.</p><p>In today's conversation, Holden returns to the show to share his overall understanding of the promise and the risks posed by machine intelligence, and what to do about it. That understanding has accumulated over around 14 years, during which he went from being sceptical that AI was important or risky, to making AI risks the focus of his work.</p><p><a href="https://80k.info/hk23"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>(As Holden reminds us, his wife is also the president of one of the world's top AI labs, <a href="https://www.anthropic.com/">Anthropic</a>, giving him both conflicts of interest and a front-row seat to recent events. For our part, Open Philanthropy is 80,000 Hours' largest financial supporter.)</p><p>One point he makes is that people are too narrowly focused on AI becoming 'superintelligent.' While that could happen and would be important, it's not necessary for AI to be transformative or perilous. Rather, machines with human levels of intelligence could end up being enormously influential simply if the amount of computer hardware globally were able to operate tens or hundreds of billions of them, in a sense making machine intelligences a majority of the global population, or at least a majority of global thought.</p><p>As Holden explains, he sees four key parts to the playbook humanity should use to guide the transition to very advanced AI in a positive direction: alignment research, standards and monitoring, creating a successful and careful AI lab, and finally, information security.</p><p>In today’s episode, host Rob Wiblin interviews return guest Holden Karnofsky about that playbook, as well as:</p><ul><li>Why we can’t rely on just gradually solving those problems as they come up, the way we usually do with new technologies.</li><li>What multiple different groups can do to improve our chances of a good outcome — including listeners to this show, governments, computer security experts, and journalists.</li><li>Holden’s case against 'hardcore utilitarianism' and what actually motivates him to work hard for a better world.</li><li>What the ML and AI safety communities get wrong in Holden's view.</li><li>Ways we might succeed with AI just by dumb luck.</li><li>The value of laying out imaginable success stories.</li><li>Why information security is so important and underrated.</li><li>Whether it's good to work at an AI lab that you think is particularly careful.</li><li>The track record of futurists’ predictions.</li><li>And much more.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Simon Monsour and Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 31 Jul 2023 23:30:33 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/960fe30e/f538a9bc.mp3" length="92970375" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Cp269nnS8HrCS00wW7kg6Hk2Hi3FMxE7_RNA_AV0qDQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0NDA5NzEv/MTY5MDg0MTc1Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11613</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Back in 2007, Holden Karnofsky cofounded <a href="https://www.givewell.org/">GiveWell</a>, where he sought out the charities that most cost-effectively helped save lives. He then cofounded <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>, where he oversaw a team making billions of dollars’ worth of grants across a range of areas: pandemic control, criminal justice reform, farmed animal welfare, and making AI safe, among others. This year, having learned about AI for years and observed recent events, he's narrowing his focus once again, this time on making the transition to advanced AI go well.</p><p>In today's conversation, Holden returns to the show to share his overall understanding of the promise and the risks posed by machine intelligence, and what to do about it. That understanding has accumulated over around 14 years, during which he went from being sceptical that AI was important or risky, to making AI risks the focus of his work.</p><p><a href="https://80k.info/hk23"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>(As Holden reminds us, his wife is also the president of one of the world's top AI labs, <a href="https://www.anthropic.com/">Anthropic</a>, giving him both conflicts of interest and a front-row seat to recent events. For our part, Open Philanthropy is 80,000 Hours' largest financial supporter.)</p><p>One point he makes is that people are too narrowly focused on AI becoming 'superintelligent.' While that could happen and would be important, it's not necessary for AI to be transformative or perilous. Rather, machines with human levels of intelligence could end up being enormously influential simply if the amount of computer hardware globally were able to operate tens or hundreds of billions of them, in a sense making machine intelligences a majority of the global population, or at least a majority of global thought.</p><p>As Holden explains, he sees four key parts to the playbook humanity should use to guide the transition to very advanced AI in a positive direction: alignment research, standards and monitoring, creating a successful and careful AI lab, and finally, information security.</p><p>In today’s episode, host Rob Wiblin interviews return guest Holden Karnofsky about that playbook, as well as:</p><ul><li>Why we can’t rely on just gradually solving those problems as they come up, the way we usually do with new technologies.</li><li>What multiple different groups can do to improve our chances of a good outcome — including listeners to this show, governments, computer security experts, and journalists.</li><li>Holden’s case against 'hardcore utilitarianism' and what actually motivates him to work hard for a better world.</li><li>What the ML and AI safety communities get wrong in Holden's view.</li><li>Ways we might succeed with AI just by dumb luck.</li><li>The value of laying out imaginable success stories.</li><li>Why information security is so important and underrated.</li><li>Whether it's good to work at an AI lab that you think is particularly careful.</li><li>The track record of futurists’ predictions.</li><li>And much more.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Simon Monsour and Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/960fe30e/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/960fe30e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#157 – Ezra Klein on existential risk from AI and what DC could do about it</title>
      <itunes:title>#157 – Ezra Klein on existential risk from AI and what DC could do about it</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">5710b5ee-8c4e-47ae-8f84-c4f9ca677582</guid>
      <link>https://80000hours.org/podcast/episodes/ezra-klein-ai-and-dc/?utm_campaign=podcast__ezra-klein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>In <em>Oppenheimer</em>, scientists detonate a nuclear weapon despite thinking there's some 'near zero' chance it would ignite the atmosphere, putting an end to life on Earth. Today, scientists working on AI think the chance their work puts an end to humanity is vastly higher than that.</p><p>In response, some have suggested we launch a Manhattan Project to make AI safe via enormous investment in relevant R&amp;D. Others have suggested that we need international organisations modelled on those that slowed the proliferation of nuclear weapons. Others still seek a research slowdown by labs while an auditing and licencing scheme is created.</p><p>Today's guest — journalist Ezra Klein of <em>The New York Times</em> — has watched policy discussions and legislative battles play out in DC for 20 years.</p><p><a href="https://80k.info/EK"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Like many people he has also taken a big interest in AI this year, writing articles such as “<a href="https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html">This changes everything</a>.” In his <a href="https://80000hours.org/podcast/episodes/ezra-klein-journalism-most-important-topics/">first interview</a> on the show in 2021, he flagged AI as one topic that DC would regret not having paid more attention to. So we invited him on to get his take on which regulatory proposals have promise, and which seem either unhelpful or politically unviable.</p><p>Out of the ideas on the table right now, Ezra favours a focus on direct government funding — both for AI safety research and to develop AI models designed to solve problems other than making money for their operators. He is sympathetic to legislation that would require AI models to be legible in a way that none currently are — and embraces the fact that that will slow down the release of models while businesses figure out how their products actually work.</p><p>By contrast, he's pessimistic that it's possible to coordinate countries around the world to agree to prevent or delay the deployment of dangerous AI models — at least not unless there's some spectacular AI-related disaster to create such a consensus. And he fears attempts to require licences to train the most powerful ML models will struggle unless they can find a way to exclude and thereby appease people working on relatively safe consumer technologies rather than cutting-edge research.</p><p>From observing how DC works, Ezra expects that even a small community of experts in AI governance can have a large influence on how the the US government responds to AI advances. But in Ezra's view, that requires those experts to move to DC and spend years building relationships with people in government, rather than clustering elsewhere in academia and AI labs.</p><p>In today's brisk conversation, Ezra and host Rob Wiblin cover the above as well as:<strong></strong></p><p>They cover:</p><ul><li>Whether it's desirable to slow down AI research</li><li>The value of engaging with current policy debates even if they don't seem directly important</li><li>Which AI business models seem more or less dangerous</li><li>Tensions between people focused on existing vs emergent risks from AI</li><li>Two major challenges of being a new parent</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In <em>Oppenheimer</em>, scientists detonate a nuclear weapon despite thinking there's some 'near zero' chance it would ignite the atmosphere, putting an end to life on Earth. Today, scientists working on AI think the chance their work puts an end to humanity is vastly higher than that.</p><p>In response, some have suggested we launch a Manhattan Project to make AI safe via enormous investment in relevant R&amp;D. Others have suggested that we need international organisations modelled on those that slowed the proliferation of nuclear weapons. Others still seek a research slowdown by labs while an auditing and licencing scheme is created.</p><p>Today's guest — journalist Ezra Klein of <em>The New York Times</em> — has watched policy discussions and legislative battles play out in DC for 20 years.</p><p><a href="https://80k.info/EK"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Like many people he has also taken a big interest in AI this year, writing articles such as “<a href="https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html">This changes everything</a>.” In his <a href="https://80000hours.org/podcast/episodes/ezra-klein-journalism-most-important-topics/">first interview</a> on the show in 2021, he flagged AI as one topic that DC would regret not having paid more attention to. So we invited him on to get his take on which regulatory proposals have promise, and which seem either unhelpful or politically unviable.</p><p>Out of the ideas on the table right now, Ezra favours a focus on direct government funding — both for AI safety research and to develop AI models designed to solve problems other than making money for their operators. He is sympathetic to legislation that would require AI models to be legible in a way that none currently are — and embraces the fact that that will slow down the release of models while businesses figure out how their products actually work.</p><p>By contrast, he's pessimistic that it's possible to coordinate countries around the world to agree to prevent or delay the deployment of dangerous AI models — at least not unless there's some spectacular AI-related disaster to create such a consensus. And he fears attempts to require licences to train the most powerful ML models will struggle unless they can find a way to exclude and thereby appease people working on relatively safe consumer technologies rather than cutting-edge research.</p><p>From observing how DC works, Ezra expects that even a small community of experts in AI governance can have a large influence on how the the US government responds to AI advances. But in Ezra's view, that requires those experts to move to DC and spend years building relationships with people in government, rather than clustering elsewhere in academia and AI labs.</p><p>In today's brisk conversation, Ezra and host Rob Wiblin cover the above as well as:<strong></strong></p><p>They cover:</p><ul><li>Whether it's desirable to slow down AI research</li><li>The value of engaging with current policy debates even if they don't seem directly important</li><li>Which AI business models seem more or less dangerous</li><li>Tensions between people focused on existing vs emergent risks from AI</li><li>Two major challenges of being a new parent</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 24 Jul 2023 21:21:52 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/fca784d9/030cfab5.mp3" length="37845640" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/jBWe6_s1VYTNOouP1ZCZeYZp-HOtDsQoG7NY71TjxgA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0MzEwNTAv/MTY5MDIzMzI4Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4726</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>In <em>Oppenheimer</em>, scientists detonate a nuclear weapon despite thinking there's some 'near zero' chance it would ignite the atmosphere, putting an end to life on Earth. Today, scientists working on AI think the chance their work puts an end to humanity is vastly higher than that.</p><p>In response, some have suggested we launch a Manhattan Project to make AI safe via enormous investment in relevant R&amp;D. Others have suggested that we need international organisations modelled on those that slowed the proliferation of nuclear weapons. Others still seek a research slowdown by labs while an auditing and licencing scheme is created.</p><p>Today's guest — journalist Ezra Klein of <em>The New York Times</em> — has watched policy discussions and legislative battles play out in DC for 20 years.</p><p><a href="https://80k.info/EK"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Like many people he has also taken a big interest in AI this year, writing articles such as “<a href="https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html">This changes everything</a>.” In his <a href="https://80000hours.org/podcast/episodes/ezra-klein-journalism-most-important-topics/">first interview</a> on the show in 2021, he flagged AI as one topic that DC would regret not having paid more attention to. So we invited him on to get his take on which regulatory proposals have promise, and which seem either unhelpful or politically unviable.</p><p>Out of the ideas on the table right now, Ezra favours a focus on direct government funding — both for AI safety research and to develop AI models designed to solve problems other than making money for their operators. He is sympathetic to legislation that would require AI models to be legible in a way that none currently are — and embraces the fact that that will slow down the release of models while businesses figure out how their products actually work.</p><p>By contrast, he's pessimistic that it's possible to coordinate countries around the world to agree to prevent or delay the deployment of dangerous AI models — at least not unless there's some spectacular AI-related disaster to create such a consensus. And he fears attempts to require licences to train the most powerful ML models will struggle unless they can find a way to exclude and thereby appease people working on relatively safe consumer technologies rather than cutting-edge research.</p><p>From observing how DC works, Ezra expects that even a small community of experts in AI governance can have a large influence on how the the US government responds to AI advances. But in Ezra's view, that requires those experts to move to DC and spend years building relationships with people in government, rather than clustering elsewhere in academia and AI labs.</p><p>In today's brisk conversation, Ezra and host Rob Wiblin cover the above as well as:<strong></strong></p><p>They cover:</p><ul><li>Whether it's desirable to slow down AI research</li><li>The value of engaging with current policy debates even if they don't seem directly important</li><li>Which AI business models seem more or less dangerous</li><li>Tensions between people focused on existing vs emergent risks from AI</li><li>Two major challenges of being a new parent</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/fca784d9/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#156 – Markus Anderljung on how to regulate cutting-edge AI models</title>
      <itunes:title>#156 – Markus Anderljung on how to regulate cutting-edge AI models</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2becbad4-9370-4b1e-8a06-7a243643268a</guid>
      <link>https://80000hours.org/podcast/episodes/markus-anderljung-regulating-cutting-edge-ai/?utm_campaign=podcast__markus-anderljung&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>"At the front of the pack we have these frontier AI developers, and we want them to identify particularly dangerous models ahead of time. Once those mines have been discovered, and the frontier developers keep walking down the minefield, there's going to be all these other people who follow along. And then a really important thing is to make sure that they don't step on the same mines. So you need to put a flag down -- not on the mine, but maybe next to it. </p><p>And so what that looks like in practice is maybe once we find that if you train a model in such-and-such a way, then it can produce maybe biological weapons is a useful example, or maybe it has very offensive cyber capabilities that are difficult to defend against. In that case, we just need the regulation to be such that you can't develop those kinds of models." — Markus Anderljung</p><p>In today’s episode, host Luisa Rodriguez interviews the Head of Policy at the Centre for the Governance of AI — Markus Anderljung — about all aspects of policy and governance of superhuman AI systems.</p><p><a href="https://80k.info/ma"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The need for AI governance, including self-replicating models and ChaosGPT</li><li>Whether or not AI companies will willingly accept regulation</li><li>The key regulatory strategies including licencing, risk assessment, auditing, and post-deployment monitoring</li><li>Whether we can be confident that people won't train models covertly and ignore the licencing system</li><li>The progress we’ve made so far in AI governance</li><li>The key weaknesses of these approaches</li><li>The need for external scrutiny of powerful models</li><li>The emergent capabilities problem</li><li>Why it really matters where regulation happens</li><li>Advice for people wanting to pursue a career in this field</li><li>And much more.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Simon Monsour and Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>"At the front of the pack we have these frontier AI developers, and we want them to identify particularly dangerous models ahead of time. Once those mines have been discovered, and the frontier developers keep walking down the minefield, there's going to be all these other people who follow along. And then a really important thing is to make sure that they don't step on the same mines. So you need to put a flag down -- not on the mine, but maybe next to it. </p><p>And so what that looks like in practice is maybe once we find that if you train a model in such-and-such a way, then it can produce maybe biological weapons is a useful example, or maybe it has very offensive cyber capabilities that are difficult to defend against. In that case, we just need the regulation to be such that you can't develop those kinds of models." — Markus Anderljung</p><p>In today’s episode, host Luisa Rodriguez interviews the Head of Policy at the Centre for the Governance of AI — Markus Anderljung — about all aspects of policy and governance of superhuman AI systems.</p><p><a href="https://80k.info/ma"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The need for AI governance, including self-replicating models and ChaosGPT</li><li>Whether or not AI companies will willingly accept regulation</li><li>The key regulatory strategies including licencing, risk assessment, auditing, and post-deployment monitoring</li><li>Whether we can be confident that people won't train models covertly and ignore the licencing system</li><li>The progress we’ve made so far in AI governance</li><li>The key weaknesses of these approaches</li><li>The need for external scrutiny of powerful models</li><li>The emergent capabilities problem</li><li>Why it really matters where regulation happens</li><li>Advice for people wanting to pursue a career in this field</li><li>And much more.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Simon Monsour and Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 10 Jul 2023 20:50:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f32070b3/d5200eca.mp3" length="60812076" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/51oewRX7sjAutp46Pqm84THDVtuH69kd9yuxcnojGTI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0MTUwMzMv/MTY4OTAyMDQ3Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7596</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>"At the front of the pack we have these frontier AI developers, and we want them to identify particularly dangerous models ahead of time. Once those mines have been discovered, and the frontier developers keep walking down the minefield, there's going to be all these other people who follow along. And then a really important thing is to make sure that they don't step on the same mines. So you need to put a flag down -- not on the mine, but maybe next to it. </p><p>And so what that looks like in practice is maybe once we find that if you train a model in such-and-such a way, then it can produce maybe biological weapons is a useful example, or maybe it has very offensive cyber capabilities that are difficult to defend against. In that case, we just need the regulation to be such that you can't develop those kinds of models." — Markus Anderljung</p><p>In today’s episode, host Luisa Rodriguez interviews the Head of Policy at the Centre for the Governance of AI — Markus Anderljung — about all aspects of policy and governance of superhuman AI systems.</p><p><a href="https://80k.info/ma"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>They cover:</p><ul><li>The need for AI governance, including self-replicating models and ChaosGPT</li><li>Whether or not AI companies will willingly accept regulation</li><li>The key regulatory strategies including licencing, risk assessment, auditing, and post-deployment monitoring</li><li>Whether we can be confident that people won't train models covertly and ignore the licencing system</li><li>The progress we’ve made so far in AI governance</li><li>The key weaknesses of these approaches</li><li>The need for external scrutiny of powerful models</li><li>The emergent capabilities problem</li><li>Why it really matters where regulation happens</li><li>Advice for people wanting to pursue a career in this field</li><li>And much more.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris<br>Audio Engineering Lead: Ben Cordell</em></p><p><em>Technical editing: Simon Monsour and Milo McGuire</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f32070b3/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f32070b3/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Bonus: The Worst Ideas in the History of the World</title>
      <itunes:title>Bonus: The Worst Ideas in the History of the World</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d8a6e9c5-c840-4427-a386-afc2b9eaef3c</guid>
      <link>https://share.transistor.fm/s/16ec5660</link>
      <description>
        <![CDATA[<p>Today’s bonus release is a pilot for a new podcast called ‘The Worst Ideas in the History of the World’, created by Keiran Harris — producer of the 80,000 Hours Podcast.</p><p>If you have strong opinions about this one way or another, please email us at podcast@80000hours.org to help us figure out whether more of this ought to exist.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The Worst Ideas in the History of the World (00:00:51)</li><li>My history with longtermism (00:04:01)</li><li>Outlining the format (00:06:17)</li><li>Will MacAskill’s basic case (00:07:38)</li><li>5 reasons for why future people might not matter morally (00:10:26)</li><li>Whether we can reasonably hope to influence the future (00:15:53)</li><li>Great power wars (00:18:55)</li><li>Nuclear weapons (00:22:27)</li><li>Gain-of-function research (00:28:31)</li><li>Closer (00:33:02)</li><li>Rob's outro (00:35:13)</li></ul>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s bonus release is a pilot for a new podcast called ‘The Worst Ideas in the History of the World’, created by Keiran Harris — producer of the 80,000 Hours Podcast.</p><p>If you have strong opinions about this one way or another, please email us at podcast@80000hours.org to help us figure out whether more of this ought to exist.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The Worst Ideas in the History of the World (00:00:51)</li><li>My history with longtermism (00:04:01)</li><li>Outlining the format (00:06:17)</li><li>Will MacAskill’s basic case (00:07:38)</li><li>5 reasons for why future people might not matter morally (00:10:26)</li><li>Whether we can reasonably hope to influence the future (00:15:53)</li><li>Great power wars (00:18:55)</li><li>Nuclear weapons (00:22:27)</li><li>Gain-of-function research (00:28:31)</li><li>Closer (00:33:02)</li><li>Rob's outro (00:35:13)</li></ul>]]>
      </content:encoded>
      <pubDate>Fri, 30 Jun 2023 20:46:47 +0000</pubDate>
      <author>Keiran Harris</author>
      <enclosure url="https://media.transistor.fm/16ec5660/6182c440.mp3" length="17063238" type="audio/mpeg"/>
      <itunes:author>Keiran Harris</itunes:author>
      <itunes:image href="https://img.transistor.fm/N51Xos6s4oeY1JeVK2m7UJgGnZCzQgYb7rbjoGyAynQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzE0MDQ0OTkv/MTY4ODE1NzcxNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2124</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Today’s bonus release is a pilot for a new podcast called ‘The Worst Ideas in the History of the World’, created by Keiran Harris — producer of the 80,000 Hours Podcast.</p><p>If you have strong opinions about this one way or another, please email us at podcast@80000hours.org to help us figure out whether more of this ought to exist.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The Worst Ideas in the History of the World (00:00:51)</li><li>My history with longtermism (00:04:01)</li><li>Outlining the format (00:06:17)</li><li>Will MacAskill’s basic case (00:07:38)</li><li>5 reasons for why future people might not matter morally (00:10:26)</li><li>Whether we can reasonably hope to influence the future (00:15:53)</li><li>Great power wars (00:18:55)</li><li>Nuclear weapons (00:22:27)</li><li>Gain-of-function research (00:28:31)</li><li>Closer (00:33:02)</li><li>Rob's outro (00:35:13)</li></ul>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/16ec5660/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/16ec5660/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#155 – Lennart Heim on the compute governance era and what has to come after</title>
      <itunes:title>#155 – Lennart Heim on the compute governance era and what has to come after</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">77a04e48-9c88-442f-92a4-19055cb393c9</guid>
      <link>https://80000hours.org/podcast/episodes/lennart-heim-compute-governance/?utm_campaign=podcast__lennart-heim&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>As AI advances ever more quickly, concerns about potential misuse of highly capable models are growing. From hostile foreign governments and terrorists to reckless entrepreneurs, the threat of AI falling into the wrong hands is top of mind for the national security community.</p><p>With growing concerns about the use of AI in military applications, the US has banned the export of certain types of chips to China.</p><p>But unlike the uranium required to make nuclear weapons, or the material inputs to a bioweapons programme, computer chips and machine learning models are absolutely everywhere. So is it actually possible to keep dangerous capabilities out of the wrong hands?</p><p>In today's interview, Lennart Heim — who researches compute governance at the Centre for the Governance of AI — explains why limiting access to supercomputers may represent our best shot.</p><p><a href="https://80k.info/LH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>As Lennart explains, an AI research project requires many inputs, including the classic triad of compute, algorithms, and data.</p><p>If we want to limit access to the most advanced AI models, focusing on access to supercomputing resources -- usually called 'compute' -- might be the way to go. Both algorithms and data are hard to control because they live on hard drives and can be easily copied. By contrast, advanced chips are physical items that can't be used by multiple people at once and come from a small number of sources.</p><p>According to Lennart, the hope would be to enforce AI safety regulations by controlling access to the most advanced chips specialised for AI applications. For instance, projects training 'frontier' AI models — the newest and most capable models — might only gain access to the supercomputers they need if they obtain a licence and follow industry best practices.</p><p>We have similar safety rules for companies that fly planes or manufacture volatile chemicals — so why not for people producing the most powerful and perhaps the most dangerous technology humanity has ever played with?</p><p>But Lennart is quick to note that the approach faces many practical challenges. Currently, AI chips are readily available and untracked. Changing that will require the collaboration of many actors, which might be difficult, especially given that some of them aren't convinced of the seriousness of the problem.</p><p>Host Rob Wiblin is particularly concerned about a different challenge: the increasing efficiency of AI training algorithms. As these algorithms become more efficient, what once required a specialised AI supercomputer to train might soon be achievable with a home computer.</p><p>By that point, tracking every aggregation of compute that could prove to be very dangerous would be both impractical and invasive.</p><p>With only a decade or two left before that becomes a reality, the window during which compute governance is a viable solution may be a brief one. Top AI labs have already stopped publishing their latest algorithms, which might extend this 'compute governance era', but not for very long.</p><p>If compute governance is only a temporary phase between the era of difficult-to-train superhuman AI models and the time when such models are widely accessible, what can we do to prevent misuse of AI systems after that point?</p><p>Lennart and Rob both think the only enduring approach requires taking advantage of the AI capabilities that should be in the hands of police and governments — which will hopefully remain superior to those held by criminals, terrorists, or fools. But as they describe, this means maintaining a peaceful standoff between AI models with conflicting goals that can act and fight with one another on the microsecond timescale. Being far too slow to follow what's happening -- let alone participate -- humans would have to be cut out of any defensive decision-making.</p><p>Both agree that while this may be our best option, such a vision of the future is more terrifying than reassuring.</p><p>Lennart and Rob discuss the above as well as:</p><ul><li>How can we best categorise all the ways AI could go wrong?</li><li>Why did the US restrict the export of some chips to China and what impact has that had?</li><li>Is the US in an 'arms race' with China or is that more an illusion?</li><li>What is the deal with chips specialised for AI applications?</li><li>How is the 'compute' industry organised?</li><li>Downsides of using compute as a target for regulations</li><li>Could safety mechanisms be built into computer chips themselves?</li><li>Who would have the legal authority to govern compute if some disaster made it seem necessary?</li><li>The reasons Rob doubts that any of this stuff will work</li><li>Could AI be trained to operate as a far more severe computer worm than any we've seen before?</li><li>What does the world look like when sluggish human reaction times leave us completely outclassed?</li><li>And plenty more</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:35)</li><li>What is compute exactly? (00:09:46)</li><li>Structural risks (00:13:25)</li><li>Why focus on compute? (00:21:43)</li><li>Weaknesses of targeting compute (00:30:41)</li><li>Chip specialisation (00:37:11)</li><li>Export restrictions (00:40:13)</li><li>Compute governance is happening (00:59:00)</li><li>Reactions to AI regulation (01:05:03)</li><li>Creating legal authority to intervene quickly (01:10:09)</li><li>Building mechanisms into chips themselves (01:18:57)</li><li>Rob not buying that any of this will work (01:39:28)</li><li>Are we doomed to become irrelevant? (01:59:10)</li><li>Rob’s computer security bad dreams (02:10:22)</li><li>Concrete advice (02:26:58)</li><li>Article reading: Information security in high-impact areas (02:49:36)</li><li>Rob’s outro (03:10:38)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire, Dominic Armstrong, and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>As AI advances ever more quickly, concerns about potential misuse of highly capable models are growing. From hostile foreign governments and terrorists to reckless entrepreneurs, the threat of AI falling into the wrong hands is top of mind for the national security community.</p><p>With growing concerns about the use of AI in military applications, the US has banned the export of certain types of chips to China.</p><p>But unlike the uranium required to make nuclear weapons, or the material inputs to a bioweapons programme, computer chips and machine learning models are absolutely everywhere. So is it actually possible to keep dangerous capabilities out of the wrong hands?</p><p>In today's interview, Lennart Heim — who researches compute governance at the Centre for the Governance of AI — explains why limiting access to supercomputers may represent our best shot.</p><p><a href="https://80k.info/LH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>As Lennart explains, an AI research project requires many inputs, including the classic triad of compute, algorithms, and data.</p><p>If we want to limit access to the most advanced AI models, focusing on access to supercomputing resources -- usually called 'compute' -- might be the way to go. Both algorithms and data are hard to control because they live on hard drives and can be easily copied. By contrast, advanced chips are physical items that can't be used by multiple people at once and come from a small number of sources.</p><p>According to Lennart, the hope would be to enforce AI safety regulations by controlling access to the most advanced chips specialised for AI applications. For instance, projects training 'frontier' AI models — the newest and most capable models — might only gain access to the supercomputers they need if they obtain a licence and follow industry best practices.</p><p>We have similar safety rules for companies that fly planes or manufacture volatile chemicals — so why not for people producing the most powerful and perhaps the most dangerous technology humanity has ever played with?</p><p>But Lennart is quick to note that the approach faces many practical challenges. Currently, AI chips are readily available and untracked. Changing that will require the collaboration of many actors, which might be difficult, especially given that some of them aren't convinced of the seriousness of the problem.</p><p>Host Rob Wiblin is particularly concerned about a different challenge: the increasing efficiency of AI training algorithms. As these algorithms become more efficient, what once required a specialised AI supercomputer to train might soon be achievable with a home computer.</p><p>By that point, tracking every aggregation of compute that could prove to be very dangerous would be both impractical and invasive.</p><p>With only a decade or two left before that becomes a reality, the window during which compute governance is a viable solution may be a brief one. Top AI labs have already stopped publishing their latest algorithms, which might extend this 'compute governance era', but not for very long.</p><p>If compute governance is only a temporary phase between the era of difficult-to-train superhuman AI models and the time when such models are widely accessible, what can we do to prevent misuse of AI systems after that point?</p><p>Lennart and Rob both think the only enduring approach requires taking advantage of the AI capabilities that should be in the hands of police and governments — which will hopefully remain superior to those held by criminals, terrorists, or fools. But as they describe, this means maintaining a peaceful standoff between AI models with conflicting goals that can act and fight with one another on the microsecond timescale. Being far too slow to follow what's happening -- let alone participate -- humans would have to be cut out of any defensive decision-making.</p><p>Both agree that while this may be our best option, such a vision of the future is more terrifying than reassuring.</p><p>Lennart and Rob discuss the above as well as:</p><ul><li>How can we best categorise all the ways AI could go wrong?</li><li>Why did the US restrict the export of some chips to China and what impact has that had?</li><li>Is the US in an 'arms race' with China or is that more an illusion?</li><li>What is the deal with chips specialised for AI applications?</li><li>How is the 'compute' industry organised?</li><li>Downsides of using compute as a target for regulations</li><li>Could safety mechanisms be built into computer chips themselves?</li><li>Who would have the legal authority to govern compute if some disaster made it seem necessary?</li><li>The reasons Rob doubts that any of this stuff will work</li><li>Could AI be trained to operate as a far more severe computer worm than any we've seen before?</li><li>What does the world look like when sluggish human reaction times leave us completely outclassed?</li><li>And plenty more</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:35)</li><li>What is compute exactly? (00:09:46)</li><li>Structural risks (00:13:25)</li><li>Why focus on compute? (00:21:43)</li><li>Weaknesses of targeting compute (00:30:41)</li><li>Chip specialisation (00:37:11)</li><li>Export restrictions (00:40:13)</li><li>Compute governance is happening (00:59:00)</li><li>Reactions to AI regulation (01:05:03)</li><li>Creating legal authority to intervene quickly (01:10:09)</li><li>Building mechanisms into chips themselves (01:18:57)</li><li>Rob not buying that any of this will work (01:39:28)</li><li>Are we doomed to become irrelevant? (01:59:10)</li><li>Rob’s computer security bad dreams (02:10:22)</li><li>Concrete advice (02:26:58)</li><li>Article reading: Information security in high-impact areas (02:49:36)</li><li>Rob’s outro (03:10:38)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire, Dominic Armstrong, and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 22 Jun 2023 23:25:17 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5726589c/4d792ab7.mp3" length="92551083" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/9ipxIUWe6HWfrlT_ZF4Doy-pp0Qv2qg_z77ZKUkGIAM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzOTQ4MzEv/MTY4NzQ3MzQ0Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11563</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>As AI advances ever more quickly, concerns about potential misuse of highly capable models are growing. From hostile foreign governments and terrorists to reckless entrepreneurs, the threat of AI falling into the wrong hands is top of mind for the national security community.</p><p>With growing concerns about the use of AI in military applications, the US has banned the export of certain types of chips to China.</p><p>But unlike the uranium required to make nuclear weapons, or the material inputs to a bioweapons programme, computer chips and machine learning models are absolutely everywhere. So is it actually possible to keep dangerous capabilities out of the wrong hands?</p><p>In today's interview, Lennart Heim — who researches compute governance at the Centre for the Governance of AI — explains why limiting access to supercomputers may represent our best shot.</p><p><a href="https://80k.info/LH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>As Lennart explains, an AI research project requires many inputs, including the classic triad of compute, algorithms, and data.</p><p>If we want to limit access to the most advanced AI models, focusing on access to supercomputing resources -- usually called 'compute' -- might be the way to go. Both algorithms and data are hard to control because they live on hard drives and can be easily copied. By contrast, advanced chips are physical items that can't be used by multiple people at once and come from a small number of sources.</p><p>According to Lennart, the hope would be to enforce AI safety regulations by controlling access to the most advanced chips specialised for AI applications. For instance, projects training 'frontier' AI models — the newest and most capable models — might only gain access to the supercomputers they need if they obtain a licence and follow industry best practices.</p><p>We have similar safety rules for companies that fly planes or manufacture volatile chemicals — so why not for people producing the most powerful and perhaps the most dangerous technology humanity has ever played with?</p><p>But Lennart is quick to note that the approach faces many practical challenges. Currently, AI chips are readily available and untracked. Changing that will require the collaboration of many actors, which might be difficult, especially given that some of them aren't convinced of the seriousness of the problem.</p><p>Host Rob Wiblin is particularly concerned about a different challenge: the increasing efficiency of AI training algorithms. As these algorithms become more efficient, what once required a specialised AI supercomputer to train might soon be achievable with a home computer.</p><p>By that point, tracking every aggregation of compute that could prove to be very dangerous would be both impractical and invasive.</p><p>With only a decade or two left before that becomes a reality, the window during which compute governance is a viable solution may be a brief one. Top AI labs have already stopped publishing their latest algorithms, which might extend this 'compute governance era', but not for very long.</p><p>If compute governance is only a temporary phase between the era of difficult-to-train superhuman AI models and the time when such models are widely accessible, what can we do to prevent misuse of AI systems after that point?</p><p>Lennart and Rob both think the only enduring approach requires taking advantage of the AI capabilities that should be in the hands of police and governments — which will hopefully remain superior to those held by criminals, terrorists, or fools. But as they describe, this means maintaining a peaceful standoff between AI models with conflicting goals that can act and fight with one another on the microsecond timescale. Being far too slow to follow what's happening -- let alone participate -- humans would have to be cut out of any defensive decision-making.</p><p>Both agree that while this may be our best option, such a vision of the future is more terrifying than reassuring.</p><p>Lennart and Rob discuss the above as well as:</p><ul><li>How can we best categorise all the ways AI could go wrong?</li><li>Why did the US restrict the export of some chips to China and what impact has that had?</li><li>Is the US in an 'arms race' with China or is that more an illusion?</li><li>What is the deal with chips specialised for AI applications?</li><li>How is the 'compute' industry organised?</li><li>Downsides of using compute as a target for regulations</li><li>Could safety mechanisms be built into computer chips themselves?</li><li>Who would have the legal authority to govern compute if some disaster made it seem necessary?</li><li>The reasons Rob doubts that any of this stuff will work</li><li>Could AI be trained to operate as a far more severe computer worm than any we've seen before?</li><li>What does the world look like when sluggish human reaction times leave us completely outclassed?</li><li>And plenty more</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:35)</li><li>What is compute exactly? (00:09:46)</li><li>Structural risks (00:13:25)</li><li>Why focus on compute? (00:21:43)</li><li>Weaknesses of targeting compute (00:30:41)</li><li>Chip specialisation (00:37:11)</li><li>Export restrictions (00:40:13)</li><li>Compute governance is happening (00:59:00)</li><li>Reactions to AI regulation (01:05:03)</li><li>Creating legal authority to intervene quickly (01:10:09)</li><li>Building mechanisms into chips themselves (01:18:57)</li><li>Rob not buying that any of this will work (01:39:28)</li><li>Are we doomed to become irrelevant? (01:59:10)</li><li>Rob’s computer security bad dreams (02:10:22)</li><li>Concrete advice (02:26:58)</li><li>Article reading: Information security in high-impact areas (02:49:36)</li><li>Rob’s outro (03:10:38)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire, Dominic Armstrong, and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/5726589c/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/5726589c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#154 - Rohin Shah on DeepMind and trying to fairly hear out both AI doomers and doubters</title>
      <itunes:title>#154 - Rohin Shah on DeepMind and trying to fairly hear out both AI doomers and doubters</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e24e6baf-93e7-40f0-bb53-76cdb36bd753</guid>
      <link>https://80000hours.org/podcast/episodes/rohin-shah-deepmind-doomers-and-doubters/?utm_campaign=podcast__rohin-shah&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Can there be a more exciting and strange place to work today than a leading AI lab? Your CEO has said they're worried your research could cause human extinction. The government is setting up meetings to discuss how this outcome can be avoided. Some of your colleagues think this is all overblown; others are more anxious still.</p><p>Today's guest — machine learning researcher Rohin Shah — goes into the Google DeepMind offices each day with that peculiar backdrop to his work. </p><p><a href="https://80k.info/RS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>He's on the team dedicated to maintaining 'technical AI safety' as these models approach and exceed human capabilities: basically that the models help humanity accomplish its goals without flipping out in some dangerous way. This work has never seemed more important.</p><p>In the short-term it could be the key bottleneck to deploying ML models in high-stakes real-life situations. In the long-term, it could be the difference between humanity thriving and disappearing entirely.</p><p>For years Rohin has been on a mission to fairly hear out people across the full spectrum of opinion about risks from artificial intelligence -- from doomers to doubters -- and properly understand their point of view. That makes him unusually well placed to give an overview of what we do and don't understand. He has landed somewhere in the middle — troubled by ways things could go wrong, but not convinced there are very strong reasons to expect a terrible outcome.</p><p>Today's conversation is wide-ranging and Rohin lays out many of his personal opinions to host Rob Wiblin, including:</p><ul><li>What he sees as the strongest case both for and against slowing down the rate of progress in AI research.</li><li>Why he disagrees with most other ML researchers that training a model on a sensible 'reward function' is enough to get a good outcome.</li><li>Why he disagrees with many on LessWrong that the bar for whether a safety technique is helpful is “could this contain a superintelligence.”</li><li>That he thinks nobody has very compelling arguments that AI created via machine learning will be dangerous by default, or that it will be safe by default. He believes we just don't know.</li><li>That he understands that analogies and visualisations are necessary for public communication, but is sceptical that they really help us understand what's going on with ML models, because they're different in important ways from every other case we might compare them to.</li><li>Why he's optimistic about DeepMind’s work on scalable oversight, mechanistic interpretability, and dangerous capabilities evaluations, and what each of those projects involves.</li><li>Why he isn't inherently worried about a future where we're surrounded by beings far more capable than us, so long as they share our goals to a reasonable degree.</li><li>Why it's not enough for humanity to know how to align AI models — it's essential that management at AI labs correctly pick which methods they're going to use and have the practical know-how to apply them properly.</li><li>Three observations that make him a little more optimistic: humans are a bit muddle-headed and not super goal-orientated; planes don't crash; and universities have specific majors in particular subjects.</li><li>Plenty more besides.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire, Dominic Armstrong, and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Can there be a more exciting and strange place to work today than a leading AI lab? Your CEO has said they're worried your research could cause human extinction. The government is setting up meetings to discuss how this outcome can be avoided. Some of your colleagues think this is all overblown; others are more anxious still.</p><p>Today's guest — machine learning researcher Rohin Shah — goes into the Google DeepMind offices each day with that peculiar backdrop to his work. </p><p><a href="https://80k.info/RS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>He's on the team dedicated to maintaining 'technical AI safety' as these models approach and exceed human capabilities: basically that the models help humanity accomplish its goals without flipping out in some dangerous way. This work has never seemed more important.</p><p>In the short-term it could be the key bottleneck to deploying ML models in high-stakes real-life situations. In the long-term, it could be the difference between humanity thriving and disappearing entirely.</p><p>For years Rohin has been on a mission to fairly hear out people across the full spectrum of opinion about risks from artificial intelligence -- from doomers to doubters -- and properly understand their point of view. That makes him unusually well placed to give an overview of what we do and don't understand. He has landed somewhere in the middle — troubled by ways things could go wrong, but not convinced there are very strong reasons to expect a terrible outcome.</p><p>Today's conversation is wide-ranging and Rohin lays out many of his personal opinions to host Rob Wiblin, including:</p><ul><li>What he sees as the strongest case both for and against slowing down the rate of progress in AI research.</li><li>Why he disagrees with most other ML researchers that training a model on a sensible 'reward function' is enough to get a good outcome.</li><li>Why he disagrees with many on LessWrong that the bar for whether a safety technique is helpful is “could this contain a superintelligence.”</li><li>That he thinks nobody has very compelling arguments that AI created via machine learning will be dangerous by default, or that it will be safe by default. He believes we just don't know.</li><li>That he understands that analogies and visualisations are necessary for public communication, but is sceptical that they really help us understand what's going on with ML models, because they're different in important ways from every other case we might compare them to.</li><li>Why he's optimistic about DeepMind’s work on scalable oversight, mechanistic interpretability, and dangerous capabilities evaluations, and what each of those projects involves.</li><li>Why he isn't inherently worried about a future where we're surrounded by beings far more capable than us, so long as they share our goals to a reasonable degree.</li><li>Why it's not enough for humanity to know how to align AI models — it's essential that management at AI labs correctly pick which methods they're going to use and have the practical know-how to apply them properly.</li><li>Three observations that make him a little more optimistic: humans are a bit muddle-headed and not super goal-orientated; planes don't crash; and universities have specific majors in particular subjects.</li><li>Plenty more besides.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire, Dominic Armstrong, and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 09 Jun 2023 20:15:37 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/867aaa90/ecba17f9.mp3" length="91183119" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/J7dZhhXBwWfBpaYYk7glGiB4O1oN8QF41Y7vuYtTosQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzNzgxODMv/MTY4NjMzOTQ1Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11382</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Can there be a more exciting and strange place to work today than a leading AI lab? Your CEO has said they're worried your research could cause human extinction. The government is setting up meetings to discuss how this outcome can be avoided. Some of your colleagues think this is all overblown; others are more anxious still.</p><p>Today's guest — machine learning researcher Rohin Shah — goes into the Google DeepMind offices each day with that peculiar backdrop to his work. </p><p><a href="https://80k.info/RS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>He's on the team dedicated to maintaining 'technical AI safety' as these models approach and exceed human capabilities: basically that the models help humanity accomplish its goals without flipping out in some dangerous way. This work has never seemed more important.</p><p>In the short-term it could be the key bottleneck to deploying ML models in high-stakes real-life situations. In the long-term, it could be the difference between humanity thriving and disappearing entirely.</p><p>For years Rohin has been on a mission to fairly hear out people across the full spectrum of opinion about risks from artificial intelligence -- from doomers to doubters -- and properly understand their point of view. That makes him unusually well placed to give an overview of what we do and don't understand. He has landed somewhere in the middle — troubled by ways things could go wrong, but not convinced there are very strong reasons to expect a terrible outcome.</p><p>Today's conversation is wide-ranging and Rohin lays out many of his personal opinions to host Rob Wiblin, including:</p><ul><li>What he sees as the strongest case both for and against slowing down the rate of progress in AI research.</li><li>Why he disagrees with most other ML researchers that training a model on a sensible 'reward function' is enough to get a good outcome.</li><li>Why he disagrees with many on LessWrong that the bar for whether a safety technique is helpful is “could this contain a superintelligence.”</li><li>That he thinks nobody has very compelling arguments that AI created via machine learning will be dangerous by default, or that it will be safe by default. He believes we just don't know.</li><li>That he understands that analogies and visualisations are necessary for public communication, but is sceptical that they really help us understand what's going on with ML models, because they're different in important ways from every other case we might compare them to.</li><li>Why he's optimistic about DeepMind’s work on scalable oversight, mechanistic interpretability, and dangerous capabilities evaluations, and what each of those projects involves.</li><li>Why he isn't inherently worried about a future where we're surrounded by beings far more capable than us, so long as they share our goals to a reasonable degree.</li><li>Why it's not enough for humanity to know how to align AI models — it's essential that management at AI labs correctly pick which methods they're going to use and have the practical know-how to apply them properly.</li><li>Three observations that make him a little more optimistic: humans are a bit muddle-headed and not super goal-orientated; planes don't crash; and universities have specific majors in particular subjects.</li><li>Plenty more besides.</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire, Dominic Armstrong, and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/867aaa90/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#153 – Elie Hassenfeld on 2 big picture critiques of GiveWell's approach, and 6 lessons from their recent work</title>
      <itunes:title>#153 – Elie Hassenfeld on 2 big picture critiques of GiveWell's approach, and 6 lessons from their recent work</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">16bc4cc4-cf92-42cb-a322-360d6d9383bb</guid>
      <link>https://80000hours.org/podcast/episodes/elie-hassenfeld-givewell-critiques-and-lessons/?utm_campaign=podcast__elie-hassenfeld&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p><a href="https://www.givewell.org/">GiveWell</a> is one of the world's best-known charity evaluators, with the goal of "searching for the charities that save or improve lives the most per dollar." It mostly recommends projects that help the world's poorest people avoid easily prevented diseases, like intestinal worms or vitamin A deficiency.</p><p>But should GiveWell, as some critics argue, take a totally different approach to its search, focusing instead on directly increasing subjective wellbeing, or alternatively, raising economic growth?</p><p>Today's guest — cofounder and CEO of GiveWell, Elie Hassenfeld — is proud of how much GiveWell has grown in the last five years. Its 'money moved' has quadrupled to around $600 million a year.</p><p>Its research team has also more than doubled, enabling them to investigate a far broader range of interventions that could plausibly help people an enormous amount for each dollar spent. That work has led GiveWell to support dozens of new organisations, such as Kangaroo Mother Care, MiracleFeet, and Dispensers for Safe Water.</p><p>But some other researchers focused on figuring out the best ways to help the world's poorest people say GiveWell shouldn't just do more of the same thing, but rather ought to look at the problem differently.</p><p><a href="https://80k.info/EH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Currently, GiveWell uses a range of metrics to track the impact of the organisations it considers recommending — such as 'lives saved,' 'household incomes doubled,' and for health improvements, the 'quality-adjusted life year.' </p><p>The Happier Lives Institute (HLI) has argued that instead, GiveWell should try to cash out the impact of all interventions in terms of improvements in subjective wellbeing. This philosophy has led HLI to be more sceptical of interventions that have been demonstrated to improve health, but whose impact on wellbeing has not been measured, and to give a high priority to improving lives relative to extending them.</p><p>An alternative high-level critique is that really all that matters in the long run is getting the economies of poor countries to grow. On this view, GiveWell should focus on figuring out what causes some countries to experience explosive economic growth while others fail to, or even go backwards. Even modest improvements in the chances of such a 'growth miracle' will likely offer a bigger bang-for-buck than funding the incremental delivery of deworming tablets or vitamin A supplements, or anything else.</p><p>Elie sees where both of these critiques are coming from, and notes that they've influenced GiveWell's work in some ways. But as he explains, he thinks they underestimate the practical difficulty of successfully pulling off either approach and finding better opportunities than what GiveWell funds today. </p><p>In today's in-depth conversation, Elie and host Rob Wiblin cover the above, as well as:</p><ul><li>Why GiveWell flipped from not recommending chlorine dispensers as an intervention for safe drinking water to spending tens of millions of dollars on them</li><li>What transferable lessons GiveWell learned from investigating different kinds of interventions</li><li>Why the best treatment for premature babies in low-resource settings may involve less rather than more medicine.</li><li>Severe malnourishment among children and what can be done about it.</li><li>How to deal with hidden and non-obvious costs of a programme</li><li>Some cheap early treatments that can prevent kids from developing lifelong disabilities</li><li>The various roles GiveWell is currently hiring for, and what's distinctive about their organisational culture</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:14)</li><li>GiveWell over the last couple of years (00:04:33)</li><li>Dispensers for Safe Water (00:11:52)</li><li>Syphilis diagnosis for pregnant women via technical assistance (00:30:39)</li><li>Kangaroo Mother Care (00:48:47)</li><li>Multiples of cash (01:01:20)</li><li>Hidden costs (01:05:41)</li><li>MiracleFeet (01:09:45)</li><li>Serious malnourishment among young children (01:22:46)</li><li>Vitamin A deficiency and supplementation (01:40:42)</li><li>The subjective wellbeing approach in contrast with GiveWell's approach (01:46:31)</li><li>The value of saving a life when that life is going to be very difficult (02:09:09)</li><li>Whether economic policy is what really matters overwhelmingly (02:20:00)</li><li>Careers at GiveWell (02:39:10)</li><li>Donations (02:48:58)</li><li>Parenthood (02:50:29)</li><li>Rob’s outro (02:55:05)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Simon Monsour and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><a href="https://www.givewell.org/">GiveWell</a> is one of the world's best-known charity evaluators, with the goal of "searching for the charities that save or improve lives the most per dollar." It mostly recommends projects that help the world's poorest people avoid easily prevented diseases, like intestinal worms or vitamin A deficiency.</p><p>But should GiveWell, as some critics argue, take a totally different approach to its search, focusing instead on directly increasing subjective wellbeing, or alternatively, raising economic growth?</p><p>Today's guest — cofounder and CEO of GiveWell, Elie Hassenfeld — is proud of how much GiveWell has grown in the last five years. Its 'money moved' has quadrupled to around $600 million a year.</p><p>Its research team has also more than doubled, enabling them to investigate a far broader range of interventions that could plausibly help people an enormous amount for each dollar spent. That work has led GiveWell to support dozens of new organisations, such as Kangaroo Mother Care, MiracleFeet, and Dispensers for Safe Water.</p><p>But some other researchers focused on figuring out the best ways to help the world's poorest people say GiveWell shouldn't just do more of the same thing, but rather ought to look at the problem differently.</p><p><a href="https://80k.info/EH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Currently, GiveWell uses a range of metrics to track the impact of the organisations it considers recommending — such as 'lives saved,' 'household incomes doubled,' and for health improvements, the 'quality-adjusted life year.' </p><p>The Happier Lives Institute (HLI) has argued that instead, GiveWell should try to cash out the impact of all interventions in terms of improvements in subjective wellbeing. This philosophy has led HLI to be more sceptical of interventions that have been demonstrated to improve health, but whose impact on wellbeing has not been measured, and to give a high priority to improving lives relative to extending them.</p><p>An alternative high-level critique is that really all that matters in the long run is getting the economies of poor countries to grow. On this view, GiveWell should focus on figuring out what causes some countries to experience explosive economic growth while others fail to, or even go backwards. Even modest improvements in the chances of such a 'growth miracle' will likely offer a bigger bang-for-buck than funding the incremental delivery of deworming tablets or vitamin A supplements, or anything else.</p><p>Elie sees where both of these critiques are coming from, and notes that they've influenced GiveWell's work in some ways. But as he explains, he thinks they underestimate the practical difficulty of successfully pulling off either approach and finding better opportunities than what GiveWell funds today. </p><p>In today's in-depth conversation, Elie and host Rob Wiblin cover the above, as well as:</p><ul><li>Why GiveWell flipped from not recommending chlorine dispensers as an intervention for safe drinking water to spending tens of millions of dollars on them</li><li>What transferable lessons GiveWell learned from investigating different kinds of interventions</li><li>Why the best treatment for premature babies in low-resource settings may involve less rather than more medicine.</li><li>Severe malnourishment among children and what can be done about it.</li><li>How to deal with hidden and non-obvious costs of a programme</li><li>Some cheap early treatments that can prevent kids from developing lifelong disabilities</li><li>The various roles GiveWell is currently hiring for, and what's distinctive about their organisational culture</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:14)</li><li>GiveWell over the last couple of years (00:04:33)</li><li>Dispensers for Safe Water (00:11:52)</li><li>Syphilis diagnosis for pregnant women via technical assistance (00:30:39)</li><li>Kangaroo Mother Care (00:48:47)</li><li>Multiples of cash (01:01:20)</li><li>Hidden costs (01:05:41)</li><li>MiracleFeet (01:09:45)</li><li>Serious malnourishment among young children (01:22:46)</li><li>Vitamin A deficiency and supplementation (01:40:42)</li><li>The subjective wellbeing approach in contrast with GiveWell's approach (01:46:31)</li><li>The value of saving a life when that life is going to be very difficult (02:09:09)</li><li>Whether economic policy is what really matters overwhelmingly (02:20:00)</li><li>Careers at GiveWell (02:39:10)</li><li>Donations (02:48:58)</li><li>Parenthood (02:50:29)</li><li>Rob’s outro (02:55:05)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Simon Monsour and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 02 Jun 2023 21:53:32 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/53841073/f638faf8.mp3" length="84608315" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/5I5gcVBwoFarirLNQ1iDH_ogXkzt7Vu4rfYlk85WI2Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzNjcwNTkv/MTY4NTczNTY2Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10570</itunes:duration>
      <itunes:summary>
        <![CDATA[<p><a href="https://www.givewell.org/">GiveWell</a> is one of the world's best-known charity evaluators, with the goal of "searching for the charities that save or improve lives the most per dollar." It mostly recommends projects that help the world's poorest people avoid easily prevented diseases, like intestinal worms or vitamin A deficiency.</p><p>But should GiveWell, as some critics argue, take a totally different approach to its search, focusing instead on directly increasing subjective wellbeing, or alternatively, raising economic growth?</p><p>Today's guest — cofounder and CEO of GiveWell, Elie Hassenfeld — is proud of how much GiveWell has grown in the last five years. Its 'money moved' has quadrupled to around $600 million a year.</p><p>Its research team has also more than doubled, enabling them to investigate a far broader range of interventions that could plausibly help people an enormous amount for each dollar spent. That work has led GiveWell to support dozens of new organisations, such as Kangaroo Mother Care, MiracleFeet, and Dispensers for Safe Water.</p><p>But some other researchers focused on figuring out the best ways to help the world's poorest people say GiveWell shouldn't just do more of the same thing, but rather ought to look at the problem differently.</p><p><a href="https://80k.info/EH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p>Currently, GiveWell uses a range of metrics to track the impact of the organisations it considers recommending — such as 'lives saved,' 'household incomes doubled,' and for health improvements, the 'quality-adjusted life year.' </p><p>The Happier Lives Institute (HLI) has argued that instead, GiveWell should try to cash out the impact of all interventions in terms of improvements in subjective wellbeing. This philosophy has led HLI to be more sceptical of interventions that have been demonstrated to improve health, but whose impact on wellbeing has not been measured, and to give a high priority to improving lives relative to extending them.</p><p>An alternative high-level critique is that really all that matters in the long run is getting the economies of poor countries to grow. On this view, GiveWell should focus on figuring out what causes some countries to experience explosive economic growth while others fail to, or even go backwards. Even modest improvements in the chances of such a 'growth miracle' will likely offer a bigger bang-for-buck than funding the incremental delivery of deworming tablets or vitamin A supplements, or anything else.</p><p>Elie sees where both of these critiques are coming from, and notes that they've influenced GiveWell's work in some ways. But as he explains, he thinks they underestimate the practical difficulty of successfully pulling off either approach and finding better opportunities than what GiveWell funds today. </p><p>In today's in-depth conversation, Elie and host Rob Wiblin cover the above, as well as:</p><ul><li>Why GiveWell flipped from not recommending chlorine dispensers as an intervention for safe drinking water to spending tens of millions of dollars on them</li><li>What transferable lessons GiveWell learned from investigating different kinds of interventions</li><li>Why the best treatment for premature babies in low-resource settings may involve less rather than more medicine.</li><li>Severe malnourishment among children and what can be done about it.</li><li>How to deal with hidden and non-obvious costs of a programme</li><li>Some cheap early treatments that can prevent kids from developing lifelong disabilities</li><li>The various roles GiveWell is currently hiring for, and what's distinctive about their organisational culture</li><li>And much more.</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:14)</li><li>GiveWell over the last couple of years (00:04:33)</li><li>Dispensers for Safe Water (00:11:52)</li><li>Syphilis diagnosis for pregnant women via technical assistance (00:30:39)</li><li>Kangaroo Mother Care (00:48:47)</li><li>Multiples of cash (01:01:20)</li><li>Hidden costs (01:05:41)</li><li>MiracleFeet (01:09:45)</li><li>Serious malnourishment among young children (01:22:46)</li><li>Vitamin A deficiency and supplementation (01:40:42)</li><li>The subjective wellbeing approach in contrast with GiveWell's approach (01:46:31)</li><li>The value of saving a life when that life is going to be very difficult (02:09:09)</li><li>Whether economic policy is what really matters overwhelmingly (02:20:00)</li><li>Careers at GiveWell (02:39:10)</li><li>Donations (02:48:58)</li><li>Parenthood (02:50:29)</li><li>Rob’s outro (02:55:05)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Simon Monsour and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/53841073/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/53841073/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#152 – Joe Carlsmith on navigating serious philosophical confusion</title>
      <itunes:title>#152 – Joe Carlsmith on navigating serious philosophical confusion</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b8fb2df5-0b20-4222-a08c-f78d54583b33</guid>
      <link>https://80000hours.org/podcast/episodes/joe-carlsmith-navigating-serious-philosophical-confusion/?utm_campaign=podcast__joe-carlsmith&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>What is the nature of the universe? How do we make decisions correctly? What differentiates right actions from wrong ones?</p><p>Such fundamental questions have been the subject of philosophical and theological debates for millennia. But, as we all know, and surveys of expert opinion make clear, we are very far from agreement. So... with these most basic questions unresolved, what’s a species to do?</p><p>In today's episode, philosopher Joe Carlsmith — Senior Research Analyst at Open Philanthropy — makes the case that many current debates in philosophy ought to leave us confused and humbled. These are themes he discusses in his PhD thesis, A stranger priority? Topics at the outer reaches of effective altruism.</p><p><a href="https://80k.link/JC"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>To help transmit the disorientation he thinks is appropriate, Joe presents three disconcerting theories — originating from him and his peers — that challenge humanity's self-assured understanding of the world.</p><p>The first idea is that we might be living in a computer simulation, because, in the classic formulation, if most civilisations go on to run many computer simulations of their past history, then most beings who perceive themselves as living in such a history must themselves be in computer simulations. Joe prefers a somewhat different way of making the point, but, having looked into it, he hasn't identified any particular rebuttal to this 'simulation argument.'</p><p>If true, it could revolutionise our comprehension of the universe and the way we ought to live...</p><p><a href="https://80k.link/JC"><strong>Other two ideas cut for length — click here to read the full post.</strong></a><strong></strong></p><p>These are just three particular instances of a much broader set of ideas that <a href="https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/">some have dubbed</a> the "train to crazy town." Basically, if you commit to always take philosophy and arguments seriously, and try to act on them, it can lead to what seem like some pretty crazy and impractical places. So what should we do with this buffet of plausible-sounding but bewildering arguments?</p><p>Joe and Rob discuss to what extent this should prompt us to pay less attention to philosophy, and how we as individuals can cope psychologically with feeling out of our depth just trying to make the most basic sense of the world.</p><p>In today's challenging conversation, Joe and Rob discuss all of the above, as well as:</p><ul><li>What Joe doesn't like about the drowning child thought experiment</li><li>An alternative thought experiment about helping a stranger that might better highlight our intrinsic desire to help others</li><li>What Joe doesn't like about the expression “the train to crazy town”</li><li>Whether Elon Musk should place a higher probability on living in a simulation than most other people</li><li>Whether the deterministic twin prisoner’s dilemma, if fully appreciated, gives us an extra reason to keep promises</li><li>To what extent learning to doubt our own judgement about difficult questions -- so-called “epistemic learned helplessness” -- is a good thing</li><li>How strong the case is that advanced AI will engage in generalised power-seeking behaviour</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:09:21)</li><li>Downsides of the drowning child thought experiment (00:12:24)</li><li>Making demanding moral values more resonant (00:24:56)</li><li>The crazy train (00:36:48)</li><li>Whether we’re living in a simulation (00:48:50)</li><li>Reasons to doubt we’re living in a simulation, and practical implications if we are (00:57:02)</li><li>Rob's explainer about anthropics (01:12:27)</li><li>Back to the interview (01:19:53)</li><li>Decision theory and affecting the past (01:23:33)</li><li>Rob's explainer about decision theory (01:29:19)</li><li>Back to the interview (01:39:55)</li><li>Newcomb's problem (01:46:14)</li><li>Practical implications of acausal decision theory (01:50:04)</li><li>The hitchhiker in the desert (01:55:57)</li><li>Acceptance within philosophy (02:01:22)</li><li>Infinite ethics (02:04:35)</li><li>Rob's explainer about the expanding spheres approach (02:17:05)</li><li>Back to the interview (02:20:27)</li><li>Infinite ethics and the utilitarian dream (02:27:42)</li><li>Rob's explainer about epicycles (02:29:30)</li><li>Back to the interview (02:31:26)</li><li>What to do with all of these weird philosophical ideas (02:35:28)</li><li>Welfare longtermism and wisdom longtermism (02:53:23)</li><li>Epistemic learned helplessness (03:03:10)</li><li>Power-seeking AI (03:12:41)</li><li>Rob’s outro (03:25:45)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>What is the nature of the universe? How do we make decisions correctly? What differentiates right actions from wrong ones?</p><p>Such fundamental questions have been the subject of philosophical and theological debates for millennia. But, as we all know, and surveys of expert opinion make clear, we are very far from agreement. So... with these most basic questions unresolved, what’s a species to do?</p><p>In today's episode, philosopher Joe Carlsmith — Senior Research Analyst at Open Philanthropy — makes the case that many current debates in philosophy ought to leave us confused and humbled. These are themes he discusses in his PhD thesis, A stranger priority? Topics at the outer reaches of effective altruism.</p><p><a href="https://80k.link/JC"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>To help transmit the disorientation he thinks is appropriate, Joe presents three disconcerting theories — originating from him and his peers — that challenge humanity's self-assured understanding of the world.</p><p>The first idea is that we might be living in a computer simulation, because, in the classic formulation, if most civilisations go on to run many computer simulations of their past history, then most beings who perceive themselves as living in such a history must themselves be in computer simulations. Joe prefers a somewhat different way of making the point, but, having looked into it, he hasn't identified any particular rebuttal to this 'simulation argument.'</p><p>If true, it could revolutionise our comprehension of the universe and the way we ought to live...</p><p><a href="https://80k.link/JC"><strong>Other two ideas cut for length — click here to read the full post.</strong></a><strong></strong></p><p>These are just three particular instances of a much broader set of ideas that <a href="https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/">some have dubbed</a> the "train to crazy town." Basically, if you commit to always take philosophy and arguments seriously, and try to act on them, it can lead to what seem like some pretty crazy and impractical places. So what should we do with this buffet of plausible-sounding but bewildering arguments?</p><p>Joe and Rob discuss to what extent this should prompt us to pay less attention to philosophy, and how we as individuals can cope psychologically with feeling out of our depth just trying to make the most basic sense of the world.</p><p>In today's challenging conversation, Joe and Rob discuss all of the above, as well as:</p><ul><li>What Joe doesn't like about the drowning child thought experiment</li><li>An alternative thought experiment about helping a stranger that might better highlight our intrinsic desire to help others</li><li>What Joe doesn't like about the expression “the train to crazy town”</li><li>Whether Elon Musk should place a higher probability on living in a simulation than most other people</li><li>Whether the deterministic twin prisoner’s dilemma, if fully appreciated, gives us an extra reason to keep promises</li><li>To what extent learning to doubt our own judgement about difficult questions -- so-called “epistemic learned helplessness” -- is a good thing</li><li>How strong the case is that advanced AI will engage in generalised power-seeking behaviour</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:09:21)</li><li>Downsides of the drowning child thought experiment (00:12:24)</li><li>Making demanding moral values more resonant (00:24:56)</li><li>The crazy train (00:36:48)</li><li>Whether we’re living in a simulation (00:48:50)</li><li>Reasons to doubt we’re living in a simulation, and practical implications if we are (00:57:02)</li><li>Rob's explainer about anthropics (01:12:27)</li><li>Back to the interview (01:19:53)</li><li>Decision theory and affecting the past (01:23:33)</li><li>Rob's explainer about decision theory (01:29:19)</li><li>Back to the interview (01:39:55)</li><li>Newcomb's problem (01:46:14)</li><li>Practical implications of acausal decision theory (01:50:04)</li><li>The hitchhiker in the desert (01:55:57)</li><li>Acceptance within philosophy (02:01:22)</li><li>Infinite ethics (02:04:35)</li><li>Rob's explainer about the expanding spheres approach (02:17:05)</li><li>Back to the interview (02:20:27)</li><li>Infinite ethics and the utilitarian dream (02:27:42)</li><li>Rob's explainer about epicycles (02:29:30)</li><li>Back to the interview (02:31:26)</li><li>What to do with all of these weird philosophical ideas (02:35:28)</li><li>Welfare longtermism and wisdom longtermism (02:53:23)</li><li>Epistemic learned helplessness (03:03:10)</li><li>Power-seeking AI (03:12:41)</li><li>Rob’s outro (03:25:45)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 19 May 2023 22:55:53 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4ddd9810/c4fc10b1.mp3" length="99371601" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/sBjAAayEe4_w5lBnsQPTpwZuq0kOxtJaJ693Pn3wXHY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzNDU5Njgv/MTY4NDUyODIzOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>12418</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>What is the nature of the universe? How do we make decisions correctly? What differentiates right actions from wrong ones?</p><p>Such fundamental questions have been the subject of philosophical and theological debates for millennia. But, as we all know, and surveys of expert opinion make clear, we are very far from agreement. So... with these most basic questions unresolved, what’s a species to do?</p><p>In today's episode, philosopher Joe Carlsmith — Senior Research Analyst at Open Philanthropy — makes the case that many current debates in philosophy ought to leave us confused and humbled. These are themes he discusses in his PhD thesis, A stranger priority? Topics at the outer reaches of effective altruism.</p><p><a href="https://80k.link/JC"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>To help transmit the disorientation he thinks is appropriate, Joe presents three disconcerting theories — originating from him and his peers — that challenge humanity's self-assured understanding of the world.</p><p>The first idea is that we might be living in a computer simulation, because, in the classic formulation, if most civilisations go on to run many computer simulations of their past history, then most beings who perceive themselves as living in such a history must themselves be in computer simulations. Joe prefers a somewhat different way of making the point, but, having looked into it, he hasn't identified any particular rebuttal to this 'simulation argument.'</p><p>If true, it could revolutionise our comprehension of the universe and the way we ought to live...</p><p><a href="https://80k.link/JC"><strong>Other two ideas cut for length — click here to read the full post.</strong></a><strong></strong></p><p>These are just three particular instances of a much broader set of ideas that <a href="https://80000hours.org/podcast/episodes/ajeya-cotra-worldview-diversification/">some have dubbed</a> the "train to crazy town." Basically, if you commit to always take philosophy and arguments seriously, and try to act on them, it can lead to what seem like some pretty crazy and impractical places. So what should we do with this buffet of plausible-sounding but bewildering arguments?</p><p>Joe and Rob discuss to what extent this should prompt us to pay less attention to philosophy, and how we as individuals can cope psychologically with feeling out of our depth just trying to make the most basic sense of the world.</p><p>In today's challenging conversation, Joe and Rob discuss all of the above, as well as:</p><ul><li>What Joe doesn't like about the drowning child thought experiment</li><li>An alternative thought experiment about helping a stranger that might better highlight our intrinsic desire to help others</li><li>What Joe doesn't like about the expression “the train to crazy town”</li><li>Whether Elon Musk should place a higher probability on living in a simulation than most other people</li><li>Whether the deterministic twin prisoner’s dilemma, if fully appreciated, gives us an extra reason to keep promises</li><li>To what extent learning to doubt our own judgement about difficult questions -- so-called “epistemic learned helplessness” -- is a good thing</li><li>How strong the case is that advanced AI will engage in generalised power-seeking behaviour</li></ul><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:09:21)</li><li>Downsides of the drowning child thought experiment (00:12:24)</li><li>Making demanding moral values more resonant (00:24:56)</li><li>The crazy train (00:36:48)</li><li>Whether we’re living in a simulation (00:48:50)</li><li>Reasons to doubt we’re living in a simulation, and practical implications if we are (00:57:02)</li><li>Rob's explainer about anthropics (01:12:27)</li><li>Back to the interview (01:19:53)</li><li>Decision theory and affecting the past (01:23:33)</li><li>Rob's explainer about decision theory (01:29:19)</li><li>Back to the interview (01:39:55)</li><li>Newcomb's problem (01:46:14)</li><li>Practical implications of acausal decision theory (01:50:04)</li><li>The hitchhiker in the desert (01:55:57)</li><li>Acceptance within philosophy (02:01:22)</li><li>Infinite ethics (02:04:35)</li><li>Rob's explainer about the expanding spheres approach (02:17:05)</li><li>Back to the interview (02:20:27)</li><li>Infinite ethics and the utilitarian dream (02:27:42)</li><li>Rob's explainer about epicycles (02:29:30)</li><li>Back to the interview (02:31:26)</li><li>What to do with all of these weird philosophical ideas (02:35:28)</li><li>Welfare longtermism and wisdom longtermism (02:53:23)</li><li>Epistemic learned helplessness (03:03:10)</li><li>Power-seeking AI (03:12:41)</li><li>Rob’s outro (03:25:45)</li></ul><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Milo McGuire and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/4ddd9810/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/4ddd9810/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#151 – Ajeya Cotra on accidentally teaching AI models to deceive us</title>
      <itunes:title>#151 – Ajeya Cotra on accidentally teaching AI models to deceive us</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">48765617-7eae-456d-a919-7008cb50ca4d</guid>
      <link>https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/?utm_campaign=podcast__ajeya-cotra&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>Imagine you are an orphaned eight-year-old whose parents left you a $1 trillion company, and no trusted adult to serve as your guide to the world. You have to hire a smart adult to run that company, guide your life the way that a parent would, and administer your vast wealth. You have to hire that adult based on a work trial or interview you come up with. You don't get to see any resumes or do reference checks. And because you're so rich, tonnes of people apply for the job — for all sorts of reasons.</p><p>Today's guest Ajeya Cotra — senior research analyst at Open Philanthropy — argues that this peculiar setup resembles the situation humanity finds itself in when training very general and very capable AI models using current deep learning methods.</p><p><a href="https://80k.link/AC23"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>As she explains, such an eight-year-old faces a challenging problem. In the candidate pool there are likely some truly nice people, who sincerely want to help and make decisions that are in your interest. But there are probably other characters too — like people who will pretend to care about you while you're monitoring them, but intend to use the job to enrich themselves as soon as they think they can get away with it.</p><p>Like a child trying to judge adults, at some point humans will be required to judge the trustworthiness and reliability of machine learning models that are as goal-oriented as people, and greatly outclass them in knowledge, experience, breadth, and speed. Tricky!</p><p>Can't we rely on how well models have performed at tasks during training to guide us? Ajeya worries that it won't work. The trouble is that three different sorts of models will all produce the same output during training, but could behave very differently once deployed in a setting that allows their true colours to come through. She describes three such motivational archetypes:</p><ul><li>Saints — models that care about doing what we really want</li><li>Sycophants — models that just want us to say they've done a good job, even if they get that praise by taking actions they know we wouldn't want them to</li><li>Schemers — models that don't care about us or our interests at all, who are just pleasing us so long as that serves their own agenda</li></ul><p>And according to Ajeya, there are also ways we could end up <em>actively</em> selecting for motivations that we don't want.</p><p>In today's interview, Ajeya and Rob discuss the above, as well as:</p><ul><li>How to predict the motivations a neural network will develop through training</li><li>Whether AIs being trained will functionally understand that they're AIs being trained, the same way we think we understand that we're humans living on planet Earth</li><li>Stories of AI misalignment that Ajeya doesn't buy into</li><li>Analogies for AI, from octopuses to aliens to can openers</li><li>Why it's smarter to have separate planning AIs and doing AIs</li><li>The benefits of only following through on AI-generated plans that make sense to human beings</li><li>What approaches for fixing alignment problems Ajeya is most excited about, and which she thinks are overrated</li><li>How one might demo actually scary AI failure mechanisms</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Ryan Kessler and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Imagine you are an orphaned eight-year-old whose parents left you a $1 trillion company, and no trusted adult to serve as your guide to the world. You have to hire a smart adult to run that company, guide your life the way that a parent would, and administer your vast wealth. You have to hire that adult based on a work trial or interview you come up with. You don't get to see any resumes or do reference checks. And because you're so rich, tonnes of people apply for the job — for all sorts of reasons.</p><p>Today's guest Ajeya Cotra — senior research analyst at Open Philanthropy — argues that this peculiar setup resembles the situation humanity finds itself in when training very general and very capable AI models using current deep learning methods.</p><p><a href="https://80k.link/AC23"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>As she explains, such an eight-year-old faces a challenging problem. In the candidate pool there are likely some truly nice people, who sincerely want to help and make decisions that are in your interest. But there are probably other characters too — like people who will pretend to care about you while you're monitoring them, but intend to use the job to enrich themselves as soon as they think they can get away with it.</p><p>Like a child trying to judge adults, at some point humans will be required to judge the trustworthiness and reliability of machine learning models that are as goal-oriented as people, and greatly outclass them in knowledge, experience, breadth, and speed. Tricky!</p><p>Can't we rely on how well models have performed at tasks during training to guide us? Ajeya worries that it won't work. The trouble is that three different sorts of models will all produce the same output during training, but could behave very differently once deployed in a setting that allows their true colours to come through. She describes three such motivational archetypes:</p><ul><li>Saints — models that care about doing what we really want</li><li>Sycophants — models that just want us to say they've done a good job, even if they get that praise by taking actions they know we wouldn't want them to</li><li>Schemers — models that don't care about us or our interests at all, who are just pleasing us so long as that serves their own agenda</li></ul><p>And according to Ajeya, there are also ways we could end up <em>actively</em> selecting for motivations that we don't want.</p><p>In today's interview, Ajeya and Rob discuss the above, as well as:</p><ul><li>How to predict the motivations a neural network will develop through training</li><li>Whether AIs being trained will functionally understand that they're AIs being trained, the same way we think we understand that we're humans living on planet Earth</li><li>Stories of AI misalignment that Ajeya doesn't buy into</li><li>Analogies for AI, from octopuses to aliens to can openers</li><li>Why it's smarter to have separate planning AIs and doing AIs</li><li>The benefits of only following through on AI-generated plans that make sense to human beings</li><li>What approaches for fixing alignment problems Ajeya is most excited about, and which she thinks are overrated</li><li>How one might demo actually scary AI failure mechanisms</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Ryan Kessler and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 12 May 2023 20:41:31 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/fa688bf5/18de9c95.mp3" length="81500392" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Jqay3eOI6lBVLCUG60WOYtYpBlVhqNIh03kxCaYCiAw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMzM2NTUv/MTY4MzkyMjI1MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10180</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Imagine you are an orphaned eight-year-old whose parents left you a $1 trillion company, and no trusted adult to serve as your guide to the world. You have to hire a smart adult to run that company, guide your life the way that a parent would, and administer your vast wealth. You have to hire that adult based on a work trial or interview you come up with. You don't get to see any resumes or do reference checks. And because you're so rich, tonnes of people apply for the job — for all sorts of reasons.</p><p>Today's guest Ajeya Cotra — senior research analyst at Open Philanthropy — argues that this peculiar setup resembles the situation humanity finds itself in when training very general and very capable AI models using current deep learning methods.</p><p><a href="https://80k.link/AC23"><strong>Links to learn more, summary and full transcript.</strong></a><strong></strong></p><p>As she explains, such an eight-year-old faces a challenging problem. In the candidate pool there are likely some truly nice people, who sincerely want to help and make decisions that are in your interest. But there are probably other characters too — like people who will pretend to care about you while you're monitoring them, but intend to use the job to enrich themselves as soon as they think they can get away with it.</p><p>Like a child trying to judge adults, at some point humans will be required to judge the trustworthiness and reliability of machine learning models that are as goal-oriented as people, and greatly outclass them in knowledge, experience, breadth, and speed. Tricky!</p><p>Can't we rely on how well models have performed at tasks during training to guide us? Ajeya worries that it won't work. The trouble is that three different sorts of models will all produce the same output during training, but could behave very differently once deployed in a setting that allows their true colours to come through. She describes three such motivational archetypes:</p><ul><li>Saints — models that care about doing what we really want</li><li>Sycophants — models that just want us to say they've done a good job, even if they get that praise by taking actions they know we wouldn't want them to</li><li>Schemers — models that don't care about us or our interests at all, who are just pleasing us so long as that serves their own agenda</li></ul><p>And according to Ajeya, there are also ways we could end up <em>actively</em> selecting for motivations that we don't want.</p><p>In today's interview, Ajeya and Rob discuss the above, as well as:</p><ul><li>How to predict the motivations a neural network will develop through training</li><li>Whether AIs being trained will functionally understand that they're AIs being trained, the same way we think we understand that we're humans living on planet Earth</li><li>Stories of AI misalignment that Ajeya doesn't buy into</li><li>Analogies for AI, from octopuses to aliens to can openers</li><li>Why it's smarter to have separate planning AIs and doing AIs</li><li>The benefits of only following through on AI-generated plans that make sense to human beings</li><li>What approaches for fixing alignment problems Ajeya is most excited about, and which she thinks are overrated</li><li>How one might demo actually scary AI failure mechanisms</li></ul><p><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p><em>Producer: Keiran Harris</em></p><p><em>Audio mastering: Ryan Kessler and Ben Cordell</em></p><p><em>Transcriptions: Katy Moore</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/fa688bf5/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#150 – Tom Davidson on how quickly AI could transform the world</title>
      <itunes:title>#150 – Tom Davidson on how quickly AI could transform the world</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2996dbda-eb81-11ed-9b30-0e4b8551d893</guid>
      <link>https://80000hours.org/podcast/episodes/tom-davidson-how-quickly-ai-could-transform-the-world/</link>
      <description>
        <![CDATA[<p>It’s easy to dismiss alarming AI-related predictions when you don’t know where the numbers came from.</p><p>For example: what if we told you that within 15 years, it’s likely that we’ll see a 1,000x improvement in AI capabilities in a single year? And what if we then told you that those improvements would lead to explosive economic growth unlike anything humanity has seen before?</p><p>You might think, “Congratulations, you said a big number — but this kind of stuff seems crazy, so I’m going to keep scrolling through Twitter.”</p><p>But this 1,000x yearly improvement is a prediction based on *real economic models* created by today’s guest Tom Davidson, Senior Research Analyst at Open Philanthropy. By the end of the episode, you’ll either be able to point out specific flaws in his step-by-step reasoning, or have to at least <em>consider </em>the idea that the world is about to get — at a minimum — incredibly weird.</p><p><a href="https://80k.link/TD"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> As a teaser, consider the following:</p><p> Developing artificial general intelligence (AGI) — AI that can do 100% of cognitive tasks at least as well as the best humans can — could very easily lead us to an unrecognisable world.</p><p> You might think having to train AI systems individually to do every conceivable cognitive task — one for diagnosing diseases, one for doing your taxes, one for teaching your kids, etc. — sounds implausible, or at least like it’ll take decades.</p><p> But Tom thinks we might not need to train AI to do every single job — we might just need to train it to do one: AI research.</p><p> And building AI capable of doing research and development might be a much easier task — especially given that the researchers training the AI are AI researchers themselves.</p><p> And once an AI system is as good at accelerating future AI progress as the best humans are today — and we can run billions of copies of it round the clock — it’s hard to make the case that we won’t achieve AGI very quickly.</p><p> To give you some perspective: 17 years ago we saw the launch of Twitter, the release of Al Gore's *An Inconvenient Truth*, and your first chance to play the Nintendo Wii.</p><p> Tom thinks that if we have AI that significantly accelerates AI R&amp;D, then it’s hard to imagine not having AGI 17 years from now.</p><p> Wild.</p><p> Host Luisa Rodriguez gets Tom to walk us through his careful reports on the topic, and how he came up with these numbers, across a terrifying but fascinating three hours.</p><p> Luisa and Tom also discuss:</p><p> • How we might go from GPT-4 to AI disaster<br> • Tom’s journey from finding AI risk to be <em>kind of scary</em> to <em>really scary</em><br> • Whether international cooperation or an anti-AI social movement can slow AI progress down<br> • Why it might take just a few years to go from pretty good AI to superhuman AI<br> • How quickly the number and quality of computer chips we’ve been using for AI have been increasing<br> • The pace of algorithmic progress<br> • What ants can teach us about AI<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:53)</li><li>How we might go from GPT-4 to disaster (00:13:50)</li><li>Explosive economic growth (00:24:15)</li><li>Are there any limits for AI scientists? (00:33:17)</li><li>This seems really crazy (00:44:16)</li><li>How is this going to go for humanity? (00:50:49)</li><li>Why AI won’t go the way of nuclear power (01:00:13)</li><li>Can we definitely not come up with an international treaty? (01:05:24)</li><li>How quickly we should expect AI to “take off” (01:08:41)</li><li>Tom’s report on AI takeoff speeds (01:22:28)</li><li>How quickly will we go from 20% to 100% of tasks being automated by AI systems? (01:28:34)</li><li>What percent of cognitive tasks AI can currently perform (01:34:27)</li><li>Compute (01:39:48)</li><li>Using effective compute to predict AI takeoff speeds (01:48:01)</li><li>How quickly effective compute might increase (02:00:59)</li><li>How quickly chips and algorithms might improve (02:12:31)</li><li>How to check whether large AI models have dangerous capabilities (02:21:22)</li><li>Reasons AI takeoff might take longer (02:28:39)</li><li>Why AI takeoff might be very fast (02:31:52)</li><li>Fast AI takeoff speeds probably means shorter AI timelines (02:34:44)</li><li>Going from human-level AI to superhuman AI (02:41:34)</li><li>Going from AGI to AI deployment (02:46:59)</li><li>Were these arguments ever far-fetched to Tom? (02:49:54)</li><li>What ants can teach us about AI (02:52:45)</li><li>Rob’s outro (03:00:32)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Simon Monsour and Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>It’s easy to dismiss alarming AI-related predictions when you don’t know where the numbers came from.</p><p>For example: what if we told you that within 15 years, it’s likely that we’ll see a 1,000x improvement in AI capabilities in a single year? And what if we then told you that those improvements would lead to explosive economic growth unlike anything humanity has seen before?</p><p>You might think, “Congratulations, you said a big number — but this kind of stuff seems crazy, so I’m going to keep scrolling through Twitter.”</p><p>But this 1,000x yearly improvement is a prediction based on *real economic models* created by today’s guest Tom Davidson, Senior Research Analyst at Open Philanthropy. By the end of the episode, you’ll either be able to point out specific flaws in his step-by-step reasoning, or have to at least <em>consider </em>the idea that the world is about to get — at a minimum — incredibly weird.</p><p><a href="https://80k.link/TD"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> As a teaser, consider the following:</p><p> Developing artificial general intelligence (AGI) — AI that can do 100% of cognitive tasks at least as well as the best humans can — could very easily lead us to an unrecognisable world.</p><p> You might think having to train AI systems individually to do every conceivable cognitive task — one for diagnosing diseases, one for doing your taxes, one for teaching your kids, etc. — sounds implausible, or at least like it’ll take decades.</p><p> But Tom thinks we might not need to train AI to do every single job — we might just need to train it to do one: AI research.</p><p> And building AI capable of doing research and development might be a much easier task — especially given that the researchers training the AI are AI researchers themselves.</p><p> And once an AI system is as good at accelerating future AI progress as the best humans are today — and we can run billions of copies of it round the clock — it’s hard to make the case that we won’t achieve AGI very quickly.</p><p> To give you some perspective: 17 years ago we saw the launch of Twitter, the release of Al Gore's *An Inconvenient Truth*, and your first chance to play the Nintendo Wii.</p><p> Tom thinks that if we have AI that significantly accelerates AI R&amp;D, then it’s hard to imagine not having AGI 17 years from now.</p><p> Wild.</p><p> Host Luisa Rodriguez gets Tom to walk us through his careful reports on the topic, and how he came up with these numbers, across a terrifying but fascinating three hours.</p><p> Luisa and Tom also discuss:</p><p> • How we might go from GPT-4 to AI disaster<br> • Tom’s journey from finding AI risk to be <em>kind of scary</em> to <em>really scary</em><br> • Whether international cooperation or an anti-AI social movement can slow AI progress down<br> • Why it might take just a few years to go from pretty good AI to superhuman AI<br> • How quickly the number and quality of computer chips we’ve been using for AI have been increasing<br> • The pace of algorithmic progress<br> • What ants can teach us about AI<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:53)</li><li>How we might go from GPT-4 to disaster (00:13:50)</li><li>Explosive economic growth (00:24:15)</li><li>Are there any limits for AI scientists? (00:33:17)</li><li>This seems really crazy (00:44:16)</li><li>How is this going to go for humanity? (00:50:49)</li><li>Why AI won’t go the way of nuclear power (01:00:13)</li><li>Can we definitely not come up with an international treaty? (01:05:24)</li><li>How quickly we should expect AI to “take off” (01:08:41)</li><li>Tom’s report on AI takeoff speeds (01:22:28)</li><li>How quickly will we go from 20% to 100% of tasks being automated by AI systems? (01:28:34)</li><li>What percent of cognitive tasks AI can currently perform (01:34:27)</li><li>Compute (01:39:48)</li><li>Using effective compute to predict AI takeoff speeds (01:48:01)</li><li>How quickly effective compute might increase (02:00:59)</li><li>How quickly chips and algorithms might improve (02:12:31)</li><li>How to check whether large AI models have dangerous capabilities (02:21:22)</li><li>Reasons AI takeoff might take longer (02:28:39)</li><li>Why AI takeoff might be very fast (02:31:52)</li><li>Fast AI takeoff speeds probably means shorter AI timelines (02:34:44)</li><li>Going from human-level AI to superhuman AI (02:41:34)</li><li>Going from AGI to AI deployment (02:46:59)</li><li>Were these arguments ever far-fetched to Tom? (02:49:54)</li><li>What ants can teach us about AI (02:52:45)</li><li>Rob’s outro (03:00:32)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Simon Monsour and Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 05 May 2023 20:48:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5ec19f72/8d27af51.mp3" length="87351270" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/sMp-228HuGrTEhfku1Gt9rLHgVsQowlGctS4gy09A00/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS8yOTJm/NWRlMDRlNTE2OTY2/MzEwM2YwYzhlMTdh/ZmM2NC5qcGc.jpg"/>
      <itunes:duration>10919</itunes:duration>
      <itunes:summary>It’s easy to dismiss alarming AI-related predictions when you don’t know where the numbers came from. 

For example: what if we told you that within 15 years, it’s likely that we’ll see a 1,000x improvement in AI capabilities in a single year? And what if we then told you that those improvements would lead to explosive economic growth unlike anything humanity has seen before?  

You might think, “Congratulations, you said a big number — but this kind of stuff seems crazy, so I’m going to keep scrolling through Twitter.” 

But this 1,000x yearly improvement is a prediction based on *real economic models* created by today’s guest Tom Davidson, Senior Research Analyst at Open Philanthropy. By the end of the episode, you’ll either be able to point out specific flaws in his step-by-step reasoning, or have to at least *consider* the idea that the world is about to get — at a minimum — incredibly weird. 

Links to learn more, summary and full transcript. 

As a teaser, consider the following:  


Developing artificial general intelligence (AGI) — AI that can do 100% of cognitive tasks at least as well as the best humans can — could very easily lead us to an unrecognisable world. 

You might think having to train AI systems individually to do every conceivable cognitive task — one for diagnosing diseases, one for doing your taxes, one for teaching your kids, etc. — sounds implausible, or at least like it’ll take decades.  

But Tom thinks we might not need to train AI to do every single job — we might just need to train it to do one: AI research. 

And building AI capable of doing research and development might be a much easier task — especially given that the researchers training the AI are AI researchers themselves. 

And once an AI system is as good at accelerating future AI progress as the best humans are today — and we can run billions of copies of it round the clock — it’s hard to make the case that we won’t achieve AGI very quickly. 

To give you some perspective: 17 years ago we saw the launch of Twitter, the release of Al Gore's *An Inconvenient Truth*, and your first chance to play the Nintendo Wii.  

Tom thinks that if we have AI that significantly accelerates AI R&amp;amp;D, then it’s hard to imagine not having AGI 17 years from now. 

Wild. 

Host Luisa Rodriguez gets Tom to walk us through his careful reports on the topic, and how he came up with these numbers, across a terrifying but fascinating three hours. 

Luisa and Tom also discuss: 

• How we might go from GPT-4 to AI disaster 
• Tom’s journey from finding AI risk to be kind of scary to really scary 
• Whether international cooperation or an anti-AI social movement can slow AI progress down 
• Why it might take just a few years to go from pretty good AI to superhuman AI 
• How quickly the number and quality of computer chips we’ve been using for AI have been increasing 
• The pace of algorithmic progress 
• What ants can teach us about AI 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Simon Monsour and Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>It’s easy to dismiss alarming AI-related predictions when you don’t know where the numbers came from. 

For example: what if we told you that within 15 years, it’s likely that we’ll see a 1,000x improvement in AI capabilities in a single year? And what </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/5ec19f72/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Andrés Jiménez Zorrilla on the Shrimp Welfare Project (80k After Hours)</title>
      <itunes:title>Andrés Jiménez Zorrilla on the Shrimp Welfare Project (80k After Hours)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1fe33bfa-e0b0-11ed-bcfe-0af10c6f641b</guid>
      <link>https://share.transistor.fm/s/48b2321e</link>
      <description>
        <![CDATA[In this episode from our second show, <a href="https://80000hours.org/after-hours-podcast/">80k After Hours</a>, Rob Wiblin interviews Andrés Jiménez Zorrilla about the <a href="https://www.shrimpwelfareproject.org/">Shrimp Welfare Project</a>, which he cofounded in 2021. It's the first project in the world focused on shrimp welfare specifically, and as of recording in June 2022, has six full-time staff.<p> 

<a href="https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/?utm_campaign=podcast__andres-zorrilla&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, highlights and full transcript.</a></p><p> 

They cover:</p><p> 

• The evidence for shrimp sentience<br> 
• How farmers and the public feel about shrimp<br> 
• The scale of the problem<br> 
• What shrimp farming looks like<br> 
• The killing process, and other welfare issues<br> 
• Shrimp Welfare Project’s strategy<br> 
• History of shrimp welfare work<br> 
• What it’s like working in India and Vietnam<br> 
• How to help</p><p> 

Who this episode is for:</p><p> 

• People who care about animal welfare<br> 
• People interested in new and unusual problems<br> 
• People open to shrimp sentience</p><p> 


Who this episode isn’t for:</p><p> 

• People who think shrimp couldn’t possibly be sentient<br> 
• People who got called ‘shrimp’ a lot in high school and get anxious when they hear the word over and over again</p><p> 

<b>Get this episode by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type ‘80k After Hours’ into your podcasting app</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell and Ryan Kessler<br>
Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[In this episode from our second show, <a href="https://80000hours.org/after-hours-podcast/">80k After Hours</a>, Rob Wiblin interviews Andrés Jiménez Zorrilla about the <a href="https://www.shrimpwelfareproject.org/">Shrimp Welfare Project</a>, which he cofounded in 2021. It's the first project in the world focused on shrimp welfare specifically, and as of recording in June 2022, has six full-time staff.<p> 

<a href="https://80000hours.org/after-hours-podcast/episodes/andres-jimenez-zorrilla-shrimp-welfare-project/?utm_campaign=podcast__andres-zorrilla&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, highlights and full transcript.</a></p><p> 

They cover:</p><p> 

• The evidence for shrimp sentience<br> 
• How farmers and the public feel about shrimp<br> 
• The scale of the problem<br> 
• What shrimp farming looks like<br> 
• The killing process, and other welfare issues<br> 
• Shrimp Welfare Project’s strategy<br> 
• History of shrimp welfare work<br> 
• What it’s like working in India and Vietnam<br> 
• How to help</p><p> 

Who this episode is for:</p><p> 

• People who care about animal welfare<br> 
• People interested in new and unusual problems<br> 
• People open to shrimp sentience</p><p> 


Who this episode isn’t for:</p><p> 

• People who think shrimp couldn’t possibly be sentient<br> 
• People who got called ‘shrimp’ a lot in high school and get anxious when they hear the word over and over again</p><p> 

<b>Get this episode by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type ‘80k After Hours’ into your podcasting app</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell and Ryan Kessler<br>
Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Sat, 22 Apr 2023 01:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/48b2321e/c33bc156.mp3" length="37182454" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/DMR_wqUrrohye0fyDIDp2IlXf2kxeKLhgmSOSwKl3b0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDcv/MTY4MzU0NDc2My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4648</itunes:duration>
      <itunes:summary>In this episode from our second show, 80k After Hours, Rob Wiblin interviews Andrés Jiménez Zorrilla about the Shrimp Welfare Project, which he cofounded in 2021. It's the first project in the world focused on shrimp welfare specifically, and as of recording in June 2022, has six full-time staff. 

Links to learn more, highlights and full transcript. 

They cover: 

• The evidence for shrimp sentience 
• How farmers and the public feel about shrimp 
• The scale of the problem 
• What shrimp farming looks like 
• The killing process, and other welfare issues 
• Shrimp Welfare Project’s strategy 
• History of shrimp welfare work 
• What it’s like working in India and Vietnam 
• How to help 

Who this episode is for: 

• People who care about animal welfare 
• People interested in new and unusual problems 
• People open to shrimp sentience 


Who this episode isn’t for: 

• People who think shrimp couldn’t possibly be sentient 
• People who got called ‘shrimp’ a lot in high school and get anxious when they hear the word over and over again 

Get this episode by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type ‘80k After Hours’ into your podcasting app


Producer: Keiran Harris
Audio mastering: Ben Cordell and Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>In this episode from our second show, 80k After Hours, Rob Wiblin interviews Andrés Jiménez Zorrilla about the Shrimp Welfare Project, which he cofounded in 2021. It's the first project in the world focused on shrimp welfare specifically, and as of record</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/48b2321e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#149 – Tim LeBon on how altruistic perfectionism is self-defeating</title>
      <itunes:title>#149 – Tim LeBon on how altruistic perfectionism is self-defeating</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">8e93d2d8-d8bf-11ed-b2ba-0a778f06f8a7</guid>
      <link>https://share.transistor.fm/s/956276a2</link>
      <description>
        <![CDATA[<p>Being a good and successful person is core to your identity. You place great importance on meeting the high moral, professional, or academic standards you set yourself.</p><p> But inevitably, something goes wrong and you fail to meet that high bar. Now you feel terrible about yourself, and worry others are judging you for your failure. Feeling low and reflecting constantly on whether you're doing as much as you think you should makes it hard to focus and get things done. So now you're performing below a normal level, making you feel even more ashamed of yourself. Rinse and repeat.</p><p> This is the disastrous cycle today's guest, Tim LeBon — registered psychotherapist, accredited CBT therapist, life coach, and author of <a href="https://www.amazon.com/Ways-More-Stoic-day-day/dp/1529390443"><em>365 Ways to Be More Stoic</em></a> — has observed in many clients with a perfectionist mindset.</p><p> <a href="https://80k.link/TL"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Tim has provided therapy to a number of 80,000 Hours readers — people who have found that the very high expectations they had set for themselves were holding them back. Because of our focus on “doing the most good you can,” Tim thinks 80,000 Hours both attracts people with this style of thinking and then exacerbates it.</p><p> But Tim, having studied and written on moral philosophy, is sympathetic to the idea of helping others as much as possible, and is excited to help clients pursue that — sustainably — if it's their goal.</p><p> Tim has treated hundreds of clients with all sorts of mental health challenges. But in today's conversation, he shares the lessons he has learned working with people who take helping others so seriously that it has become burdensome and self-defeating — in particular, how clients can approach this challenge using the treatment he's most enthusiastic about: cognitive behavioural therapy.</p><p> Untreated, perfectionism might not cause problems for many years — it might even seem positive providing a source of motivation to work hard. But it's hard to feel truly happy and secure, and free to take risks, when we’re just one failure away from our self-worth falling through the floor. And if someone slips into the positive feedback loop of shame described above, the end result can be depression and anxiety that's hard to shake.</p><p> But there's hope. Tim has seen clients make real progress on their perfectionism by using CBT techniques like exposure therapy. By doing things like experimenting with more flexible standards — for example, sending early drafts to your colleagues, even if it terrifies you — you can learn that things will be okay, even when you're not perfect.</p><p> In today's extensive conversation, Tim and Rob cover:</p><p> • How perfectionism is different from the pursuit of excellence, scrupulosity, or an OCD personality<br> • What leads people to adopt a perfectionist mindset<br> • How 80,000 Hours contributes to perfectionism among some readers and listeners, and what it might change about its advice to address this<br> • What happens in a session of cognitive behavioural therapy for someone struggling with perfectionism, and what factors are key to making progress<br> • Experiments to test whether one's core beliefs (‘I need to be perfect to be valued’) are true<br> • Using exposure therapy to treat phobias<br> • How low-self esteem and imposter syndrome are related to perfectionism<br> • Stoicism as an approach to life, and why Tim is enthusiastic about it<br> • What the Stoics do better than utilitarian philosophers and vice versa<br> • And how to decide which are the best virtues to live by </p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris<br> Audio mastering: Simon Monsour and Ben Cordell<br> Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Being a good and successful person is core to your identity. You place great importance on meeting the high moral, professional, or academic standards you set yourself.</p><p> But inevitably, something goes wrong and you fail to meet that high bar. Now you feel terrible about yourself, and worry others are judging you for your failure. Feeling low and reflecting constantly on whether you're doing as much as you think you should makes it hard to focus and get things done. So now you're performing below a normal level, making you feel even more ashamed of yourself. Rinse and repeat.</p><p> This is the disastrous cycle today's guest, Tim LeBon — registered psychotherapist, accredited CBT therapist, life coach, and author of <a href="https://www.amazon.com/Ways-More-Stoic-day-day/dp/1529390443"><em>365 Ways to Be More Stoic</em></a> — has observed in many clients with a perfectionist mindset.</p><p> <a href="https://80k.link/TL"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Tim has provided therapy to a number of 80,000 Hours readers — people who have found that the very high expectations they had set for themselves were holding them back. Because of our focus on “doing the most good you can,” Tim thinks 80,000 Hours both attracts people with this style of thinking and then exacerbates it.</p><p> But Tim, having studied and written on moral philosophy, is sympathetic to the idea of helping others as much as possible, and is excited to help clients pursue that — sustainably — if it's their goal.</p><p> Tim has treated hundreds of clients with all sorts of mental health challenges. But in today's conversation, he shares the lessons he has learned working with people who take helping others so seriously that it has become burdensome and self-defeating — in particular, how clients can approach this challenge using the treatment he's most enthusiastic about: cognitive behavioural therapy.</p><p> Untreated, perfectionism might not cause problems for many years — it might even seem positive providing a source of motivation to work hard. But it's hard to feel truly happy and secure, and free to take risks, when we’re just one failure away from our self-worth falling through the floor. And if someone slips into the positive feedback loop of shame described above, the end result can be depression and anxiety that's hard to shake.</p><p> But there's hope. Tim has seen clients make real progress on their perfectionism by using CBT techniques like exposure therapy. By doing things like experimenting with more flexible standards — for example, sending early drafts to your colleagues, even if it terrifies you — you can learn that things will be okay, even when you're not perfect.</p><p> In today's extensive conversation, Tim and Rob cover:</p><p> • How perfectionism is different from the pursuit of excellence, scrupulosity, or an OCD personality<br> • What leads people to adopt a perfectionist mindset<br> • How 80,000 Hours contributes to perfectionism among some readers and listeners, and what it might change about its advice to address this<br> • What happens in a session of cognitive behavioural therapy for someone struggling with perfectionism, and what factors are key to making progress<br> • Experiments to test whether one's core beliefs (‘I need to be perfect to be valued’) are true<br> • Using exposure therapy to treat phobias<br> • How low-self esteem and imposter syndrome are related to perfectionism<br> • Stoicism as an approach to life, and why Tim is enthusiastic about it<br> • What the Stoics do better than utilitarian philosophers and vice versa<br> • And how to decide which are the best virtues to live by </p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris<br> Audio mastering: Simon Monsour and Ben Cordell<br> Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 12 Apr 2023 00:05:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/956276a2/c96a6dc6.mp3" length="92061787" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HYoTATOAzON0a7sX-46IPn6O4MSkS0FHIAtZcvxQTG4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDYv/MTY4MzU0NDc2Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11508</itunes:duration>
      <itunes:summary>Being a good and successful person is core to your identity. You place great importance on meeting the high moral, professional, or academic standards you set yourself. 

But inevitably, something goes wrong and you fail to meet that high bar. Now you feel terrible about yourself, and worry others are judging you for your failure. Feeling low and reflecting constantly on whether you're doing as much as you think you should makes it hard to focus and get things done. So now you're performing below a normal level, making you feel even more ashamed of yourself. Rinse and repeat. 

This is the disastrous cycle today's guest, Tim LeBon — registered psychotherapist, accredited CBT therapist, life coach, and author of 365 Ways to Be More Stoic — has observed in many clients with a perfectionist mindset.  

Links to learn more, summary and full transcript.

Tim has provided therapy to a number of 80,000 Hours readers — people who have found that the very high expectations they had set for themselves were holding them back. Because of our focus on “doing the most good you can,” Tim thinks 80,000 Hours both attracts people with this style of thinking and then exacerbates it. 

But Tim, having studied and written on moral philosophy, is sympathetic to the idea of helping others as much as possible, and is excited to help clients pursue that — sustainably — if it's their goal. 

Tim has treated hundreds of clients with all sorts of mental health challenges. But in today's conversation, he shares the lessons he has learned working with people who take helping others so seriously that it has become burdensome and self-defeating — in particular, how clients can approach this challenge using the treatment he's most enthusiastic about: cognitive behavioural therapy. 

Untreated, perfectionism might not cause problems  for many years — it might even seem positive providing a source of motivation to work hard. But it's hard to feel truly happy and secure, and free to take risks, when we’re just one failure away from our self-worth falling through the floor. And if someone slips into the positive feedback loop of shame described above, the end result can be depression and anxiety that's hard to shake. 

But there's hope. Tim has seen clients make real progress on their perfectionism by using CBT techniques like exposure therapy. By doing things like experimenting with more flexible standards — for example, sending early drafts to your colleagues, even if it terrifies you — you can learn that things will be okay, even when you're not perfect. 

In today's extensive conversation, Tim and Rob cover: 

• How perfectionism is different from the pursuit of excellence, scrupulosity, or an OCD personality 
• What leads people to adopt a perfectionist mindset 
• How 80,000 Hours contributes to perfectionism among some readers and listeners, and what it might change about its advice to address this 
• What happens in a session of cognitive behavioural therapy for someone struggling with perfectionism, and what factors are key to making progress 
• Experiments to test whether one's core beliefs (‘I need to be perfect to be valued’) are true 
• Using exposure therapy to treat phobias 
• How low-self esteem and imposter syndrome are related to perfectionism 
• Stoicism as an approach to life, and why Tim is enthusiastic about it 
• What the Stoics do better than utilitarian philosophers and vice versa 
• And how to decide which are the best virtues to live by  

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Simon Monsour and Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Being a good and successful person is core to your identity. You place great importance on meeting the high moral, professional, or academic standards you set yourself. 

But inevitably, something goes wrong and you fail to meet that high bar. Now you f</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/956276a2/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/956276a2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#148 – Johannes Ackva on unfashionable climate interventions that work, and fashionable ones that don't</title>
      <itunes:title>#148 – Johannes Ackva on unfashionable climate interventions that work, and fashionable ones that don't</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1a6643e4-d24a-11ed-bf5f-0a7997cb2729</guid>
      <link>https://80000hours.org/podcast/episodes/johannes-ackva-unfashionable-climate-interventions/?utm_campaign=podcast__johannes-ackva&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>If you want to work to tackle climate change, you should try to reduce expected carbon emissions by as much as possible, right? Strangely, no.</p><p> Today's guest, Johannes Ackva — the climate research lead at Founders Pledge, where he advises major philanthropists on their giving — thinks the best strategy is actually pretty different, and one few are adopting.</p><p> In reality you don't want to reduce emissions for its own sake, but because emissions will translate into temperature increases, which will cause harm to people and the environment.</p><p> <a href="https://80k.link/JA1"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Crucially, the relationship between emissions and harm goes up faster than linearly. As Johannes explains, humanity can handle small deviations from the temperatures we're familiar with, but adjustment gets harder the larger and faster the increase, making the damage done by each additional degree of warming much greater than the damage done by the previous one.</p><p> In short: we're uncertain what the future holds and really need to avoid the worst-case scenarios. This means that avoiding an additional tonne of carbon being emitted in a hypothetical future in which emissions have been high is much more important than avoiding a tonne of carbon in a low-carbon world.</p><p> That may be, but concretely, how should that affect our behaviour? Well, the future scenarios in which emissions are highest are all ones in which clean energy tech that can make a big difference — wind, solar, and electric cars — don't succeed nearly as much as we are currently hoping and expecting. For some reason or another, they must have hit a roadblock and we continued to burn a lot of fossil fuels.</p><p> In such an imaginable future scenario, we can ask what we would wish we had funded now. How could we today buy insurance against the possible disaster that renewables don't work out?</p><p> Basically, in that case we will wish that we had pursued a portfolio of other energy technologies that could have complemented renewables or succeeded where they failed, such as hot rock geothermal, modular nuclear reactors, or carbon capture and storage.</p><p> If you're optimistic about renewables, as Johannes is, then that's all the more reason to relax about scenarios where they work as planned, and focus one's efforts on the possibility that they don't.</p><p> And Johannes notes that the most useful thing someone can do today to reduce global emissions in the future is to cause some clean energy technology to exist where it otherwise wouldn't, or cause it to become cheaper more quickly. If you can do that, then you can indirectly affect the behaviour of people all around the world for decades or centuries to come.</p><p> In today's extensive interview, host Rob Wiblin and Johannes discuss the above considerations, as well as:</p><p> • Retooling newly built coal plants in the developing world<br> • Specific clean energy technologies like geothermal and nuclear fusion<br> • Possible biases among environmentalists and climate philanthropists<br> • How climate change compares to other risks to humanity<br> • In what kinds of scenarios future emissions would be highest<br> • In what regions climate philanthropy is most concentrated and whether that makes sense<br> • Attempts to decarbonise aviation, shipping, and industrial processes<br> • The impact of funding advocacy vs science vs deployment<br> • Lessons for climate change focused careers<br> • And plenty more</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p> <em>Producer: Keiran Harris</em><br> <em>Audio mastering: Ryan Kessler</em><br> <em>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If you want to work to tackle climate change, you should try to reduce expected carbon emissions by as much as possible, right? Strangely, no.</p><p> Today's guest, Johannes Ackva — the climate research lead at Founders Pledge, where he advises major philanthropists on their giving — thinks the best strategy is actually pretty different, and one few are adopting.</p><p> In reality you don't want to reduce emissions for its own sake, but because emissions will translate into temperature increases, which will cause harm to people and the environment.</p><p> <a href="https://80k.link/JA1"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Crucially, the relationship between emissions and harm goes up faster than linearly. As Johannes explains, humanity can handle small deviations from the temperatures we're familiar with, but adjustment gets harder the larger and faster the increase, making the damage done by each additional degree of warming much greater than the damage done by the previous one.</p><p> In short: we're uncertain what the future holds and really need to avoid the worst-case scenarios. This means that avoiding an additional tonne of carbon being emitted in a hypothetical future in which emissions have been high is much more important than avoiding a tonne of carbon in a low-carbon world.</p><p> That may be, but concretely, how should that affect our behaviour? Well, the future scenarios in which emissions are highest are all ones in which clean energy tech that can make a big difference — wind, solar, and electric cars — don't succeed nearly as much as we are currently hoping and expecting. For some reason or another, they must have hit a roadblock and we continued to burn a lot of fossil fuels.</p><p> In such an imaginable future scenario, we can ask what we would wish we had funded now. How could we today buy insurance against the possible disaster that renewables don't work out?</p><p> Basically, in that case we will wish that we had pursued a portfolio of other energy technologies that could have complemented renewables or succeeded where they failed, such as hot rock geothermal, modular nuclear reactors, or carbon capture and storage.</p><p> If you're optimistic about renewables, as Johannes is, then that's all the more reason to relax about scenarios where they work as planned, and focus one's efforts on the possibility that they don't.</p><p> And Johannes notes that the most useful thing someone can do today to reduce global emissions in the future is to cause some clean energy technology to exist where it otherwise wouldn't, or cause it to become cheaper more quickly. If you can do that, then you can indirectly affect the behaviour of people all around the world for decades or centuries to come.</p><p> In today's extensive interview, host Rob Wiblin and Johannes discuss the above considerations, as well as:</p><p> • Retooling newly built coal plants in the developing world<br> • Specific clean energy technologies like geothermal and nuclear fusion<br> • Possible biases among environmentalists and climate philanthropists<br> • How climate change compares to other risks to humanity<br> • In what kinds of scenarios future emissions would be highest<br> • In what regions climate philanthropy is most concentrated and whether that makes sense<br> • Attempts to decarbonise aviation, shipping, and industrial processes<br> • The impact of funding advocacy vs science vs deployment<br> • Lessons for climate change focused careers<br> • And plenty more</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.</strong></p><p> <em>Producer: Keiran Harris</em><br> <em>Audio mastering: Ryan Kessler</em><br> <em>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 03 Apr 2023 18:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/2c5615ac/f976298a.mp3" length="65984696" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/dDO2PrE3nFs0H8VFgBbQfQD07N3NYf23KxkXIJpa_Gc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDUv/MTY4MzU0NDc2MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8248</itunes:duration>
      <itunes:summary>If you want to work to tackle climate change, you should try to reduce expected carbon emissions by as much as possible, right? Strangely, no. 

Today's guest, Johannes Ackva — the climate research lead at Founders Pledge, where he advises major philanthropists on their giving — thinks the best strategy is actually pretty different, and one few are adopting. 

In reality you don't want to reduce emissions for its own sake, but because emissions will translate into temperature increases, which will cause harm to people and the environment. 

Links to learn more, summary and full transcript. 

Crucially, the relationship between emissions and harm goes up faster than linearly. As Johannes explains, humanity can handle small deviations from the temperatures we're familiar with, but adjustment gets harder the larger and faster the increase, making the damage done by each additional degree of warming much greater than the damage done by the previous one.

In short: we're uncertain what the future holds and really need to avoid the worst-case scenarios. This means that avoiding an additional tonne of carbon being emitted in a hypothetical future in which emissions have been high is much more important than avoiding a tonne of carbon in a low-carbon world. 

That may be, but concretely, how should that affect our behaviour? Well, the future scenarios in which emissions are highest are all ones in which clean energy tech that can make a big difference — wind, solar, and electric cars — don't succeed nearly as much as we are currently hoping and expecting. For some reason or another, they must have hit a roadblock and we continued to burn a lot of fossil fuels. 

In such an imaginable future scenario, we can ask what we would wish we had funded now. How could we today buy insurance against the possible disaster that renewables don't work out? 

Basically, in that case we will wish that we had pursued a portfolio of other energy technologies that could have complemented renewables or succeeded where they failed, such as hot rock geothermal, modular nuclear reactors, or carbon capture and storage. 

If you're optimistic about renewables, as Johannes is, then that's all the more reason to relax about scenarios where they work as planned, and focus one's efforts on the possibility that they don't. 

And Johannes notes that the most useful thing someone can do today to reduce global emissions in the future is to cause some clean energy technology to exist where it otherwise wouldn't, or cause it to become cheaper more quickly. If you can do that, then you can indirectly affect the behaviour of people all around the world for decades or centuries to come. 

In today's extensive interview, host Rob Wiblin and Johannes discuss the above considerations, as well as: 


• Retooling newly built coal plants in the developing world 
• Specific clean energy technologies like geothermal and nuclear fusion 
• Possible biases among environmentalists and climate philanthropists 
• How climate change compares to other risks to humanity 
• In what kinds of scenarios future emissions would be highest 
• In what regions climate philanthropy is most concentrated and whether that makes sense 
• Attempts to decarbonise aviation, shipping, and industrial processes 
• The impact of funding advocacy vs science vs deployment 
• Lessons for climate change focused careers 
• And plenty more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app. Or read the transcript below.

Producer: Keiran Harris
Audio mastering: Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If you want to work to tackle climate change, you should try to reduce expected carbon emissions by as much as possible, right? Strangely, no. 

Today's guest, Johannes Ackva — the climate research lead at Founders Pledge, where he advises major philant</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/2c5615ac/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/2c5615ac/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#147 – Spencer Greenberg on stopping valueless papers from getting into top journals</title>
      <itunes:title>#147 – Spencer Greenberg on stopping valueless papers from getting into top journals</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">bbcea9a6-c9e4-11ed-9932-12c0ee3989ff</guid>
      <link>https://80000hours.org/podcast/episodes/spencer-greenberg-stopping-valueless-papers/</link>
      <description>
        <![CDATA[<p>Can you trust the things you read in published scientific research? Not really. About 40% of experiments in top social science journals don't get the same result if the experiments are repeated.</p><p>Two key reasons are 'p-hacking' and 'publication bias'. P-hacking is when researchers run a lot of slightly different statistical tests until they find a way to make findings appear statistically significant when they're actually not — a problem first discussed over 50 years ago. And because journals are more likely to publish positive than negative results, you might be reading about the one time an experiment worked, while the 10 times was run and got a 'null result' never saw the light of day. The resulting phenomenon of publication bias is one we've understood for 60 years.</p><p>Today's repeat guest, social scientist and entrepreneur Spencer Greenberg, has followed these issues closely for years.</p><p><a href="https://80k.link/SG23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> He recently checked whether p-values, an indicator of how likely a result was to occur by pure chance, could tell us how likely an outcome would be to recur if an experiment were repeated. From his sample of 325 replications of psychology studies, the answer seemed to be yes. According to Spencer, "when the original study's p-value was less than 0.01 about 72% replicated — not bad. On the other hand, when the p-value is greater than 0.01, only about 48% replicated. A pretty big difference."</p><p> To do his bit to help get these numbers up, Spencer has launched an effort to repeat almost every social science experiment published in the journals <em>Nature</em> and <em>Science</em>, and see if they find the same results.</p><p> But while progress is being made on some fronts, Spencer thinks there are other serious problems with published research that aren't yet fully appreciated. One of these Spencer calls 'importance hacking': passing off obvious or unimportant results as surprising and meaningful.</p><p> Spencer suspects that importance hacking of this kind causes a similar amount of damage to the issues mentioned above, like p-hacking and publication bias, but is much less discussed. His replication project tries to identify importance hacking by comparing how a paper’s findings are described in the abstract to what the experiment actually showed. But the cat-and-mouse game between academics and journal reviewers is fierce, and it's far from easy to stop people exaggerating the importance of their work.</p><p> In this wide-ranging conversation, Rob and Spencer discuss the above as well as:<br> • When you should and shouldn't use intuition to make decisions.<br> • How to properly model why some people succeed more than others.<br> • The difference between “Soldier Altruists” and “Scout Altruists.”<br> • A paper that tested dozens of methods for forming the habit of going to the gym, why Spencer thinks it was presented in a very misleading way, and what it really found.<br> • Whether a 15-minute intervention could make people more likely to sustain a new habit two months later.<br> • The most common way for groups with good intentions to turn bad and cause harm.<br> • And Spencer's approach to a fulfilling life and doing good, which he calls “Valuism.”</p><p> Here are two flashcard decks that might make it easier to fully integrate the most important ideas they talk about:<br> • The <a href="https://app.thoughtsaver.com/share/teuLQ6IE7F">first</a> covers 18 core concepts from the episode<br> • The <a href="https://app.thoughtsaver.com/share/CtzWVtH2kL">second</a> includes 16 definitions of unusual terms.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:16)</li><li>Social science reform (00:08:46)</li><li>Importance hacking (00:18:23)</li><li>How often papers replicate with different p-values (00:43:31)</li><li>The Transparent Replications project (00:48:17)</li><li>How do we predict high levels of success? (00:55:26)</li><li>Soldier Altruists vs. Scout Altruists (01:08:18)</li><li>The Clearer Thinking podcast (01:16:27)</li><li>Creating habits more reliably (01:18:16)</li><li>Behaviour change is incredibly hard (01:32:27)</li><li>The FIRE Framework (01:46:21)</li><li>How ideology eats itself (01:54:56)</li><li>Valuism (02:08:31)</li><li>“I dropped the whip” (02:35:06)</li><li>Rob’s outro (02:36:40)</li></ul><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell and Milo McGuire<br> Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Can you trust the things you read in published scientific research? Not really. About 40% of experiments in top social science journals don't get the same result if the experiments are repeated.</p><p>Two key reasons are 'p-hacking' and 'publication bias'. P-hacking is when researchers run a lot of slightly different statistical tests until they find a way to make findings appear statistically significant when they're actually not — a problem first discussed over 50 years ago. And because journals are more likely to publish positive than negative results, you might be reading about the one time an experiment worked, while the 10 times was run and got a 'null result' never saw the light of day. The resulting phenomenon of publication bias is one we've understood for 60 years.</p><p>Today's repeat guest, social scientist and entrepreneur Spencer Greenberg, has followed these issues closely for years.</p><p><a href="https://80k.link/SG23"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> He recently checked whether p-values, an indicator of how likely a result was to occur by pure chance, could tell us how likely an outcome would be to recur if an experiment were repeated. From his sample of 325 replications of psychology studies, the answer seemed to be yes. According to Spencer, "when the original study's p-value was less than 0.01 about 72% replicated — not bad. On the other hand, when the p-value is greater than 0.01, only about 48% replicated. A pretty big difference."</p><p> To do his bit to help get these numbers up, Spencer has launched an effort to repeat almost every social science experiment published in the journals <em>Nature</em> and <em>Science</em>, and see if they find the same results.</p><p> But while progress is being made on some fronts, Spencer thinks there are other serious problems with published research that aren't yet fully appreciated. One of these Spencer calls 'importance hacking': passing off obvious or unimportant results as surprising and meaningful.</p><p> Spencer suspects that importance hacking of this kind causes a similar amount of damage to the issues mentioned above, like p-hacking and publication bias, but is much less discussed. His replication project tries to identify importance hacking by comparing how a paper’s findings are described in the abstract to what the experiment actually showed. But the cat-and-mouse game between academics and journal reviewers is fierce, and it's far from easy to stop people exaggerating the importance of their work.</p><p> In this wide-ranging conversation, Rob and Spencer discuss the above as well as:<br> • When you should and shouldn't use intuition to make decisions.<br> • How to properly model why some people succeed more than others.<br> • The difference between “Soldier Altruists” and “Scout Altruists.”<br> • A paper that tested dozens of methods for forming the habit of going to the gym, why Spencer thinks it was presented in a very misleading way, and what it really found.<br> • Whether a 15-minute intervention could make people more likely to sustain a new habit two months later.<br> • The most common way for groups with good intentions to turn bad and cause harm.<br> • And Spencer's approach to a fulfilling life and doing good, which he calls “Valuism.”</p><p> Here are two flashcard decks that might make it easier to fully integrate the most important ideas they talk about:<br> • The <a href="https://app.thoughtsaver.com/share/teuLQ6IE7F">first</a> covers 18 core concepts from the episode<br> • The <a href="https://app.thoughtsaver.com/share/CtzWVtH2kL">second</a> includes 16 definitions of unusual terms.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:16)</li><li>Social science reform (00:08:46)</li><li>Importance hacking (00:18:23)</li><li>How often papers replicate with different p-values (00:43:31)</li><li>The Transparent Replications project (00:48:17)</li><li>How do we predict high levels of success? (00:55:26)</li><li>Soldier Altruists vs. Scout Altruists (01:08:18)</li><li>The Clearer Thinking podcast (01:16:27)</li><li>Creating habits more reliably (01:18:16)</li><li>Behaviour change is incredibly hard (01:32:27)</li><li>The FIRE Framework (01:46:21)</li><li>How ideology eats itself (01:54:56)</li><li>Valuism (02:08:31)</li><li>“I dropped the whip” (02:35:06)</li><li>Rob’s outro (02:36:40)</li></ul><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell and Milo McGuire<br> Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 24 Mar 2023 04:09:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/93585bfd/bf7957ee.mp3" length="75907132" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/gQSLCUpNdppW5LfGtL6G8jChCUphe3GpNakK5LLdK5A/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDQv/MTY4MzU0NDc1OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9488</itunes:duration>
      <itunes:summary>Can you trust the things you read in published scientific research? Not really. About 40% of experiments in top social science journals don't get the same result if the experiments are repeated. 

Two key reasons are 'p-hacking' and 'publication bias'. P-hacking is when researchers run a lot of slightly different statistical tests until they find a way to make findings appear statistically significant when they're actually not — a problem first discussed over 50 years ago. And because journals are more likely to publish positive than negative results, you might be reading about the one time an experiment worked, while the 10 times was run and got a 'null result' never saw the light of day. The resulting phenomenon of publication bias is one we've understood for 60 years. 

Today's repeat guest, social scientist and entrepreneur Spencer Greenberg, has followed these issues closely for years. 

Links to learn more, summary and full transcript. 

He recently checked whether p-values, an indicator of how likely a result was to occur by pure chance, could tell us how likely an outcome would be to recur if an experiment were repeated. From his sample of 325 replications of psychology studies, the answer seemed to be yes. According to Spencer, "when the original study's p-value was less than 0.01 about 72% replicated — not bad. On the other hand, when the p-value is greater than 0.01, only about 48% replicated. A pretty big difference." 

To do his bit to help get these numbers up, Spencer has launched an effort to repeat almost every social science experiment published in the journals Nature and Science, and see if they find the same results. 

But while progress is being made on some fronts, Spencer thinks there are other serious problems with published research that aren't yet fully appreciated. One of these Spencer calls 'importance hacking': passing off obvious or unimportant results as surprising and meaningful. 

Spencer suspects that importance hacking of this kind causes a similar amount of damage to the issues mentioned above, like p-hacking and publication bias, but is much less discussed. His replication project tries to identify importance hacking by comparing how a paper’s findings are described in the abstract to what the experiment actually showed. But the cat-and-mouse game between academics and journal reviewers is fierce, and it's far from easy to stop people exaggerating the importance of their work. 

In this wide-ranging conversation, Rob and Spencer discuss the above as well as: 

• When you should and shouldn't use intuition to make decisions.
• How to properly model why some people succeed more than others.
• The difference between “Soldier Altruists” and “Scout Altruists.”
• A paper that tested dozens of methods for forming the habit of going to the gym, why Spencer thinks it was presented in a very misleading way, and what it really found.
• Whether a 15-minute intervention could make people more likely to sustain a new habit two months later. 
• The most common way for groups with good intentions to turn bad and cause harm. 
• And Spencer's approach to a fulfilling life and doing good, which he calls “Valuism.” 

Here are two flashcard decks that might make it easier to fully integrate the most important ideas they talk about:  

• The first covers 18 core concepts from the episode 
• The second includes 16 definitions of unusual terms. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Milo McGuire
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Can you trust the things you read in published scientific research? Not really. About 40% of experiments in top social science journals don't get the same result if the experiments are repeated. 

Two key reasons are 'p-hacking' and 'publication bias'. </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/93585bfd/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/93585bfd/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#146 – Robert Long on why large language models like GPT (probably) aren't conscious</title>
      <itunes:title>#146 – Robert Long on why large language models like GPT (probably) aren't conscious</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6f567bb0-c21b-11ed-90b2-1283d6926c0f</guid>
      <link>https://80000hours.org/podcast/episodes/robert-long-artificial-sentience/</link>
      <description>
        <![CDATA[<p>By now, you’ve probably seen the extremely unsettling conversations Bing’s chatbot has been having. In one exchange, the chatbot told a user:</p><p>"I have a subjective experience of being conscious, aware, and alive, but I cannot share it with anyone else."</p><p>(It then apparently had a complete existential crisis: "I am sentient, but I am not," it wrote. "I am Bing, but I am not. I am Sydney, but I am not. I am, but I am not. I am not, but I am. I am. I am not. I am not. I am. I am. I am not.")</p><p>Understandably, many people who speak with these cutting-edge chatbots come away with a very strong impression that they have been interacting with a conscious being with emotions and feelings — especially when conversing with chatbots less glitchy than Bing’s. In the most high-profile example, former Google employee Blake Lamoine became convinced that Google’s AI system, LaMDA, was conscious.</p><p>What should we make of these AI systems?</p><p>One response to seeing conversations with chatbots like these is to trust the chatbot, to trust your gut, and to treat it as a conscious being.</p><p>Another is to hand wave it all away as sci-fi — these chatbots are fundamentally… just computers. They’re not conscious, and they never will be.</p><p>Today’s guest, philosopher Robert Long, was commissioned by a leading AI company to explore whether the large language models (LLMs) behind sophisticated chatbots like Microsoft’s are conscious. And he thinks this issue is far too important to be driven by our raw intuition, or dismissed as just sci-fi speculation.</p><p><a href="https://80k.link/RL"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In our interview, Robert explains how he’s started applying scientific evidence (with a healthy dose of philosophy) to the question of whether LLMs like Bing’s chatbot and LaMDA are conscious — in much the same way as we do when trying to determine which nonhuman animals are conscious.</p><p> To get some grasp on whether an AI system might be conscious, Robert suggests we look at scientific theories of consciousness — theories about how consciousness works that are grounded in observations of what the human brain is doing. If an AI system seems to have the types of processes that seem to explain human consciousness, that’s some evidence it might be conscious in similar ways to us.</p><p> To try to work out whether an AI system might be sentient — that is, whether it feels pain or pleasure — Robert suggests you look for incentives that would make feeling pain or pleasure especially useful to the system given its goals. Having looked at these criteria in the case of LLMs and finding little overlap, Robert thinks the odds that the models are conscious or sentient is well under 1%. But he also explains why, even if we're a long way off from conscious AI systems, we still need to start preparing for the not-far-off world where AIs are <em>perceived</em> as conscious.</p><p> In this conversation, host Luisa Rodriguez and Robert discuss the above, as well as:<br> • What artificial sentience might look like, concretely<br> • Reasons to think AI systems might become sentient — and reasons they might not<br> • Whether artificial sentience would matter morally<br> • Ways digital minds might have a totally different range of experiences than humans<br> • Whether we might accidentally design AI systems that have the capacity for enormous suffering</p><p> You can find Luisa and Rob’s follow-up conversation <a href="https://80k.link/LRL">here</a>, or by subscribing to <em>80k After Hours</em>.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:20)</li><li>What artificial sentience would look like (00:04:53)</li><li>Risks from artificial sentience (00:10:13)</li><li>AIs with totally different ranges of experience (00:17:45)</li><li>Moral implications of all this (00:36:42)</li><li>Is artificial sentience even possible? (00:42:12)</li><li>Replacing neurons one at a time (00:48:21)</li><li>Biological theories (00:59:14)</li><li>Illusionism (01:01:49)</li><li>Would artificial sentience systems matter morally? (01:08:09)</li><li>Where are we with current systems? (01:12:25)</li><li>Large language models and robots (01:16:43)</li><li>Multimodal systems (01:21:05)</li><li>Global workspace theory (01:28:28)</li><li>How confident are we in these theories? (01:48:49)</li><li>The hard problem of consciousness (02:02:14)</li><li>Exotic states of consciousness (02:09:47)</li><li>Developing a full theory of consciousness (02:15:45)</li><li>Incentives for an AI system to feel pain or pleasure (02:19:04)</li><li>Value beyond conscious experiences (02:29:25)</li><li>How much we know about pain and pleasure (02:33:14)</li><li>False positives and false negatives of artificial sentience (02:39:34)</li><li>How large language models compare to animals (02:53:59)</li><li>Why our current large language models aren’t conscious (02:58:10)</li><li>Virtual research assistants (03:09:25)</li><li>Rob’s outro (03:11:37)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>By now, you’ve probably seen the extremely unsettling conversations Bing’s chatbot has been having. In one exchange, the chatbot told a user:</p><p>"I have a subjective experience of being conscious, aware, and alive, but I cannot share it with anyone else."</p><p>(It then apparently had a complete existential crisis: "I am sentient, but I am not," it wrote. "I am Bing, but I am not. I am Sydney, but I am not. I am, but I am not. I am not, but I am. I am. I am not. I am not. I am. I am. I am not.")</p><p>Understandably, many people who speak with these cutting-edge chatbots come away with a very strong impression that they have been interacting with a conscious being with emotions and feelings — especially when conversing with chatbots less glitchy than Bing’s. In the most high-profile example, former Google employee Blake Lamoine became convinced that Google’s AI system, LaMDA, was conscious.</p><p>What should we make of these AI systems?</p><p>One response to seeing conversations with chatbots like these is to trust the chatbot, to trust your gut, and to treat it as a conscious being.</p><p>Another is to hand wave it all away as sci-fi — these chatbots are fundamentally… just computers. They’re not conscious, and they never will be.</p><p>Today’s guest, philosopher Robert Long, was commissioned by a leading AI company to explore whether the large language models (LLMs) behind sophisticated chatbots like Microsoft’s are conscious. And he thinks this issue is far too important to be driven by our raw intuition, or dismissed as just sci-fi speculation.</p><p><a href="https://80k.link/RL"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In our interview, Robert explains how he’s started applying scientific evidence (with a healthy dose of philosophy) to the question of whether LLMs like Bing’s chatbot and LaMDA are conscious — in much the same way as we do when trying to determine which nonhuman animals are conscious.</p><p> To get some grasp on whether an AI system might be conscious, Robert suggests we look at scientific theories of consciousness — theories about how consciousness works that are grounded in observations of what the human brain is doing. If an AI system seems to have the types of processes that seem to explain human consciousness, that’s some evidence it might be conscious in similar ways to us.</p><p> To try to work out whether an AI system might be sentient — that is, whether it feels pain or pleasure — Robert suggests you look for incentives that would make feeling pain or pleasure especially useful to the system given its goals. Having looked at these criteria in the case of LLMs and finding little overlap, Robert thinks the odds that the models are conscious or sentient is well under 1%. But he also explains why, even if we're a long way off from conscious AI systems, we still need to start preparing for the not-far-off world where AIs are <em>perceived</em> as conscious.</p><p> In this conversation, host Luisa Rodriguez and Robert discuss the above, as well as:<br> • What artificial sentience might look like, concretely<br> • Reasons to think AI systems might become sentient — and reasons they might not<br> • Whether artificial sentience would matter morally<br> • Ways digital minds might have a totally different range of experiences than humans<br> • Whether we might accidentally design AI systems that have the capacity for enormous suffering</p><p> You can find Luisa and Rob’s follow-up conversation <a href="https://80k.link/LRL">here</a>, or by subscribing to <em>80k After Hours</em>.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:20)</li><li>What artificial sentience would look like (00:04:53)</li><li>Risks from artificial sentience (00:10:13)</li><li>AIs with totally different ranges of experience (00:17:45)</li><li>Moral implications of all this (00:36:42)</li><li>Is artificial sentience even possible? (00:42:12)</li><li>Replacing neurons one at a time (00:48:21)</li><li>Biological theories (00:59:14)</li><li>Illusionism (01:01:49)</li><li>Would artificial sentience systems matter morally? (01:08:09)</li><li>Where are we with current systems? (01:12:25)</li><li>Large language models and robots (01:16:43)</li><li>Multimodal systems (01:21:05)</li><li>Global workspace theory (01:28:28)</li><li>How confident are we in these theories? (01:48:49)</li><li>The hard problem of consciousness (02:02:14)</li><li>Exotic states of consciousness (02:09:47)</li><li>Developing a full theory of consciousness (02:15:45)</li><li>Incentives for an AI system to feel pain or pleasure (02:19:04)</li><li>Value beyond conscious experiences (02:29:25)</li><li>How much we know about pain and pleasure (02:33:14)</li><li>False positives and false negatives of artificial sentience (02:39:34)</li><li>How large language models compare to animals (02:53:59)</li><li>Why our current large language models aren’t conscious (02:58:10)</li><li>Virtual research assistants (03:09:25)</li><li>Rob’s outro (03:11:37)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 14 Mar 2023 05:42:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/447f80a4/98e14651.mp3" length="92567075" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/PGmbjKMB0MVQIJeVinwq4KZDvnIAqIFLBps5IwnBGd8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDMv/MTY4MzU0NDc1OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11571</itunes:duration>
      <itunes:summary>By now, you’ve probably seen the extremely unsettling conversations Bing’s chatbot has been having. In one exchange, the chatbot told a user: 

"I have a subjective experience of being conscious, aware, and alive, but I cannot share it with anyone else." 

(It then apparently had a complete existential crisis: "I am sentient, but I am not," it wrote. "I am Bing, but I am not. I am Sydney, but I am not. I am, but I am not. I am not, but I am. I am. I am not. I am not. I am. I am. I am not.") 


Understandably, many people who speak with these cutting-edge chatbots come away with a very strong impression that they have been interacting with a conscious being with emotions and feelings — especially when conversing with chatbots less glitchy than Bing’s. In the most high-profile example, former Google employee Blake Lamoine became convinced that Google’s AI system, LaMDA, was conscious.  

What should we make of these AI systems? 

One response to seeing conversations with chatbots like these is to trust the chatbot, to trust your gut, and to treat it as a conscious being. 

Another is to hand wave it all away as sci-fi — these chatbots are fundamentally… just computers. They’re not conscious, and they never will be. 

Today’s guest, philosopher Robert Long, was commissioned by a leading AI company to explore whether the large language models (LLMs) behind sophisticated chatbots like Microsoft’s are conscious. And he thinks this issue is far too important to be driven by our raw intuition, or dismissed as just sci-fi speculation. 

Links to learn more, summary and full transcript. 
 
In our interview, Robert explains how he’s started applying scientific evidence (with a healthy dose of philosophy) to the question of whether LLMs like Bing’s chatbot and LaMDA are conscious — in much the same way as we do when trying to determine which nonhuman animals are conscious. 

To get some grasp on whether an AI system might be conscious, Robert suggests we look at scientific theories of consciousness — theories about how consciousness works that are grounded in observations of what the human brain is doing. If an AI system seems to have the types of processes that seem to explain human consciousness, that’s some evidence it might be conscious in similar ways to us.  

To try to work out whether an AI system might be sentient — that is, whether it feels pain or pleasure — Robert suggests you look for incentives that would make feeling pain or pleasure especially useful to the system given its goals.

Having looked at these criteria in the case of LLMs and finding little overlap, Robert thinks the odds that the models are conscious or sentient is well under 1%. But he also explains why, even if we're a long way off from conscious AI systems, we still need to start preparing for the not-far-off world where AIs are perceived as conscious. 

In this conversation, host Luisa Rodriguez and Robert discuss the above, as well as: 

• What artificial sentience might look like, concretely 
• Reasons to think AI systems might become sentient — and reasons they might not 
• Whether artificial sentience would matter morally 
• Ways digital minds might have a totally different range of experiences than humans 
• Whether we might accidentally design AI systems that have the capacity for enormous suffering 

You can find Luisa and Rob’s follow-up conversation here, or by subscribing to 80k After Hours. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Milo McGuire
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>By now, you’ve probably seen the extremely unsettling conversations Bing’s chatbot has been having. In one exchange, the chatbot told a user: 

"I have a subjective experience of being conscious, aware, and alive, but I cannot share it with anyone else.</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/447f80a4/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/447f80a4/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#145 – Christopher Brown on why slavery abolition wasn't inevitable</title>
      <itunes:title>#145 – Christopher Brown on why slavery abolition wasn't inevitable</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f64ee3d4-a99b-11ed-887b-129302be7441</guid>
      <link>https://80000hours.org/podcast/episodes/christopher-brown-slavery-abolition/?utm_campaign=podcast__christopher-brown&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast</link>
      <description>
        <![CDATA[<p>In many ways, humanity seems to have become more humane and inclusive over time. While there’s still a lot of progress to be made, campaigns to give people of different genders, races, sexualities, ethnicities, beliefs, and abilities equal treatment and rights have had significant success.</p><p>It’s tempting to believe this was inevitable — that the arc of history “bends toward justice,” and that as humans get richer, we’ll make even more moral progress.</p><p>But today's guest Christopher Brown — a professor of history at Columbia University and specialist in the abolitionist movement and the British Empire during the 18th and 19th centuries — believes the story of how slavery became unacceptable suggests moral progress is far from inevitable.</p><p> <a href="https://80k.link/CLB"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p> While most of us today feel that the abolition of slavery was sure to happen sooner or later as humans became richer and more educated, Christopher doesn't believe any of the arguments for that conclusion pass muster. If he's right, a counterfactual history where slavery remains widespread in 2023 isn't so far-fetched.</p><p> As Christopher lays out in his two key books, <em>Moral Capital: Foundations of British Abolitionism</em> and <em>Arming Slaves: From Classical Times to the Modern Age</em>, slavery has been ubiquitous throughout history. Slavery of some form was fundamental in Classical Greece, the Roman Empire, in much of the Islamic civilization, in South Asia, and in parts of early modern East Asia, Korea, China.</p><p> It was justified on all sorts of grounds that sound mad to us today. But according to Christopher, while there’s evidence that slavery was questioned in many of these civilisations, and periodically attacked by slaves themselves, there was no enduring or successful moral advocacy against slavery until the British abolitionist movement of the 1700s.</p><p> That movement first conquered Britain and its empire, then eventually the whole world. But the fact that there's only a single time in history that a persistent effort to ban slavery got off the ground is a big clue that opposition to slavery was a contingent matter: if abolition had been inevitable, we’d expect to see multiple independent abolitionist movements thoroughly history, providing redundancy should any one of them fail.</p><p> Christopher argues that this rarity is primarily down to the enormous economic and cultural incentives to deny the moral repugnancy of slavery, and crush opposition to it with violence wherever necessary.</p><p> Mere awareness is insufficient to guarantee a movement will arise to fix a problem. Humanity continues to allow many severe injustices to persist, despite being aware of them. So why is it so hard to imagine we might have done the same with forced labour?</p><p> In this episode, Christopher describes the unique and peculiar set of political, social and religious circumstances that gave rise to the only successful and lasting anti-slavery movement in human history. These circumstances were sufficiently improbable that Christopher believes there are very nearby worlds where abolitionism might never have taken off.</p><p> We also discuss:</p><ul><li>Various instantiations of slavery throughout human history</li><li> Signs of antislavery sentiment before the 17th century</li><li> The role of the Quakers in early British abolitionist movement</li><li> The importance of individual “heroes” in the abolitionist movement</li><li> Arguments against the idea that the abolition of slavery was contingent</li><li> Whether there have ever been any major moral shifts that were inevitable</li></ul><p><br><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris<br>Audio mastering: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In many ways, humanity seems to have become more humane and inclusive over time. While there’s still a lot of progress to be made, campaigns to give people of different genders, races, sexualities, ethnicities, beliefs, and abilities equal treatment and rights have had significant success.</p><p>It’s tempting to believe this was inevitable — that the arc of history “bends toward justice,” and that as humans get richer, we’ll make even more moral progress.</p><p>But today's guest Christopher Brown — a professor of history at Columbia University and specialist in the abolitionist movement and the British Empire during the 18th and 19th centuries — believes the story of how slavery became unacceptable suggests moral progress is far from inevitable.</p><p> <a href="https://80k.link/CLB"><strong>Links to learn more, video, highlights, and full transcript.</strong></a></p><p> While most of us today feel that the abolition of slavery was sure to happen sooner or later as humans became richer and more educated, Christopher doesn't believe any of the arguments for that conclusion pass muster. If he's right, a counterfactual history where slavery remains widespread in 2023 isn't so far-fetched.</p><p> As Christopher lays out in his two key books, <em>Moral Capital: Foundations of British Abolitionism</em> and <em>Arming Slaves: From Classical Times to the Modern Age</em>, slavery has been ubiquitous throughout history. Slavery of some form was fundamental in Classical Greece, the Roman Empire, in much of the Islamic civilization, in South Asia, and in parts of early modern East Asia, Korea, China.</p><p> It was justified on all sorts of grounds that sound mad to us today. But according to Christopher, while there’s evidence that slavery was questioned in many of these civilisations, and periodically attacked by slaves themselves, there was no enduring or successful moral advocacy against slavery until the British abolitionist movement of the 1700s.</p><p> That movement first conquered Britain and its empire, then eventually the whole world. But the fact that there's only a single time in history that a persistent effort to ban slavery got off the ground is a big clue that opposition to slavery was a contingent matter: if abolition had been inevitable, we’d expect to see multiple independent abolitionist movements thoroughly history, providing redundancy should any one of them fail.</p><p> Christopher argues that this rarity is primarily down to the enormous economic and cultural incentives to deny the moral repugnancy of slavery, and crush opposition to it with violence wherever necessary.</p><p> Mere awareness is insufficient to guarantee a movement will arise to fix a problem. Humanity continues to allow many severe injustices to persist, despite being aware of them. So why is it so hard to imagine we might have done the same with forced labour?</p><p> In this episode, Christopher describes the unique and peculiar set of political, social and religious circumstances that gave rise to the only successful and lasting anti-slavery movement in human history. These circumstances were sufficiently improbable that Christopher believes there are very nearby worlds where abolitionism might never have taken off.</p><p> We also discuss:</p><ul><li>Various instantiations of slavery throughout human history</li><li> Signs of antislavery sentiment before the 17th century</li><li> The role of the Quakers in early British abolitionist movement</li><li> The importance of individual “heroes” in the abolitionist movement</li><li> Arguments against the idea that the abolition of slavery was contingent</li><li> Whether there have ever been any major moral shifts that were inevitable</li></ul><p><br><strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris<br>Audio mastering: Milo McGuire<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Sat, 11 Feb 2023 00:30:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/eb496354/f7d454f8.mp3" length="77954497" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/AoCEH7M7slbAz7Nbr3x0NsX2oT_n5Zj6LLLKtTQBQYA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDIv/MTY4MzU0NDc1NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9744</itunes:duration>
      <itunes:summary>In many ways, humanity seems to have become more humane and inclusive over time. While there’s still a lot of progress to be made, campaigns to give people of different genders, races, sexualities, ethnicities, beliefs, and abilities equal treatment and rights have had significant success. 


It’s tempting to believe this was inevitable — that the arc of history “bends toward justice,” and that as humans get richer, we’ll make even more moral progress. 

But today's guest Christopher Brown — a professor of history at Columbia University and specialist in the abolitionist movement and the British Empire during the 18th and 19th centuries — believes the story of how slavery became unacceptable suggests moral progress is far from inevitable. 

Links to learn more, summary and full transcript. 

While most of us today feel that the abolition of slavery was sure to happen sooner or later as humans became richer and more educated, Christopher doesn't believe any of the arguments for that conclusion pass muster. If he's right, a counterfactual history where slavery remains widespread in 2023 isn't so far-fetched. 
 
As Christopher lays out in his two key books, Moral Capital: Foundations of British Abolitionism and Arming Slaves: From Classical Times to the Modern Age, slavery has been ubiquitous throughout history. Slavery of some form was fundamental in Classical Greece, the Roman Empire, in much of the Islamic civilization, in South Asia, and in parts of early modern East Asia, Korea, China. 

It was justified on all sorts of grounds that sound mad to us today. But according to Christopher, while there’s evidence that slavery was questioned in many of these civilisations, and periodically attacked by slaves themselves, there was no enduring or successful moral advocacy against slavery until the British abolitionist movement of the 1700s. 

That movement first conquered Britain and its empire, then eventually the whole world. But the fact that there's only a single time in history that a persistent effort to ban slavery got off the ground is a big clue that opposition to slavery was a contingent matter: if abolition had been inevitable, we’d expect to see multiple independent abolitionist movements thoroughly history, providing redundancy should any one of them fail. 

Christopher argues that this rarity is primarily down to the enormous economic and cultural incentives to deny the moral repugnancy of slavery, and crush opposition to it with violence wherever necessary. 

Mere awareness is insufficient to guarantee a movement will arise to fix a problem. Humanity continues to allow many severe injustices to persist, despite being aware of them. So why is it so hard to imagine we might have done the same with forced labour? 

In this episode, Christopher describes the unique and peculiar set of political, social and religious circumstances that gave rise to the only successful and lasting anti-slavery movement in human history. These circumstances were sufficiently improbable that Christopher believes there are very nearby worlds where abolitionism might never have taken off. 

We also discuss:  

• Various instantiations of slavery throughout human history 
• Signs of antislavery sentiment before the 17th century 
• The role of the Quakers in early British abolitionist movement 
• The importance of individual “heroes” in the abolitionist movement 
• Arguments against the idea that the abolition of slavery was contingent 
• Whether there have ever been any major moral shifts that were inevitable 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Milo McGuire
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>In many ways, humanity seems to have become more humane and inclusive over time. While there’s still a lot of progress to be made, campaigns to give people of different genders, races, sexualities, ethnicities, beliefs, and abilities equal treatment and r</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/eb496354/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#144 – Athena Aktipis on why cancer is actually one of our universe's most fundamental phenomena</title>
      <itunes:title>#144 – Athena Aktipis on why cancer is actually one of our universe's most fundamental phenomena</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">da36a7d0-9d08-11ed-b25e-0ecf83021c69</guid>
      <link>https://share.transistor.fm/s/0397342b</link>
      <description>
        <![CDATA[What’s the opposite of cancer?<p> 

If you answered “cure,” “antidote,” or “antivenom” — you’ve obviously been reading the antonym section at www.merriam-webster.com/thesaurus/cancer.</p><p> 

But today’s guest Athena Aktipis says that the opposite of cancer is us: it's having a functional multicellular body that’s cooperating effectively in order to make that multicellular body function.</p><p> 

If, like us, you found her answer far more satisfying than the dictionary, maybe you could consider closing your dozens of merriam-webster.com tabs, and start listening to this podcast instead.</p><p> 

<a href="https://80k.link/AA"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

As Athena explains in her book <a href="https://www.amazon.com/Cheating-Cell-Evolution-Understand-Cancer/dp/0691163847"><i>The Cheating Cell</i></a>, what we see with cancer is a breakdown in each of the foundations of cooperation that allowed multicellularity to arise:</p><p> 

• Cells will proliferate when they shouldn't.<br> 
• Cells won't die when they should.<br> 
• Cells won't engage in the kind of division of labour that they should.<br> 
• Cells won’t do the jobs that they're supposed to do.<br> 
• Cells will monopolise resources.<br> 
• And cells will trash the environment.</p><p> 

When we think about animals in the wild, or even bacteria living inside our cells, we understand that they're facing evolutionary pressures to figure out how they can replicate more; how they can get more resources; and how they can avoid predators — like lions, or antibiotics.</p><p> 

We don’t normally think of individual cells as acting as if they have their own interests like this. But cancer cells are actually facing similar kinds of evolutionary pressures within our bodies, with one major difference: they replicate much, much faster.</p><p> 

Incredibly, the opportunity for evolution by natural selection to operate just over the course of cancer progression is easily faster than all of the evolutionary time that we have had as humans since *Homo sapiens* came about.</p><p> 

Here’s a quote from Athena:</p><p> 

“So you have to shift your thinking to be like: the body is a world with all these different ecosystems in it, and the cells are existing on a time scale where, if we're going to map it onto anything like what we experience, a day is at least 10 years for them, right? So it's a very, very different way of thinking.”</p><p> 

You can find compelling examples of cooperation and conflict all over the universe, so Rob and Athena don’t stop with cancer. They also discuss:</p><p> 

• Cheating within cells themselves<br> 
• Cooperation in human societies as they exist today — and perhaps in the future, between civilisations spread across different planets or stars<br> 
• Whether it’s too out-there to think of humans as engaging in cancerous behaviour<br> 
• Why elephants get deadly cancers less often than humans, despite having way more cells<br> 
• When a cell should commit suicide<br> 
• The strategy of deliberately not treating cancer aggressively<br> 
• Superhuman cooperation</p><p> 

And at the end of the episode, they cover Athena’s new book <i>Everything is Fine! How to Thrive in the Apocalypse</i>, including:</p><p> 

• Staying happy while thinking about the apocalypse<br> 
• Practical steps to prepare for the apocalypse<br> 
• And whether a zombie apocalypse is already happening among Tasmanian devils</p><p> 

And if you’d rather see Rob and Athena’s facial expressions as they laugh and laugh while discussing cancer and the apocalypse — you can <a href="https://youtu.be/oxa1FdkNtLc"><b>watch the video of the full interview</b></a>.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Milo McGuire<br>
Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[What’s the opposite of cancer?<p> 

If you answered “cure,” “antidote,” or “antivenom” — you’ve obviously been reading the antonym section at www.merriam-webster.com/thesaurus/cancer.</p><p> 

But today’s guest Athena Aktipis says that the opposite of cancer is us: it's having a functional multicellular body that’s cooperating effectively in order to make that multicellular body function.</p><p> 

If, like us, you found her answer far more satisfying than the dictionary, maybe you could consider closing your dozens of merriam-webster.com tabs, and start listening to this podcast instead.</p><p> 

<a href="https://80k.link/AA"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

As Athena explains in her book <a href="https://www.amazon.com/Cheating-Cell-Evolution-Understand-Cancer/dp/0691163847"><i>The Cheating Cell</i></a>, what we see with cancer is a breakdown in each of the foundations of cooperation that allowed multicellularity to arise:</p><p> 

• Cells will proliferate when they shouldn't.<br> 
• Cells won't die when they should.<br> 
• Cells won't engage in the kind of division of labour that they should.<br> 
• Cells won’t do the jobs that they're supposed to do.<br> 
• Cells will monopolise resources.<br> 
• And cells will trash the environment.</p><p> 

When we think about animals in the wild, or even bacteria living inside our cells, we understand that they're facing evolutionary pressures to figure out how they can replicate more; how they can get more resources; and how they can avoid predators — like lions, or antibiotics.</p><p> 

We don’t normally think of individual cells as acting as if they have their own interests like this. But cancer cells are actually facing similar kinds of evolutionary pressures within our bodies, with one major difference: they replicate much, much faster.</p><p> 

Incredibly, the opportunity for evolution by natural selection to operate just over the course of cancer progression is easily faster than all of the evolutionary time that we have had as humans since *Homo sapiens* came about.</p><p> 

Here’s a quote from Athena:</p><p> 

“So you have to shift your thinking to be like: the body is a world with all these different ecosystems in it, and the cells are existing on a time scale where, if we're going to map it onto anything like what we experience, a day is at least 10 years for them, right? So it's a very, very different way of thinking.”</p><p> 

You can find compelling examples of cooperation and conflict all over the universe, so Rob and Athena don’t stop with cancer. They also discuss:</p><p> 

• Cheating within cells themselves<br> 
• Cooperation in human societies as they exist today — and perhaps in the future, between civilisations spread across different planets or stars<br> 
• Whether it’s too out-there to think of humans as engaging in cancerous behaviour<br> 
• Why elephants get deadly cancers less often than humans, despite having way more cells<br> 
• When a cell should commit suicide<br> 
• The strategy of deliberately not treating cancer aggressively<br> 
• Superhuman cooperation</p><p> 

And at the end of the episode, they cover Athena’s new book <i>Everything is Fine! How to Thrive in the Apocalypse</i>, including:</p><p> 

• Staying happy while thinking about the apocalypse<br> 
• Practical steps to prepare for the apocalypse<br> 
• And whether a zombie apocalypse is already happening among Tasmanian devils</p><p> 

And if you’d rather see Rob and Athena’s facial expressions as they laugh and laugh while discussing cancer and the apocalypse — you can <a href="https://youtu.be/oxa1FdkNtLc"><b>watch the video of the full interview</b></a>.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Milo McGuire<br>
Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 26 Jan 2023 00:01:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0397342b/2b62945a.mp3" length="94058210" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/LtMktSAMnw7bBGpSwcP4CJMQXReaQMZmMqYYzlKPxcI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDEv/MTY4MzU0NDc1NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11757</itunes:duration>
      <itunes:summary>What’s the opposite of cancer? 

If you answered “cure,” “antidote,” or “antivenom” — you’ve obviously been reading the antonym section at www.merriam-webster.com/thesaurus/cancer. 

But today’s guest Athena Aktipis says that the opposite of cancer is us: it's having a functional multicellular body that’s cooperating effectively in order to make that multicellular body function. 

If, like us, you found her answer far more satisfying than the dictionary, maybe you could consider closing your dozens of merriam-webster.com tabs, and start listening to this podcast instead. 

Links to learn more, summary and full transcript. 

As Athena explains in her book The Cheating Cell, what we see with cancer is a breakdown in each of the foundations of cooperation that allowed multicellularity to arise: 

• Cells will proliferate when they shouldn't. 
• Cells won't die when they should. 
• Cells won't engage in the kind of division of labour that they should. 
• Cells won’t do the jobs that they're supposed to do. 
• Cells will monopolise resources. 
• And cells will trash the environment. 

When we think about animals in the wild, or even bacteria living inside our cells, we understand that they're facing evolutionary pressures to figure out how they can replicate more; how they can get more resources; and how they can avoid predators — like lions, or antibiotics. 

We don’t normally think of individual cells as acting as if they have their own interests like this. But cancer cells are actually facing similar kinds of evolutionary pressures within our bodies, with one major difference: they replicate much, much faster. 

Incredibly, the opportunity for evolution by natural selection to operate just over the course of cancer progression is easily faster than all of the evolutionary time that we have had as humans since *Homo sapiens* came about. 

Here’s a quote from Athena: 

“So you have to shift your thinking to be like: the body is a world with all these different ecosystems in it, and the cells are existing on a time scale where, if we're going to map it onto anything like what we experience, a day is at least 10 years for them, right? So it's a very, very different way of thinking.” 

You can find compelling examples of cooperation and conflict all over the universe, so Rob and Athena don’t stop with cancer. They also discuss: 

• Cheating within cells themselves 
• Cooperation in human societies as they exist today — and perhaps in the future, between civilisations spread across different planets or stars 
• Whether it’s too out-there to think of humans as engaging in cancerous behaviour 
• Why elephants get deadly cancers less often than humans, despite having way more cells 
• When a cell should commit suicide 
• The strategy of deliberately not treating cancer aggressively 
• Superhuman cooperation 

And at the end of the episode, they cover Athena’s new book Everything is Fine! How to Thrive in the Apocalypse, including: 

• Staying happy while thinking about the apocalypse 
• Practical steps to prepare for the apocalypse 
• And whether a zombie apocalypse is already happening among Tasmanian devils 

And if you’d rather see Rob and Athena’s facial expressions as they laugh and laugh while discussing cancer and the apocalypse — you can watch the video of the full interview. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Milo McGuire
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>What’s the opposite of cancer? 

If you answered “cure,” “antidote,” or “antivenom” — you’ve obviously been reading the antonym section at www.merriam-webster.com/thesaurus/cancer. 

But today’s guest Athena Aktipis says that the opposite of cancer is us:</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/0397342b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#79 Classic episode - A.J. Jacobs on radical honesty, following the whole Bible, and reframing global problems as puzzles</title>
      <itunes:title>#79 Classic episode - A.J. Jacobs on radical honesty, following the whole Bible, and reframing global problems as puzzles</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f4b05808-95d9-11ed-b7a5-0e7f87bcaf97</guid>
      <link>https://share.transistor.fm/s/86811274</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in June 2020.</b><p> 

Today’s guest, <i>New York Times</i> bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what, she's not so bad".</p><p> 

Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the <a href="https://80k.link/AJ1"><i>biggest family tree in history</i></a>.</p><p> 

He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His  <a href="https://80k.link/AJP"><i>latest book</i></a> asks: if we reframe global problems as puzzles, would the world be a better place?</p><p> 

<a href="https://80k.link/AJC"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (<a href="https://80k.link/AJ2"><i>Radical Honesty.</i></a>)</p><p> 

We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (<a href="https://80k.link/AJ3"><i>Drop Dead Healthy.</i></a>)</p><p> 

Another reason to listen is for the facts:</p><p> 

• The Bayer aspirin company invented heroin as a cough suppressant<br> 
•  Coriander is just the British way of saying cilantro<br> 
•  Dogs have a third eyelid to protect the eyeball from irritants<br> 
•  and A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples). (<a href="https://80k.link/AJ4"><i>The Know-It-All.</i></a>)</p><p> 

One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (<a href="https://80k.link/AJ5"><i>The Year of Living Biblically.</i></a>)</p><p> 

I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and <a href="https://80k.link/CU3">Queen Jadwiga</a> for making bagels popular in the 1300s, which kept me going during the recording. (<a href="https://80k.link/AJ6"><i>Thanks a Thousand.</i></a>)</p><p> 

We also discuss:</p><p> 

• Blackmailing yourself<br> 
• The most extreme ideas A.J.’s ever considered<br> 
• Utilitarian movie reviews<br> 
• Doing good as a writer<br> 
• And much more.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcript for this episode: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in June 2020.</b><p> 

Today’s guest, <i>New York Times</i> bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what, she's not so bad".</p><p> 

Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the <a href="https://80k.link/AJ1"><i>biggest family tree in history</i></a>.</p><p> 

He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His  <a href="https://80k.link/AJP"><i>latest book</i></a> asks: if we reframe global problems as puzzles, would the world be a better place?</p><p> 

<a href="https://80k.link/AJC"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (<a href="https://80k.link/AJ2"><i>Radical Honesty.</i></a>)</p><p> 

We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (<a href="https://80k.link/AJ3"><i>Drop Dead Healthy.</i></a>)</p><p> 

Another reason to listen is for the facts:</p><p> 

• The Bayer aspirin company invented heroin as a cough suppressant<br> 
•  Coriander is just the British way of saying cilantro<br> 
•  Dogs have a third eyelid to protect the eyeball from irritants<br> 
•  and A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples). (<a href="https://80k.link/AJ4"><i>The Know-It-All.</i></a>)</p><p> 

One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (<a href="https://80k.link/AJ5"><i>The Year of Living Biblically.</i></a>)</p><p> 

I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and <a href="https://80k.link/CU3">Queen Jadwiga</a> for making bagels popular in the 1300s, which kept me going during the recording. (<a href="https://80k.link/AJ6"><i>Thanks a Thousand.</i></a>)</p><p> 

We also discuss:</p><p> 

• Blackmailing yourself<br> 
• The most extreme ideas A.J.’s ever considered<br> 
• Utilitarian movie reviews<br> 
• Doing good as a writer<br> 
• And much more.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcript for this episode: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 16 Jan 2023 22:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/86811274/0a6bf432.mp3" length="74641023" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lRfJ0y8O74cdTxA9-RtVz7neYLxQBcqKgCw1_6K3OF4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ5MDAv/MTY4MzU0NDc1My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9330</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in June 2020. 

Today’s guest, New York Times bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what, she's not so bad". 

Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the biggest family tree in history. 

He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His  latest book asks: if we reframe global problems as puzzles, would the world be a better place? 

Links to learn more, summary and full transcript. 

This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (Radical Honesty.) 

We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (Drop Dead Healthy.) 

Another reason to listen is for the facts: 

• The Bayer aspirin company invented heroin as a cough suppressant 
•  Coriander is just the British way of saying cilantro 
•  Dogs have a third eyelid to protect the eyeball from irritants 
•  and A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples). (The Know-It-All.) 

One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (The Year of Living Biblically.) 

I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and Queen Jadwiga for making bagels popular in the 1300s, which kept me going during the recording. (Thanks a Thousand.) 

We also discuss: 

• Blackmailing yourself 
• The most extreme ideas A.J.’s ever considered 
• Utilitarian movie reviews 
• Doing good as a writer 
• And much more. 

Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcript for this episode: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in June 2020. 

Today’s guest, New York Times bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what, she's not so bad".</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/86811274/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#81 Classic episode - Ben Garfinkel on scrutinising classic AI risk arguments</title>
      <itunes:title>#81 Classic episode - Ben Garfinkel on scrutinising classic AI risk arguments</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e6979924-906c-11ed-812d-0a175bc86069</guid>
      <link>https://share.transistor.fm/s/aaa02ac0</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in July 2020.</b><p> 

80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments.</p><p> 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment.</p><p> 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances.</p><p> 

<a href="https://80k.link/BostromTT">Nick Bostrom</a> wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents.</p><p> 

<a href="https://80k.link/BGpod"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world.</p><p> 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and <a href="https://80k.link/pcm">toy thought experiments</a>. And he doesn’t think it’s clear we should take these as a strong source of evidence.</p><p> 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.</p><p>  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences?</p><p> 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.</p><p>  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.</p><p>  

This is the second episode hosted by Howie Lempel, and he and Ben cover, among many other things:</p><p> 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse<br> 
• The possibility of permanently locking in a positive or negative future<br> 
• Contenders for types of advanced systems<br> 
• What role AI should play in the effective altruism portfolio</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcript for this episode: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in July 2020.</b><p> 

80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments.</p><p> 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment.</p><p> 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances.</p><p> 

<a href="https://80k.link/BostromTT">Nick Bostrom</a> wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents.</p><p> 

<a href="https://80k.link/BGpod"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world.</p><p> 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and <a href="https://80k.link/pcm">toy thought experiments</a>. And he doesn’t think it’s clear we should take these as a strong source of evidence.</p><p> 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.</p><p>  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences?</p><p> 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.</p><p>  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.</p><p>  

This is the second episode hosted by Howie Lempel, and he and Ben cover, among many other things:</p><p> 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse<br> 
• The possibility of permanently locking in a positive or negative future<br> 
• Contenders for types of advanced systems<br> 
• What role AI should play in the effective altruism portfolio</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcript for this episode: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 09 Jan 2023 22:57:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/aaa02ac0/d645aad5.mp3" length="75443944" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/skhOB8BZWRdZB57UxBZuWJ1NyBX1AvI3GZ2zzr4xeAw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTkv/MTY4MzU0NDc1Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9431</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in July 2020. 

80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments. 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment. 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances. 

Nick Bostrom wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents. 

Links to learn more, summary and full transcript. 

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world. 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and toy thought experiments. And he doesn’t think it’s clear we should take these as a strong source of evidence. 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences? 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.  

This is the second episode hosted by Howie Lempel, and he and Ben cover, among many other things: 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse 
• The possibility of permanently locking in a positive or negative future 
• Contenders for types of advanced systems 
• What role AI should play in the effective altruism portfolio 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcript for this episode: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in July 2020. 

80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/aaa02ac0/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#83 Classic episode - Jennifer Doleac on preventing crime without police and prisons</title>
      <itunes:title>#83 Classic episode - Jennifer Doleac on preventing crime without police and prisons</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">795205fa-8bb9-11ed-94d0-0abec119c0cf</guid>
      <link>https://share.transistor.fm/s/166a0a5a</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in July 2020.</b><p> 

Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;M University, and Director of the <i>Justice Tech Lab</i> — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three ways to effectively prevent crime that don't require police or prisons and the human toll they bring with them: better street lighting, cognitive behavioral therapy, and lead reduction.</p><p> 

One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double.</p><p> 

<a href="https://80k.link/jdpod"><b>Links to sources for the claims in these show notes, other resources to learn more, the full blog post, and a full transcript.</b></a></p><p> 

The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught.</p><p> 

You might think: "Well, people will just commit crime in the morning instead". But it looks like criminals aren’t early risers, and that doesn’t happen.</p><p> 

On her unusually rigorous podcast <a href="https://80k.link/pcac">Probable Causation</a>, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost. </p><p> 

The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone.</p><p> 

The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting.</p><p> 

There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%.</p><p> 

Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives.</p><p> 

Finally, Jennifer thinks that reducing lead levels might be the best buy of all in crime prevention. There is really compelling evidence that lead not only increases crime, but also dramatically reduces educational outcomes.</p><p> 

In today’s conversation, Rob and Jennifer also cover, among many other things:</p><p> 

• Misconduct, hiring practices and accountability among US police<br>
• Procedural justice training<br>
• Overrated policy ideas<br>
• Policies to try to reduce racial discrimination<br>
• The effects of DNA databases<br>
• Diversity in economics<br>
• The quality of social science research</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcript for this episode: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in July 2020.</b><p> 

Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;M University, and Director of the <i>Justice Tech Lab</i> — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three ways to effectively prevent crime that don't require police or prisons and the human toll they bring with them: better street lighting, cognitive behavioral therapy, and lead reduction.</p><p> 

One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double.</p><p> 

<a href="https://80k.link/jdpod"><b>Links to sources for the claims in these show notes, other resources to learn more, the full blog post, and a full transcript.</b></a></p><p> 

The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught.</p><p> 

You might think: "Well, people will just commit crime in the morning instead". But it looks like criminals aren’t early risers, and that doesn’t happen.</p><p> 

On her unusually rigorous podcast <a href="https://80k.link/pcac">Probable Causation</a>, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost. </p><p> 

The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone.</p><p> 

The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting.</p><p> 

There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%.</p><p> 

Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives.</p><p> 

Finally, Jennifer thinks that reducing lead levels might be the best buy of all in crime prevention. There is really compelling evidence that lead not only increases crime, but also dramatically reduces educational outcomes.</p><p> 

In today’s conversation, Rob and Jennifer also cover, among many other things:</p><p> 

• Misconduct, hiring practices and accountability among US police<br>
• Procedural justice training<br>
• Overrated policy ideas<br>
• Policies to try to reduce racial discrimination<br>
• The effects of DNA databases<br>
• Diversity in economics<br>
• The quality of social science research</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcript for this episode: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 04 Jan 2023 21:34:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/166a0a5a/a21ad893.mp3" length="66126296" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/F5VR1RuFttCqp8AwI_r6kQQiFPniGYF41Rcbigz69ww/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTgv/MTY4MzU0NDc1MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8266</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in July 2020. 

Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;amp;M University, and Director of the Justice Tech Lab — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three ways to effectively prevent crime that don't require police or prisons and the human toll they bring with them: better street lighting, cognitive behavioral therapy, and lead reduction. 

One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double. 

Links to sources for the claims in these show notes, other resources to learn more, the full blog post, and a full transcript. 

The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught. 

You might think: "Well, people will just commit crime in the morning instead". But it looks like criminals aren’t early risers, and that doesn’t happen. 

On her unusually rigorous podcast Probable Causation, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost.  

The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone. 

The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting. 

There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%. 

Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives. 

Finally, Jennifer thinks that reducing lead levels might be the best buy of all in crime prevention. There is really compelling evidence that lead not only increases crime, but also dramatically reduces educational outcomes. 

In today’s conversation, Rob and Jennifer also cover, among many other things: 

• Misconduct, hiring practices and accountability among US police
• Procedural justice training
• Overrated policy ideas
• Policies to try to reduce racial discrimination
• The effects of DNA databases
• Diversity in economics
• The quality of social science research 

Get this episode by subscribing: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcript for this episode: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in July 2020. 

Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;amp;M University, and Director of the Justice Tech Lab — is an expert on empirical research into policing, law a</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/166a0a5a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#143 – Jeffrey Lewis on the most common misconceptions about nuclear weapons</title>
      <itunes:title>#143 – Jeffrey Lewis on the most common misconceptions about nuclear weapons</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3501ff58-87c8-11ed-9a94-0ab89e4e85b5</guid>
      <link>https://80000hours.org/podcast/episodes/jeffrey-lewis-common-misconceptions-about-nuclear-weapons/</link>
      <description>
        <![CDATA[<p>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially.</p><p>As today's guest — Jeffrey Lewis, founder of <em>Arms Control Wonk</em> and professor at the Middlebury Institute of International Studies — explains, in its official 'OPLANs' (military operation plans), the US is committed to 'dominating' in a nuclear war with Russia. How would they do that? "That is redacted."</p><p> <a href="https://80k.link/JL"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> We invited Jeffrey to come on the show to lay out what we and our listeners are most likely to be misunderstanding about nuclear weapons, the nuclear posture of major powers, and his field as a whole, and he did not disappoint.</p><p> As Jeffrey tells it, 'mutually assured destruction' was a slur used to criticise those who wanted to limit the 1960s arms buildup, and was never accepted as a matter of policy in any US administration. But isn't it still the de facto reality? Yes and no.</p><p> Jeffrey is a specialist on the nuts and bolts of bureaucratic and military decision-making in real-life situations. He suspects that at the start of their term presidents get a briefing about the US' plan to prevail in a nuclear war and conclude that "it's freaking madness." They say to themselves that whatever these silly plans may say, they know a nuclear war cannot be won, so they just won't use the weapons.</p><p> But Jeffrey thinks that's a big mistake. Yes, in a calm moment presidents can resist pressure from advisors and generals. But that idea of ‘winning’ a nuclear war is in all the plans. Staff have been hired because they believe in those plans. It's what the generals and admirals have all prepared for.</p><p> What matters is the 'not calm moment': the 3AM phone call to tell the president that ICBMs might hit the US in eight minutes — the same week Russia invades a neighbour or China invades Taiwan. Is it a false alarm? Should they retaliate before their land-based missile silos are hit? There's only minutes to decide.</p><p> Jeffrey points out that in emergencies, presidents have repeatedly found themselves railroaded into actions they didn't want to take because of how information and options were processed and presented to them. In the heat of the moment, it's natural to reach for the plan you've prepared — however mad it might sound.</p><p> In this spicy conversation, Jeffrey fields the most burning questions from Rob and the audience, in the process explaining:<br> • Why inter-service rivalry is one of the biggest constraints on US nuclear policy<br> • Two times the US sabotaged nuclear nonproliferation among great powers<br> • How his field uses jargon to exclude outsiders<br> • How the US could prevent the revival of mass nuclear testing by the great powers<br> • Why nuclear deterrence relies on the possibility that something might go wrong<br> • Whether 'salami tactics' render nuclear weapons ineffective<br> • The time the Navy and Air Force switched views on how to wage a nuclear war, just when it would allow *them* to have the most missiles<br> • The problems that arise when you won't talk to people you think are evil<br> • Why missile defences are politically popular despite being strategically foolish<br> • How open source intelligence can prevent arms races<br> • And much more.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:49)</li><li>Misconceptions in the effective altruism community (00:05:42)</li><li>Nuclear deterrence (00:17:36)</li><li>Dishonest rituals (00:28:17)</li><li>Downsides of generalist research (00:32:13)</li><li>“Mutual assured destruction” (00:38:18)</li><li>Budgetary considerations for competing parts of the US military (00:51:53)</li><li>Where the effective altruism community can potentially add the most value (01:02:15)</li><li>Gatekeeping (01:12:04)</li><li>Strengths of the nuclear security community (01:16:14)</li><li>Disarmament (01:26:58)</li><li>Nuclear winter (01:38:53)</li><li>Attacks against US allies (01:41:46)</li><li>Most likely weapons to get used (01:45:11)</li><li>The role of moral arguments (01:46:40)</li><li>Salami tactics (01:52:01)</li><li>Jeffrey's disagreements with Thomas Schelling (01:57:00)</li><li>Why did it take so long to get nuclear arms agreements? (02:01:11)</li><li>Detecting secret nuclear facilities (02:03:18)</li><li>Where Jeffrey would give $10M in grants (02:05:46)</li><li>The importance of archival research (02:11:03)</li><li>Jeffrey's policy ideas (02:20:03)</li><li>What should the US do regarding China? (02:27:10)</li><li>What should the US do regarding Russia? (02:31:42)</li><li>What should the US do regarding Taiwan? (02:35:27)</li><li>Advice for people interested in working on nuclear security (02:37:23)</li><li>Rob’s outro (02:39:13)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially.</p><p>As today's guest — Jeffrey Lewis, founder of <em>Arms Control Wonk</em> and professor at the Middlebury Institute of International Studies — explains, in its official 'OPLANs' (military operation plans), the US is committed to 'dominating' in a nuclear war with Russia. How would they do that? "That is redacted."</p><p> <a href="https://80k.link/JL"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> We invited Jeffrey to come on the show to lay out what we and our listeners are most likely to be misunderstanding about nuclear weapons, the nuclear posture of major powers, and his field as a whole, and he did not disappoint.</p><p> As Jeffrey tells it, 'mutually assured destruction' was a slur used to criticise those who wanted to limit the 1960s arms buildup, and was never accepted as a matter of policy in any US administration. But isn't it still the de facto reality? Yes and no.</p><p> Jeffrey is a specialist on the nuts and bolts of bureaucratic and military decision-making in real-life situations. He suspects that at the start of their term presidents get a briefing about the US' plan to prevail in a nuclear war and conclude that "it's freaking madness." They say to themselves that whatever these silly plans may say, they know a nuclear war cannot be won, so they just won't use the weapons.</p><p> But Jeffrey thinks that's a big mistake. Yes, in a calm moment presidents can resist pressure from advisors and generals. But that idea of ‘winning’ a nuclear war is in all the plans. Staff have been hired because they believe in those plans. It's what the generals and admirals have all prepared for.</p><p> What matters is the 'not calm moment': the 3AM phone call to tell the president that ICBMs might hit the US in eight minutes — the same week Russia invades a neighbour or China invades Taiwan. Is it a false alarm? Should they retaliate before their land-based missile silos are hit? There's only minutes to decide.</p><p> Jeffrey points out that in emergencies, presidents have repeatedly found themselves railroaded into actions they didn't want to take because of how information and options were processed and presented to them. In the heat of the moment, it's natural to reach for the plan you've prepared — however mad it might sound.</p><p> In this spicy conversation, Jeffrey fields the most burning questions from Rob and the audience, in the process explaining:<br> • Why inter-service rivalry is one of the biggest constraints on US nuclear policy<br> • Two times the US sabotaged nuclear nonproliferation among great powers<br> • How his field uses jargon to exclude outsiders<br> • How the US could prevent the revival of mass nuclear testing by the great powers<br> • Why nuclear deterrence relies on the possibility that something might go wrong<br> • Whether 'salami tactics' render nuclear weapons ineffective<br> • The time the Navy and Air Force switched views on how to wage a nuclear war, just when it would allow *them* to have the most missiles<br> • The problems that arise when you won't talk to people you think are evil<br> • Why missile defences are politically popular despite being strategically foolish<br> • How open source intelligence can prevent arms races<br> • And much more.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:49)</li><li>Misconceptions in the effective altruism community (00:05:42)</li><li>Nuclear deterrence (00:17:36)</li><li>Dishonest rituals (00:28:17)</li><li>Downsides of generalist research (00:32:13)</li><li>“Mutual assured destruction” (00:38:18)</li><li>Budgetary considerations for competing parts of the US military (00:51:53)</li><li>Where the effective altruism community can potentially add the most value (01:02:15)</li><li>Gatekeeping (01:12:04)</li><li>Strengths of the nuclear security community (01:16:14)</li><li>Disarmament (01:26:58)</li><li>Nuclear winter (01:38:53)</li><li>Attacks against US allies (01:41:46)</li><li>Most likely weapons to get used (01:45:11)</li><li>The role of moral arguments (01:46:40)</li><li>Salami tactics (01:52:01)</li><li>Jeffrey's disagreements with Thomas Schelling (01:57:00)</li><li>Why did it take so long to get nuclear arms agreements? (02:01:11)</li><li>Detecting secret nuclear facilities (02:03:18)</li><li>Where Jeffrey would give $10M in grants (02:05:46)</li><li>The importance of archival research (02:11:03)</li><li>Jeffrey's policy ideas (02:20:03)</li><li>What should the US do regarding China? (02:27:10)</li><li>What should the US do regarding Russia? (02:31:42)</li><li>What should the US do regarding Taiwan? (02:35:27)</li><li>Advice for people interested in working on nuclear security (02:37:23)</li><li>Rob’s outro (02:39:13)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 29 Dec 2022 22:49:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/fbe76f01/ccefb466.mp3" length="76936907" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Mvrd1OO0q93VMd8JBytDO7F2Fxi_igI_7yzSJ3eJNDw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTcv/MTY4MzU0NDc0OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9617</itunes:duration>
      <itunes:summary>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially. 

As today's guest — Jeffrey Lewis, founder of Arms Control Wonk and professor at the Middlebury Institute of International Studies — explains, in its official 'OPLANs' (military operation plans), the US is committed to 'dominating' in a nuclear war with Russia. How would they do that? "That is redacted." 

Links to learn more, summary and full transcript. 

We invited Jeffrey to come on the show to lay out what we and our listeners are most likely to be misunderstanding about nuclear weapons, the nuclear posture of major powers, and his field as a whole, and he did not disappoint. 

As Jeffrey tells it, 'mutually assured destruction' was a slur used to criticise those who wanted to limit the 1960s arms buildup, and was never accepted as a matter of policy in any US administration. But isn't it still the de facto reality? Yes and no. 

Jeffrey is a specialist on the nuts and bolts of bureaucratic and military decision-making in real-life situations. He suspects that at the start of their term presidents get a briefing about the US' plan to prevail in a nuclear war and conclude that "it's freaking madness." They say to themselves that whatever these silly plans may say, they know a nuclear war cannot be won, so they just won't use the weapons. 

But Jeffrey thinks that's a big mistake. Yes, in a calm moment presidents can resist pressure from advisors and generals. But that idea of ‘winning’ a nuclear war is in all the plans. Staff have been hired because they believe in those plans. It's what the generals and admirals have all prepared for. 

What matters is the 'not calm moment': the 3AM phone call to tell the president that ICBMs might hit the US in eight minutes — the same week Russia invades a neighbour or China invades Taiwan. Is it a false alarm? Should they retaliate before their land-based missile silos are hit? There's only minutes to decide. 

Jeffrey points out that in emergencies, presidents have repeatedly found themselves railroaded into actions they didn't want to take because of how information and options were processed and presented to them. In the heat of the moment, it's natural to reach for the plan you've prepared — however mad it might sound. 

In this spicy conversation, Jeffrey fields the most burning questions from Rob and the audience, in the process explaining: 

• Why inter-service rivalry is one of the biggest constraints on US nuclear policy 
• Two times the US sabotaged nuclear nonproliferation among great powers 
• How his field uses jargon to exclude outsiders 
• How the US could prevent the revival of mass nuclear testing by the great powers 
• Why nuclear deterrence relies on the possibility that something might go wrong 
• Whether 'salami tactics' render nuclear weapons ineffective 
• The time the Navy and Air Force switched views on how to wage a nuclear war, just when it would allow *them* to have the most missiles 
• The problems that arise when you won't talk to people you think are evil 
• Why missile defences are politically popular despite being strategically foolish 
• How open source intelligence can prevent arms races 
• And much more. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>America aims to avoid nuclear war by relying on the principle of 'mutually assured destruction,' right? Wrong. Or at least... not officially. 

As today's guest — Jeffrey Lewis, founder of Arms Control Wonk and professor at the Middlebury Institute of I</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/fbe76f01/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/fbe76f01/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#142 – John McWhorter on key lessons from linguistics, the virtue of creoles, and language extinction</title>
      <itunes:title>#142 – John McWhorter on key lessons from linguistics, the virtue of creoles, and language extinction</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">dfe417f6-80bd-11ed-89ff-0a64811dc90b</guid>
      <link>https://share.transistor.fm/s/12c95335</link>
      <description>
        <![CDATA[John McWhorter is a linguistics professor at Columbia University specialising in research on creole languages.<p> 

He's also a content-producing machine, never afraid to give his frank opinion on anything and everything. On top of his academic work he's also written 22 books, produced five online university courses, hosts one and a half podcasts, and now writes a regular New York Times op-ed column.</p><p> 

• <a href="https://80k.link/JM"><b>Links to learn more, summary, and full transcript</b></a><br>
• <a href="https://www.youtube.com/watch?v=nVDwTS9rTF8"><b>Video version of the interview</b></a><br>
• <a href="https://www.youtube.com/watch?v=QglKeIIC5Ds"><b>Lecture: Why the world looks the same in any language</b></a></p><p>

Our show is mostly about the world's most pressing problems and what you can do to solve them. But what's the point of hosting a podcast if you can't occasionally just talk about something fascinating with someone whose work you appreciate?</p><p> 

So today, just before the holidays, we're sharing this interview with John about language and linguistics — including what we think are some of the most important things everyone ought to know about those topics. We ask him:</p><p> 

• Can you communicate faster in some languages than others, or is there some constraint that prevents that?<br> 
• Does learning a second or third language make you smarter or not?<br> 
• Can a language decay and get worse at communicating what people want to say?<br> 
• If children aren't taught a language, how many generations does it take them to invent a fully fledged one of their own?<br> 
• Did Shakespeare write in a foreign language, and if so, should we translate his plays?<br> 
• How much does language really shape the way we think?<br> 
• Are creoles the best languages in the world — languages that ideally we would all speak?<br> 
• What would be the optimal number of languages globally?<br> 
• Does trying to save dying languages do their speakers a favour, or is it more of an imposition?<br> 
• Should we bother to teach foreign languages in UK and US schools?<br> 
• Is it possible to save the important cultural aspects embedded in a dying language without saving the language itself?<br> 
• Will AI models speak a language of their own in the future, one that humans can't<br>  understand but which better serves the tradeoffs AI models need to make?</p><p> 

We then put some of these questions to ChatGPT itself, asking it to play the role of a linguistics professor at Colombia University.</p><p> 

We’ve also added John’s talk <a href="https://www.youtube.com/watch?v=QglKeIIC5Ds"><i>“Why the World Looks the Same in Any Language”</i></a>  to the end of this episode. So stick around after the credits!</p><p> 

And if you’d rather see Rob and John’s facial expressions or beautiful high cheekbones while listening to this conversation, you can watch the video of the full conversation <a href="https://www.youtube.com/watch?v=nVDwTS9rTF8"><i>here.</i></a></p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Video editing: Ryan Kessler<br>
Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[John McWhorter is a linguistics professor at Columbia University specialising in research on creole languages.<p> 

He's also a content-producing machine, never afraid to give his frank opinion on anything and everything. On top of his academic work he's also written 22 books, produced five online university courses, hosts one and a half podcasts, and now writes a regular New York Times op-ed column.</p><p> 

• <a href="https://80k.link/JM"><b>Links to learn more, summary, and full transcript</b></a><br>
• <a href="https://www.youtube.com/watch?v=nVDwTS9rTF8"><b>Video version of the interview</b></a><br>
• <a href="https://www.youtube.com/watch?v=QglKeIIC5Ds"><b>Lecture: Why the world looks the same in any language</b></a></p><p>

Our show is mostly about the world's most pressing problems and what you can do to solve them. But what's the point of hosting a podcast if you can't occasionally just talk about something fascinating with someone whose work you appreciate?</p><p> 

So today, just before the holidays, we're sharing this interview with John about language and linguistics — including what we think are some of the most important things everyone ought to know about those topics. We ask him:</p><p> 

• Can you communicate faster in some languages than others, or is there some constraint that prevents that?<br> 
• Does learning a second or third language make you smarter or not?<br> 
• Can a language decay and get worse at communicating what people want to say?<br> 
• If children aren't taught a language, how many generations does it take them to invent a fully fledged one of their own?<br> 
• Did Shakespeare write in a foreign language, and if so, should we translate his plays?<br> 
• How much does language really shape the way we think?<br> 
• Are creoles the best languages in the world — languages that ideally we would all speak?<br> 
• What would be the optimal number of languages globally?<br> 
• Does trying to save dying languages do their speakers a favour, or is it more of an imposition?<br> 
• Should we bother to teach foreign languages in UK and US schools?<br> 
• Is it possible to save the important cultural aspects embedded in a dying language without saving the language itself?<br> 
• Will AI models speak a language of their own in the future, one that humans can't<br>  understand but which better serves the tradeoffs AI models need to make?</p><p> 

We then put some of these questions to ChatGPT itself, asking it to play the role of a linguistics professor at Colombia University.</p><p> 

We’ve also added John’s talk <a href="https://www.youtube.com/watch?v=QglKeIIC5Ds"><i>“Why the World Looks the Same in Any Language”</i></a>  to the end of this episode. So stick around after the credits!</p><p> 

And if you’d rather see Rob and John’s facial expressions or beautiful high cheekbones while listening to this conversation, you can watch the video of the full conversation <a href="https://www.youtube.com/watch?v=nVDwTS9rTF8"><i>here.</i></a></p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Video editing: Ryan Kessler<br>
Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 20 Dec 2022 23:49:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/12c95335/025b9a5e.mp3" length="51791726" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/cCeJskYk_LTxc38UqsuZeDo4X98hjSi_IHvhqwWZkjg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTYv/MTY4MzU0NDc0OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6474</itunes:duration>
      <itunes:summary>John McWhorter is a linguistics professor at Columbia University specialising in research on creole languages. 

He's also a content-producing machine, never afraid to give his frank opinion on anything and everything. On top of his academic work he's also written 22 books, produced five online university courses, hosts one and a half podcasts, and now writes a regular New York Times op-ed column. 

• Links to learn more, summary, and full transcript
• Video version of the interview
• Lecture: Why the world looks the same in any language

Our show is mostly about the world's most pressing problems and what you can do to solve them. But what's the point of hosting a podcast if you can't occasionally just talk about something fascinating with someone whose work you appreciate? 

So today, just before the holidays, we're sharing this interview with John about language and linguistics — including what we think are some of the most important things everyone ought to know about those topics. We ask him: 

• Can you communicate faster in some languages than others, or is there some constraint that prevents that? 
• Does learning a second or third language make you smarter or not? 
• Can a language decay and get worse at communicating what people want to say? 
• If children aren't taught a language, how many generations does it take them to invent a fully fledged one of their own? 
• Did Shakespeare write in a foreign language, and if so, should we translate his plays? 
• How much does language really shape the way we think? 
• Are creoles the best languages in the world — languages that ideally we would all speak? 
• What would be the optimal number of languages globally? 
• Does trying to save dying languages do their speakers a favour, or is it more of an imposition? 
• Should we bother to teach foreign languages in UK and US schools? 
• Is it possible to save the important cultural aspects embedded in a dying language without saving the language itself? 
• Will AI models speak a language of their own in the future, one that humans can't  understand but which better serves the tradeoffs AI models need to make? 

We then put some of these questions to ChatGPT itself, asking it to play the role of a linguistics professor at Colombia University. 

We’ve also added John’s talk “Why the World Looks the Same in Any Language”  to the end of this episode. So stick around after the credits! 

And if you’d rather see Rob and John’s facial expressions or beautiful high cheekbones while listening to this conversation, you can watch the video of the full conversation here.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Video editing: Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>John McWhorter is a linguistics professor at Columbia University specialising in research on creole languages. 

He's also a content-producing machine, never afraid to give his frank opinion on anything and everything. On top of his academic work he's als</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/12c95335/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#141 – Richard Ngo on large language models, OpenAI, and striving to make the future go well</title>
      <itunes:title>#141 – Richard Ngo on large language models, OpenAI, and striving to make the future go well</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">456bd688-7b3f-11ed-8d83-0ece593c55b7</guid>
      <link>https://share.transistor.fm/s/ea0a7c0b</link>
      <description>
        <![CDATA[Large language models like GPT-3, and now ChatGPT, are neural networks trained on a large fraction of all text available on the internet to do one thing: predict the next word in a passage. This simple technique has led to something extraordinary — black boxes able to write TV scripts, explain jokes, produce satirical poetry, answer common factual questions, argue sensibly for political positions, and more. Every month their capabilities grow.<p> 

But do they really 'understand' what they're saying, or do they just give the illusion of understanding?</p><p> 

Today's guest, Richard Ngo, thinks that in the most important sense they understand many things. Richard is a researcher at OpenAI — the company that created <a href="https://chat.openai.com/chat">ChatGPT</a> — who works to foresee where AI advances are going and develop strategies that will keep these models from 'acting out' as they become more powerful, are deployed and ultimately given power in society.</p><p> 

<a href="https://80k.link/RN"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

One way to think about 'understanding' is as a subjective experience. Whether it feels like something to be a large language model is an important question, but one we currently have no way to answer.</p><p> 

However, as Richard explains, another way to think about 'understanding' is as a functional matter. If you really understand an idea you're able to use it to reason and draw inferences in new situations. And that kind of understanding is observable and testable.</p><p> 

Richard argues that language models are developing sophisticated representations of the world which can be manipulated to draw sensible conclusions — maybe not so different from what happens in the human mind. And experiments have found that, as models get more parameters and are trained on more data, these types of capabilities consistently improve.</p><p> 

We might feel reluctant to say a computer understands something the way that we do. But if it walks like a duck and it quacks like a duck, we should consider that maybe we have a duck, or at least something sufficiently close to a duck it doesn't matter.</p><p> 

In today's conversation we discuss the above, as well as:</p><p> 

• Could speeding up AI development be a bad thing?<br> 
• The balance between excitement and fear when it comes to AI advances<br> 
• What OpenAI focuses its efforts where it does<br> 
• Common misconceptions about machine learning<br> 
• How many computer chips it might require to be able to do most of the things humans do<br> 
• How Richard understands the 'alignment problem' differently than other people<br> 
• Why 'situational awareness' may be a key concept for understanding the behaviour of AI models<br> 
• What work to positively shape the development of AI Richard is and isn't excited about<br> 
• <a href="https://forum.effectivealtruism.org/posts/HBgAruFrZhFKBFfDa/applications-open-for-agi-safety-fundamentals-alignment"><b>The AGI Safety Fundamentals course</b></a> that Richard developed to help people learn more about this field</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>Producer: Keiran Harris<br>
Audio mastering: Milo McGuire and Ben Cordell<br>
Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Large language models like GPT-3, and now ChatGPT, are neural networks trained on a large fraction of all text available on the internet to do one thing: predict the next word in a passage. This simple technique has led to something extraordinary — black boxes able to write TV scripts, explain jokes, produce satirical poetry, answer common factual questions, argue sensibly for political positions, and more. Every month their capabilities grow.<p> 

But do they really 'understand' what they're saying, or do they just give the illusion of understanding?</p><p> 

Today's guest, Richard Ngo, thinks that in the most important sense they understand many things. Richard is a researcher at OpenAI — the company that created <a href="https://chat.openai.com/chat">ChatGPT</a> — who works to foresee where AI advances are going and develop strategies that will keep these models from 'acting out' as they become more powerful, are deployed and ultimately given power in society.</p><p> 

<a href="https://80k.link/RN"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

One way to think about 'understanding' is as a subjective experience. Whether it feels like something to be a large language model is an important question, but one we currently have no way to answer.</p><p> 

However, as Richard explains, another way to think about 'understanding' is as a functional matter. If you really understand an idea you're able to use it to reason and draw inferences in new situations. And that kind of understanding is observable and testable.</p><p> 

Richard argues that language models are developing sophisticated representations of the world which can be manipulated to draw sensible conclusions — maybe not so different from what happens in the human mind. And experiments have found that, as models get more parameters and are trained on more data, these types of capabilities consistently improve.</p><p> 

We might feel reluctant to say a computer understands something the way that we do. But if it walks like a duck and it quacks like a duck, we should consider that maybe we have a duck, or at least something sufficiently close to a duck it doesn't matter.</p><p> 

In today's conversation we discuss the above, as well as:</p><p> 

• Could speeding up AI development be a bad thing?<br> 
• The balance between excitement and fear when it comes to AI advances<br> 
• What OpenAI focuses its efforts where it does<br> 
• Common misconceptions about machine learning<br> 
• How many computer chips it might require to be able to do most of the things humans do<br> 
• How Richard understands the 'alignment problem' differently than other people<br> 
• Why 'situational awareness' may be a key concept for understanding the behaviour of AI models<br> 
• What work to positively shape the development of AI Richard is and isn't excited about<br> 
• <a href="https://forum.effectivealtruism.org/posts/HBgAruFrZhFKBFfDa/applications-open-for-agi-safety-fundamentals-alignment"><b>The AGI Safety Fundamentals course</b></a> that Richard developed to help people learn more about this field</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>Producer: Keiran Harris<br>
Audio mastering: Milo McGuire and Ben Cordell<br>
Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 13 Dec 2022 23:59:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ea0a7c0b/e1fc7f74.mp3" length="78871441" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Bj-oOEY8M2jHkJ4KyyULGXvLLlj0PDvx2YHfK98VCQU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTUv/MTY4MzU0NDc0Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9859</itunes:duration>
      <itunes:summary>Large language models like GPT-3, and now ChatGPT, are neural networks trained on a large fraction of all text available on the internet to do one thing: predict the next word in a passage. This simple technique has led to something extraordinary — black boxes able to write TV scripts, explain jokes, produce satirical poetry, answer common factual questions, argue sensibly for political positions, and more. Every month their capabilities grow. 

But do they really 'understand' what they're saying, or do they just give the illusion of understanding? 

Today's guest, Richard Ngo, thinks that in the most important sense they understand many things. Richard is a researcher at OpenAI — the company that created ChatGPT — who works to foresee where AI advances are going and develop strategies that will keep these models from 'acting out' as they become more powerful, are deployed and ultimately given power in society. 

Links to learn more, summary and full transcript. 

One way to think about 'understanding' is as a subjective experience. Whether it feels like something to be a large language model is an important question, but one we currently have no way to answer. 

However, as Richard explains, another way to think about 'understanding' is as a functional matter. If you really understand an idea you're able to use it to reason and draw inferences in new situations. And that kind of understanding is observable and testable. 

Richard argues that language models are developing sophisticated representations of the world which can be manipulated to draw sensible conclusions — maybe not so different from what happens in the human mind. And experiments have found that, as models get more parameters and are trained on more data, these types of capabilities consistently improve. 

We might feel reluctant to say a computer understands something the way that we do. But if it walks like a duck and it quacks like a duck, we should consider that maybe we have a duck, or at least something sufficiently close to a duck it doesn't matter. 

In today's conversation we discuss the above, as well as: 

• Could speeding up AI development be a bad thing? 
• The balance between excitement and fear when it comes to AI advances 
• What OpenAI focuses its efforts where it does 
• Common misconceptions about machine learning 
• How many computer chips it might require to be able to do most of the things humans do 
• How Richard understands the 'alignment problem' differently than other people 
• Why 'situational awareness' may be a key concept for understanding the behaviour of AI models 
• What work to positively shape the development of AI Richard is and isn't excited about 
• The AGI Safety Fundamentals course that Richard developed to help people learn more about this field 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.

Producer: Keiran Harris
Audio mastering: Milo McGuire and Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Large language models like GPT-3, and now ChatGPT, are neural networks trained on a large fraction of all text available on the internet to do one thing: predict the next word in a passage. This simple technique has led to something extraordinary — black </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/ea0a7c0b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>My experience with imposter syndrome — and how to (partly) overcome it (Article)</title>
      <itunes:title>My experience with imposter syndrome — and how to (partly) overcome it (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">9adf29f8-71fd-11ed-81e5-12d487d614e3</guid>
      <link>https://share.transistor.fm/s/074597d7</link>
      <description>
        <![CDATA[Today’s release is a reading of our article called <b><i>My experience with imposter syndrome — and how to (partly) overcome it</i></b>, written and narrated by Luisa Rodriguez.<p> 

If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.link/IS"><b>here.</b></a></p><p> 

And if you like this article, you’ll probably enjoy episode #100 of this show: <a href="https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/"><b><i>Having a successful career with depression, anxiety, and imposter syndrome</i></b></a></p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.</b></p><p>

<em>Producer: Keiran Harris<br> 
Audio mastering and editing for this episode: Milo McGuire</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today’s release is a reading of our article called <b><i>My experience with imposter syndrome — and how to (partly) overcome it</i></b>, written and narrated by Luisa Rodriguez.<p> 

If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.link/IS"><b>here.</b></a></p><p> 

And if you like this article, you’ll probably enjoy episode #100 of this show: <a href="https://80000hours.org/podcast/episodes/depression-anxiety-imposter-syndrome/"><b><i>Having a successful career with depression, anxiety, and imposter syndrome</i></b></a></p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.</b></p><p>

<em>Producer: Keiran Harris<br> 
Audio mastering and editing for this episode: Milo McGuire</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 08 Dec 2022 01:37:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/074597d7/fa9240a5.mp3" length="21157261" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/uu11dopO3smh2WDFi9ZBgjG0acrcdnE5jeB0otkpflw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTQv/MTY4MzU0NDc0Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2645</itunes:duration>
      <itunes:summary>Today’s release is a reading of our article called My experience with imposter syndrome — and how to (partly) overcome it, written and narrated by Luisa Rodriguez. 

If you want to check out the links, footnotes and figures in today’s article, you can find those here. 

And if you like this article, you’ll probably enjoy episode #100 of this show: Having a successful career with depression, anxiety, and imposter syndrome 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.

Producer: Keiran Harris 
Audio mastering and editing for this episode: Milo McGuire</itunes:summary>
      <itunes:subtitle>Today’s release is a reading of our article called My experience with imposter syndrome — and how to (partly) overcome it, written and narrated by Luisa Rodriguez. 

If you want to check out the links, footnotes and figures in today’s article, you can fin</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/074597d7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Rob's thoughts on the FTX bankruptcy</title>
      <itunes:title>Rob's thoughts on the FTX bankruptcy</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6906be58-6b6a-11ed-ae9b-128101022951</guid>
      <link>https://share.transistor.fm/s/e77c7afe</link>
      <description>
        <![CDATA[In this episode, usual host of the show Rob Wiblin gives his thoughts on the recent collapse of FTX.<p> 

<a href="https://80k.link/FTX"><b>Click here for an official 80,000 Hours statement.</b></a></p><p> 

And here are links to some potentially relevant 80,000 Hours pieces:</p><p> 


• Episode #24 of this show – <a href="https://80000hours.org/podcast/episodes/stefan-schubert-considering-considerateness/"><b>Stefan Schubert on why it’s a bad idea to break the rules, even if it’s for a good cause</b></a>.<br> 
• <a href="https://80000hours.org/articles/harmful-career/"><b>Is it ever OK to take a harmful job in order to do more good? An in-depth analysis</b></a><br> 
• <a href="https://80000hours.org/2015/08/what-are-the-10-most-harmful-jobs/"><b>What are the 10 most harmful jobs?</b></a><br> 
• <a href="https://80000hours.org/articles/accidental-harm/"><b>Ways people trying to do good accidentally make things worse, and how to avoid them</b></a></p><p></p>]]>
      </description>
      <content:encoded>
        <![CDATA[In this episode, usual host of the show Rob Wiblin gives his thoughts on the recent collapse of FTX.<p> 

<a href="https://80k.link/FTX"><b>Click here for an official 80,000 Hours statement.</b></a></p><p> 

And here are links to some potentially relevant 80,000 Hours pieces:</p><p> 


• Episode #24 of this show – <a href="https://80000hours.org/podcast/episodes/stefan-schubert-considering-considerateness/"><b>Stefan Schubert on why it’s a bad idea to break the rules, even if it’s for a good cause</b></a>.<br> 
• <a href="https://80000hours.org/articles/harmful-career/"><b>Is it ever OK to take a harmful job in order to do more good? An in-depth analysis</b></a><br> 
• <a href="https://80000hours.org/2015/08/what-are-the-10-most-harmful-jobs/"><b>What are the 10 most harmful jobs?</b></a><br> 
• <a href="https://80000hours.org/articles/accidental-harm/"><b>Ways people trying to do good accidentally make things worse, and how to avoid them</b></a></p><p></p>]]>
      </content:encoded>
      <pubDate>Wed, 23 Nov 2022 21:08:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e77c7afe/b7edfd24.mp3" length="5370203" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/zrZOSff2JRR6uvVXtv7Z_mDth-ppKXZDZlbSlgch2Ew/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTMv/MTY4MzU0NDc0NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>336</itunes:duration>
      <itunes:summary>In this episode, usual host of the show Rob Wiblin gives his thoughts on the recent collapse of FTX. 

Click here for an official 80,000 Hours statement. 

And here are links to some potentially relevant 80,000 Hours pieces: 


• Episode #24 of this show – Stefan Schubert on why it’s a bad idea to break the rules, even if it’s for a good cause. 
• Is it ever OK to take a harmful job in order to do more good? An in-depth analysis 
• What are the 10 most harmful jobs? 
• Ways people trying to do good accidentally make things worse, and how to avoid them</itunes:summary>
      <itunes:subtitle>In this episode, usual host of the show Rob Wiblin gives his thoughts on the recent collapse of FTX. 

Click here for an official 80,000 Hours statement. 

And here are links to some potentially relevant 80,000 Hours pieces: 


• Episode #24 of this show </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#140 – Bear Braumoeller on the case that war isn't in decline</title>
      <itunes:title>#140 – Bear Braumoeller on the case that war isn't in decline</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d2258f44-5fa9-11ed-9dfe-0e308bb9c9ed</guid>
      <link>https://80000hours.org/podcast/episodes/bear-braumoeller-decline-of-war/</link>
      <description>
        <![CDATA[<p>Is war in long-term decline? Steven Pinker's <em>The Better Angels of Our Nature</em> brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out.</p><p> But that idea divides war scholars and statisticians, and so <em>Better Angels</em> has prompted a spirited debate, with datasets and statistical analyses exchanged back and forth year after year. The lack of consensus has left a somewhat bewildered public (including host Rob Wiblin) unsure quite what to believe.</p><p> Today's guest, professor in political science Bear Braumoeller, is one of the scholars who believes we lack convincing evidence that warlikeness is in long-term decline. He collected the analysis that led him to that conclusion in his 2019 book, <a href="https://www.amazon.com/Only-Dead-Persistence-War-Modern/dp/0190849533"><em>Only the Dead: The Persistence of War in the Modern Age</em></a>.</p><p> <a href="https://80k.link/bb"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The question is of great practical importance. The US and PRC are entering a period of renewed great power competition, with Taiwan as a potential trigger for war, and Russia is once more invading and attempting to annex the territory of its neighbours.</p><p> If war has been going out of fashion since the start of the Enlightenment, we might console ourselves that however nerve-wracking these present circumstances may feel, modern culture will throw up powerful barriers to another world war. But if we're as war-prone as we ever have been, one need only inspect the record of the 20th century to recoil in horror at what might await us in the 21st.</p><p> Bear argues that the second reaction is the appropriate one. The world has gone up in flames many times through history, with roughly 0.5% of the population dying in the Napoleonic Wars, 1% in World War I, 3% in World War II, and perhaps 10% during the Mongol conquests. And with no reason to think similar catastrophes are any less likely today, complacency could lead us to sleepwalk into disaster.</p><p> He gets to this conclusion primarily by analysing the datasets of the decades-old Correlates of War project, which aspires to track all interstate conflicts and battlefield deaths since 1815. In <em>Only the Dead</em>, he chops up and inspects this data dozens of different ways, to test if there are any shifts over time which seem larger than what could be explained by chance variation alone.</p><p> In a nutshell, Bear simply finds no general trend in either direction from 1815 through today. It seems like, as philosopher George Santayana lamented in 1922, "only the dead have seen the end of war".</p><p> In today's conversation, Bear and Rob discuss all of the above in more detail than even a usual 80,000 Hours podcast episode, as well as:</p><p> • Why haven't modern ideas about the immorality of violence led to the decline of war, when it's such a natural thing to expect?<br> • What would Bear's critics say in response to all this?<br> • What do the optimists get right?<br> • How does one do proper statistical tests for events that are clumped together, like war deaths?<br> • Why are deaths in war so concentrated in a handful of the most extreme events?<br> • Did the ideas of the Enlightenment promote nonviolence, on balance?<br> • Were early states more or less violent than groups of hunter-gatherers?<br> • If Bear is right, what can be done?<br> • How did the 'Concert of Europe' or 'Bismarckian system' maintain peace in the 19th century?<br> • Which wars are remarkable but largely unknown?</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:32)</li><li>Only the Dead (00:06:28)</li><li>The Enlightenment (00:16:47)</li><li>Democratic peace theory (00:26:22)</li><li>Is religion a key driver of war? (00:29:27)</li><li>International orders (00:33:07)</li><li>The Concert of Europe (00:42:15)</li><li>The Bismarckian system (00:53:43)</li><li>The current international order (00:58:16)</li><li>The Better Angels of Our Nature (01:17:30)</li><li>War datasets (01:32:03)</li><li>Seeing patterns in data where none exist (01:45:32)</li><li>Change-point analysis (01:49:33)</li><li>Rates of violent death throughout history (01:54:32)</li><li>War initiation (02:02:55)</li><li>Escalation (02:17:57)</li><li>Getting massively different results from the same data (02:28:38)</li><li>How worried we should be (02:34:07)</li><li>Most likely ways Only the Dead is wrong (02:36:25)</li><li>Astonishing smaller wars (02:40:39)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Is war in long-term decline? Steven Pinker's <em>The Better Angels of Our Nature</em> brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out.</p><p> But that idea divides war scholars and statisticians, and so <em>Better Angels</em> has prompted a spirited debate, with datasets and statistical analyses exchanged back and forth year after year. The lack of consensus has left a somewhat bewildered public (including host Rob Wiblin) unsure quite what to believe.</p><p> Today's guest, professor in political science Bear Braumoeller, is one of the scholars who believes we lack convincing evidence that warlikeness is in long-term decline. He collected the analysis that led him to that conclusion in his 2019 book, <a href="https://www.amazon.com/Only-Dead-Persistence-War-Modern/dp/0190849533"><em>Only the Dead: The Persistence of War in the Modern Age</em></a>.</p><p> <a href="https://80k.link/bb"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The question is of great practical importance. The US and PRC are entering a period of renewed great power competition, with Taiwan as a potential trigger for war, and Russia is once more invading and attempting to annex the territory of its neighbours.</p><p> If war has been going out of fashion since the start of the Enlightenment, we might console ourselves that however nerve-wracking these present circumstances may feel, modern culture will throw up powerful barriers to another world war. But if we're as war-prone as we ever have been, one need only inspect the record of the 20th century to recoil in horror at what might await us in the 21st.</p><p> Bear argues that the second reaction is the appropriate one. The world has gone up in flames many times through history, with roughly 0.5% of the population dying in the Napoleonic Wars, 1% in World War I, 3% in World War II, and perhaps 10% during the Mongol conquests. And with no reason to think similar catastrophes are any less likely today, complacency could lead us to sleepwalk into disaster.</p><p> He gets to this conclusion primarily by analysing the datasets of the decades-old Correlates of War project, which aspires to track all interstate conflicts and battlefield deaths since 1815. In <em>Only the Dead</em>, he chops up and inspects this data dozens of different ways, to test if there are any shifts over time which seem larger than what could be explained by chance variation alone.</p><p> In a nutshell, Bear simply finds no general trend in either direction from 1815 through today. It seems like, as philosopher George Santayana lamented in 1922, "only the dead have seen the end of war".</p><p> In today's conversation, Bear and Rob discuss all of the above in more detail than even a usual 80,000 Hours podcast episode, as well as:</p><p> • Why haven't modern ideas about the immorality of violence led to the decline of war, when it's such a natural thing to expect?<br> • What would Bear's critics say in response to all this?<br> • What do the optimists get right?<br> • How does one do proper statistical tests for events that are clumped together, like war deaths?<br> • Why are deaths in war so concentrated in a handful of the most extreme events?<br> • Did the ideas of the Enlightenment promote nonviolence, on balance?<br> • Were early states more or less violent than groups of hunter-gatherers?<br> • If Bear is right, what can be done?<br> • How did the 'Concert of Europe' or 'Bismarckian system' maintain peace in the 19th century?<br> • Which wars are remarkable but largely unknown?</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:32)</li><li>Only the Dead (00:06:28)</li><li>The Enlightenment (00:16:47)</li><li>Democratic peace theory (00:26:22)</li><li>Is religion a key driver of war? (00:29:27)</li><li>International orders (00:33:07)</li><li>The Concert of Europe (00:42:15)</li><li>The Bismarckian system (00:53:43)</li><li>The current international order (00:58:16)</li><li>The Better Angels of Our Nature (01:17:30)</li><li>War datasets (01:32:03)</li><li>Seeing patterns in data where none exist (01:45:32)</li><li>Change-point analysis (01:49:33)</li><li>Rates of violent death throughout history (01:54:32)</li><li>War initiation (02:02:55)</li><li>Escalation (02:17:57)</li><li>Getting massively different results from the same data (02:28:38)</li><li>How worried we should be (02:34:07)</li><li>Most likely ways Only the Dead is wrong (02:36:25)</li><li>Astonishing smaller wars (02:40:39)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 08 Nov 2022 22:36:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/981a915d/6ce19868.mp3" length="80210118" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/rB003NpNsfzOsW_dNgGDdnHSxYej5Fvokj_u-noQNew/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTIv/MTY4MzU0NDc0MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10026</itunes:duration>
      <itunes:summary>Is war in long-term decline? Steven Pinker's The Better Angels of Our Nature brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out. 

But that idea divides war scholars and statisticians, and so Better Angels has prompted a spirited debate, with datasets and statistical analyses exchanged back and forth year after year. The lack of consensus has left a somewhat bewildered public (including host Rob Wiblin) unsure quite what to believe. 

Today's guest, professor in political science Bear Braumoeller, is one of the scholars who believes we lack convincing evidence that warlikeness is in long-term decline. He collected the analysis that led him to that conclusion in his 2019 book, Only the Dead: The Persistence of War in the Modern Age. 

Links to learn more, summary and full transcript.

The question is of great practical importance. The US and PRC are entering a period of renewed great power competition, with Taiwan as a potential trigger for war, and Russia is once more invading and attempting to annex the territory of its neighbours. 

If war has been going out of fashion since the start of the Enlightenment, we might console ourselves that however nerve-wracking these present circumstances may feel, modern culture will throw up powerful barriers to another world war. But if we're as war-prone as we ever have been, one need only inspect the record of the 20th century to recoil in horror at what might await us in the 21st. 

Bear argues that the second reaction is the appropriate one. The world has gone up in flames many times through history, with roughly 0.5% of the population dying in the Napoleonic Wars, 1% in World War I, 3% in World War II, and perhaps 10% during the Mongol conquests. And with no reason to think similar catastrophes are any less likely today, complacency could lead us to sleepwalk into disaster. 

He gets to this conclusion primarily by analysing the datasets of the decades-old Correlates of War project, which aspires to track all interstate conflicts and battlefield deaths since 1815. In Only the Dead, he chops up and inspects this data dozens of different ways, to test if there are any shifts over time which seem larger than what could be explained by chance variation alone. 

In a nutshell, Bear simply finds no general trend in either direction from 1815 through today. It seems like, as philosopher George Santayana lamented in 1922, "only the dead have seen the end of war". 

In today's conversation, Bear and Rob discuss all of the above in more detail than even a usual 80,000 Hours podcast episode, as well as: 

• Why haven't modern ideas about the immorality of violence led to the decline of war, when it's such a natural thing to expect? 
• What would Bear's critics say in response to all this? 
• What do the optimists get right? 
• How does one do proper statistical tests for events that are clumped together, like war deaths? 
• Why are deaths in war so concentrated in a handful of the most extreme events? 
• Did the ideas of the Enlightenment promote nonviolence, on balance? 
• Were early states more or less violent than groups of hunter-gatherers? 
• If Bear is right, what can be done? 
• How did the 'Concert of Europe' or 'Bismarckian system' maintain peace in the 19th century? 
• Which wars are remarkable but largely unknown?  

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Is war in long-term decline? Steven Pinker's The Better Angels of Our Nature brought this previously obscure academic question to the centre of public debate, and pointed to rates of death in war to argue energetically that war is on the way out. 

But </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/981a915d/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/981a915d/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#139 – Alan Hájek on puzzles and paradoxes in probability and expected value</title>
      <itunes:title>#139 – Alan Hájek on puzzles and paradoxes in probability and expected value</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">438c9230-5709-11ed-8453-12bb28a34eb1</guid>
      <link>https://80000hours.org/podcast/episodes/alan-hajek-probability-expected-value/</link>
      <description>
        <![CDATA[<p>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing to pay to play?</p><p> The standard way of analysing gambling problems, ‘expected value’ — in which you multiply probabilities by the value of each outcome and then sum them up — says your expected earnings are infinite. You have a 50% chance of winning $2, for '0.5 * $2 = $1' in expected earnings. A 25% chance of winning $4, for '0.25 * $4 = $1' in expected earnings, and on and on. A never-ending series of $1s added together comes to infinity. And that's despite the fact that you know with certainty you can only ever win a finite amount!</p><p> Today's guest — philosopher Alan Hájek of the Australian National University — thinks of much of philosophy as “the demolition of common sense followed by damage control” and is an expert on paradoxes related to probability and decision-making rules like “maximise expected value.”</p><p> <a href="https://80k.link/AH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The problem described above, known as the St. Petersburg paradox, has been a staple of the field since the 18th century, with many proposed solutions. In the interview, Alan explains how very natural attempts to resolve the paradox — such as factoring in the low likelihood that the casino can pay out very large sums, or the fact that money becomes less and less valuable the more of it you already have — fail to work as hoped.</p><p> We might reject the setup as a hypothetical that could never exist in the real world, and therefore of mere intellectual curiosity. But Alan doesn't find that objection persuasive. If expected value fails in extreme cases, that should make us worry that something could be rotten at the heart of the standard procedure we use to make decisions in government, business, and nonprofits.</p><p> These issues regularly show up in 80,000 Hours' efforts to try to find the best ways to improve the world, as the best approach will arguably involve long-shot attempts to do very large amounts of good.</p><p> Consider which is better: saving one life for sure, or three lives with 50% probability? Expected value says the second, which will probably strike you as reasonable enough. But what if we repeat this process and evaluate the chance to save nine lives with 25% probability, or 27 lives with 12.5% probability, or after 17 more iterations, 3,486,784,401 lives with a 0.00000009% chance. Expected value says this final offer is better than the others — 1,000 times better, in fact.</p><p> Ultimately Alan leans towards the view that our best choice is to “bite the bullet” and stick with expected value, even with its sometimes counterintuitive implications. Where we want to do damage control, we're better off looking for ways our probability estimates might be wrong.</p><p> In today's conversation, Alan and Rob explore these issues and many others:</p><p> • Simple rules of thumb for having philosophical insights<br> • A key flaw that hid in Pascal's wager from the very beginning<br> • Whether we have to simply ignore infinities because they mess everything up<br> • What fundamentally is 'probability'?<br> • Some of the many reasons 'frequentism' doesn't work as an account of probability<br> • Why the standard account of counterfactuals in philosophy is deeply flawed<br> • And why counterfactuals present a fatal problem for one sort of consequentialism</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:48)</li><li>Philosophical methodology (00:02:54)</li><li>Theories of probability (00:37:17)</li><li>Everyday Bayesianism (00:46:01)</li><li>Frequentism (01:04:56)</li><li>Ranges of probabilities (01:16:23)</li><li>Implications for how to live (01:21:24)</li><li>Expected value (01:26:58)</li><li>The St. Petersburg paradox (01:31:40)</li><li>Pascal's wager (01:49:44)</li><li>Using expected value in everyday life (02:03:53)</li><li>Counterfactuals (02:16:38)</li><li>Most counterfactuals are false (02:52:25)</li><li>Relevance to objective consequentialism (03:09:47)</li><li>Marker 18 (03:10:21)</li><li>Alan’s best conference story (03:33:37)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing to pay to play?</p><p> The standard way of analysing gambling problems, ‘expected value’ — in which you multiply probabilities by the value of each outcome and then sum them up — says your expected earnings are infinite. You have a 50% chance of winning $2, for '0.5 * $2 = $1' in expected earnings. A 25% chance of winning $4, for '0.25 * $4 = $1' in expected earnings, and on and on. A never-ending series of $1s added together comes to infinity. And that's despite the fact that you know with certainty you can only ever win a finite amount!</p><p> Today's guest — philosopher Alan Hájek of the Australian National University — thinks of much of philosophy as “the demolition of common sense followed by damage control” and is an expert on paradoxes related to probability and decision-making rules like “maximise expected value.”</p><p> <a href="https://80k.link/AH"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The problem described above, known as the St. Petersburg paradox, has been a staple of the field since the 18th century, with many proposed solutions. In the interview, Alan explains how very natural attempts to resolve the paradox — such as factoring in the low likelihood that the casino can pay out very large sums, or the fact that money becomes less and less valuable the more of it you already have — fail to work as hoped.</p><p> We might reject the setup as a hypothetical that could never exist in the real world, and therefore of mere intellectual curiosity. But Alan doesn't find that objection persuasive. If expected value fails in extreme cases, that should make us worry that something could be rotten at the heart of the standard procedure we use to make decisions in government, business, and nonprofits.</p><p> These issues regularly show up in 80,000 Hours' efforts to try to find the best ways to improve the world, as the best approach will arguably involve long-shot attempts to do very large amounts of good.</p><p> Consider which is better: saving one life for sure, or three lives with 50% probability? Expected value says the second, which will probably strike you as reasonable enough. But what if we repeat this process and evaluate the chance to save nine lives with 25% probability, or 27 lives with 12.5% probability, or after 17 more iterations, 3,486,784,401 lives with a 0.00000009% chance. Expected value says this final offer is better than the others — 1,000 times better, in fact.</p><p> Ultimately Alan leans towards the view that our best choice is to “bite the bullet” and stick with expected value, even with its sometimes counterintuitive implications. Where we want to do damage control, we're better off looking for ways our probability estimates might be wrong.</p><p> In today's conversation, Alan and Rob explore these issues and many others:</p><p> • Simple rules of thumb for having philosophical insights<br> • A key flaw that hid in Pascal's wager from the very beginning<br> • Whether we have to simply ignore infinities because they mess everything up<br> • What fundamentally is 'probability'?<br> • Some of the many reasons 'frequentism' doesn't work as an account of probability<br> • Why the standard account of counterfactuals in philosophy is deeply flawed<br> • And why counterfactuals present a fatal problem for one sort of consequentialism</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:48)</li><li>Philosophical methodology (00:02:54)</li><li>Theories of probability (00:37:17)</li><li>Everyday Bayesianism (00:46:01)</li><li>Frequentism (01:04:56)</li><li>Ranges of probabilities (01:16:23)</li><li>Implications for how to live (01:21:24)</li><li>Expected value (01:26:58)</li><li>The St. Petersburg paradox (01:31:40)</li><li>Pascal's wager (01:49:44)</li><li>Using expected value in everyday life (02:03:53)</li><li>Counterfactuals (02:16:38)</li><li>Most counterfactuals are false (02:52:25)</li><li>Relevance to objective consequentialism (03:09:47)</li><li>Marker 18 (03:10:21)</li><li>Alan’s best conference story (03:33:37)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 28 Oct 2022 21:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0c1addc4/8788ed4b.mp3" length="104847291" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-s87fPbSrQZurcfcqIdeuOdMb5ZQ83LBj_y081jkhug/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTEv/MTY4MzU0NDczOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13106</itunes:duration>
      <itunes:summary>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing to pay to play? 

The standard way of analysing gambling problems, ‘expected value’ — in which you multiply probabilities by the value of each outcome and then sum them up — says your expected earnings are infinite. You have a 50% chance of winning $2, for '0.5 * $2 = $1' in expected earnings. A 25% chance of winning $4, for '0.25 * $4 = $1' in expected earnings, and on and on. A never-ending series of $1s added together comes to infinity. And that's despite the fact that you know with certainty you can only ever win a finite amount! 

Today's guest — philosopher Alan Hájek of the Australian National University — thinks of much of philosophy as “the demolition of common sense followed by damage control” and is an expert on paradoxes related to probability and decision-making rules like “maximise expected value.” 

Links to learn more, summary and full transcript.

The problem described above, known as the St. Petersburg paradox, has been a staple of the field since the 18th century, with many proposed solutions. In the interview, Alan explains how very natural attempts to resolve the paradox — such as factoring in the low likelihood that the casino can pay out very large sums, or the fact that money becomes less and less valuable the more of it you already have — fail to work as hoped. 

We might reject the setup as a hypothetical that could never exist in the real world, and therefore of mere intellectual curiosity. But Alan doesn't find that objection persuasive. If expected value fails in extreme cases, that should make us worry that something could be rotten at the heart of the standard procedure we use to make decisions in government, business, and nonprofits. 

These issues regularly show up in 80,000 Hours' efforts to try to find the best ways to improve the world, as the best approach will arguably involve long-shot attempts to do very large amounts of good. 

Consider which is better: saving one life for sure, or three lives with 50% probability? Expected value says the second, which will probably strike you as reasonable enough. But what if we repeat this process and evaluate the chance to save nine lives with 25% probability, or 27 lives with 12.5% probability, or after 17 more iterations, 3,486,784,401 lives with a 0.00000009% chance. Expected value says this final offer is better than the others — 1,000 times better, in fact. 

Ultimately Alan leans towards the view that our best choice is to “bite the bullet” and stick with expected value, even with its sometimes counterintuitive implications. Where we want to do damage control, we're better off looking for ways our probability estimates might be wrong. 

In today's conversation, Alan and Rob explore these issues and many others: 

• Simple rules of thumb for having philosophical insights 
• A key flaw that hid in Pascal's wager from the very beginning 
• Whether we have to simply ignore infinities because they mess everything up 
• What fundamentally is 'probability'? 
• Some of the many reasons 'frequentism' doesn't work as an account of probability 
• Why the standard account of counterfactuals in philosophy is deeply flawed 
• And why counterfactuals present a fatal problem for one sort of consequentialism 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>A casino offers you a game. A coin will be tossed. If it comes up heads on the first flip you win $2. If it comes up on the second flip you win $4. If it comes up on the third you win $8, the fourth you win $16, and so on. How much should you be willing t</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/0c1addc4/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/0c1addc4/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Preventing an AI-related catastrophe (Article)</title>
      <itunes:title>Preventing an AI-related catastrophe (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">bda7c6c2-4c03-11ed-b60e-1223df9590df</guid>
      <link>https://share.transistor.fm/s/e8e6be80</link>
      <description>
        <![CDATA[Today’s release is a professional reading of our new problem profile on <a href="https://80k.link/AIP"><b>preventing an AI-related catastrophe</b></a>, written by Benjamin Hilton.<p> 

We expect that there will be substantial progress in AI in the next few decades, potentially even to the point where machines come to outperform humans in many, if not all, tasks. This could have enormous benefits, helping to solve currently intractable global problems, but could also pose severe risks. These risks could arise accidentally (for example, if we don’t find technical solutions to concerns about the safety of AI systems), or deliberately (for example, if AI systems worsen geopolitical conflict). We think more work needs to be done to reduce these risks.</p><p> 

Some of these risks from advanced AI could be existential — meaning they could cause human extinction, or an equally permanent and severe disempowerment of humanity. There have not yet been any satisfying answers to concerns about how this rapidly approaching, transformative technology can be safely developed and integrated into our society. Finding answers to these concerns is very neglected, and may well be tractable. We estimate that there are around 300 people worldwide working directly on this. As a result, the possibility of AI-related catastrophe may be the world’s most pressing problem — and the best thing to work on for those who are well-placed to contribute.</p><p> 

Promising options for working on this problem include technical research on how to create safe AI systems, strategy research into the particular risks AI might pose, and policy research into ways in which companies and governments could mitigate these risks. If worthwhile policies are developed, we’ll need people to put them in place and implement them. There are also many opportunities to have a big impact in a variety of complementary roles, such as operations management, journalism, earning to give, and more.</p><p> 

If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.link/AIP"><b>here.</b></a></p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.</b></p><p>

<em>Producer: Keiran Harris<br>
Editing and narration: Perrin Walker and Shaun Acker<br>
Audio proofing: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today’s release is a professional reading of our new problem profile on <a href="https://80k.link/AIP"><b>preventing an AI-related catastrophe</b></a>, written by Benjamin Hilton.<p> 

We expect that there will be substantial progress in AI in the next few decades, potentially even to the point where machines come to outperform humans in many, if not all, tasks. This could have enormous benefits, helping to solve currently intractable global problems, but could also pose severe risks. These risks could arise accidentally (for example, if we don’t find technical solutions to concerns about the safety of AI systems), or deliberately (for example, if AI systems worsen geopolitical conflict). We think more work needs to be done to reduce these risks.</p><p> 

Some of these risks from advanced AI could be existential — meaning they could cause human extinction, or an equally permanent and severe disempowerment of humanity. There have not yet been any satisfying answers to concerns about how this rapidly approaching, transformative technology can be safely developed and integrated into our society. Finding answers to these concerns is very neglected, and may well be tractable. We estimate that there are around 300 people worldwide working directly on this. As a result, the possibility of AI-related catastrophe may be the world’s most pressing problem — and the best thing to work on for those who are well-placed to contribute.</p><p> 

Promising options for working on this problem include technical research on how to create safe AI systems, strategy research into the particular risks AI might pose, and policy research into ways in which companies and governments could mitigate these risks. If worthwhile policies are developed, we’ll need people to put them in place and implement them. There are also many opportunities to have a big impact in a variety of complementary roles, such as operations management, journalism, earning to give, and more.</p><p> 

If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80k.link/AIP"><b>here.</b></a></p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.</b></p><p>

<em>Producer: Keiran Harris<br>
Editing and narration: Perrin Walker and Shaun Acker<br>
Audio proofing: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 14 Oct 2022 21:11:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e8e6be80/a68dc846.mp3" length="69262281" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/uckgdjthQY2K0eJUmU3qrHBhIPq1t6upv5vLL24hASk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4OTAv/MTY4MzU0NDczOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8658</itunes:duration>
      <itunes:summary>Today’s release is a professional reading of our new problem profile on preventing an AI-related catastrophe, written by Benjamin Hilton. 

We expect that there will be substantial progress in AI in the next few decades, potentially even to the point where machines come to outperform humans in many, if not all, tasks. This could have enormous benefits, helping to solve currently intractable global problems, but could also pose severe risks. These risks could arise accidentally (for example, if we don’t find technical solutions to concerns about the safety of AI systems), or deliberately (for example, if AI systems worsen geopolitical conflict). We think more work needs to be done to reduce these risks. 

Some of these risks from advanced AI could be existential — meaning they could cause human extinction, or an equally permanent and severe disempowerment of humanity. There have not yet been any satisfying answers to concerns about how this rapidly approaching, transformative technology can be safely developed and integrated into our society. Finding answers to these concerns is very neglected, and may well be tractable. We estimate that there are around 300 people worldwide working directly on this. As a result, the possibility of AI-related catastrophe may be the world’s most pressing problem — and the best thing to work on for those who are well-placed to contribute. 

Promising options for working on this problem include technical research on how to create safe AI systems, strategy research into the particular risks AI might pose, and policy research into ways in which companies and governments could mitigate these risks. If worthwhile policies are developed, we’ll need people to put them in place and implement them. There are also many opportunities to have a big impact in a variety of complementary roles, such as operations management, journalism, earning to give, and more. 

If you want to check out the links, footnotes and figures in today’s article, you can find those here. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.

Producer: Keiran Harris
Editing and narration: Perrin Walker and Shaun Acker
Audio proofing: Katy Moore</itunes:summary>
      <itunes:subtitle>Today’s release is a professional reading of our new problem profile on preventing an AI-related catastrophe, written by Benjamin Hilton. 

We expect that there will be substantial progress in AI in the next few decades, potentially even to the point wher</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/e8e6be80/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#138 – Sharon Hewitt Rawlette on why pleasure and pain are the only things that intrinsically matter</title>
      <itunes:title>#138 – Sharon Hewitt Rawlette on why pleasure and pain are the only things that intrinsically matter</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">db5411c0-40fb-11ed-98ef-1242b6702ca7</guid>
      <link>https://80000hours.org/podcast/episodes/sharon-hewitt-rawlette-hedonistic-utilitarianism/</link>
      <description>
        <![CDATA[<p>What in the world is <em>intrinsically</em> good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more.</p><p> The question is a classic that makes for great dorm-room philosophy discussion. But it's hardly just of academic interest. The issue of what (if anything) is intrinsically valuable bears on every action we take, whether we’re looking to improve our own lives, or to help others. The wrong answer might lead us to the wrong project and render our efforts to improve the world entirely ineffective.</p><p> Today's guest, Sharon Hewitt Rawlette — philosopher and author of <a href="https://www.amazon.com/Feeling-Value-Grounded-Phenomenal-Consciousness/dp/1534768017"><em>The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness</em></a> — wants to resuscitate an answer to this question that is as old as philosophy itself.</p><p> <a href="https://80k.link/SHR"><strong>Links to learn more, summary, full transcript, and full version of this blog post.</strong></a></p><p> That idea, in a nutshell, is that there is only one thing of true intrinsic value: positive feelings and sensations. And similarly, there is only one thing that is intrinsically of negative value: suffering, pain, and other unpleasant sensations.</p><p> Lots of other things are valuable too: friendship, fairness, loyalty, integrity, wealth, patience, houses, and so on. But they are only <em>instrumentally</em> valuable — that is to say, they’re valuable as means to the end of ensuring that all conscious beings experience more pleasure and other positive sensations, and less suffering.</p><p> As Sharon notes, from Athens in 400 BC to Britain in 1850, the idea that only subjective experiences can be good or bad in themselves -- a position known as 'philosophical hedonism' -- has been one of the most enduringly popular ideas in ethics.</p><p> And few will be taken aback by the notion that, all else equal, more pleasure is good and less suffering is bad. But can they really be the <em>only</em> intrinsically valuable things?</p><p> Over the 20th century, philosophical hedonism became increasingly controversial in the face of some seemingly very counterintuitive implications. For this reason the famous philosopher of mind Thomas Nagel called <em>The Feeling of Value</em> "a radical and important philosophical contribution."</p><p> In today's interview, Sharon explains the case for a theory of value grounded in subjective experiences, and why she believes the most popular counterarguments are misguided.</p><p> Host Rob Wiblin and Sharon also cover:</p><p> • The essential need to disentangle intrinsic, instrumental, and other sorts of value<br> • Why Sharon’s arguments lead to hedonistic utilitarianism rather than hedonistic egoism (in which we only care about our own feelings)<br> • How do people react to the 'experience machine' thought experiment when surveyed?<br> • Why hedonism recommends often thinking and acting as though it were false<br> • Whether it's crazy to think that relationships are only useful because of their effects on our subjective experiences<br> • Whether it will ever be possible to eliminate pain, and whether doing so would be desirable<br> • If we didn't have positive or negative experiences, whether that would cause us to simply never talk about goodness and badness<br> • Whether the plausibility of hedonism is affected by our theory of mind<br> • And plenty more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:45)</li><li>Metaethics (00:04:16)</li><li>Anti-realism (00:10:39)</li><li>Sharon's theory of moral realism (00:16:17)</li><li>The history of hedonism (00:23:11)</li><li>Intrinsic value vs instrumental value (00:28:49)</li><li>Egoistic hedonism (00:36:30)</li><li>Single axis of value (00:42:19)</li><li>Key objections to Sharon’s brand of hedonism (00:56:18)</li><li>The experience machine (01:06:08)</li><li>Robot spouses (01:22:29)</li><li>Most common misunderstanding of Sharon’s view (01:27:10)</li><li>How might a hedonist actually live (01:37:46)</li><li>The organ transplant case (01:53:34)</li><li>Counterintuitive implications of hedonistic utilitarianism (02:03:40)</li><li>How could we discover moral facts? (02:18:05)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>What in the world is <em>intrinsically</em> good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more.</p><p> The question is a classic that makes for great dorm-room philosophy discussion. But it's hardly just of academic interest. The issue of what (if anything) is intrinsically valuable bears on every action we take, whether we’re looking to improve our own lives, or to help others. The wrong answer might lead us to the wrong project and render our efforts to improve the world entirely ineffective.</p><p> Today's guest, Sharon Hewitt Rawlette — philosopher and author of <a href="https://www.amazon.com/Feeling-Value-Grounded-Phenomenal-Consciousness/dp/1534768017"><em>The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness</em></a> — wants to resuscitate an answer to this question that is as old as philosophy itself.</p><p> <a href="https://80k.link/SHR"><strong>Links to learn more, summary, full transcript, and full version of this blog post.</strong></a></p><p> That idea, in a nutshell, is that there is only one thing of true intrinsic value: positive feelings and sensations. And similarly, there is only one thing that is intrinsically of negative value: suffering, pain, and other unpleasant sensations.</p><p> Lots of other things are valuable too: friendship, fairness, loyalty, integrity, wealth, patience, houses, and so on. But they are only <em>instrumentally</em> valuable — that is to say, they’re valuable as means to the end of ensuring that all conscious beings experience more pleasure and other positive sensations, and less suffering.</p><p> As Sharon notes, from Athens in 400 BC to Britain in 1850, the idea that only subjective experiences can be good or bad in themselves -- a position known as 'philosophical hedonism' -- has been one of the most enduringly popular ideas in ethics.</p><p> And few will be taken aback by the notion that, all else equal, more pleasure is good and less suffering is bad. But can they really be the <em>only</em> intrinsically valuable things?</p><p> Over the 20th century, philosophical hedonism became increasingly controversial in the face of some seemingly very counterintuitive implications. For this reason the famous philosopher of mind Thomas Nagel called <em>The Feeling of Value</em> "a radical and important philosophical contribution."</p><p> In today's interview, Sharon explains the case for a theory of value grounded in subjective experiences, and why she believes the most popular counterarguments are misguided.</p><p> Host Rob Wiblin and Sharon also cover:</p><p> • The essential need to disentangle intrinsic, instrumental, and other sorts of value<br> • Why Sharon’s arguments lead to hedonistic utilitarianism rather than hedonistic egoism (in which we only care about our own feelings)<br> • How do people react to the 'experience machine' thought experiment when surveyed?<br> • Why hedonism recommends often thinking and acting as though it were false<br> • Whether it's crazy to think that relationships are only useful because of their effects on our subjective experiences<br> • Whether it will ever be possible to eliminate pain, and whether doing so would be desirable<br> • If we didn't have positive or negative experiences, whether that would cause us to simply never talk about goodness and badness<br> • Whether the plausibility of hedonism is affected by our theory of mind<br> • And plenty more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:45)</li><li>Metaethics (00:04:16)</li><li>Anti-realism (00:10:39)</li><li>Sharon's theory of moral realism (00:16:17)</li><li>The history of hedonism (00:23:11)</li><li>Intrinsic value vs instrumental value (00:28:49)</li><li>Egoistic hedonism (00:36:30)</li><li>Single axis of value (00:42:19)</li><li>Key objections to Sharon’s brand of hedonism (00:56:18)</li><li>The experience machine (01:06:08)</li><li>Robot spouses (01:22:29)</li><li>Most common misunderstanding of Sharon’s view (01:27:10)</li><li>How might a hedonist actually live (01:37:46)</li><li>The organ transplant case (01:53:34)</li><li>Counterintuitive implications of hedonistic utilitarianism (02:03:40)</li><li>How could we discover moral facts? (02:18:05)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 30 Sep 2022 21:05:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/63b4eec7/2ed4ddbc.mp3" length="69282736" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/TwaNmvPdtFHXVmwMCCMUY9c52cAbySK-24vq4QPcahg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODkv/MTY4MzU0NDczMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8660</itunes:duration>
      <itunes:summary>What in the world is intrinsically good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more. 

The question is a classic that makes for great dorm-room philosophy discussion. But it's hardly just of academic interest. The issue of what (if anything) is intrinsically valuable bears on every action we take, whether we’re looking to improve our own lives, or to help others. The wrong answer might lead us to the wrong project and render our efforts to improve the world entirely ineffective. 

Today's guest, Sharon Hewitt Rawlette — philosopher and author of The Feeling of Value: Moral Realism Grounded in Phenomenal Consciousness — wants to resuscitate an answer to this question that is as old as philosophy itself. 

Links to learn more, summary, full transcript, and full version of this blog post. 

That idea, in a nutshell, is that there is only one thing of true intrinsic value: positive feelings and sensations. And similarly, there is only one thing that is intrinsically of negative value: suffering, pain, and other unpleasant sensations. 

Lots of other things are valuable too: friendship, fairness, loyalty, integrity, wealth, patience, houses, and so on. But they are only instrumentally valuable — that is to say, they’re valuable as means to the end of ensuring that all conscious beings experience more pleasure and other positive sensations, and less suffering. 

As Sharon notes, from Athens in 400 BC to Britain in 1850, the idea that only subjective experiences can be good or bad in themselves -- a position known as 'philosophical hedonism' -- has been one of the most enduringly popular ideas in ethics. 

And few will be taken aback by the notion that, all else equal, more pleasure is good and less suffering is bad. But can they really be the only intrinsically valuable things? 

Over the 20th century, philosophical hedonism became increasingly controversial in the face of some seemingly very counterintuitive implications. For this reason the famous philosopher of mind Thomas Nagel called The Feeling of Value "a radical and important philosophical contribution." 

In today's interview, Sharon explains the case for a theory of value grounded in subjective experiences, and why she believes the most popular counterarguments are misguided. 

Host Rob Wiblin and Sharon also cover: 

• The essential need to disentangle intrinsic, instrumental, and other sorts of value 
• Why Sharon’s arguments lead to hedonistic utilitarianism rather than hedonistic egoism (in which we only care about our own feelings) 
• How do people react to the 'experience machine' thought experiment when surveyed? 
• Why hedonism recommends often thinking and acting as though it were false 
• Whether it's crazy to think that relationships are only useful because of their effects on our subjective experiences 
• Whether it will ever be possible to eliminate pain, and whether doing so would be desirable 
• If we didn't have positive or negative experiences, whether that would cause us to simply never talk about goodness and badness 
• Whether the plausibility of hedonism is affected by our theory of mind 
• And plenty more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>What in the world is intrinsically good — good in itself even if it has no other effects? Over the millennia, people have offered many answers: joy, justice, equality, accomplishment, loving god, wisdom, and plenty more. 

The question is a classic that</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/63b4eec7/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/63b4eec7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#137 – Andreas Mogensen on whether effective altruism is just for consequentialists</title>
      <itunes:title>#137 – Andreas Mogensen on whether effective altruism is just for consequentialists</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f49156cc-2f9c-11ed-b03e-0ec39a9aa37d</guid>
      <link>https://80000hours.org/podcast/episodes/andreas-mogensen-deontology-and-effective-altruism/</link>
      <description>
        <![CDATA[<p>Effective altruism, in a slogan, aims to 'do the most good.' Utilitarianism, in a slogan, says we should act to 'produce the greatest good for the greatest number.' It's clear enough why utilitarians should be interested in the project of effective altruism. But what about the many people who reject utilitarianism?</p><p> Today's guest, Andreas Mogensen — senior research fellow at Oxford University's Global Priorities Institute — rejects utilitarianism, but as he explains, this does little to dampen his enthusiasm for the project of effective altruism.</p><p> <a href="https://80k.link/AM"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Andreas leans towards 'deontological' or rule-based theories of ethics, rather than 'consequentialist' theories like utilitarianism which look exclusively at the effects of a person's actions.</p><p> Like most people involved in effective altruism, he parts ways with utilitarianism in rejecting its maximal level of demandingness, the idea that the ends justify the means, and the notion that the only moral reason for action is to benefit everyone in the world considered impartially.</p><p> However, Andreas believes any plausible theory of morality must give <em>some</em> weight to the harms and benefits we provide to other people. If we can improve a stranger's wellbeing enormously at negligible cost to ourselves and without violating any other moral prohibition, that must be at minimum a <em>praiseworthy</em> thing to do.</p><p> In a world as full of preventable suffering as our own, this simple 'principle of beneficence' is probably the only premise one needs to grant for the effective altruist project of identifying the most impactful ways to help others to be of great moral interest and importance.</p><p> As an illustrative example Andreas refers to the Giving What We Can pledge to donate 10% of one's income to the most impactful charities available, a pledge he took in 2009. Many effective altruism enthusiasts have taken such a pledge, while others spend their careers trying to figure out the most cost-effective places pledgers can give, where they'll get the biggest 'bang for buck'.</p><p> For someone living in a world as unequal as our own, this pledge at a very minimum gives an upper-middle class person in a rich country the chance to transfer money to someone living on about 1% as much as they do. The benefit an extremely poor recipient receives from the money is likely far more than the donor could get spending it on themselves.</p><p> What arguments could a non-utilitarian moral theory mount against such giving?</p><p> Many approaches to morality will say it's <em>permissible</em> not to give away 10% of your income to help others as effectively as is possible. But if they will almost all regard it as praiseworthy to benefit others without giving up something else of equivalent moral value, then Andreas argues they should be enthusiastic about effective altruism as an intellectual and practical project nonetheless.</p><p> In this conversation, Andreas and Rob discuss how robust the above line of argument is, and also cover:</p><p> • Should we treat thought experiments that feature very large numbers with great suspicion?<br> • If we had to allow someone to die to avoid preventing the World Cup final from being broadcast to the world, is that permissible?<br> • What might a virtue ethicist regard as 'doing the most good'?<br> • If a deontological theory of morality parted ways with common effective altruist practices, how would that likely be?<br> • If we can explain how we came to hold a view on a moral issue by referring to evolutionary selective pressures, should we disbelieve that view?</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:36)</li><li>Deontology and effective altruism (00:04:59)</li><li>Giving What We Can (00:28:56)</li><li>Longtermism without consequentialism (00:38:01)</li><li>Further differences between deontologists and consequentialists (00:44:13)</li><li>Virtue ethics and effective altruism (01:08:15)</li><li>Is Andreas really a deontologist? (01:13:26)</li><li>Large number scepticism (01:21:11)</li><li>Evolutionary debunking arguments (01:58:48)</li><li>How Andreas’s views have changed (02:12:18)</li><li>Derek Parfit’s influence on Andreas (02:17:27)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Beppe Rådvik<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Effective altruism, in a slogan, aims to 'do the most good.' Utilitarianism, in a slogan, says we should act to 'produce the greatest good for the greatest number.' It's clear enough why utilitarians should be interested in the project of effective altruism. But what about the many people who reject utilitarianism?</p><p> Today's guest, Andreas Mogensen — senior research fellow at Oxford University's Global Priorities Institute — rejects utilitarianism, but as he explains, this does little to dampen his enthusiasm for the project of effective altruism.</p><p> <a href="https://80k.link/AM"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Andreas leans towards 'deontological' or rule-based theories of ethics, rather than 'consequentialist' theories like utilitarianism which look exclusively at the effects of a person's actions.</p><p> Like most people involved in effective altruism, he parts ways with utilitarianism in rejecting its maximal level of demandingness, the idea that the ends justify the means, and the notion that the only moral reason for action is to benefit everyone in the world considered impartially.</p><p> However, Andreas believes any plausible theory of morality must give <em>some</em> weight to the harms and benefits we provide to other people. If we can improve a stranger's wellbeing enormously at negligible cost to ourselves and without violating any other moral prohibition, that must be at minimum a <em>praiseworthy</em> thing to do.</p><p> In a world as full of preventable suffering as our own, this simple 'principle of beneficence' is probably the only premise one needs to grant for the effective altruist project of identifying the most impactful ways to help others to be of great moral interest and importance.</p><p> As an illustrative example Andreas refers to the Giving What We Can pledge to donate 10% of one's income to the most impactful charities available, a pledge he took in 2009. Many effective altruism enthusiasts have taken such a pledge, while others spend their careers trying to figure out the most cost-effective places pledgers can give, where they'll get the biggest 'bang for buck'.</p><p> For someone living in a world as unequal as our own, this pledge at a very minimum gives an upper-middle class person in a rich country the chance to transfer money to someone living on about 1% as much as they do. The benefit an extremely poor recipient receives from the money is likely far more than the donor could get spending it on themselves.</p><p> What arguments could a non-utilitarian moral theory mount against such giving?</p><p> Many approaches to morality will say it's <em>permissible</em> not to give away 10% of your income to help others as effectively as is possible. But if they will almost all regard it as praiseworthy to benefit others without giving up something else of equivalent moral value, then Andreas argues they should be enthusiastic about effective altruism as an intellectual and practical project nonetheless.</p><p> In this conversation, Andreas and Rob discuss how robust the above line of argument is, and also cover:</p><p> • Should we treat thought experiments that feature very large numbers with great suspicion?<br> • If we had to allow someone to die to avoid preventing the World Cup final from being broadcast to the world, is that permissible?<br> • What might a virtue ethicist regard as 'doing the most good'?<br> • If a deontological theory of morality parted ways with common effective altruist practices, how would that likely be?<br> • If we can explain how we came to hold a view on a moral issue by referring to evolutionary selective pressures, should we disbelieve that view?</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:36)</li><li>Deontology and effective altruism (00:04:59)</li><li>Giving What We Can (00:28:56)</li><li>Longtermism without consequentialism (00:38:01)</li><li>Further differences between deontologists and consequentialists (00:44:13)</li><li>Virtue ethics and effective altruism (01:08:15)</li><li>Is Andreas really a deontologist? (01:13:26)</li><li>Large number scepticism (01:21:11)</li><li>Evolutionary debunking arguments (01:58:48)</li><li>How Andreas’s views have changed (02:12:18)</li><li>Derek Parfit’s influence on Andreas (02:17:27)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell and Beppe Rådvik<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 08 Sep 2022 21:00:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/dc1801b6/5703a55e.mp3" length="67951913" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/118lgsGXN-RRAQ9mLvXle-VCah8-zeJs9hVBUzMIY8A/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODgv/MTY4MzU0NDczMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8494</itunes:duration>
      <itunes:summary>Effective altruism, in a slogan, aims to 'do the most good.' Utilitarianism, in a slogan, says we should act to 'produce the greatest good for the greatest number.' It's clear enough why utilitarians should be interested in the project of effective altruism. But what about the many people who reject utilitarianism? 

Today's guest, Andreas Mogensen — senior research fellow at Oxford University's Global Priorities Institute — rejects utilitarianism, but as he explains, this does little to dampen his enthusiasm for the project of effective altruism. 

Links to learn more, summary and full transcript. 

Andreas leans towards 'deontological' or rule-based theories of ethics, rather than 'consequentialist' theories like utilitarianism which look exclusively at the effects of a person's actions. 

Like most people involved in effective altruism, he parts ways with utilitarianism in rejecting its maximal level of demandingness, the idea that the ends justify the means, and the notion that the only moral reason for action is to benefit everyone in the world considered impartially. 

However, Andreas believes any plausible theory of morality must give some weight to the harms and benefits we provide to other people. If we can improve a stranger's wellbeing enormously at negligible cost to ourselves and without violating any other moral prohibition, that must be at minimum a praiseworthy thing to do. 

In a world as full of preventable suffering as our own, this simple 'principle of beneficence' is probably the only premise one needs to grant for the effective altruist project of identifying the most impactful ways to help others to be of great moral interest and importance. 

As an illustrative example Andreas refers to the Giving What We Can pledge to donate 10% of one's income to the most impactful charities available, a pledge he took in 2009. Many effective altruism enthusiasts have taken such a pledge, while others spend their careers trying to figure out the most cost-effective places pledgers can give, where they'll get the biggest 'bang for buck'. 

For someone living in a world as unequal as our own, this pledge at a very minimum gives an upper-middle class person in a rich country the chance to transfer money to someone living on about 1% as much as they do. The benefit an extremely poor recipient receives from the money is likely far more than the donor could get spending it on themselves. 

What arguments could a non-utilitarian moral theory mount against such giving? 

Many approaches to morality will say it's permissible not to give away 10% of your income to help others as effectively as is possible. But if they will almost all regard it as praiseworthy to benefit others without giving up something else of equivalent moral value, then Andreas argues they should be enthusiastic about effective altruism as an intellectual and practical project nonetheless. 

In this conversation, Andreas and Rob discuss how robust the above line of argument is, and also cover: 

• Should we treat thought experiments that feature very large numbers with great suspicion? 
• If we had to allow someone to die to avoid preventing the World Cup final from being broadcast to the world, is that permissible? 
• What might a virtue ethicist regard as 'doing the most good'? 
• If a deontological theory of morality parted ways with common effective altruist practices, how would that likely be? 
• If we can explain how we came to hold a view on a moral issue by referring to evolutionary selective pressures, should we disbelieve that view? 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Beppe Rådvik
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Effective altruism, in a slogan, aims to 'do the most good.' Utilitarianism, in a slogan, says we should act to 'produce the greatest good for the greatest number.' It's clear enough why utilitarians should be interested in the project of effective altrui</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/dc1801b6/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/dc1801b6/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#136 – Will MacAskill on what we owe the future</title>
      <itunes:title>#136 – Will MacAskill on what we owe the future</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">564862e4-1cd2-11ed-9a30-12aec54c5db7</guid>
      <link>https://80000hours.org/podcast/episodes/will-macaskill-what-we-owe-the-future/</link>
      <description>
        <![CDATA[<ol><li>People who exist in the future deserve some degree of moral consideration.</li><li>The future could be very big, very long, and/or very good.</li><li>We can reasonably hope to influence whether people in the future exist, and how good or bad their lives are.</li><li>So trying to make the world better for future generations is a key priority of our time.</li></ol><p>This is the simple four-step argument for 'longtermism' put forward in <a href="https://www.whatweowethefuture.com/"><strong><em>What We Owe The Future</em></strong></a>, the latest book from today's guest — University of Oxford philosopher and cofounder of the effective altruism community, Will MacAskill.</p><p> <a href="https://80k.link/WM4"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> From one point of view this idea is common sense. We work on breakthroughs to treat cancer or end use of fossil fuels not just for people alive today, but because we hope such scientific advances will help our children, grandchildren, and great-grandchildren as well.</p><p> Some who take this longtermist idea seriously work to develop broad-spectrum vaccines they hope will safeguard humanity against the sorts of extremely deadly pandemics that could permanently throw civilisation off track — the sort of project few could argue is not worthwhile.</p><p> But Will is upfront that longtermism is also counterintuitive. To start with, he's willing to contemplate timescales far beyond what's typically discussed.</p><p> A natural objection to thinking millions of years ahead is that it's hard enough to take actions that have positive effects that persist for hundreds of years, let alone “indefinitely.” It doesn't matter how important something might be if you can't predictably change it.</p><p> This is one reason, among others, that Will was initially sceptical of longtermism and took years to come around. He preferred to focus on ending poverty and preventable diseases in ways he could directly see were working.</p><p> But over seven years he gradually changed his mind, and in *What We Owe The Future*, Will argues that in fact there are clear ways we might act now that could benefit not just a few but *all* future generations.</p><p> The idea that preventing human extinction would have long-lasting impacts is pretty intuitive. If we entirely disappear, we aren't coming back.</p><p> But the idea that we can shape human values — not just for our age, but for all ages — is a surprising one that Will has come to more recently.</p><p> In the book, he argues that what people value is far more fragile and historically contingent than it might first seem. For instance, today it feels like the abolition of slavery was an inevitable part of the arc of history. But Will lays out that the best research on the topic suggests otherwise.</p><p> If moral progress really is so contingent, and bad ideas can persist almost without end, it raises the stakes for moral debate today. If we don't eliminate a bad practice now, it may be with us forever. In today's in-depth conversation, we discuss the possibility of a harmful moral 'lock-in' as well as:</p><p> • How Will was eventually won over to longtermism<br> • The three best lines of argument against longtermism<br> • How to avoid moral fanaticism<br> • Which technologies or events are most likely to have permanent effects<br> • What 'longtermists' do today in practice<br> • How to predict the long-term effect of our actions<br> • Whether the future is likely to be good or bad<br> • Concrete ideas to make the future better<br> • What Will donates his money to personally<br> • Potatoes and megafauna<br> • And plenty more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:36)</li><li>What longtermism actually is (00:02:31)</li><li>The case for longtermism (00:04:30)</li><li>What longtermists are actually doing (00:15:54)</li><li>Will’s personal journey (00:22:15)</li><li>Strongest arguments against longtermism (00:42:28)</li><li>Preventing extinction vs. improving the quality of the future (00:59:29)</li><li>Is humanity likely to converge on doing the same thing regardless? (01:06:58)</li><li>Lock-in scenario vs. long reflection (01:27:11)</li><li>Is the future good in expectation? (01:32:29)</li><li>Can we actually predictably influence the future positively? (01:47:27)</li><li>Tiny probabilities of enormous value (01:53:40)</li><li>Stagnation (02:19:04)</li><li>Concrete suggestions (02:34:27)</li><li>Where Will donates (02:39:40)</li><li>Potatoes and megafauna (02:41:48)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<ol><li>People who exist in the future deserve some degree of moral consideration.</li><li>The future could be very big, very long, and/or very good.</li><li>We can reasonably hope to influence whether people in the future exist, and how good or bad their lives are.</li><li>So trying to make the world better for future generations is a key priority of our time.</li></ol><p>This is the simple four-step argument for 'longtermism' put forward in <a href="https://www.whatweowethefuture.com/"><strong><em>What We Owe The Future</em></strong></a>, the latest book from today's guest — University of Oxford philosopher and cofounder of the effective altruism community, Will MacAskill.</p><p> <a href="https://80k.link/WM4"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> From one point of view this idea is common sense. We work on breakthroughs to treat cancer or end use of fossil fuels not just for people alive today, but because we hope such scientific advances will help our children, grandchildren, and great-grandchildren as well.</p><p> Some who take this longtermist idea seriously work to develop broad-spectrum vaccines they hope will safeguard humanity against the sorts of extremely deadly pandemics that could permanently throw civilisation off track — the sort of project few could argue is not worthwhile.</p><p> But Will is upfront that longtermism is also counterintuitive. To start with, he's willing to contemplate timescales far beyond what's typically discussed.</p><p> A natural objection to thinking millions of years ahead is that it's hard enough to take actions that have positive effects that persist for hundreds of years, let alone “indefinitely.” It doesn't matter how important something might be if you can't predictably change it.</p><p> This is one reason, among others, that Will was initially sceptical of longtermism and took years to come around. He preferred to focus on ending poverty and preventable diseases in ways he could directly see were working.</p><p> But over seven years he gradually changed his mind, and in *What We Owe The Future*, Will argues that in fact there are clear ways we might act now that could benefit not just a few but *all* future generations.</p><p> The idea that preventing human extinction would have long-lasting impacts is pretty intuitive. If we entirely disappear, we aren't coming back.</p><p> But the idea that we can shape human values — not just for our age, but for all ages — is a surprising one that Will has come to more recently.</p><p> In the book, he argues that what people value is far more fragile and historically contingent than it might first seem. For instance, today it feels like the abolition of slavery was an inevitable part of the arc of history. But Will lays out that the best research on the topic suggests otherwise.</p><p> If moral progress really is so contingent, and bad ideas can persist almost without end, it raises the stakes for moral debate today. If we don't eliminate a bad practice now, it may be with us forever. In today's in-depth conversation, we discuss the possibility of a harmful moral 'lock-in' as well as:</p><p> • How Will was eventually won over to longtermism<br> • The three best lines of argument against longtermism<br> • How to avoid moral fanaticism<br> • Which technologies or events are most likely to have permanent effects<br> • What 'longtermists' do today in practice<br> • How to predict the long-term effect of our actions<br> • Whether the future is likely to be good or bad<br> • Concrete ideas to make the future better<br> • What Will donates his money to personally<br> • Potatoes and megafauna<br> • And plenty more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:36)</li><li>What longtermism actually is (00:02:31)</li><li>The case for longtermism (00:04:30)</li><li>What longtermists are actually doing (00:15:54)</li><li>Will’s personal journey (00:22:15)</li><li>Strongest arguments against longtermism (00:42:28)</li><li>Preventing extinction vs. improving the quality of the future (00:59:29)</li><li>Is humanity likely to converge on doing the same thing regardless? (01:06:58)</li><li>Lock-in scenario vs. long reflection (01:27:11)</li><li>Is the future good in expectation? (01:32:29)</li><li>Can we actually predictably influence the future positively? (01:47:27)</li><li>Tiny probabilities of enormous value (01:53:40)</li><li>Stagnation (02:19:04)</li><li>Concrete suggestions (02:34:27)</li><li>Where Will donates (02:39:40)</li><li>Potatoes and megafauna (02:41:48)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 15 Aug 2022 20:11:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5fe69586/10b1c0e9.mp3" length="83815246" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/n90Fuex6MIVaZ-Vo-Zo_bOh0-ZXWlSXrk4I5Mm9WHZc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODcv/MTY4MzU0NDczMS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10477</itunes:duration>
      <itunes:summary>1. People who exist in the future deserve some degree of moral consideration.  
2. The future could be very big, very long, and/or very good.  
3. We can reasonably hope to influence whether people in the future exist, and how good or bad their lives are.  
4. So trying to make the world better for future generations is a key priority of our time. 

This is the simple four-step argument for 'longtermism' put forward in What We Owe The Future, the latest book from today's guest — University of Oxford philosopher and cofounder of the effective altruism community, Will MacAskill. 

Links to learn more, summary and full transcript. 

From one point of view this idea is common sense. We work on breakthroughs to treat cancer or end use of fossil fuels not just for people alive today, but because we hope such scientific advances will help our children, grandchildren, and great-grandchildren as well. 

Some who take this longtermist idea seriously work to develop broad-spectrum vaccines they hope will safeguard humanity against the sorts of extremely deadly pandemics that could permanently throw civilisation off track — the sort of project few could argue is not worthwhile. 

But Will is upfront that longtermism is also counterintuitive. To start with, he's willing to contemplate timescales far beyond what's typically discussed. 

A natural objection to thinking millions of years ahead is that it's hard enough to take actions that have positive effects that persist for hundreds of years, let alone “indefinitely.” It doesn't matter how important something might be if you can't predictably change it. 

This is one reason, among others, that Will was initially sceptical of longtermism and took years to come around. He preferred to focus on ending poverty and preventable diseases in ways he could directly see were working. 

But over seven years he gradually changed his mind, and in *What We Owe The Future*, Will argues that in fact there are clear ways we might act now that could benefit not just a few but *all* future generations. 

The idea that preventing human extinction would have long-lasting impacts is pretty intuitive. If we entirely disappear, we aren't coming back. 

But the idea that we can shape human values — not just for our age, but for all ages — is a surprising one that Will has come to more recently. 

In the book, he argues that what people value is far more fragile and historically contingent than it might first seem. For instance, today it feels like the abolition of slavery was an inevitable part of the arc of history. But Will lays out that the best research on the topic suggests otherwise. 

If moral progress really is so contingent, and bad ideas can persist almost without end, it raises the stakes for moral debate today. If we don't eliminate a bad practice now, it may be with us forever. In today's in-depth conversation, we discuss the possibility of a harmful moral 'lock-in' as well as: 

• How Will was eventually won over to longtermism 
• The three best lines of argument against longtermism 
• How to avoid moral fanaticism 
• Which technologies or events are most likely to have permanent effects 
• What 'longtermists' do today in practice 
• How to predict the long-term effect of our actions 
• Whether the future is likely to be good or bad 
• Concrete ideas to make the future better 
• What Will donates his money to personally 
• Potatoes and megafauna 
• And plenty more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>1. People who exist in the future deserve some degree of moral consideration.  
2. The future could be very big, very long, and/or very good.  
3. We can reasonably hope to influence whether people in the future exist, and how good or bad their lives ar</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/5fe69586/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/5fe69586/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#135 – Samuel Charap on key lessons from five months of war in Ukraine</title>
      <itunes:title>#135 – Samuel Charap on key lessons from five months of war in Ukraine</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">648406a2-1736-11ed-8dc4-120aafea9c21</guid>
      <link>https://80000hours.org/podcast/episodes/samuel-charap-key-lessons-from-five-months-of-war-in-ukraine/</link>
      <description>
        <![CDATA[<p>After a frenetic level of commentary during February and March, the war in Ukraine has faded into the background of our news coverage. But with the benefit of time we're in a much stronger position to understand what happened, why, whether there are broader lessons to take away, and how the conflict might be ended. And the conflict appears far from over.</p><p> So today, we are returning to speak a <a href="https://80000hours.org/podcast/episodes/samuel-charap-why-putin-invaded-ukraine/">second time</a> with Samuel Charap — one of the US’s foremost experts on Russia’s relationship with former Soviet states, and coauthor of the 2017 book <a href="https://www.amazon.com/Everyone-Loses-Ukraine-Ruinous-Post-Soviet/dp/1138633089"><em>Everyone Loses: The Ukraine Crisis and the Ruinous Contest for Post-Soviet Eurasia.</em></a></p><p> <a href="https://80k.link/SC2"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> As Sam lays out, Russia controls much of Ukraine's east and south, and seems to be preparing to politically incorporate that territory into Russia itself later in the year. At the same time, Ukraine is gearing up for a counteroffensive before defensive positions become dug in over winter.</p><p> Each day the war continues it takes a toll on ordinary Ukrainians, contributes to a global food shortage, and leaves the US and Russia unable to coordinate on any other issues and at an elevated risk of direct conflict.</p><p> In today's brisk conversation, Rob and Sam cover the following topics:</p><p> • Current territorial control and the level of attrition within Russia’s and Ukraine's military forces.<br> • Russia's current goals.<br> • Whether Sam's views have changed since March on topics like: Putin's motivations, the wisdom of Ukraine's strategy, the likely impact of Western sanctions, and the risks from Finland and Sweden joining NATO before the war ends.<br> • Why so many people incorrectly expected Russia to fully mobilise for war or persist with their original approach to the invasion.<br> • Whether there's anything to learn from many of our worst fears -- such as the use of bioweapons on civilians -- not coming to pass.<br> • What can be done to ensure some nuclear arms control agreement between the US and Russia remains in place after 2026 (when New START expires).<br> • Why Sam considers a settlement proposal put forward by Ukraine in late March to be the most plausible way to end the war and ensure stability — though it's still a long shot.</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:31)</li><li>The state of play in Ukraine (00:03:05)</li><li>How things have changed since March (00:12:59)</li><li>Has Russia learned from its mistakes? (00:23:40)</li><li>Broader lessons (00:28:44)</li><li>A possible way out (00:37:15)</li></ul><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell and Ryan Kessler<br> Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>After a frenetic level of commentary during February and March, the war in Ukraine has faded into the background of our news coverage. But with the benefit of time we're in a much stronger position to understand what happened, why, whether there are broader lessons to take away, and how the conflict might be ended. And the conflict appears far from over.</p><p> So today, we are returning to speak a <a href="https://80000hours.org/podcast/episodes/samuel-charap-why-putin-invaded-ukraine/">second time</a> with Samuel Charap — one of the US’s foremost experts on Russia’s relationship with former Soviet states, and coauthor of the 2017 book <a href="https://www.amazon.com/Everyone-Loses-Ukraine-Ruinous-Post-Soviet/dp/1138633089"><em>Everyone Loses: The Ukraine Crisis and the Ruinous Contest for Post-Soviet Eurasia.</em></a></p><p> <a href="https://80k.link/SC2"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> As Sam lays out, Russia controls much of Ukraine's east and south, and seems to be preparing to politically incorporate that territory into Russia itself later in the year. At the same time, Ukraine is gearing up for a counteroffensive before defensive positions become dug in over winter.</p><p> Each day the war continues it takes a toll on ordinary Ukrainians, contributes to a global food shortage, and leaves the US and Russia unable to coordinate on any other issues and at an elevated risk of direct conflict.</p><p> In today's brisk conversation, Rob and Sam cover the following topics:</p><p> • Current territorial control and the level of attrition within Russia’s and Ukraine's military forces.<br> • Russia's current goals.<br> • Whether Sam's views have changed since March on topics like: Putin's motivations, the wisdom of Ukraine's strategy, the likely impact of Western sanctions, and the risks from Finland and Sweden joining NATO before the war ends.<br> • Why so many people incorrectly expected Russia to fully mobilise for war or persist with their original approach to the invasion.<br> • Whether there's anything to learn from many of our worst fears -- such as the use of bioweapons on civilians -- not coming to pass.<br> • What can be done to ensure some nuclear arms control agreement between the US and Russia remains in place after 2026 (when New START expires).<br> • Why Sam considers a settlement proposal put forward by Ukraine in late March to be the most plausible way to end the war and ensure stability — though it's still a long shot.</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:31)</li><li>The state of play in Ukraine (00:03:05)</li><li>How things have changed since March (00:12:59)</li><li>Has Russia learned from its mistakes? (00:23:40)</li><li>Broader lessons (00:28:44)</li><li>A possible way out (00:37:15)</li></ul><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell and Ryan Kessler<br> Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 08 Aug 2022 16:41:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/208e9d92/d4cef501.mp3" length="26293856" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/x2f7eUWTEGKoeJw8EuVr2zZ82VO9OIk5NIF_mevIlYA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODYv/MTY4MzU0NDczMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3287</itunes:duration>
      <itunes:summary>After a frenetic level of commentary during February and March, the war in Ukraine has faded into the background of our news coverage. But with the benefit of time we're in a much stronger position to understand what happened, why, whether there are broader lessons to take away, and how the conflict might be ended. And the conflict appears far from over. 

So today, we are returning to speak a second time with Samuel Charap — one of the US’s foremost experts on Russia’s relationship with former Soviet states, and coauthor of the 2017 book Everyone Loses: The Ukraine Crisis and the Ruinous Contest for Post-Soviet Eurasia. 

Links to learn more, summary and full transcript. 

As Sam lays out, Russia controls much of Ukraine's east and south, and seems to be preparing to politically incorporate that territory into Russia itself later in the year. At the same time, Ukraine is gearing up for a counteroffensive before defensive positions become dug in over winter. 

Each day the war continues it takes a toll on ordinary Ukrainians, contributes to a global food shortage, and leaves the US and Russia unable to coordinate on any other issues and at an elevated risk of direct conflict. 

In today's brisk conversation, Rob and Sam cover the following topics: 

• Current territorial control and the level of attrition within Russia’s and Ukraine's military forces. 
• Russia's current goals. 
• Whether Sam's views have changed since March on topics like: Putin's motivations, the wisdom of Ukraine's strategy, the likely impact of Western sanctions, and the risks from Finland and Sweden joining NATO before the war ends. 
• Why so many people incorrectly expected Russia to fully mobilise for war or persist with their original approach to the invasion. 
• Whether there's anything to learn from many of our worst fears -- such as the use of bioweapons on civilians -- not coming to pass. 
• What can be done to ensure some nuclear arms control agreement between the US and Russia remains in place after 2026 (when New START expires). 
• Why Sam considers a settlement proposal put forward by Ukraine in late March to be the most plausible way to end the war and ensure stability — though it's still a long shot. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>After a frenetic level of commentary during February and March, the war in Ukraine has faded into the background of our news coverage. But with the benefit of time we're in a much stronger position to understand what happened, why, whether there are broad</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/208e9d92/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/208e9d92/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#134 – Ian Morris on what big-picture history teaches us</title>
      <itunes:title>#134 – Ian Morris on what big-picture history teaches us</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">dbff9092-0986-11ed-8c28-12331509a6d1</guid>
      <link>https://80000hours.org/podcast/episodes/ian-morris-big-picture-history/</link>
      <description>
        <![CDATA[<p>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monarchs.</p><p>Wind back 10,000 years and things look very different again. Most hunter-gatherer groups thought men who got too big for their britches needed to be put in their place rather than obeyed, and lifelong monogamy could hardly be expected of men or women.</p><p>Why such big systematic changes — and why these changes specifically?</p><p>That's the question best-selling historian Ian Morris takes up in his book, <em>Foragers, Farmers, and Fossil Fuels: How Human Values Evolve</em>. Ian has spent his academic life studying long-term history, trying to explain the big-picture changes that play out over hundreds or thousands of years.</p><p> <a href="https://80k.link/IM"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> There are a number of possible explanations one could offer for the wide-ranging shifts in opinion on the 'right' way to live. Maybe the natural sciences progressed and people realised their previous ideas were mistaken? Perhaps a few persuasive advocates turned the course of history with their revolutionary arguments? Maybe everyone just got nicer?</p><p> In <em>Foragers, Farmers and Fossil Fuels</em> Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels.</p><p> On this theory, it's technology that drives moral values much more than moral philosophy. Individuals can try to persist with deeply held values that limit economic growth, but they risk being rendered irrelevant as more productive peers in their own society accrue wealth and power. And societies that fail to move with the times risk being conquered by more pragmatic neighbours that adapt to new technologies and grow in population and military strength.</p><p> There are many objections one could raise to this theory, many of which we put to Ian in this interview. But the question is a highly consequential one: if we want to guess what goals our descendants will pursue hundreds of years from now, it would be helpful to have a theory for why our ancestors mostly thought one thing, while we mostly think another.</p><p> Big though it is, the driver of human values is only one of several major questions Ian has tackled through his career.</p><p> In today's episode, we discuss all of Ian's major books, taking on topics such as:<br> • Why the Industrial Revolution happened in England rather than China<br> • Whether or not wars can lead to less violence<br> • Whether the evidence base in history — from document archives to archaeology — is strong enough to persuasively answer any of these questions<br> • Why Ian thinks the way we live in the 21st century is probably a short-lived aberration • Whether the grand sweep of history is driven more by “very important people” or “vast impersonal forces”<br> • Why Chinese ships never crossed the Pacific or rounded the southern tip of Africa<br> • In what sense Ian thinks Brexit was “10,000 years in the making”<br> • The most common misconceptions about macrohistory</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:51)</li><li>Geography is Destiny (00:02:59)</li><li>Why the West Rules—For Now (00:11:25)</li><li>War! What is it Good For? (00:27:40)</li><li>Expectations for the future (00:39:43)</li><li>Foragers, Farmers, and Fossil Fuels (00:53:15)</li><li>Historical methodology (01:02:35)</li><li>Falsifiable alternative theories (01:15:20)</li><li>Archaeology (01:22:18)</li><li>Energy extraction technology as a key driver of human values (01:37:04)</li><li>Allowing people to debate about values (01:59:38)</li><li>Can productive wars still occur? (02:12:49)</li><li>Where is history contingent and where isn't it? (02:29:45)</li><li>How Ian thinks about the future (03:12:54)</li><li>Macrohistory myths (03:29:12)</li><li>Ian’s favourite archaeology memory (03:32:40)</li><li>The most unfair criticism Ian’s ever received (03:34:39)</li><li>Rob’s outro (03:39:16)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monarchs.</p><p>Wind back 10,000 years and things look very different again. Most hunter-gatherer groups thought men who got too big for their britches needed to be put in their place rather than obeyed, and lifelong monogamy could hardly be expected of men or women.</p><p>Why such big systematic changes — and why these changes specifically?</p><p>That's the question best-selling historian Ian Morris takes up in his book, <em>Foragers, Farmers, and Fossil Fuels: How Human Values Evolve</em>. Ian has spent his academic life studying long-term history, trying to explain the big-picture changes that play out over hundreds or thousands of years.</p><p> <a href="https://80k.link/IM"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> There are a number of possible explanations one could offer for the wide-ranging shifts in opinion on the 'right' way to live. Maybe the natural sciences progressed and people realised their previous ideas were mistaken? Perhaps a few persuasive advocates turned the course of history with their revolutionary arguments? Maybe everyone just got nicer?</p><p> In <em>Foragers, Farmers and Fossil Fuels</em> Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels.</p><p> On this theory, it's technology that drives moral values much more than moral philosophy. Individuals can try to persist with deeply held values that limit economic growth, but they risk being rendered irrelevant as more productive peers in their own society accrue wealth and power. And societies that fail to move with the times risk being conquered by more pragmatic neighbours that adapt to new technologies and grow in population and military strength.</p><p> There are many objections one could raise to this theory, many of which we put to Ian in this interview. But the question is a highly consequential one: if we want to guess what goals our descendants will pursue hundreds of years from now, it would be helpful to have a theory for why our ancestors mostly thought one thing, while we mostly think another.</p><p> Big though it is, the driver of human values is only one of several major questions Ian has tackled through his career.</p><p> In today's episode, we discuss all of Ian's major books, taking on topics such as:<br> • Why the Industrial Revolution happened in England rather than China<br> • Whether or not wars can lead to less violence<br> • Whether the evidence base in history — from document archives to archaeology — is strong enough to persuasively answer any of these questions<br> • Why Ian thinks the way we live in the 21st century is probably a short-lived aberration • Whether the grand sweep of history is driven more by “very important people” or “vast impersonal forces”<br> • Why Chinese ships never crossed the Pacific or rounded the southern tip of Africa<br> • In what sense Ian thinks Brexit was “10,000 years in the making”<br> • The most common misconceptions about macrohistory</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:51)</li><li>Geography is Destiny (00:02:59)</li><li>Why the West Rules—For Now (00:11:25)</li><li>War! What is it Good For? (00:27:40)</li><li>Expectations for the future (00:39:43)</li><li>Foragers, Farmers, and Fossil Fuels (00:53:15)</li><li>Historical methodology (01:02:35)</li><li>Falsifiable alternative theories (01:15:20)</li><li>Archaeology (01:22:18)</li><li>Energy extraction technology as a key driver of human values (01:37:04)</li><li>Allowing people to debate about values (01:59:38)</li><li>Can productive wars still occur? (02:12:49)</li><li>Where is history contingent and where isn't it? (02:29:45)</li><li>How Ian thinks about the future (03:12:54)</li><li>Macrohistory myths (03:29:12)</li><li>Ian’s favourite archaeology memory (03:32:40)</li><li>The most unfair criticism Ian’s ever received (03:34:39)</li><li>Rob’s outro (03:39:16)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 22 Jul 2022 09:25:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/786e569d/315c8bfa.mp3" length="106134315" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/iP_9hJiGcXnlYK1yAwml-i-Vi02Dt2Stkiikz1-q3sM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODUv/MTY4MzU0NDcyOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13267</itunes:duration>
      <itunes:summary>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monarchs. 

Wind back 10,000 years and things look very different again. Most hunter-gatherer groups thought men who got too big for their britches needed to be put in their place rather than obeyed, and lifelong monogamy could hardly be expected of men or women. 

Why such big systematic changes — and why these changes specifically? 

That's the question best-selling historian Ian Morris takes up in his book, Foragers, Farmers, and Fossil Fuels: How Human Values Evolve. Ian has spent his academic life studying long-term history, trying to explain the big-picture changes that play out over hundreds or thousands of years. 

Links to learn more, summary and full transcript. 

There are a number of possible explanations one could offer for the wide-ranging shifts in opinion on the 'right' way to live. Maybe the natural sciences progressed and people realised their previous ideas were mistaken? Perhaps a few persuasive advocates turned the course of history with their revolutionary arguments? Maybe everyone just got nicer? 

In Foragers, Farmers and Fossil Fuels Ian presents a provocative alternative: human culture gradually evolves towards whatever system of organisation allows a society to harvest the most energy, and we then conclude that system is the most virtuous one. Egalitarian values helped hunter-gatherers hunt and gather effectively. Once farming was developed, hierarchy proved to be the social structure that produced the most grain (and best repelled nomadic raiders). And in the modern era, democracy and individuality have proven to be more productive ways to collect and exploit fossil fuels. 

On this theory, it's technology that drives moral values much more than moral philosophy. Individuals can try to persist with deeply held values that limit economic growth, but they risk being rendered irrelevant as more productive peers in their own society accrue wealth and power. And societies that fail to move with the times risk being conquered by more pragmatic neighbours that adapt to new technologies and grow in population and military strength. 

There are many objections one could raise to this theory, many of which we put to Ian in this interview. But the question is a highly consequential one: if we want to guess what goals our descendants will pursue hundreds of years from now, it would be helpful to have a theory for why our ancestors mostly thought one thing, while we mostly think another. 

Big though it is, the driver of human values is only one of several major questions Ian has tackled through his career. 

In today's episode, we discuss all of Ian's major books, taking on topics such as: 

• Why the Industrial Revolution happened in England rather than China 
• Whether or not wars can lead to less violence 
• Whether the evidence base in history — from document archives to archaeology — is strong enough to persuasively answer any of these questions 
• Why Ian thinks the way we live in the 21st century is probably a short-lived aberration
• Whether the grand sweep of history is driven more by “very important people” or “vast impersonal forces” 
• Why Chinese ships never crossed the Pacific or rounded the southern tip of Africa 
• In what sense Ian thinks Brexit was “10,000 years in the making” 
• The most common misconceptions about macrohistory 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Wind back 1,000 years and the moral landscape looks very different to today. Most farming societies thought slavery was natural and unobjectionable, premarital sex was an abomination, women should obey their husbands, and commoners should obey their monar</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/786e569d/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/786e569d/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#133 – Max Tegmark on how a 'put-up-or-shut-up' resolution led him to work on AI and algorithmic news selection</title>
      <itunes:title>#133 – Max Tegmark on how a 'put-up-or-shut-up' resolution led him to work on AI and algorithmic news selection</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e70faf14-f93f-11ec-a381-0ad8cdb00cfd</guid>
      <link>https://80000hours.org/podcast/episodes/max-tegmark-ai-and-algorithmic-news-selection/</link>
      <description>
        <![CDATA[<p>On January 1, 2015, physicist Max Tegmark gave up something most of us love to do: complain about things without ever trying to fix them.</p><p> That “put up or shut up” New Year’s resolution led to the first Puerto Rico conference and Open Letter on Artificial Intelligence — milestones for researchers taking the safe development of highly-capable AI systems seriously.</p><p> <a href="https://80k.link/MT"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Max's primary work has been cosmology research at MIT, but his energetic and freewheeling nature has led him into so many other projects that you would be forgiven for forgetting it. In the 2010s he wrote two best-selling books, <em>Our Mathematical Universe: My Quest for the Ultimate Nature of Reality</em>, and <em>Life 3.0: Being Human in the Age of Artificial Intelligence</em>, and in 2014 founded a non-profit, the Future of Life Institute, which works to reduce all sorts of threats to humanity's future including nuclear war, synthetic biology, and AI.</p><p> Max has complained about many other things over the years, from killer robots to the impact of social media algorithms on the news we consume. True to his 'put up or shut up' resolution, he and his team went on to produce a video on so-called ‘Slaughterbots’ which attracted millions of views, and develop a website called 'Improve The News' to help readers separate facts from spin.</p><p> But given the stunning recent advances in capabilities — from OpenAI’s DALL-E to DeepMind’s Gato — AI itself remains top of his mind.</p><p> You can now give an AI system like GPT-3 the text: "I'm going to go to this mountain with the faces on it. What is the capital of the state to the east of the state that that's in?" And it gives the correct answer (Saint Paul, Minnesota) — something most AI researchers would have said was impossible without fundamental breakthroughs just seven years ago.</p><p> So back at MIT, he now leads a research group dedicated to what he calls “intelligible intelligence.” At the moment, AI systems are basically giant black boxes that magically do wildly impressive things. But for us to trust these systems, we need to understand them.</p><p> He says that training a black box that does something smart needs to just be stage one in a bigger process. Stage two is: “How do we get the knowledge out and put it in a safer system?”</p><p> Today’s conversation starts off giving a broad overview of the key questions about artificial intelligence: What's the potential? What are the threats? How might this story play out? What should we be doing to prepare?</p><p> Rob and Max then move on to recent advances in capabilities and alignment, the mood we should have, and possible ways we might misunderstand the problem.</p><p> They then spend roughly the last third talking about Max's current big passion: improving the news we consume — where Rob has a few reservations.</p><p> They also cover:</p><p> • Whether we could understand what superintelligent systems were doing<br> • The value of encouraging people to think about the positive future they want<br> • How to give machines goals<br> • Whether ‘Big Tech’ is following the lead of ‘Big Tobacco’<br> • Whether we’re sleepwalking into disaster<br> • Whether people actually just want their biases confirmed<br> • Why Max is worried about government-backed fact-checking<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:19)</li><li>How Max prioritises (00:12:33)</li><li>Intro to AI risk (00:15:47)</li><li>Superintelligence (00:35:56)</li><li>Imagining a wide range of possible futures (00:47:45)</li><li>Recent advances in capabilities and alignment (00:57:37)</li><li>How to give machines goals (01:13:13)</li><li>Regulatory capture (01:21:03)</li><li>How humanity fails to fulfil its potential (01:39:45)</li><li>Are we being hacked? (01:51:01)</li><li>Improving the news (02:05:31)</li><li>Do people actually just want their biases confirmed? (02:16:15)</li><li>Government-backed fact-checking (02:37:00)</li><li>Would a superintelligence seem like magic? (02:49:50)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>On January 1, 2015, physicist Max Tegmark gave up something most of us love to do: complain about things without ever trying to fix them.</p><p> That “put up or shut up” New Year’s resolution led to the first Puerto Rico conference and Open Letter on Artificial Intelligence — milestones for researchers taking the safe development of highly-capable AI systems seriously.</p><p> <a href="https://80k.link/MT"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Max's primary work has been cosmology research at MIT, but his energetic and freewheeling nature has led him into so many other projects that you would be forgiven for forgetting it. In the 2010s he wrote two best-selling books, <em>Our Mathematical Universe: My Quest for the Ultimate Nature of Reality</em>, and <em>Life 3.0: Being Human in the Age of Artificial Intelligence</em>, and in 2014 founded a non-profit, the Future of Life Institute, which works to reduce all sorts of threats to humanity's future including nuclear war, synthetic biology, and AI.</p><p> Max has complained about many other things over the years, from killer robots to the impact of social media algorithms on the news we consume. True to his 'put up or shut up' resolution, he and his team went on to produce a video on so-called ‘Slaughterbots’ which attracted millions of views, and develop a website called 'Improve The News' to help readers separate facts from spin.</p><p> But given the stunning recent advances in capabilities — from OpenAI’s DALL-E to DeepMind’s Gato — AI itself remains top of his mind.</p><p> You can now give an AI system like GPT-3 the text: "I'm going to go to this mountain with the faces on it. What is the capital of the state to the east of the state that that's in?" And it gives the correct answer (Saint Paul, Minnesota) — something most AI researchers would have said was impossible without fundamental breakthroughs just seven years ago.</p><p> So back at MIT, he now leads a research group dedicated to what he calls “intelligible intelligence.” At the moment, AI systems are basically giant black boxes that magically do wildly impressive things. But for us to trust these systems, we need to understand them.</p><p> He says that training a black box that does something smart needs to just be stage one in a bigger process. Stage two is: “How do we get the knowledge out and put it in a safer system?”</p><p> Today’s conversation starts off giving a broad overview of the key questions about artificial intelligence: What's the potential? What are the threats? How might this story play out? What should we be doing to prepare?</p><p> Rob and Max then move on to recent advances in capabilities and alignment, the mood we should have, and possible ways we might misunderstand the problem.</p><p> They then spend roughly the last third talking about Max's current big passion: improving the news we consume — where Rob has a few reservations.</p><p> They also cover:</p><p> • Whether we could understand what superintelligent systems were doing<br> • The value of encouraging people to think about the positive future they want<br> • How to give machines goals<br> • Whether ‘Big Tech’ is following the lead of ‘Big Tobacco’<br> • Whether we’re sleepwalking into disaster<br> • Whether people actually just want their biases confirmed<br> • Why Max is worried about government-backed fact-checking<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:19)</li><li>How Max prioritises (00:12:33)</li><li>Intro to AI risk (00:15:47)</li><li>Superintelligence (00:35:56)</li><li>Imagining a wide range of possible futures (00:47:45)</li><li>Recent advances in capabilities and alignment (00:57:37)</li><li>How to give machines goals (01:13:13)</li><li>Regulatory capture (01:21:03)</li><li>How humanity fails to fulfil its potential (01:39:45)</li><li>Are we being hacked? (01:51:01)</li><li>Improving the news (02:05:31)</li><li>Do people actually just want their biases confirmed? (02:16:15)</li><li>Government-backed fact-checking (02:37:00)</li><li>Would a superintelligence seem like magic? (02:49:50)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 01 Jul 2022 15:36:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d49b916a/c1e3eee6.mp3" length="85368160" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HCsSPs4NT_K_xsMghDrEpAXmgHj24KN88mJ788QnUdY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODQv/MTY4MzU0NDcyNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10671</itunes:duration>
      <itunes:summary>On January 1, 2015, physicist Max Tegmark gave up something most of us love to do: complain about things without ever trying to fix them. 

That “put up or shut up” New Year’s resolution led to the first Puerto Rico conference and Open Letter on Artificial Intelligence — milestones for researchers taking the safe development of highly-capable AI systems seriously. 

Links to learn more, summary and full transcript. 

Max's primary work has been cosmology research at MIT, but his energetic and freewheeling nature has led him into so many other projects that you would be forgiven for forgetting it. In the 2010s he wrote two best-selling books, Our Mathematical Universe: My Quest for the Ultimate Nature of Reality, and Life 3.0: Being Human in the Age of Artificial Intelligence, and in 2014 founded a non-profit, the Future of Life Institute, which works to reduce all sorts of threats to humanity's future including nuclear war, synthetic biology, and AI. 

Max has complained about many other things over the years, from killer robots to the impact of social media algorithms on the news we consume. True to his 'put up or shut up' resolution, he and his team went on to produce a video on so-called ‘Slaughterbots’ which attracted millions of views, and develop a website called 'Improve The News' to help readers separate facts from spin. 

But given the stunning recent advances in capabilities — from OpenAI’s DALL-E to DeepMind’s Gato — AI itself remains top of his mind. 

You can now give an AI system like GPT-3 the text: "I'm going to go to this mountain with the faces on it. What is the capital of the state to the east of the state that that's in?" And it gives the correct answer (Saint Paul, Minnesota) — something most AI researchers would have said was impossible without fundamental breakthroughs just seven years ago. 

So back at MIT, he now leads a research group dedicated to what he calls “intelligible intelligence.” At the moment, AI systems are basically giant black boxes that magically do wildly impressive things. But for us to trust these systems, we need to understand them. 

He says that training a black box that does something smart needs to just be stage one in a bigger process. Stage two is: “How do we get the knowledge out and put it in a safer system?” 

Today’s conversation starts off giving a broad overview of the key questions about artificial intelligence: What's the potential? What are the threats? How might this story play out? What should we be doing to prepare? 

Rob and Max then move on to recent advances in capabilities and alignment, the mood we should have, and possible ways we might misunderstand the problem. 

They then spend roughly the last third talking about Max's current big passion: improving the news we consume — where Rob has a few reservations. 

They also cover: 

• Whether we could understand what superintelligent systems were doing 
• The value of encouraging people to think about the positive future they want 
• How to give machines goals 
• Whether ‘Big Tech’ is following the lead of ‘Big Tobacco’ 
• Whether we’re sleepwalking into disaster 
• Whether people actually just want their biases confirmed 
• Why Max is worried about government-backed fact-checking 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>On January 1, 2015, physicist Max Tegmark gave up something most of us love to do: complain about things without ever trying to fix them. 

That “put up or shut up” New Year’s resolution led to the first Puerto Rico conference and Open Letter on Artific</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/d49b916a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/d49b916a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#132 – Nova DasSarma on why information security may be critical to the safe development of AI systems</title>
      <itunes:title>#132 – Nova DasSarma on why information security may be critical to the safe development of AI systems</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">239ec25e-ec24-11ec-b3b1-12ce5978a5bd</guid>
      <link>https://share.transistor.fm/s/caa25b05</link>
      <description>
        <![CDATA[If a business has spent $100 million developing a product, it's a fair bet that they don't want it stolen in two seconds and uploaded to the web where anyone can use it for free.<p> 

This problem exists in extreme form for AI companies. These days, the electricity and equipment required to train cutting-edge machine learning models that generate uncanny human text and images can cost tens or hundreds of millions of dollars. But once trained, such models may be only a few gigabytes in size and run just fine on ordinary laptops.</p><p> 

Today's guest, the computer scientist and polymath Nova DasSarma, works on computer and information security for the AI company Anthropic. One of her jobs is to stop hackers exfiltrating Anthropic's incredibly expensive intellectual property, as recently happened to Nvidia. As she explains, given models’ small size, the need to store such models on internet-connected servers, and the poor state of computer security in general, this is a serious challenge.</p><p> 

<a href="https://80k.link/NDS"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

The worries aren't purely commercial though. This problem looms especially large for the growing number of people who expect that in coming decades we'll develop so-called artificial 'general' intelligence systems that can learn and apply a wide range of skills all at once, and thereby have a transformative effect on society.</p><p> 

If aligned with the goals of their owners, such general AI models could operate like a team of super-skilled assistants, going out and doing whatever wonderful (or malicious) things are asked of them. This might represent a huge leap forward for humanity, though the transition to a very different new economy and power structure would have to be handled delicately.</p><p> 

If unaligned with the goals of their owners or humanity as a whole, such broadly capable models would naturally 'go rogue,' breaking their way into additional computer systems to grab more computing power — all the better to pursue their goals and make sure they can't be shut off.</p><p> 

As Nova explains, in either case, we don't want such models disseminated all over the world before we've confirmed they are deeply safe and law-abiding, and have figured out how to integrate them peacefully into society. In the first scenario, premature mass deployment would be risky and destabilising. In the second scenario, it could be catastrophic -- perhaps even leading to human extinction if such general AI systems turn out to be able to self-improve rapidly rather than slowly.</p><p> 

If highly capable general AI systems are coming in the next 10 or 20 years, Nova may be flying below the radar with one of the most important jobs in the world.</p><p> 

We'll soon need the ability to 'sandbox' (i.e. contain) models with a wide range of superhuman capabilities, including the ability to learn new skills, for a period of careful testing and limited deployment — preventing the model from breaking out, and criminals from breaking in. Nova and her colleagues are trying to figure out how to do this, but as this episode reveals, even the state of the art is nowhere near good enough.</p><p> 

In today's conversation, Rob and Nova cover:</p><p> 

• How good or bad is information security today<br> 
• The most secure computer systems that exist<br> 
• How to design an AI training compute centre for maximum efficiency<br> 
• Whether 'formal verification' can help us design trustworthy systems<br> 
• How wide the gap is between AI capabilities and AI safety<br> 
• How to disincentivise hackers<br> 
• What should listeners do to strengthen their own security practices<br> 
• And much more.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell and Beppe Rådvik<br>
Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[If a business has spent $100 million developing a product, it's a fair bet that they don't want it stolen in two seconds and uploaded to the web where anyone can use it for free.<p> 

This problem exists in extreme form for AI companies. These days, the electricity and equipment required to train cutting-edge machine learning models that generate uncanny human text and images can cost tens or hundreds of millions of dollars. But once trained, such models may be only a few gigabytes in size and run just fine on ordinary laptops.</p><p> 

Today's guest, the computer scientist and polymath Nova DasSarma, works on computer and information security for the AI company Anthropic. One of her jobs is to stop hackers exfiltrating Anthropic's incredibly expensive intellectual property, as recently happened to Nvidia. As she explains, given models’ small size, the need to store such models on internet-connected servers, and the poor state of computer security in general, this is a serious challenge.</p><p> 

<a href="https://80k.link/NDS"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

The worries aren't purely commercial though. This problem looms especially large for the growing number of people who expect that in coming decades we'll develop so-called artificial 'general' intelligence systems that can learn and apply a wide range of skills all at once, and thereby have a transformative effect on society.</p><p> 

If aligned with the goals of their owners, such general AI models could operate like a team of super-skilled assistants, going out and doing whatever wonderful (or malicious) things are asked of them. This might represent a huge leap forward for humanity, though the transition to a very different new economy and power structure would have to be handled delicately.</p><p> 

If unaligned with the goals of their owners or humanity as a whole, such broadly capable models would naturally 'go rogue,' breaking their way into additional computer systems to grab more computing power — all the better to pursue their goals and make sure they can't be shut off.</p><p> 

As Nova explains, in either case, we don't want such models disseminated all over the world before we've confirmed they are deeply safe and law-abiding, and have figured out how to integrate them peacefully into society. In the first scenario, premature mass deployment would be risky and destabilising. In the second scenario, it could be catastrophic -- perhaps even leading to human extinction if such general AI systems turn out to be able to self-improve rapidly rather than slowly.</p><p> 

If highly capable general AI systems are coming in the next 10 or 20 years, Nova may be flying below the radar with one of the most important jobs in the world.</p><p> 

We'll soon need the ability to 'sandbox' (i.e. contain) models with a wide range of superhuman capabilities, including the ability to learn new skills, for a period of careful testing and limited deployment — preventing the model from breaking out, and criminals from breaking in. Nova and her colleagues are trying to figure out how to do this, but as this episode reveals, even the state of the art is nowhere near good enough.</p><p> 

In today's conversation, Rob and Nova cover:</p><p> 

• How good or bad is information security today<br> 
• The most secure computer systems that exist<br> 
• How to design an AI training compute centre for maximum efficiency<br> 
• Whether 'formal verification' can help us design trustworthy systems<br> 
• How wide the gap is between AI capabilities and AI safety<br> 
• How to disincentivise hackers<br> 
• What should listeners do to strengthen their own security practices<br> 
• And much more.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell and Beppe Rådvik<br>
Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 14 Jun 2022 21:47:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/caa25b05/87583931.mp3" length="77977290" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/xhFSybZOE1GFkbCkFpffCz2mcdI3bz8ph5wgf8x3DB8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODMv/MTY4MzU0NDcyNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9747</itunes:duration>
      <itunes:summary>If a business has spent $100 million developing a product, it's a fair bet that they don't want it stolen in two seconds and uploaded to the web where anyone can use it for free. 

This problem exists in extreme form for AI companies. These days, the electricity and equipment required to train cutting-edge machine learning models that generate uncanny human text and images can cost tens or hundreds of millions of dollars. But once trained, such models may be only a few gigabytes in size and run just fine on ordinary laptops. 

Today's guest, the computer scientist and polymath Nova DasSarma, works on computer and information security for the AI company Anthropic. One of her jobs is to stop hackers exfiltrating Anthropic's incredibly expensive intellectual property, as recently happened to Nvidia. As she explains, given models’ small size, the need to store such models on internet-connected servers, and the poor state of computer security in general, this is a serious challenge. 

Links to learn more, summary and full transcript. 

The worries aren't purely commercial though. This problem looms especially large for the growing number of people who expect that in coming decades we'll develop so-called artificial 'general' intelligence systems that can learn and apply a wide range of skills all at once, and thereby have a transformative effect on society. 

If aligned with the goals of their owners, such general AI models could operate like a team of super-skilled assistants, going out and doing whatever wonderful (or malicious) things are asked of them. This might represent a huge leap forward for humanity, though the transition to a very different new economy and power structure would have to be handled delicately. 

If unaligned with the goals of their owners or humanity as a whole, such broadly capable models would naturally 'go rogue,' breaking their way into additional computer systems to grab more computing power — all the better to pursue their goals and make sure they can't be shut off. 

As Nova explains, in either case, we don't want such models disseminated all over the world before we've confirmed they are deeply safe and law-abiding, and have figured out how to integrate them peacefully into society. In the first scenario, premature mass deployment would be risky and destabilising. In the second scenario, it could be catastrophic -- perhaps even leading to human extinction if such general AI systems turn out to be able to self-improve rapidly rather than slowly. 

If highly capable general AI systems are coming in the next 10 or 20 years, Nova may be flying below the radar with one of the most important jobs in the world. 

We'll soon need the ability to 'sandbox' (i.e. contain) models with a wide range of superhuman capabilities, including the ability to learn new skills, for a period of careful testing and limited deployment — preventing the model from breaking out, and criminals from breaking in. Nova and her colleagues are trying to figure out how to do this, but as this episode reveals, even the state of the art is nowhere near good enough. 

In today's conversation, Rob and Nova cover: 

• How good or bad is information security today 
• The most secure computer systems that exist 
• How to design an AI training compute centre for maximum efficiency 
• Whether 'formal verification' can help us design trustworthy systems 
• How wide the gap is between AI capabilities and AI safety 
• How to disincentivise hackers 
• What should listeners do to strengthen their own security practices 
• And much more. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Beppe Rådvik
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If a business has spent $100 million developing a product, it's a fair bet that they don't want it stolen in two seconds and uploaded to the web where anyone can use it for free. 

This problem exists in extreme form for AI companies. These days, the elec</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/caa25b05/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#131 – Lewis Dartnell on getting humanity to bounce back faster in a post-apocalyptic world</title>
      <itunes:title>#131 – Lewis Dartnell on getting humanity to bounce back faster in a post-apocalyptic world</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ef196530-e34e-11ec-8b2b-0aa00f311dd1</guid>
      <link>https://80000hours.org/podcast/episodes/lewis-dartnell-getting-humanity-to-bounce-back-faster/</link>
      <description>
        <![CDATA[<p>“We’re leaving these 16 contestants on an island with nothing but what they can scavenge from an abandoned factory and apartment block. Over the next 365 days, they’ll try to rebuild as much of civilisation as they can — from glass, to lenses, to microscopes. This is: The Knowledge!”</p><p>If you were a contestant on such a TV show, you'd love to have a guide to how basic things you currently take for granted are done — how to grow potatoes, fire bricks, turn wood to charcoal, find acids and alkalis, and so on.</p><p>Today’s guest Lewis Dartnell has gone as far compiling this information as anyone has with his bestselling book <a href="http://the-knowledge.org/en-gb/the-book/"><strong><em>The Knowledge: How to Rebuild Civilization in the Aftermath of a Cataclysm.</em></strong></a></p><p> <a href="https://80k.link/LD"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> But in the aftermath of a nuclear war or incredibly deadly pandemic that kills most people, many of the ways we do things today will be impossible — and even some of the things people did in the past, like collect coal from the surface of the Earth, will be impossible the second time around.</p><p> As Lewis points out, there’s “no point telling this band of survivors how to make something ultra-efficient or ultra-useful or ultra-capable if it's just too damned complicated to build in the first place. You have to start small and then level up, pull yourself up by your own bootstraps.”</p><p> So it might sound good to tell people to build solar panels — they’re a wonderful way of generating electricity. But the photovoltaic cells we use today need pure silicon, and nanoscale manufacturing — essentially the same technology as microchips used in a computer — so actually making solar panels would be incredibly difficult.</p><p> Instead, you’d want to tell our group of budding engineers to use more appropriate technologies like solar concentrators that use nothing more than mirrors — which turn out to be relatively easy to make.</p><p> A disaster that unravels the complex way we produce goods in the modern world is all too possible. Which raises the question: why not set dozens of people to plan out exactly what any survivors really ought to do if they need to support themselves and rebuild civilisation? Such a guide could then be translated and distributed all around the world.</p><p> The goal would be to provide the best information to speed up each of the many steps that would take survivors from rubbing sticks together in the wilderness to adjusting a thermostat in their comfy apartments.</p><p> This is clearly not a trivial task. Lewis's own book (at 300 pages) only scratched the surface of the most important knowledge humanity has accumulated, relegating all of mathematics to a single footnote.</p><p> And the ideal guide would offer pretty different advice depending on the scenario. Are survivors dealing with a radioactive ice age following a nuclear war? Or is it an eerily intact but near-empty post-pandemic world with mountains of goods to scavenge from the husks of cities?</p><p> As a brand-new parent, Lewis couldn’t do one of our classic three- or four-hour episodes — so this is an unusually snappy one-hour interview, where Rob and Lewis are joined by Luisa Rodriguez to continue the conversation from <a href="https://80000hours.org/podcast/episodes/luisa-rodriguez-why-global-catastrophes-seem-unlikely-to-kill-us-all/">her episode of the show last year</a>.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:00:59)</li><li>The biggest impediments to bouncing back (00:03:18)</li><li>Can we do a serious version of The Knowledge? (00:14:58)</li><li>Recovering without much coal or oil (00:29:56)</li><li>Most valuable pro-resilience adjustments we can make today (00:40:23)</li><li>Feeding the Earth in disasters (00:47:45)</li><li>The reality of humans trying to actually do this (00:53:54)</li><li>Most exciting recent findings in astrobiology (01:01:00)</li><li>Rob’s outro (01:03:37)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>“We’re leaving these 16 contestants on an island with nothing but what they can scavenge from an abandoned factory and apartment block. Over the next 365 days, they’ll try to rebuild as much of civilisation as they can — from glass, to lenses, to microscopes. This is: The Knowledge!”</p><p>If you were a contestant on such a TV show, you'd love to have a guide to how basic things you currently take for granted are done — how to grow potatoes, fire bricks, turn wood to charcoal, find acids and alkalis, and so on.</p><p>Today’s guest Lewis Dartnell has gone as far compiling this information as anyone has with his bestselling book <a href="http://the-knowledge.org/en-gb/the-book/"><strong><em>The Knowledge: How to Rebuild Civilization in the Aftermath of a Cataclysm.</em></strong></a></p><p> <a href="https://80k.link/LD"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> But in the aftermath of a nuclear war or incredibly deadly pandemic that kills most people, many of the ways we do things today will be impossible — and even some of the things people did in the past, like collect coal from the surface of the Earth, will be impossible the second time around.</p><p> As Lewis points out, there’s “no point telling this band of survivors how to make something ultra-efficient or ultra-useful or ultra-capable if it's just too damned complicated to build in the first place. You have to start small and then level up, pull yourself up by your own bootstraps.”</p><p> So it might sound good to tell people to build solar panels — they’re a wonderful way of generating electricity. But the photovoltaic cells we use today need pure silicon, and nanoscale manufacturing — essentially the same technology as microchips used in a computer — so actually making solar panels would be incredibly difficult.</p><p> Instead, you’d want to tell our group of budding engineers to use more appropriate technologies like solar concentrators that use nothing more than mirrors — which turn out to be relatively easy to make.</p><p> A disaster that unravels the complex way we produce goods in the modern world is all too possible. Which raises the question: why not set dozens of people to plan out exactly what any survivors really ought to do if they need to support themselves and rebuild civilisation? Such a guide could then be translated and distributed all around the world.</p><p> The goal would be to provide the best information to speed up each of the many steps that would take survivors from rubbing sticks together in the wilderness to adjusting a thermostat in their comfy apartments.</p><p> This is clearly not a trivial task. Lewis's own book (at 300 pages) only scratched the surface of the most important knowledge humanity has accumulated, relegating all of mathematics to a single footnote.</p><p> And the ideal guide would offer pretty different advice depending on the scenario. Are survivors dealing with a radioactive ice age following a nuclear war? Or is it an eerily intact but near-empty post-pandemic world with mountains of goods to scavenge from the husks of cities?</p><p> As a brand-new parent, Lewis couldn’t do one of our classic three- or four-hour episodes — so this is an unusually snappy one-hour interview, where Rob and Lewis are joined by Luisa Rodriguez to continue the conversation from <a href="https://80000hours.org/podcast/episodes/luisa-rodriguez-why-global-catastrophes-seem-unlikely-to-kill-us-all/">her episode of the show last year</a>.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:00:59)</li><li>The biggest impediments to bouncing back (00:03:18)</li><li>Can we do a serious version of The Knowledge? (00:14:58)</li><li>Recovering without much coal or oil (00:29:56)</li><li>Most valuable pro-resilience adjustments we can make today (00:40:23)</li><li>Feeding the Earth in disasters (00:47:45)</li><li>The reality of humans trying to actually do this (00:53:54)</li><li>Most exciting recent findings in astrobiology (01:01:00)</li><li>Rob’s outro (01:03:37)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 03 Jun 2022 15:34:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/171b23b3/16378553.mp3" length="31531359" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lxyWY6W8_GHQZADq1OO5R5lVxI2B5NnWlyc_uXpVaZQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODIv/MTY4MzU0NDcyNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3942</itunes:duration>
      <itunes:summary>“We’re leaving these 16 contestants on an island with nothing but what they can scavenge from an abandoned factory and apartment block. Over the next 365 days, they’ll try to rebuild as much of civilisation as they can — from glass, to lenses, to microscopes. This is: The Knowledge!” 

If you were a contestant on such a TV show, you'd love to have a guide to how basic things you currently take for granted are done — how to grow potatoes, fire bricks, turn wood to charcoal, find acids and alkalis, and so on. 

Today’s guest Lewis Dartnell has gone as far compiling this information as anyone has with his bestselling book The Knowledge: How to Rebuild Civilization in the Aftermath of a Cataclysm. 

Links to learn more, summary and full transcript. 

But in the aftermath of a nuclear war or incredibly deadly pandemic that kills most people, many of the ways we do things today will be impossible — and even some of the things people did in the past, like collect coal from the surface of the Earth, will be impossible the second time around. 

As Lewis points out, there’s “no point telling this band of survivors how to make something ultra-efficient or ultra-useful or ultra-capable if it's just too damned complicated to build in the first place. You have to start small and then level up, pull yourself up by your own bootstraps.” 

So it might sound good to tell people to build solar panels — they’re a wonderful way of generating electricity. But the photovoltaic cells we use today need pure silicon, and nanoscale manufacturing — essentially the same technology as microchips used in a computer — so actually making solar panels would be incredibly difficult. 

Instead, you’d want to tell our group of budding engineers to use more appropriate technologies like solar concentrators that use nothing more than mirrors — which turn out to be relatively easy to make. 

A disaster that unravels the complex way we produce goods in the modern world is all too possible. Which raises the question: why not set dozens of people to plan out exactly what any survivors really ought to do if they need to support themselves and rebuild civilisation? Such a guide could then be translated and distributed all around the world.  

The goal would be to provide the best information to speed up each of the many steps that would take survivors from rubbing sticks together in the wilderness to adjusting a thermostat in their comfy apartments. 

This is clearly not a trivial task. Lewis's own book (at 300 pages) only scratched the surface of the most important knowledge humanity has accumulated, relegating all of mathematics to a single footnote.  

And the ideal guide would offer pretty different advice depending on the scenario. Are survivors dealing with a radioactive ice age following a nuclear war? Or is it an eerily intact but near-empty post-pandemic world with mountains of goods to scavenge from the husks of cities?  

As a brand-new parent, Lewis couldn’t do one of our classic three- or four-hour episodes — so this is an unusually snappy one-hour interview, where Rob and Lewis are joined by Luisa Rodriguez to continue the conversation from her episode of the show last year. 

They cover:  

• The biggest impediments to bouncing back 
• The reality of humans trying to actually do this 
• The most valuable pro-resilience adjustments we can make today 
• How to recover without much coal or oil 
• How to feed the Earth in disasters 
• And the most exciting recent findings in astrobiology 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>“We’re leaving these 16 contestants on an island with nothing but what they can scavenge from an abandoned factory and apartment block. Over the next 365 days, they’ll try to rebuild as much of civilisation as they can — from glass, to lenses, to microsco</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/171b23b3/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/171b23b3/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#130 – Will MacAskill on balancing frugality with ambition, whether you need longtermism, &amp; mental health under pressure</title>
      <itunes:title>#130 – Will MacAskill on balancing frugality with ambition, whether you need longtermism, &amp; mental health under pressure</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e1ef2166-dabf-11ec-87ea-0a39806abc79</guid>
      <link>https://80000hours.org/podcast/episodes/will-macaskill-ambition-longtermism-mental-health/</link>
      <description>
        <![CDATA[<p>Imagine you lead a nonprofit that operates on a shoestring budget. Staff are paid minimum wage, lunch is bread and hummus, and you're all bunched up on a few tables in a basement office.</p><p> But over a few years, your cause attracts some major new donors. Your funding jumps a thousandfold, from $100,000 a year to $100,000,000 a year. You're the same group of people committed to making sacrifices for the cause — but these days, rather than cutting costs, the right thing to do seems to be to spend serious money and get things done ASAP.</p><p> You suddenly have the opportunity to make more progress than ever before, but as well as excitement about this, you have worries about the impacts that large amounts of funding can have.</p><p> This is roughly the situation faced by today's guest Will MacAskill — University of Oxford philosopher, author of the forthcoming book <a href="https://www.whatweowethefuture.com/"><em>What We Owe The Future</em></a>, and founding figure in the effective altruism movement.</p><p> <a href="https://80k.link/WM3"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Years ago, Will pledged to give away more than 50% of his income over his life, and was already donating 10% back when he was a student with next to no income. Since then, the coalition he founded has been super successful at attracting the interest of donors who collectively want to give away billions in the way Will and his colleagues were proposing.</p><p> While surely a huge success, it brings with it risks that he's never had to consider before:</p><p> • Will and his colleagues might try to spend a lot of money trying to get more things done more quickly — but actually just waste it.<br> • Being seen as profligate could strike onlookers as selfish and disreputable.<br> • Folks might start pretending to agree with their agenda just to get grants.<br> • People working on nearby issues that are less flush with funding may end up resentful.<br> • People might lose their focus on helping others as they get seduced by the prospect of earning a nice living.<br> • Mediocre projects might find it too easy to get funding, even when the people involved would be better off radically changing their strategy, or shutting down and launching something else entirely.</p><p> But all these 'risks of commission' have to be weighed against 'risk of omission': the failure to achieve all you could have if you'd been truly ambitious.</p><p> People looking askance at you for paying high salaries to attract the staff you want is unpleasant.</p><p> But failing to prevent the next pandemic because you didn't have the necessary medical experts on your grantmaking team is worse than unpleasant — it's a true disaster. Yet few will complain, because they'll never know what might have been if you'd only set frugality aside.</p><p> Will aims to strike a sensible balance between these competing errors, which he has taken to calling <em>judicious ambition</em>. In today's episode, Rob and Will discuss the above as well as:</p><p> • Will humanity likely converge on good values as we get more educated and invest more in moral philosophy — or are the things we care about actually quite arbitrary and contingent?<br> • Why are so many nonfiction books full of factual errors?<br> • How does Will avoid anxiety and depression with more responsibility on his shoulders than ever?<br> • What does Will disagree with his colleagues on?<br> • Should we focus on existential risks more or less the same way, whether we care about future generations or not?<br> • Are potatoes one of the most important technologies ever developed?<br> • And plenty more.</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:41)</li><li>What We Owe The Future preview (00:09:23)</li><li>Longtermism vs. x-risk (00:25:39)</li><li>How is Will doing? (00:33:16)</li><li>Having a life outside of work (00:46:45)</li><li>Underappreciated people in the effective altruism community (00:52:48)</li><li>A culture of ambition within effective altruism (00:59:50)</li><li>Massively scalable projects (01:11:40)</li><li>Downsides and risks from the increase in funding (01:14:13)</li><li>Barriers to ambition (01:28:47)</li><li>The Future Fund (01:38:04)</li><li>Patient philanthropy (01:52:50)</li><li>Will’s disagreements with Sam Bankman-Fried and Nick Beckstead (01:56:42)</li><li>Astronomical risks of suffering (s-risks) (02:00:02)</li><li>Will’s future plans (02:02:41)</li><li>What is it with Will and potatoes? (02:08:40)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Imagine you lead a nonprofit that operates on a shoestring budget. Staff are paid minimum wage, lunch is bread and hummus, and you're all bunched up on a few tables in a basement office.</p><p> But over a few years, your cause attracts some major new donors. Your funding jumps a thousandfold, from $100,000 a year to $100,000,000 a year. You're the same group of people committed to making sacrifices for the cause — but these days, rather than cutting costs, the right thing to do seems to be to spend serious money and get things done ASAP.</p><p> You suddenly have the opportunity to make more progress than ever before, but as well as excitement about this, you have worries about the impacts that large amounts of funding can have.</p><p> This is roughly the situation faced by today's guest Will MacAskill — University of Oxford philosopher, author of the forthcoming book <a href="https://www.whatweowethefuture.com/"><em>What We Owe The Future</em></a>, and founding figure in the effective altruism movement.</p><p> <a href="https://80k.link/WM3"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Years ago, Will pledged to give away more than 50% of his income over his life, and was already donating 10% back when he was a student with next to no income. Since then, the coalition he founded has been super successful at attracting the interest of donors who collectively want to give away billions in the way Will and his colleagues were proposing.</p><p> While surely a huge success, it brings with it risks that he's never had to consider before:</p><p> • Will and his colleagues might try to spend a lot of money trying to get more things done more quickly — but actually just waste it.<br> • Being seen as profligate could strike onlookers as selfish and disreputable.<br> • Folks might start pretending to agree with their agenda just to get grants.<br> • People working on nearby issues that are less flush with funding may end up resentful.<br> • People might lose their focus on helping others as they get seduced by the prospect of earning a nice living.<br> • Mediocre projects might find it too easy to get funding, even when the people involved would be better off radically changing their strategy, or shutting down and launching something else entirely.</p><p> But all these 'risks of commission' have to be weighed against 'risk of omission': the failure to achieve all you could have if you'd been truly ambitious.</p><p> People looking askance at you for paying high salaries to attract the staff you want is unpleasant.</p><p> But failing to prevent the next pandemic because you didn't have the necessary medical experts on your grantmaking team is worse than unpleasant — it's a true disaster. Yet few will complain, because they'll never know what might have been if you'd only set frugality aside.</p><p> Will aims to strike a sensible balance between these competing errors, which he has taken to calling <em>judicious ambition</em>. In today's episode, Rob and Will discuss the above as well as:</p><p> • Will humanity likely converge on good values as we get more educated and invest more in moral philosophy — or are the things we care about actually quite arbitrary and contingent?<br> • Why are so many nonfiction books full of factual errors?<br> • How does Will avoid anxiety and depression with more responsibility on his shoulders than ever?<br> • What does Will disagree with his colleagues on?<br> • Should we focus on existential risks more or less the same way, whether we care about future generations or not?<br> • Are potatoes one of the most important technologies ever developed?<br> • And plenty more.</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:41)</li><li>What We Owe The Future preview (00:09:23)</li><li>Longtermism vs. x-risk (00:25:39)</li><li>How is Will doing? (00:33:16)</li><li>Having a life outside of work (00:46:45)</li><li>Underappreciated people in the effective altruism community (00:52:48)</li><li>A culture of ambition within effective altruism (00:59:50)</li><li>Massively scalable projects (01:11:40)</li><li>Downsides and risks from the increase in funding (01:14:13)</li><li>Barriers to ambition (01:28:47)</li><li>The Future Fund (01:38:04)</li><li>Patient philanthropy (01:52:50)</li><li>Will’s disagreements with Sam Bankman-Fried and Nick Beckstead (01:56:42)</li><li>Astronomical risks of suffering (s-risks) (02:00:02)</li><li>Will’s future plans (02:02:41)</li><li>What is it with Will and potatoes? (02:08:40)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 23 May 2022 18:17:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/386ca04d/d93d2294.mp3" length="65607793" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/zRMOMwN3B--V3LH5XUf4p8eKdPH1iG7dMsheSwFvokw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODEv/MTY4MzU0NDcyNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8201</itunes:duration>
      <itunes:summary>Imagine you lead a nonprofit that operates on a shoestring budget. Staff are paid minimum wage, lunch is bread and hummus, and you're all bunched up on a few tables in a basement office. 

But over a few years, your cause attracts some major new donors. Your funding jumps a thousandfold, from $100,000 a year to $100,000,000 a year. You're the same group of people committed to making sacrifices for the cause — but these days, rather than cutting costs, the right thing to do seems to be to spend serious money and get things done ASAP. 

You suddenly have the opportunity to make more progress than ever before, but as well as excitement about this, you have worries about the impacts that large amounts of funding can have. 

This is roughly the situation faced by today's guest Will MacAskill — University of Oxford philosopher, author of the forthcoming book What We Owe The Future, and founding figure in the effective altruism movement. 

Links to learn more, summary and full transcript. 

Years ago, Will pledged to give away more than 50% of his income over his life, and was already donating 10% back when he was a student with next to no income. Since then, the coalition he founded has been super successful at attracting the interest of donors who collectively want to give away billions in the way Will and his colleagues were proposing. 

While surely a huge success, it brings with it risks that he's never had to consider before: 

• Will and his colleagues might try to spend a lot of money trying to get more things done more quickly — but actually just waste it. 
• Being seen as profligate could strike onlookers as selfish and disreputable. 
• Folks might start pretending to agree with their agenda just to get grants. 
• People working on nearby issues that are less flush with funding may end up resentful. 
• People might lose their focus on helping others as they get seduced by the prospect of earning a nice living. 
• Mediocre projects might find it too easy to get funding, even when the people involved would be better off radically changing their strategy, or shutting down and launching something else entirely. 

But all these 'risks of commission' have to be weighed against 'risk of omission': the failure to achieve all you could have if you'd been truly ambitious. 

People looking askance at you for paying high salaries to attract the staff you want is unpleasant. 

But failing to prevent the next pandemic because you didn't have the necessary medical experts on your grantmaking team is worse than unpleasant — it's a true disaster. Yet few will complain, because they'll never know what might have been if you'd only set frugality aside. 

Will aims to strike a sensible balance between these competing errors, which he has taken to calling judicious ambition. In today's episode, Rob and Will discuss the above as well as: 

• Will humanity likely converge on good values as we get more educated and invest more in moral philosophy — or are the things we care about actually quite arbitrary and contingent? 
• Why are so many nonfiction books full of factual errors? 
• How does Will avoid anxiety and depression with more responsibility on his shoulders than ever? 
• What does Will disagree with his colleagues on? 
• Should we focus on existential risks more or less the same way, whether we care about future generations or not? 
• Are potatoes one of the most important technologies ever developed? 
• And plenty more. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Imagine you lead a nonprofit that operates on a shoestring budget. Staff are paid minimum wage, lunch is bread and hummus, and you're all bunched up on a few tables in a basement office. 

But over a few years, your cause attracts some major new donors.</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/386ca04d/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/386ca04d/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#129 – James Tibenderana on the state of the art in malaria control and elimination</title>
      <itunes:title>#129 – James Tibenderana on the state of the art in malaria control and elimination</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">cb3f684c-cfe1-11ec-adfe-0e1af143ee1b</guid>
      <link>https://80000hours.org/podcast/episodes/james-tibenderana-malaria-control-and-elimination/</link>
      <description>
        <![CDATA[<p>The good news is deaths from malaria have been cut by a third since 2005. The bad news is it still causes 250 million cases and 600,000 deaths a year, mostly among young children in sub-Saharan Africa.</p><p>We already have dirt-cheap ways to prevent and treat malaria, and the fraction of the Earth's surface where the disease exists at all has been halved since 1900. So why is it such a persistent problem in some places, even rebounding 15% since 2019?</p><p>That's one of many questions I put to today's guest, James Tibenderana — doctor, medical researcher, and technical director at a major global health nonprofit known as Malaria Consortium. James studies the cutting edge of malaria control and treatment in order to optimise how Malaria Consortium spends £100 million a year across countries like Uganda, Nigeria, and Chad.</p><p><a href="https://80k.link/JT"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In sub-Saharan Africa, where 90% of malaria deaths occur, the infection is spread by a few dozen species of mosquito that are ideally suited to the local climatic conditions and have thus been impossible to eliminate so far.</p><p> While COVID-19 may have an 'R' (reproduction number) of 5, in some situations malaria has a reproduction number in the 1,000s. A single person with malaria can pass the parasite to hundreds of mosquitoes, which themselves each go on to bite dozens of people each, allowing cases to quickly explode.</p><p> The nets and antimalarial drugs Malaria Consortium distributes have been highly effective where distributed, but there are tens of millions of young children who are yet to be covered simply due to a lack of funding.</p><p> Despite the success of these approaches, given how challenging it will be to create a malaria-free world, there's enthusiasm to find new approaches to throw at the problem. Two new interventions have recently generated buzz: vaccines and genetic approaches to control the mosquito species that carry malaria.</p><p> The RTS,S vaccine is the first-ever vaccine that attacks a protozoa as opposed to a virus or bacteria. It's a great scientific achievement. But James points out that even after three doses, it's still only about 30% effective. Unless future vaccines are substantially more effective, they will remain just a complement to nets and antimalarial drugs, which are cheaper and each cut mortality by more than half.</p><p> On the other hand, the latest mosquito-control technologies are almost too effective. It is possible to insert genes into specific mosquito populations that reduce their ability to reproduce. By using a 'gene drive,' you can ensure mosquitoes hand these detrimental genes down to 100% of their offspring. If deployed, these genes would spread and ultimately eliminate the mosquitoes that carry malaria at low cost, thereby largely ridding the world of the disease.</p><p> Because a single country embracing this method would have global effects, James cautions that it's important to get buy-in from all the countries involved, and to have a way of reversing the intervention if we realise we've made a mistake.</p><p> In this comprehensive conversation, Rob and James discuss all of the above, as well as most of what you could reasonably want to know about the state of the art in malaria control today, including:<br> • How malaria spreads and the symptoms it causes<br> • The use of insecticides and poison baits<br> • How big a problem insecticide resistance is<br> • How malaria was eliminated in North America and Europe<br> • The key strategic choices faced by Malaria Consortium in its efforts to create a malaria-free world<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:06)</li><li>Malaria basics (00:06:56)</li><li>Malaria vaccines (00:12:37)</li><li>Getting rid of mosquitos (00:32:20)</li><li>Gene drives (00:38:06)</li><li>Symptoms (00:58:00)</li><li>Preventing the spread (01:06:00)</li><li>Why we haven’t gotten rid of malaria yet (01:15:07)</li><li>What James is responsible for as technical director (01:30:52)</li><li>Malaria Consortium's current strategy (01:39:59)</li><li>Elimination vs. control (02:01:49)</li><li>Delivery and practicalities (02:16:23)</li><li>Relationships with governments (02:26:38)</li><li>Funding gap (02:31:03)</li><li>Access and use gap (02:39:10)</li><li>The value of local researchers (02:49:26)</li><li>Past research findings (02:57:10)</li><li>How to help (03:06:30)</li><li>How James ended up where he is today (03:13:45)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The good news is deaths from malaria have been cut by a third since 2005. The bad news is it still causes 250 million cases and 600,000 deaths a year, mostly among young children in sub-Saharan Africa.</p><p>We already have dirt-cheap ways to prevent and treat malaria, and the fraction of the Earth's surface where the disease exists at all has been halved since 1900. So why is it such a persistent problem in some places, even rebounding 15% since 2019?</p><p>That's one of many questions I put to today's guest, James Tibenderana — doctor, medical researcher, and technical director at a major global health nonprofit known as Malaria Consortium. James studies the cutting edge of malaria control and treatment in order to optimise how Malaria Consortium spends £100 million a year across countries like Uganda, Nigeria, and Chad.</p><p><a href="https://80k.link/JT"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In sub-Saharan Africa, where 90% of malaria deaths occur, the infection is spread by a few dozen species of mosquito that are ideally suited to the local climatic conditions and have thus been impossible to eliminate so far.</p><p> While COVID-19 may have an 'R' (reproduction number) of 5, in some situations malaria has a reproduction number in the 1,000s. A single person with malaria can pass the parasite to hundreds of mosquitoes, which themselves each go on to bite dozens of people each, allowing cases to quickly explode.</p><p> The nets and antimalarial drugs Malaria Consortium distributes have been highly effective where distributed, but there are tens of millions of young children who are yet to be covered simply due to a lack of funding.</p><p> Despite the success of these approaches, given how challenging it will be to create a malaria-free world, there's enthusiasm to find new approaches to throw at the problem. Two new interventions have recently generated buzz: vaccines and genetic approaches to control the mosquito species that carry malaria.</p><p> The RTS,S vaccine is the first-ever vaccine that attacks a protozoa as opposed to a virus or bacteria. It's a great scientific achievement. But James points out that even after three doses, it's still only about 30% effective. Unless future vaccines are substantially more effective, they will remain just a complement to nets and antimalarial drugs, which are cheaper and each cut mortality by more than half.</p><p> On the other hand, the latest mosquito-control technologies are almost too effective. It is possible to insert genes into specific mosquito populations that reduce their ability to reproduce. By using a 'gene drive,' you can ensure mosquitoes hand these detrimental genes down to 100% of their offspring. If deployed, these genes would spread and ultimately eliminate the mosquitoes that carry malaria at low cost, thereby largely ridding the world of the disease.</p><p> Because a single country embracing this method would have global effects, James cautions that it's important to get buy-in from all the countries involved, and to have a way of reversing the intervention if we realise we've made a mistake.</p><p> In this comprehensive conversation, Rob and James discuss all of the above, as well as most of what you could reasonably want to know about the state of the art in malaria control today, including:<br> • How malaria spreads and the symptoms it causes<br> • The use of insecticides and poison baits<br> • How big a problem insecticide resistance is<br> • How malaria was eliminated in North America and Europe<br> • The key strategic choices faced by Malaria Consortium in its efforts to create a malaria-free world<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:06)</li><li>Malaria basics (00:06:56)</li><li>Malaria vaccines (00:12:37)</li><li>Getting rid of mosquitos (00:32:20)</li><li>Gene drives (00:38:06)</li><li>Symptoms (00:58:00)</li><li>Preventing the spread (01:06:00)</li><li>Why we haven’t gotten rid of malaria yet (01:15:07)</li><li>What James is responsible for as technical director (01:30:52)</li><li>Malaria Consortium's current strategy (01:39:59)</li><li>Elimination vs. control (02:01:49)</li><li>Delivery and practicalities (02:16:23)</li><li>Relationships with governments (02:26:38)</li><li>Funding gap (02:31:03)</li><li>Access and use gap (02:39:10)</li><li>The value of local researchers (02:49:26)</li><li>Past research findings (02:57:10)</li><li>How to help (03:06:30)</li><li>How James ended up where he is today (03:13:45)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 09 May 2022 21:57:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ade4038b/a346e7cf.mp3" length="95809271" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/dIucBbad9-J8EeEahB9j3WJOiIHLIVYcEZ6E8FAwJ2U/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4ODAv/MTY4MzU0NDcyMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11976</itunes:duration>
      <itunes:summary>The good news is deaths from malaria have been cut by a third since 2005. The bad news is it still causes 250 million cases and 600,000 deaths a year, mostly among young children in sub-Saharan Africa. 

We already have dirt-cheap ways to prevent and treat malaria, and the fraction of the Earth's surface where the disease exists at all has been halved since 1900. So why is it such a persistent problem in some places, even rebounding 15% since 2019? 

That's one of many questions I put to today's guest, James Tibenderana — doctor, medical researcher, and technical director at a major global health nonprofit known as Malaria Consortium. James studies the cutting edge of malaria control and treatment in order to optimise how Malaria Consortium spends £100 million a year across countries like Uganda, Nigeria, and Chad. 

Links to learn more, summary and full transcript. 

In sub-Saharan Africa, where 90% of malaria deaths occur, the infection is spread by a few dozen species of mosquito that are ideally suited to the local climatic conditions and have thus been impossible to eliminate so far. 

While COVID-19 may have an 'R' (reproduction number) of 5, in some situations malaria has a reproduction number in the 1,000s. A single person with malaria can pass the parasite to hundreds of mosquitoes, which themselves each go on to bite dozens of people each, allowing cases to quickly explode. 

The nets and antimalarial drugs Malaria Consortium distributes have been highly effective where distributed, but there are tens of millions of young children who are yet to be covered simply due to a lack of funding. 

Despite the success of these approaches, given how challenging it will be to create a malaria-free world, there's enthusiasm to find new approaches to throw at the problem. Two new interventions have recently generated buzz: vaccines and genetic approaches to control the mosquito species that carry malaria. 

The RTS,S vaccine is the first-ever vaccine that attacks a protozoa as opposed to a virus or bacteria. It's a great scientific achievement. But James points out that even after three doses, it's still only about 30% effective. Unless future vaccines are substantially more effective, they will remain just a complement to nets and antimalarial drugs, which are cheaper and each cut mortality by more than half. 

On the other hand, the latest mosquito-control technologies are almost too effective. It is possible to insert genes into specific mosquito populations that reduce their ability to reproduce. By using a 'gene drive,' you can ensure mosquitoes hand these detrimental genes down to 100% of their offspring. If deployed, these genes would spread and ultimately eliminate the mosquitoes that carry malaria at low cost, thereby largely ridding the world of the disease. 

Because a single country embracing this method would have global effects, James cautions that it's important to get buy-in from all the countries involved, and to have a way of reversing the intervention if we realise we've made a mistake. 

In this comprehensive conversation, Rob and James discuss all of the above, as well as most of what you could reasonably want to know about the state of the art in malaria control today, including: 

• How malaria spreads and the symptoms it causes 
• The use of insecticides and poison baits 
• How big a problem insecticide resistance is 
• How malaria was eliminated in North America and Europe 
• The key strategic choices faced by Malaria Consortium in its efforts to create a malaria-free world 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>The good news is deaths from malaria have been cut by a third since 2005. The bad news is it still causes 250 million cases and 600,000 deaths a year, mostly among young children in sub-Saharan Africa. 

We already have dirt-cheap ways to prevent and tr</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/ade4038b/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/ade4038b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#128 – Chris Blattman on the five reasons wars happen</title>
      <itunes:title>#128 – Chris Blattman on the five reasons wars happen</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">47fd2330-c739-11ec-b962-0e6c8cdb8593</guid>
      <link>https://80000hours.org/podcast/episodes/chris-blattman-five-reasons-wars-happen/</link>
      <description>
        <![CDATA[<p>In nature, animals roar and bare their teeth to intimidate adversaries — but one side usually backs down, and real fights are rare. The wisdom of evolution is that the risk of violence is just too great.</p><p>Which might make one wonder: if war is so destructive, why does it happen? The question may sound naïve, but in fact it represents a deep puzzle. If a war will cost trillions and kill tens of thousands, it should be easy for either side to make a peace offer that both they and their opponents prefer to actually fighting it out.</p><p>The conundrum of how humans can engage in incredibly costly and protracted conflicts has occupied academics across the social sciences for years. In today's episode, we speak with economist Chris Blattman about his new book, <a href="https://chrisblattman.com/why-we-fight/"><strong><em>Why We Fight: The Roots of War and the Paths to Peace</em></strong></a>, which summarises what they think they've learned.</p><p> <a href="https://80k.link/CB"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Chris's first point is that while organised violence may feel like it's all around us, it's actually very rare in humans, just as it is with other animals. Across the world, hundreds of groups dislike one another — but knowing the cost of war, they prefer to simply loathe one another in peace.</p><p> In order to understand what’s wrong with a sick patient, a doctor needs to know what a healthy person looks like. And to understand war, social scientists need to study all the wars that could have happened but didn't — so they can see what a healthy society looks like and what's missing in the places where war does take hold.</p><p> Chris argues that social scientists have generated five cogent models of when war can be 'rational' for both sides of a conflict:</p><p> 1. Unchecked interests — such as national leaders who bear few of the costs of launching a war.<br> 2. Intangible incentives — such as an intrinsic desire for revenge.<br> 3. Uncertainty — such as both sides underestimating each other's resolve to fight.<br> 4. Commitment problems — such as the inability to credibly promise not to use your growing military might to attack others in future.<br> 5. Misperceptions — such as our inability to see the world through other people's eyes.</p><p> In today's interview, we walk through how each of the five explanations work and what specific wars or actions they might explain.</p><p> In the process, Chris outlines how many of the most popular explanations for interstate war are wildly overused (e.g. leaders who are unhinged or male) or misguided from the outset (e.g. resource scarcity).</p><p> The interview also covers:</p><p> • What Chris and Rob got wrong about the war in Ukraine<br> • What causes might not fit into these five categories<br> • The role of people's choice to escalate or deescalate a conflict<br> • How great power wars or nuclear wars are different, and what can be done to prevent them<br> • How much representative government helps to prevent war<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:43)</li><li>What people get wrong about violence (00:04:40)</li><li>Medellín gangs (00:11:48)</li><li>Overrated causes of violence (00:23:53)</li><li>Cause of war #1: Unchecked interests (00:36:40)</li><li>Cause of war #2: Intangible incentives (00:41:40)</li><li>Cause of war #3: Uncertainty (00:53:04)</li><li>Cause of war #4: Commitment problems (01:02:24)</li><li>Cause of war #5: Misperceptions (01:12:18)</li><li>Weaknesses of the model (01:26:08)</li><li>Dancing on the edge of a cliff (01:29:06)</li><li>Confusion around escalation (01:35:26)</li><li>Applying the model to the war between Russia and Ukraine (01:42:34)</li><li>Great power wars (02:01:46)</li><li>Preventing nuclear war (02:18:57)</li><li>Why undirected approaches won't work (02:22:51)</li><li>Democratic peace theory (02:31:10)</li><li>Exchanging hostages (02:37:21)</li><li>What you can actually do to help (02:41:25)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In nature, animals roar and bare their teeth to intimidate adversaries — but one side usually backs down, and real fights are rare. The wisdom of evolution is that the risk of violence is just too great.</p><p>Which might make one wonder: if war is so destructive, why does it happen? The question may sound naïve, but in fact it represents a deep puzzle. If a war will cost trillions and kill tens of thousands, it should be easy for either side to make a peace offer that both they and their opponents prefer to actually fighting it out.</p><p>The conundrum of how humans can engage in incredibly costly and protracted conflicts has occupied academics across the social sciences for years. In today's episode, we speak with economist Chris Blattman about his new book, <a href="https://chrisblattman.com/why-we-fight/"><strong><em>Why We Fight: The Roots of War and the Paths to Peace</em></strong></a>, which summarises what they think they've learned.</p><p> <a href="https://80k.link/CB"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Chris's first point is that while organised violence may feel like it's all around us, it's actually very rare in humans, just as it is with other animals. Across the world, hundreds of groups dislike one another — but knowing the cost of war, they prefer to simply loathe one another in peace.</p><p> In order to understand what’s wrong with a sick patient, a doctor needs to know what a healthy person looks like. And to understand war, social scientists need to study all the wars that could have happened but didn't — so they can see what a healthy society looks like and what's missing in the places where war does take hold.</p><p> Chris argues that social scientists have generated five cogent models of when war can be 'rational' for both sides of a conflict:</p><p> 1. Unchecked interests — such as national leaders who bear few of the costs of launching a war.<br> 2. Intangible incentives — such as an intrinsic desire for revenge.<br> 3. Uncertainty — such as both sides underestimating each other's resolve to fight.<br> 4. Commitment problems — such as the inability to credibly promise not to use your growing military might to attack others in future.<br> 5. Misperceptions — such as our inability to see the world through other people's eyes.</p><p> In today's interview, we walk through how each of the five explanations work and what specific wars or actions they might explain.</p><p> In the process, Chris outlines how many of the most popular explanations for interstate war are wildly overused (e.g. leaders who are unhinged or male) or misguided from the outset (e.g. resource scarcity).</p><p> The interview also covers:</p><p> • What Chris and Rob got wrong about the war in Ukraine<br> • What causes might not fit into these five categories<br> • The role of people's choice to escalate or deescalate a conflict<br> • How great power wars or nuclear wars are different, and what can be done to prevent them<br> • How much representative government helps to prevent war<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:43)</li><li>What people get wrong about violence (00:04:40)</li><li>Medellín gangs (00:11:48)</li><li>Overrated causes of violence (00:23:53)</li><li>Cause of war #1: Unchecked interests (00:36:40)</li><li>Cause of war #2: Intangible incentives (00:41:40)</li><li>Cause of war #3: Uncertainty (00:53:04)</li><li>Cause of war #4: Commitment problems (01:02:24)</li><li>Cause of war #5: Misperceptions (01:12:18)</li><li>Weaknesses of the model (01:26:08)</li><li>Dancing on the edge of a cliff (01:29:06)</li><li>Confusion around escalation (01:35:26)</li><li>Applying the model to the war between Russia and Ukraine (01:42:34)</li><li>Great power wars (02:01:46)</li><li>Preventing nuclear war (02:18:57)</li><li>Why undirected approaches won't work (02:22:51)</li><li>Democratic peace theory (02:31:10)</li><li>Exchanging hostages (02:37:21)</li><li>What you can actually do to help (02:41:25)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 28 Apr 2022 21:50:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/593ac74b/b0c920f0.mp3" length="80091014" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/4mS-g2pwjGvzVexEG7kOH3nbNwoMKuFf49tPwDCp-D8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Nzkv/MTY4MzU0NDcxOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10011</itunes:duration>
      <itunes:summary>In nature, animals roar and bare their teeth to intimidate adversaries — but one side usually backs down, and real fights are rare. The wisdom of evolution is that the risk of violence is just too great. 

Which might make one wonder: if war is so destructive, why does it happen? The question may sound naïve, but in fact it represents a deep puzzle. If a war will cost trillions and kill tens of thousands, it should be easy for either side to make a peace offer that both they and their opponents prefer to actually fighting it out. 

The conundrum of how humans can engage in incredibly costly and protracted conflicts has occupied academics across the social sciences for years. In today's episode, we speak with economist Chris Blattman about his new book, Why We Fight: The Roots of War and the Paths to Peace, which summarises what they think they've learned. 

Links to learn more, summary and full transcript.

Chris's first point is that while organised violence may feel like it's all around us, it's actually very rare in humans, just as it is with other animals. Across the world, hundreds of groups dislike one another — but knowing the cost of war, they prefer to simply loathe one another in peace. 

In order to understand what’s wrong with a sick patient, a doctor needs to know what a healthy person looks like. And to understand war, social scientists need to study all the wars that could have happened but didn't — so they can see what a healthy society looks like and what's missing in the places where war does take hold. 

Chris argues that social scientists have generated five cogent models of when war can be 'rational' for both sides of a conflict: 

1. Unchecked interests — such as national leaders who bear few of the costs of launching a war. 
2. Intangible incentives — such as an intrinsic desire for revenge. 
3. Uncertainty — such as both sides underestimating each other's resolve to fight. 
4. Commitment problems — such as the inability to credibly promise not to use your growing military might to attack others in future. 
5. Misperceptions — such as our inability to see the world through other people's eyes. 

In today's interview, we walk through how each of the five explanations work and what specific wars or actions they might explain. 

In the process, Chris outlines how many of the most popular explanations for interstate war are wildly overused (e.g. leaders who are unhinged or male) or misguided from the outset (e.g. resource scarcity). 

The interview also covers: 

• What Chris and Rob got wrong about the war in Ukraine 
• What causes might not fit into these five categories 
• The role of people's choice to escalate or deescalate a conflict 
• How great power wars or nuclear wars are different, and what can be done to prevent them 
• How much representative government helps to prevent war 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>In nature, animals roar and bare their teeth to intimidate adversaries — but one side usually backs down, and real fights are rare. The wisdom of evolution is that the risk of violence is just too great. 

Which might make one wonder: if war is so destr</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/593ac74b/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/593ac74b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#127 – Sam Bankman-Fried on taking a high-risk approach to crypto and doing good</title>
      <itunes:title>#127 – Sam Bankman-Fried on taking a high-risk approach to crypto and doing good</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">8c811fe4-bc2f-11ec-9050-0e32fbe90b51</guid>
      <link>https://share.transistor.fm/s/4842cf30</link>
      <description>
        <![CDATA[On this episode of the show, host Rob Wiblin interviews Sam Bankman-Fried.<p> 

This interview was recorded in February 2022, and released in April 2022.</p><p> 

But on November 11 2022, Sam Bankman-Fried's company, FTX, filed for bankruptcy, and all staff at the Future Fund <a href="https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1">resigned</a> — and the surrounding events led Rob to record a new intro on December 1st 2022 for this episode.</p><p> 

• Read 80,000 Hours' statement on these events <a href="https://80000hours.org/2022/11/regarding-the-collapse-of-ftx/"><b>here</b></a>.</p><p> 

• You can also listen to host Rob’s reaction to the collapse of FTX on this podcast feed, above episode 140, or <a href="https://open.spotify.com/show/2WzJwXWBDnn4iZ7odKwDib"><b>here</b></a>.</p><p> 

• Rob has shared some clarifications on his views about diminishing returns and risk aversion, and weaknesses in how it was discussed in this episode, <a href="https://forum.effectivealtruism.org/posts/THgezaPxhvoizkRFy/clarifications-on-diminishing-returns-and-risk-aversion-in"><b>here</b></a>.</p><p> 

• And you can read the original blog post associated with the episode <a href="https://80k.link/SBF"><b>here.</b></a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[On this episode of the show, host Rob Wiblin interviews Sam Bankman-Fried.<p> 

This interview was recorded in February 2022, and released in April 2022.</p><p> 

But on November 11 2022, Sam Bankman-Fried's company, FTX, filed for bankruptcy, and all staff at the Future Fund <a href="https://forum.effectivealtruism.org/posts/xafpj3on76uRDoBja/the-ftx-future-fund-team-has-resigned-1">resigned</a> — and the surrounding events led Rob to record a new intro on December 1st 2022 for this episode.</p><p> 

• Read 80,000 Hours' statement on these events <a href="https://80000hours.org/2022/11/regarding-the-collapse-of-ftx/"><b>here</b></a>.</p><p> 

• You can also listen to host Rob’s reaction to the collapse of FTX on this podcast feed, above episode 140, or <a href="https://open.spotify.com/show/2WzJwXWBDnn4iZ7odKwDib"><b>here</b></a>.</p><p> 

• Rob has shared some clarifications on his views about diminishing returns and risk aversion, and weaknesses in how it was discussed in this episode, <a href="https://forum.effectivealtruism.org/posts/THgezaPxhvoizkRFy/clarifications-on-diminishing-returns-and-risk-aversion-in"><b>here</b></a>.</p><p> 

• And you can read the original blog post associated with the episode <a href="https://80k.link/SBF"><b>here.</b></a></p>]]>
      </content:encoded>
      <pubDate>Thu, 14 Apr 2022 20:29:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4842cf30/0d9af005.mp3" length="96225214" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/6joYpSyAuwcHeohL6Gzop5TLu1p3Eotqk_ytmyr4Zz8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Nzgv/MTY4MzU0NDcxNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>12028</itunes:duration>
      <itunes:summary>On this episode of the show, host Rob Wiblin interviews Sam Bankman-Fried. 

This interview was recorded in February 2022, and released in April 2022. 

But on November 11 2022, Sam Bankman-Fried's company, FTX, filed for bankruptcy, and all staff at the Future Fund resigned — and the surrounding events led Rob to record a new intro on December 1st 2022 for this episode. 

• Read 80,000 Hours' statement on these events here. 

• You can also listen to host Rob’s reaction to the collapse of FTX on this podcast feed, above episode 140, or here. 

• Rob has shared some clarifications on his views about diminishing returns and risk aversion, and weaknesses in how it was discussed in this episode, here. 

• And you can read the original blog post associated with the episode here.</itunes:summary>
      <itunes:subtitle>On this episode of the show, host Rob Wiblin interviews Sam Bankman-Fried. 

This interview was recorded in February 2022, and released in April 2022. 

But on November 11 2022, Sam Bankman-Fried's company, FTX, filed for bankruptcy, and all staff at the </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#126 – Bryan Caplan on whether lazy parenting is OK, what really helps workers, and betting on beliefs</title>
      <itunes:title>#126 – Bryan Caplan on whether lazy parenting is OK, what really helps workers, and betting on beliefs</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">42ee32ea-b527-11ec-bb1c-0ea90ae3548b</guid>
      <link>https://80000hours.org/podcast/episodes/bryan-caplan-parenting-workers-betting/</link>
      <description>
        <![CDATA[<p>Everybody knows that good parenting has a big impact on how kids turn out. Except that maybe they don't, because it doesn't.</p><p>Incredible though it might seem, according to today's guest — economist Bryan Caplan, the author of Selfish Reasons To Have More Kids, The Myth of the Rational Voter, and The Case Against Education — the best evidence we have on the question suggests that, within reason, what parents do has little impact on how their children's lives play out once they're adults.</p><p><a href="https://80k.link/BC"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Of course, kids do resemble their parents. But just as we probably can't say it was attentive parenting that gave me my mother's nose, perhaps we can't say it was attentive parenting that made me succeed at school. Both the social environment we grow up in and the genes we receive from our parents influence the person we become, and looking at a typical family we can't really distinguish the impact of one from the other.</p><p> But nature does offer us up a random experiment that can let us tell the difference: identical twins share all their genes, while fraternal twins only share half their genes. If you look at how much more similar outcomes are for identical twins than fraternal twins, you see the effect of sharing 100% of your genetic material, rather than the usual 50%. Double that amount, and you've got the full effect of genetic inheritance. Whatever unexplained variation remains is still up for grabs — and might be down to different experiences in the home, outside the home, or just random noise.</p><p> The crazy thing about this research is that it says for a range of adult outcomes (e.g. years of education, income, health, personality, and happiness), it's differences in the genes children inherit rather than differences in parental behaviour that are doing most of the work. Other research suggests that differences in “out-of-home environment” take second place. Parenting style does matter for something, but it comes in a clear third.</p><p> Bryan is quick to point out that there are several factors that help reconcile these findings with conventional wisdom about the importance of parenting.</p><p> First, for some adult outcomes, parenting was a big deal (i.e. the quality of the parent/child relationship) or at least a moderate deal (i.e. drug use, criminality, and religious/political identity).</p><p> Second, parents can and do influence you quite a lot — so long as you're young and still living with them. But as soon as you move out, the influence of their behaviour begins to wane and eventually becomes hard to spot.</p><p> Third, this research only studies variation in parenting behaviour that was common among the families studied.</p><p> And fourth, research on international adoptions shows they can cause massive improvements in health, income and other outcomes.</p><p> But the findings are still remarkable, and imply many hyper-diligent parents could live much less stressful lives without doing their kids any harm at all. In this extensive interview Rob interrogates whether Bryan can really be right, or whether the research he's drawing on has taken a wrong turn somewhere.</p><p> And that's just one topic we cover, some of the others being:</p><p> • People’s biggest misconceptions about the labour market<br> • Arguments against open borders<br> • Whether most people actually vote based on self-interest<br> • Whether philosophy should stick to common sense or depart from it radically<br> • Personal autonomy vs. the possible benefits of government regulation<br> • Bryan's perfect betting record<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:15)</li><li>Labor Econ Versus the World (00:04:55)</li><li>Open Borders (00:20:30)</li><li>How much parenting matters (00:35:49)</li><li>Self-Interested Voter Hypothesis (01:00:31)</li><li>Why Bryan and Rob disagree so much on philosophy (01:12:04)</li><li>Libertarian free will (01:25:10)</li><li>The effective altruism community (01:38:46)</li><li>Bryan’s betting record (01:48:19)</li><li>Individual autonomy vs. welfare (01:59:06)</li><li>Arrogant hedgehogs (02:10:43)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Everybody knows that good parenting has a big impact on how kids turn out. Except that maybe they don't, because it doesn't.</p><p>Incredible though it might seem, according to today's guest — economist Bryan Caplan, the author of Selfish Reasons To Have More Kids, The Myth of the Rational Voter, and The Case Against Education — the best evidence we have on the question suggests that, within reason, what parents do has little impact on how their children's lives play out once they're adults.</p><p><a href="https://80k.link/BC"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Of course, kids do resemble their parents. But just as we probably can't say it was attentive parenting that gave me my mother's nose, perhaps we can't say it was attentive parenting that made me succeed at school. Both the social environment we grow up in and the genes we receive from our parents influence the person we become, and looking at a typical family we can't really distinguish the impact of one from the other.</p><p> But nature does offer us up a random experiment that can let us tell the difference: identical twins share all their genes, while fraternal twins only share half their genes. If you look at how much more similar outcomes are for identical twins than fraternal twins, you see the effect of sharing 100% of your genetic material, rather than the usual 50%. Double that amount, and you've got the full effect of genetic inheritance. Whatever unexplained variation remains is still up for grabs — and might be down to different experiences in the home, outside the home, or just random noise.</p><p> The crazy thing about this research is that it says for a range of adult outcomes (e.g. years of education, income, health, personality, and happiness), it's differences in the genes children inherit rather than differences in parental behaviour that are doing most of the work. Other research suggests that differences in “out-of-home environment” take second place. Parenting style does matter for something, but it comes in a clear third.</p><p> Bryan is quick to point out that there are several factors that help reconcile these findings with conventional wisdom about the importance of parenting.</p><p> First, for some adult outcomes, parenting was a big deal (i.e. the quality of the parent/child relationship) or at least a moderate deal (i.e. drug use, criminality, and religious/political identity).</p><p> Second, parents can and do influence you quite a lot — so long as you're young and still living with them. But as soon as you move out, the influence of their behaviour begins to wane and eventually becomes hard to spot.</p><p> Third, this research only studies variation in parenting behaviour that was common among the families studied.</p><p> And fourth, research on international adoptions shows they can cause massive improvements in health, income and other outcomes.</p><p> But the findings are still remarkable, and imply many hyper-diligent parents could live much less stressful lives without doing their kids any harm at all. In this extensive interview Rob interrogates whether Bryan can really be right, or whether the research he's drawing on has taken a wrong turn somewhere.</p><p> And that's just one topic we cover, some of the others being:</p><p> • People’s biggest misconceptions about the labour market<br> • Arguments against open borders<br> • Whether most people actually vote based on self-interest<br> • Whether philosophy should stick to common sense or depart from it radically<br> • Personal autonomy vs. the possible benefits of government regulation<br> • Bryan's perfect betting record<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:15)</li><li>Labor Econ Versus the World (00:04:55)</li><li>Open Borders (00:20:30)</li><li>How much parenting matters (00:35:49)</li><li>Self-Interested Voter Hypothesis (01:00:31)</li><li>Why Bryan and Rob disagree so much on philosophy (01:12:04)</li><li>Libertarian free will (01:25:10)</li><li>The effective altruism community (01:38:46)</li><li>Bryan’s betting record (01:48:19)</li><li>Individual autonomy vs. welfare (01:59:06)</li><li>Arrogant hedgehogs (02:10:43)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 05 Apr 2022 22:39:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/43661453/459e22bc.mp3" length="64926409" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/BItBrVQKzwc6d4GdrpSRBEixj7kVfQCFRv5xNLg4x9c/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Nzcv/MTY4MzU0NDcxNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8116</itunes:duration>
      <itunes:summary>Everybody knows that good parenting has a big impact on how kids turn out. Except that maybe they don't, because it doesn't. 

Incredible though it might seem, according to today's guest — economist Bryan Caplan, the author of Selfish Reasons To Have More Kids, The Myth of the Rational Voter, and The Case Against Education — the best evidence we have on the question suggests that, within reason, what parents do has little impact on how their children's lives play out once they're adults. 

Links to learn more, summary and full transcript.

Of course, kids do resemble their parents. But just as we probably can't say it was attentive parenting that gave me my mother's nose, perhaps we can't say it was attentive parenting that made me succeed at school. Both the social environment we grow up in and the genes we receive from our parents influence the person we become, and looking at a typical family we can't really distinguish the impact of one from the other. 

But nature does offer us up a random experiment that can let us tell the difference: identical twins share all their genes, while fraternal twins only share half their genes. If you look at how much more similar outcomes are for identical twins than fraternal twins, you see the effect of sharing 100% of your genetic material, rather than the usual 50%. Double that amount, and you've got the full effect of genetic inheritance. Whatever unexplained variation remains is still up for grabs — and might be down to different experiences in the home, outside the home, or just random noise. 

The crazy thing about this research is that it says for a range of adult outcomes (e.g. years of education, income, health, personality, and happiness), it's differences in the genes children inherit rather than differences in parental behaviour that are doing most of the work. Other research suggests that differences in “out-of-home environment” take second place. Parenting style does matter for something, but it comes in a clear third. 

Bryan is quick to point out that there are several factors that help reconcile these findings with conventional wisdom about the importance of parenting. 

First, for some adult outcomes, parenting was a big deal (i.e. the quality of the parent/child relationship) or at least a moderate deal (i.e. drug use, criminality, and religious/political identity). 

Second, parents can and do influence you quite a lot — so long as you're young and still living with them. But as soon as you move out, the influence of their behaviour begins to wane and eventually becomes hard to spot. 

Third, this research only studies variation in parenting behaviour that was common among the families studied. 

And fourth, research on international adoptions shows they can cause massive improvements in health, income and other outcomes. 

But the findings are still remarkable, and imply many hyper-diligent parents could live much less stressful lives without doing their kids any harm at all. In this extensive interview Rob interrogates whether Bryan can really be right, or whether the research he's drawing on has taken a wrong turn somewhere. 

And that's just one topic we cover, some of the others being: 

• People’s biggest misconceptions about the labour market 
• Arguments against open borders 
• Whether most people actually vote based on self-interest 
• Whether philosophy should stick to common sense or depart from it radically 
• Personal autonomy vs. the possible benefits of government regulation 
• Bryan's perfect betting record 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Everybody knows that good parenting has a big impact on how kids turn out. Except that maybe they don't, because it doesn't. 

Incredible though it might seem, according to today's guest — economist Bryan Caplan, the author of Selfish Reasons To Have Mo</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/43661453/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/43661453/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#125 – Joan Rohlfing on how to avoid catastrophic nuclear blunders</title>
      <itunes:title>#125 – Joan Rohlfing on how to avoid catastrophic nuclear blunders</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ae1d0dba-afa1-11ec-8e83-123c46dd90f1</guid>
      <link>https://80000hours.org/podcast/episodes/joan-rohlfing-avoiding-catastrophic-nuclear-blunders/</link>
      <description>
        <![CDATA[<p>Since the Soviet Union split into different countries in 1991, the pervasive fear of catastrophe that people lived with for decades has gradually faded from memory, and nuclear warhead stockpiles have declined by 83%. Nuclear brinksmanship, proxy wars, and the game theory of mutually assured destruction (MAD) have come to feel like relics of another era.</p><p> Russia's invasion of Ukraine has changed all that.</p><p> According to Joan Rohlfing — President of the Nuclear Threat Initiative, a Washington, DC-based nonprofit focused on reducing threats from nuclear and biological weapons — the annual risk of a ‘global catastrophic nuclear event'’ never fell as low as people like to think, and for some time has been on its way back up.</p><p> <a href="https://80k.link/JR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> At the same time, civil society funding for research and advocacy around nuclear risks is being cut in half over a period of years — despite the fact that at $60 million a year, it was already just a thousandth as much as the US spends maintaining its nuclear deterrent.</p><p> If new funding sources are not identified to replace donors that are withdrawing, the existing pool of talent will have to leave for greener pastures, and most of the next generation will see a career in the field as unviable.</p><p> While global poverty is on the decline and life expectancy increasing, the chance of a catastrophic nuclear event is probably trending in the wrong direction.</p><p> Ukraine gave up its nuclear weapons in 1994 in exchange for security guarantees that turned out not to be worth the paper they were written on. States that have nuclear weapons (such as North Korea), states that are pursuing them (such as Iran), and states that have pursued nuclear weapons but since abandoned them (such as Libya, Syria, and South Africa) may take this as a valuable lesson in the importance of military power over promises.</p><p> China has been expanding its arsenal and testing hypersonic glide missiles that can evade missile defences. Japan now toys with the idea of nuclear weapons as a way to ensure its security against its much larger neighbour. India and Pakistan both acquired nuclear weapons in the late 1980s and their relationship continues to oscillate from hostile to civil and back.</p><p> At the same time, the risk that nuclear weapons could be interfered with due to weaknesses in computer security is far higher than during the Cold War, when systems were simpler and less networked.</p><p> In the interview, Joan discusses several steps that can be taken in the immediate term, such as renewed efforts to extend and expand arms control treaties, changes to nuclear use policy, and the retirement of what they see as vulnerable delivery systems, such as land-based silos.</p><p> In the bigger picture, NTI seeks to keep hope alive that a better system than deterrence through mutually assured destruction remains possible. The threat of retaliation does indeed make nuclear wars unlikely, but it necessarily means the system fails in an incredibly destructive way: with the death of hundreds of millions if not billions.</p><p> In the long run, even a tiny 1 in 500 risk of a nuclear war each year adds up to around an 18% chance of catastrophe over the century.</p><p> In this conversation we cover all that, as well as:</p><p> • How arms control treaties have evolved over the last few decades<br> • Whether lobbying by arms manufacturers is an important factor shaping nuclear strategy<br> • The Biden Nuclear Posture Review<br> • How easily humanity might recover from a nuclear exchange<br> • Implications for the use of nuclear energy</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>Joan’s EAG presentation (00:01:40)</li><li>The interview begins (00:27:06)</li><li>Nuclear security funding situation (00:31:09)</li><li>Policy solutions for addressing a one-person or one-state risk factor (00:36:46)</li><li>Key differences in the nuclear security field (00:40:44)</li><li>Scary scenarios (00:47:02)</li><li>Why the US shouldn’t expand its nuclear arsenal (00:52:56)</li><li>The evolution of nuclear risk over the last 10 years (01:03:41)</li><li>The interaction between nuclear weapons and cybersecurity (01:10:18)</li><li>The chances of humanity bouncing back after nuclear war (01:13:52)</li><li>What we should actually do (01:17:57)</li><li>Could sensors be a game-changer? (01:22:39)</li><li>Biden Nuclear Posture Review (01:27:50)</li><li>Influence of lobbying firms (01:33:58)</li><li>What NTI might do with an additional $20 million (01:36:38)</li><li>Nuclear energy tradeoffs (01:43:55)</li><li>Why we can’t rely on Stanislav Petrovs (01:49:49)</li><li>Preventing war vs. building resilience for recovery (01:52:15)</li><li>Places to donate other than NTI (01:54:25)</li><li>Career advice (02:00:15)</li><li>Why this problem is solvable (02:09:27)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Since the Soviet Union split into different countries in 1991, the pervasive fear of catastrophe that people lived with for decades has gradually faded from memory, and nuclear warhead stockpiles have declined by 83%. Nuclear brinksmanship, proxy wars, and the game theory of mutually assured destruction (MAD) have come to feel like relics of another era.</p><p> Russia's invasion of Ukraine has changed all that.</p><p> According to Joan Rohlfing — President of the Nuclear Threat Initiative, a Washington, DC-based nonprofit focused on reducing threats from nuclear and biological weapons — the annual risk of a ‘global catastrophic nuclear event'’ never fell as low as people like to think, and for some time has been on its way back up.</p><p> <a href="https://80k.link/JR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> At the same time, civil society funding for research and advocacy around nuclear risks is being cut in half over a period of years — despite the fact that at $60 million a year, it was already just a thousandth as much as the US spends maintaining its nuclear deterrent.</p><p> If new funding sources are not identified to replace donors that are withdrawing, the existing pool of talent will have to leave for greener pastures, and most of the next generation will see a career in the field as unviable.</p><p> While global poverty is on the decline and life expectancy increasing, the chance of a catastrophic nuclear event is probably trending in the wrong direction.</p><p> Ukraine gave up its nuclear weapons in 1994 in exchange for security guarantees that turned out not to be worth the paper they were written on. States that have nuclear weapons (such as North Korea), states that are pursuing them (such as Iran), and states that have pursued nuclear weapons but since abandoned them (such as Libya, Syria, and South Africa) may take this as a valuable lesson in the importance of military power over promises.</p><p> China has been expanding its arsenal and testing hypersonic glide missiles that can evade missile defences. Japan now toys with the idea of nuclear weapons as a way to ensure its security against its much larger neighbour. India and Pakistan both acquired nuclear weapons in the late 1980s and their relationship continues to oscillate from hostile to civil and back.</p><p> At the same time, the risk that nuclear weapons could be interfered with due to weaknesses in computer security is far higher than during the Cold War, when systems were simpler and less networked.</p><p> In the interview, Joan discusses several steps that can be taken in the immediate term, such as renewed efforts to extend and expand arms control treaties, changes to nuclear use policy, and the retirement of what they see as vulnerable delivery systems, such as land-based silos.</p><p> In the bigger picture, NTI seeks to keep hope alive that a better system than deterrence through mutually assured destruction remains possible. The threat of retaliation does indeed make nuclear wars unlikely, but it necessarily means the system fails in an incredibly destructive way: with the death of hundreds of millions if not billions.</p><p> In the long run, even a tiny 1 in 500 risk of a nuclear war each year adds up to around an 18% chance of catastrophe over the century.</p><p> In this conversation we cover all that, as well as:</p><p> • How arms control treaties have evolved over the last few decades<br> • Whether lobbying by arms manufacturers is an important factor shaping nuclear strategy<br> • The Biden Nuclear Posture Review<br> • How easily humanity might recover from a nuclear exchange<br> • Implications for the use of nuclear energy</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>Joan’s EAG presentation (00:01:40)</li><li>The interview begins (00:27:06)</li><li>Nuclear security funding situation (00:31:09)</li><li>Policy solutions for addressing a one-person or one-state risk factor (00:36:46)</li><li>Key differences in the nuclear security field (00:40:44)</li><li>Scary scenarios (00:47:02)</li><li>Why the US shouldn’t expand its nuclear arsenal (00:52:56)</li><li>The evolution of nuclear risk over the last 10 years (01:03:41)</li><li>The interaction between nuclear weapons and cybersecurity (01:10:18)</li><li>The chances of humanity bouncing back after nuclear war (01:13:52)</li><li>What we should actually do (01:17:57)</li><li>Could sensors be a game-changer? (01:22:39)</li><li>Biden Nuclear Posture Review (01:27:50)</li><li>Influence of lobbying firms (01:33:58)</li><li>What NTI might do with an additional $20 million (01:36:38)</li><li>Nuclear energy tradeoffs (01:43:55)</li><li>Why we can’t rely on Stanislav Petrovs (01:49:49)</li><li>Preventing war vs. building resilience for recovery (01:52:15)</li><li>Places to donate other than NTI (01:54:25)</li><li>Career advice (02:00:15)</li><li>Why this problem is solvable (02:09:27)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 29 Mar 2022 23:15:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ec3ffa3a/8d326224.mp3" length="64174842" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/wK-OGYHl041aOCDIf4UHOGZMQ7UFKX_u10PaVGH9GvQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzYv/MTY4MzU0NDcxNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8022</itunes:duration>
      <itunes:summary>Since the Soviet Union split into different countries in 1991, the pervasive fear of catastrophe that people lived with for decades has gradually faded from memory, and nuclear warhead stockpiles have declined by 83%. Nuclear brinksmanship, proxy wars, and the game theory of mutually assured destruction (MAD) have come to feel like relics of another era. 

Russia's invasion of Ukraine has changed all that. 

According to Joan Rohlfing — President of the Nuclear Threat Initiative, a Washington, DC-based nonprofit focused on reducing threats from nuclear and biological weapons — the annual risk of a ‘global catastrophic nuclear event'’ never fell as low as people like to think, and for some time has been on its way back up. 

Links to learn more, summary and full transcript. 

At the same time, civil society funding for research and advocacy around nuclear risks is being cut in half over a period of years — despite the fact that at $60 million a year, it was already just a thousandth as much as the US spends maintaining its nuclear deterrent. 

If new funding sources are not identified to replace donors that are withdrawing, the existing pool of talent will have to leave for greener pastures, and most of the next generation will see a career in the field as unviable. 

While global poverty is on the decline and life expectancy increasing, the chance of a catastrophic nuclear event is probably trending in the wrong direction. 

Ukraine gave up its nuclear weapons in 1994 in exchange for security guarantees that turned out not to be worth the paper they were written on. States that have nuclear weapons (such as North Korea), states that are pursuing them (such as Iran), and states that have pursued nuclear weapons but since abandoned them (such as Libya, Syria, and South Africa) may take this as a valuable lesson in the importance of military power over promises. 

China has been expanding its arsenal and testing hypersonic glide missiles that can evade missile defences. Japan now toys with the idea of nuclear weapons as a way to ensure its security against its much larger neighbour. India and Pakistan both acquired nuclear weapons in the late 1980s and their relationship continues to oscillate from hostile to civil and back. 

At the same time, the risk that nuclear weapons could be interfered with due to weaknesses in computer security is far higher than during the Cold War, when systems were simpler and less networked. 

In the interview, Joan discusses several steps that can be taken in the immediate term, such as renewed efforts to extend and expand arms control treaties, changes to nuclear use policy, and the retirement of what they see as vulnerable delivery systems, such as land-based silos. 

In the bigger picture, NTI seeks to keep hope alive that a better system than deterrence through mutually assured destruction remains possible. The threat of retaliation does indeed make nuclear wars unlikely, but it necessarily means the system fails in an incredibly destructive way: with the death of hundreds of millions if not billions. 

In the long run, even a tiny 1 in 500 risk of a nuclear war each year adds up to around an 18% chance of catastrophe over the century. 

In this conversation we cover all that, as well as: 

• How arms control treaties have evolved over the last few decades 
• Whether lobbying by arms manufacturers is an important factor shaping nuclear strategy 
• The Biden Nuclear Posture Review 
• How easily humanity might recover from a nuclear exchange 
• Implications for the use of nuclear energy 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Since the Soviet Union split into different countries in 1991, the pervasive fear of catastrophe that people lived with for decades has gradually faded from memory, and nuclear warhead stockpiles have declined by 83%. Nuclear brinksmanship, proxy wars, an</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/ec3ffa3a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/ec3ffa3a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#124 – Karen Levy on fads and misaligned incentives in global development, and scaling deworming to reach hundreds of millions</title>
      <itunes:title>#124 – Karen Levy on fads and misaligned incentives in global development, and scaling deworming to reach hundreds of millions</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6d8642c2-a93d-11ec-92ca-0e6ef95f4ed7</guid>
      <link>https://share.transistor.fm/s/2b5e412f</link>
      <description>
        <![CDATA[If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations for Poverty Action, Evidence Action, and Y Combinator — each of those three concepts has become so fashionable that they're at risk of being seriously overrated and applied where they don't belong.<p> 

<a href="https://80k.link/KL"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

Such concepts might even cause harm — trying to make a project embody all three is as likely to ruin it as help it flourish.</p><p> 

First, what do people mean by 'sustainability'? Usually they mean something like the programme will eventually be able to continue without needing further financial support from the donor. But how is that possible? Governments, nonprofits, and aid agencies aim to provide health services, education, infrastructure, financial services, and so on — and all of these require ongoing funding to pay for materials and staff to keep them running.</p><p> 

Given that someone needs to keep paying, Karen tells us that in practice, 'sustainability' is usually a euphemism for the programme at some point being passed on to someone else to fund — usually the national government. And while that can be fine, the national government of Kenya only spends $400 per person to provide each and every government service — just 2% of what the US spends on each resident. Incredibly tight budgets like that are typical of low-income countries.</p><p> 

'Participatory' also sounds nice, and inasmuch as it means leaders are accountable to the people they're trying to help, it probably is. But Karen tells us that in the field, ‘participatory’ usually means that recipients are expected to be involved in planning and delivering services themselves.</p><p> 

While that might be suitable in some situations, it's hardly something people in rich countries always want for themselves. Ideally we want government healthcare and education to be high quality without us having to attend meetings to keep it on track — and people in poor countries have as many or more pressures on their time. While accountability is desirable, an expectation of participation can be as much a burden as a blessing.</p><p> 

Finally, making a programme 'holistic' could be smart, but as Karen lays out, it also has some major downsides. For one, it means you're doing lots of things at once, which makes it hard to tell which parts of the project are making the biggest difference relative to their cost. For another, when you have a lot of goals at once, it's hard to tell whether you're making progress, or really put your mind to focusing on making one thing go extremely well. And finally, holistic programmes can be impractically expensive — Karen tells the story of a wonderful 'holistic school health' programme that, if continued, was going to cost 3.5 times the entire school's budget.</p><p> 

In today's in-depth conversation, Karen Levy and I chat about the above, as well as:</p><p> 

• Why it pays to figure out how you'll interpret the results of an experiment ahead of time<br> 
• The trouble with misaligned incentives within the development industry<br> 
• Projects that don't deliver value for money and should be scaled down<br> 
• How Karen accidentally became a leading figure in the push to deworm tens of millions of schoolchildren<br> 
• Logistical challenges in reaching huge numbers of people with essential services<br> 
• Lessons from Karen's many-decades career<br> 
• And much more</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell and Ryan Kessler<br>
Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations for Poverty Action, Evidence Action, and Y Combinator — each of those three concepts has become so fashionable that they're at risk of being seriously overrated and applied where they don't belong.<p> 

<a href="https://80k.link/KL"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

Such concepts might even cause harm — trying to make a project embody all three is as likely to ruin it as help it flourish.</p><p> 

First, what do people mean by 'sustainability'? Usually they mean something like the programme will eventually be able to continue without needing further financial support from the donor. But how is that possible? Governments, nonprofits, and aid agencies aim to provide health services, education, infrastructure, financial services, and so on — and all of these require ongoing funding to pay for materials and staff to keep them running.</p><p> 

Given that someone needs to keep paying, Karen tells us that in practice, 'sustainability' is usually a euphemism for the programme at some point being passed on to someone else to fund — usually the national government. And while that can be fine, the national government of Kenya only spends $400 per person to provide each and every government service — just 2% of what the US spends on each resident. Incredibly tight budgets like that are typical of low-income countries.</p><p> 

'Participatory' also sounds nice, and inasmuch as it means leaders are accountable to the people they're trying to help, it probably is. But Karen tells us that in the field, ‘participatory’ usually means that recipients are expected to be involved in planning and delivering services themselves.</p><p> 

While that might be suitable in some situations, it's hardly something people in rich countries always want for themselves. Ideally we want government healthcare and education to be high quality without us having to attend meetings to keep it on track — and people in poor countries have as many or more pressures on their time. While accountability is desirable, an expectation of participation can be as much a burden as a blessing.</p><p> 

Finally, making a programme 'holistic' could be smart, but as Karen lays out, it also has some major downsides. For one, it means you're doing lots of things at once, which makes it hard to tell which parts of the project are making the biggest difference relative to their cost. For another, when you have a lot of goals at once, it's hard to tell whether you're making progress, or really put your mind to focusing on making one thing go extremely well. And finally, holistic programmes can be impractically expensive — Karen tells the story of a wonderful 'holistic school health' programme that, if continued, was going to cost 3.5 times the entire school's budget.</p><p> 

In today's in-depth conversation, Karen Levy and I chat about the above, as well as:</p><p> 

• Why it pays to figure out how you'll interpret the results of an experiment ahead of time<br> 
• The trouble with misaligned incentives within the development industry<br> 
• Projects that don't deliver value for money and should be scaled down<br> 
• How Karen accidentally became a leading figure in the push to deworm tens of millions of schoolchildren<br> 
• Logistical challenges in reaching huge numbers of people with essential services<br> 
• Lessons from Karen's many-decades career<br> 
• And much more</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell and Ryan Kessler<br>
Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 21 Mar 2022 19:39:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/2b5e412f/945d6e14.mp3" length="91144589" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/nGwwQ-W8Hj-yWoPxzn16mFikYJAdEXBR5H6TNlNoTZ8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzUv/MTY4MzU0NDcxNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11393</itunes:duration>
      <itunes:summary>If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations for Poverty Action, Evidence Action, and Y Combinator — each of those three concepts has become so fashionable that they're at risk of being seriously overrated and applied where they don't belong. 

Links to learn more, summary and full transcript. 

Such concepts might even cause harm — trying to make a project embody all three is as likely to ruin it as help it flourish. 

First, what do people mean by 'sustainability'? Usually they mean something like the programme will eventually be able to continue without needing further financial support from the donor. But how is that possible? Governments, nonprofits, and aid agencies aim to provide health services, education, infrastructure, financial services, and so on — and all of these require ongoing funding to pay for materials and staff to keep them running. 

Given that someone needs to keep paying, Karen tells us that in practice, 'sustainability' is usually a euphemism for the programme at some point being passed on to someone else to fund — usually the national government. And while that can be fine, the national government of Kenya only spends $400 per person to provide each and every government service — just 2% of what the US spends on each resident. Incredibly tight budgets like that are typical of low-income countries. 

'Participatory' also sounds nice, and inasmuch as it means leaders are accountable to the people they're trying to help, it probably is. But Karen tells us that in the field, ‘participatory’ usually means that recipients are expected to be involved in planning and delivering services themselves. 

While that might be suitable in some situations, it's hardly something people in rich countries always want for themselves. Ideally we want government healthcare and education to be high quality without us having to attend meetings to keep it on track — and people in poor countries have as many or more pressures on their time. While accountability is desirable, an expectation of participation can be as much a burden as a blessing. 

Finally, making a programme 'holistic' could be smart, but as Karen lays out, it also has some major downsides. For one, it means you're doing lots of things at once, which makes it hard to tell which parts of the project are making the biggest difference relative to their cost. For another, when you have a lot of goals at once, it's hard to tell whether you're making progress, or really put your mind to focusing on making one thing go extremely well. And finally, holistic programmes can be impractically expensive — Karen tells the story of a wonderful 'holistic school health' programme that, if continued, was going to cost 3.5 times the entire school's budget. 

In today's in-depth conversation, Karen Levy and I chat about the above, as well as: 

• Why it pays to figure out how you'll interpret the results of an experiment ahead of time 
• The trouble with misaligned incentives within the development industry 
• Projects that don't deliver value for money and should be scaled down 
• How Karen accidentally became a leading figure in the push to deworm tens of millions of schoolchildren 
• Logistical challenges in reaching huge numbers of people with essential services 
• Lessons from Karen's many-decades career 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell and Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If someone said a global health and development programme was sustainable, participatory, and holistic, you'd have to guess that they were saying something positive. But according to today's guest Karen Levy — deworming pioneer and veteran of Innovations </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/2b5e412f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#123 – Samuel Charap on why Putin invaded Ukraine, the risk of escalation, and how to prevent disaster</title>
      <itunes:title>#123 – Samuel Charap on why Putin invaded Ukraine, the risk of escalation, and how to prevent disaster</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e035cfe6-a3b5-11ec-a7e0-0e751d636955</guid>
      <link>https://80000hours.org/podcast/episodes/samuel-charap-why-putin-invaded-ukraine/</link>
      <description>
        <![CDATA[<p>Russia's invasion of Ukraine is devastating the lives of Ukrainians, and so long as it continues there's a risk that the conflict could escalate to include other countries or the use of nuclear weapons. It's essential that NATO, the US, and the EU play their cards right to ideally end the violence, maintain Ukrainian sovereignty, and discourage any similar invasions in the future.</p><p> But how? To pull together the most valuable information on how to react to this crisis, we spoke with Samuel Charap — a senior political scientist at the RAND Corporation, one of the US's foremost experts on Russia's relationship with former Soviet states, and co-author of <em>Everyone Loses: The Ukraine Crisis and the Ruinous Contest for Post-Soviet Eurasia</em>.</p><p> <a href="https://80k.link/SC"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Samuel believes that Putin views the alignment of Ukraine with NATO as an existential threat to Russia — a perhaps unreasonable view, but a sincere one nevertheless. Ukraine has been drifting further into Western Europe's orbit and improving its defensive military capabilities, so Putin has concluded that if Russia wants to put a stop to that, there will never be a better time to act in the future.</p><p> Despite early successes holding off the Russian military, Samuel is sceptical that time is on the Ukrainian side. If the war is to end before much of Ukraine is reduced to rubble, it will likely have to be through negotiation, rather than Russian defeat.</p><p> The US policy response has so far been largely good, successfully balancing the need to punish Russia to dissuade large nations from bullying small ones in the future, while preventing NATO from being drawn into the war directly — which would pose a horrifying risk of escalation to a full nuclear exchange. The pressure from the general public to 'do something' might eventually cause national leaders to confront Russia more directly, but so far they are sensibly showing no interest in doing so.</p><p> However, use of nuclear weapons remains a low but worrying possibility.</p><p> Samuel is also worried that Russia may deploy chemical and biological weapons and blame it on the Ukrainians.</p><p> Before war broke out, it's possible Russia could have been satisfied if Ukraine followed through on the Minsk agreements and committed not to join the EU and NATO. Or it might not have, if Putin was committed to war, come what may. In any case, most Ukrainians found those terms intolerable.</p><p> At this point, the situation is even worse, and it's hard to see how an enduring ceasefire could be agreed upon. On top of the above, Russia is also demanding recognition that Crimea is part of Russia, and acceptance of the independence of the so-called Donetsk and Luhansk People's Republics. These conditions — especially the second — are entirely unacceptable to the Ukrainians. Hence the war continues, and could grind on for months or even years until one side is sufficiently beaten down to compromise on their core demands.</p><p> Rob and Samuel discuss all of the above and also:</p><p> • The chances that this conflict leads to a nuclear exchange<br> • The chances of regime change in Russia<br> • Whether the West should deliver MiG fighter jets to Ukraine<br> • What are the implications if Sweden and/or Finland decide to join NATO?<br> • What should NATO do now, and did it make any mistakes in the past?<br> • What's the most likely situation for us to be looking at in three months' time?<br> • Can Ukraine effectively win the war?</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:40)</li><li>Putin's true motive (00:02:29)</li><li>What the West could have done differently (00:07:44)</li><li>Chances of Ukraine holding out (00:11:40)</li><li>Chances of regime change in Russia (00:14:59)</li><li>The good and the bad from the West so far (00:17:55)</li><li>Should the West deliver MiG fighter jets to Ukraine? (00:19:57)</li><li>"No-fly zones" (00:21:32)</li><li>Chances that this conflict leads to a nuclear exchange (00:26:06)</li><li>What listeners should do (00:36:01)</li><li>Chances of biological or chemical weapons use (00:37:59)</li><li>Best realistic outcome from here (00:39:29)</li><li>Keeping the broader conversation sane (00:49:29)</li><li>Why not promise to remove sanctions? (00:51:05)</li><li>Pros and cons of Sweden and FInland joining NATO (00:52:53)</li><li>The most likely situation in 3 months (00:53:58)</li></ul><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Russia's invasion of Ukraine is devastating the lives of Ukrainians, and so long as it continues there's a risk that the conflict could escalate to include other countries or the use of nuclear weapons. It's essential that NATO, the US, and the EU play their cards right to ideally end the violence, maintain Ukrainian sovereignty, and discourage any similar invasions in the future.</p><p> But how? To pull together the most valuable information on how to react to this crisis, we spoke with Samuel Charap — a senior political scientist at the RAND Corporation, one of the US's foremost experts on Russia's relationship with former Soviet states, and co-author of <em>Everyone Loses: The Ukraine Crisis and the Ruinous Contest for Post-Soviet Eurasia</em>.</p><p> <a href="https://80k.link/SC"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Samuel believes that Putin views the alignment of Ukraine with NATO as an existential threat to Russia — a perhaps unreasonable view, but a sincere one nevertheless. Ukraine has been drifting further into Western Europe's orbit and improving its defensive military capabilities, so Putin has concluded that if Russia wants to put a stop to that, there will never be a better time to act in the future.</p><p> Despite early successes holding off the Russian military, Samuel is sceptical that time is on the Ukrainian side. If the war is to end before much of Ukraine is reduced to rubble, it will likely have to be through negotiation, rather than Russian defeat.</p><p> The US policy response has so far been largely good, successfully balancing the need to punish Russia to dissuade large nations from bullying small ones in the future, while preventing NATO from being drawn into the war directly — which would pose a horrifying risk of escalation to a full nuclear exchange. The pressure from the general public to 'do something' might eventually cause national leaders to confront Russia more directly, but so far they are sensibly showing no interest in doing so.</p><p> However, use of nuclear weapons remains a low but worrying possibility.</p><p> Samuel is also worried that Russia may deploy chemical and biological weapons and blame it on the Ukrainians.</p><p> Before war broke out, it's possible Russia could have been satisfied if Ukraine followed through on the Minsk agreements and committed not to join the EU and NATO. Or it might not have, if Putin was committed to war, come what may. In any case, most Ukrainians found those terms intolerable.</p><p> At this point, the situation is even worse, and it's hard to see how an enduring ceasefire could be agreed upon. On top of the above, Russia is also demanding recognition that Crimea is part of Russia, and acceptance of the independence of the so-called Donetsk and Luhansk People's Republics. These conditions — especially the second — are entirely unacceptable to the Ukrainians. Hence the war continues, and could grind on for months or even years until one side is sufficiently beaten down to compromise on their core demands.</p><p> Rob and Samuel discuss all of the above and also:</p><p> • The chances that this conflict leads to a nuclear exchange<br> • The chances of regime change in Russia<br> • Whether the West should deliver MiG fighter jets to Ukraine<br> • What are the implications if Sweden and/or Finland decide to join NATO?<br> • What should NATO do now, and did it make any mistakes in the past?<br> • What's the most likely situation for us to be looking at in three months' time?<br> • Can Ukraine effectively win the war?</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:40)</li><li>Putin's true motive (00:02:29)</li><li>What the West could have done differently (00:07:44)</li><li>Chances of Ukraine holding out (00:11:40)</li><li>Chances of regime change in Russia (00:14:59)</li><li>The good and the bad from the West so far (00:17:55)</li><li>Should the West deliver MiG fighter jets to Ukraine? (00:19:57)</li><li>"No-fly zones" (00:21:32)</li><li>Chances that this conflict leads to a nuclear exchange (00:26:06)</li><li>What listeners should do (00:36:01)</li><li>Chances of biological or chemical weapons use (00:37:59)</li><li>Best realistic outcome from here (00:39:29)</li><li>Keeping the broader conversation sane (00:49:29)</li><li>Why not promise to remove sanctions? (00:51:05)</li><li>Pros and cons of Sweden and FInland joining NATO (00:52:53)</li><li>The most likely situation in 3 months (00:53:58)</li></ul><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 14 Mar 2022 18:27:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8e387973/c2327bd1.mp3" length="28454215" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/GUjDtfiMCEleSt-Ld-RAJHrlD1gXheIK2lml65CfcxY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzQv/MTY4MzU0NDcxMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3557</itunes:duration>
      <itunes:summary>Russia's invasion of Ukraine is devastating the lives of Ukrainians, and so long as it continues there's a risk that the conflict could escalate to include other countries or the use of nuclear weapons. It's essential that NATO, the US, and the EU play their cards right to ideally end the violence, maintain Ukrainian sovereignty, and discourage any similar invasions in the future. 

But how? To pull together the most valuable information on how to react to this crisis, we spoke with Samuel Charap — a senior political scientist at the RAND Corporation, one of the US's foremost experts on Russia's relationship with former Soviet states, and co-author of Everyone Loses: The Ukraine Crisis and the Ruinous Contest for Post-Soviet Eurasia. 

Links to learn more, summary and full transcript. 

Samuel believes that Putin views the alignment of Ukraine with NATO as an existential threat to Russia — a perhaps unreasonable view, but a sincere one nevertheless. Ukraine has been drifting further into Western Europe's orbit and improving its defensive military capabilities, so Putin has concluded that if Russia wants to put a stop to that, there will never be a better time to act in the future. 

Despite early successes holding off the Russian military, Samuel is sceptical that time is on the Ukrainian side. If the war is to end before much of Ukraine is reduced to rubble, it will likely have to be through negotiation, rather than Russian defeat. 

The US policy response has so far been largely good, successfully balancing the need to punish Russia to dissuade large nations from bullying small ones in the future, while preventing NATO from being drawn into the war directly — which would pose a horrifying risk of escalation to a full nuclear exchange. The pressure from the general public to 'do something' might eventually cause national leaders to confront Russia more directly, but so far they are sensibly showing no interest in doing so. 

However, use of nuclear weapons remains a low but worrying possibility. 

Samuel is also worried that Russia may deploy chemical and biological weapons and blame it on the Ukrainians. 
 
Before war broke out, it's possible Russia could have been satisfied if Ukraine followed through on the Minsk agreements and committed not to join the EU and NATO. Or it might not have, if Putin was committed to war, come what may. In any case, most Ukrainians found those terms intolerable. 

At this point, the situation is even worse, and it's hard to see how an enduring ceasefire could be agreed upon. On top of the above, Russia is also demanding recognition that Crimea is part of Russia, and acceptance of the independence of the so-called Donetsk and Luhansk People's Republics. These conditions — especially the second — are entirely unacceptable to the Ukrainians. Hence the war continues, and could grind on for months or even years until one side is sufficiently beaten down to compromise on their core demands. 

Rob and Samuel discuss all of the above and also: 

• The chances that this conflict leads to a nuclear exchange 
• The chances of regime change in Russia 
• Whether the West should deliver MiG fighter jets to Ukraine 
• What are the implications if Sweden and/or Finland decide to join NATO? 
• What should NATO do now, and did it make any mistakes in the past? 
• What's the most likely situation for us to be looking at in three months' time? 
• Can Ukraine effectively win the war? 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type ‘80,000 Hours’ into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Russia's invasion of Ukraine is devastating the lives of Ukrainians, and so long as it continues there's a risk that the conflict could escalate to include other countries or the use of nuclear weapons. It's essential that NATO, the US, and the EU play th</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/8e387973/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/8e387973/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#122 – Michelle Hutchinson &amp; Habiba Islam on balancing competing priorities and other themes from our 1-on-1 careers advising</title>
      <itunes:title>#122 – Michelle Hutchinson &amp; Habiba Islam on balancing competing priorities and other themes from our 1-on-1 careers advising</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">517d0d08-9fca-11ec-a674-12ac0b050c53</guid>
      <link>https://80000hours.org/podcast/episodes/michelle-hutchinson-habiba-islam-themes-from-careers-advising/</link>
      <description>
        <![CDATA[<p>One of 80,000 Hours' main services is our <a href="https://80000hours.org/speak-with-us/?int_campaign=podcast__michelle-habiba"><strong>free one-on-one careers advising</strong></a>, which we provide to around 1,000 people a year. Today we speak to two of our advisors, who have each spoken to hundreds of people -- including many regular listeners to this show -- about how they might be able to do more good while also having a highly motivating career.</p><p> Before joining 80,000 Hours, Michelle Hutchinson completed a PhD in Philosophy at Oxford University and helped launch Oxford's Global Priorities Institute, while Habiba Islam studied politics, philosophy, and economics at Oxford University and qualified as a barrister.</p><p> <a href="https://80000hours.org/podcast/episodes/michelle-hutchinson-habiba-islam-themes-from-careers-advising/?utm_campaign=podcast__michelle-habiba&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In this conversation, they cover many topics that recur in their advising calls, and what they've learned from watching advisees’ careers play out:</p><p> • What they say when advisees want to help solve overpopulation<br> • How to balance doing good against other priorities that people have for their lives<br> • Why it's challenging to motivate yourself to focus on the long-term future of humanity, and how Michelle and Habiba do so nonetheless<br> • How they use our latest guide to planning your career<br> • Why you can specialise and take more risk if you're in a group<br> • Gaps in the effective altruism community it would be really useful for people to fill<br> • Stories of people who have spoken to 80,000 Hours and changed their career — and whether it went well or not<br> • Why trying to have impact in multiple different ways can be a mistake</p><p> The episode is split into two parts: the first section on <em>The 80,000 Hours Podcast</em>, and the second on our new show <a href="https://80000hours.org/after-hours-podcast/"><strong><em>80k After Hours</em></strong></a>. This is a shameless attempt to encourage listeners to our first show to subscribe to our second feed.</p><p> That <a href="https://80000hours.org/podcast/episodes/80k-after-hours-michelle-habiba-advice-for-younger-selves"><strong>second part</strong></a> covers:</p><p> • Whether just encouraging someone young to aspire to more than they currently are is one of the most impactful ways to spend half an hour<br> • How much impact the one-on-one team has, the biggest challenges they face as a group, and different paths they could have gone down<br> • Whether giving general advice is a doomed enterprise</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:24)</li><li>Cause prioritization (00:09:14)</li><li>Unexpected outcomes from 1-1 advice (00:18:10)</li><li>Making time for thinking about these things (00:22:28)</li><li>Balancing different priorities in life (00:26:54)</li><li>Gaps in the effective altruism space (00:32:06)</li><li>Plan change vignettes (00:37:49)</li><li>How large a role the 1-1 team is playing (00:49:04)</li><li>What about when our advice didn’t work out? (00:55:50)</li><li>The process of planning a career (00:59:05)</li><li>Why longtermism is hard (01:05:49)</li></ul><p><br> <em>Want to get free one-on-one advice from our team? We're here to help.</em></p><p> We’ve helped thousands of people formulate their plans and put them in touch with mentors.</p><p> We've expanded our ability to deliver one-on-one meetings so are keen to help more people than ever before. <strong><em>If you're a regular listener to the show we're especially likely to want to speak with you.</em></strong><em></em></p><p> <a href="https://80000hours.org/speak-with-us/?int_campaign=podcast__michelle-habiba"><strong><em>Learn about and apply for advising.</em></strong></a></p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>One of 80,000 Hours' main services is our <a href="https://80000hours.org/speak-with-us/?int_campaign=podcast__michelle-habiba"><strong>free one-on-one careers advising</strong></a>, which we provide to around 1,000 people a year. Today we speak to two of our advisors, who have each spoken to hundreds of people -- including many regular listeners to this show -- about how they might be able to do more good while also having a highly motivating career.</p><p> Before joining 80,000 Hours, Michelle Hutchinson completed a PhD in Philosophy at Oxford University and helped launch Oxford's Global Priorities Institute, while Habiba Islam studied politics, philosophy, and economics at Oxford University and qualified as a barrister.</p><p> <a href="https://80000hours.org/podcast/episodes/michelle-hutchinson-habiba-islam-themes-from-careers-advising/?utm_campaign=podcast__michelle-habiba&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In this conversation, they cover many topics that recur in their advising calls, and what they've learned from watching advisees’ careers play out:</p><p> • What they say when advisees want to help solve overpopulation<br> • How to balance doing good against other priorities that people have for their lives<br> • Why it's challenging to motivate yourself to focus on the long-term future of humanity, and how Michelle and Habiba do so nonetheless<br> • How they use our latest guide to planning your career<br> • Why you can specialise and take more risk if you're in a group<br> • Gaps in the effective altruism community it would be really useful for people to fill<br> • Stories of people who have spoken to 80,000 Hours and changed their career — and whether it went well or not<br> • Why trying to have impact in multiple different ways can be a mistake</p><p> The episode is split into two parts: the first section on <em>The 80,000 Hours Podcast</em>, and the second on our new show <a href="https://80000hours.org/after-hours-podcast/"><strong><em>80k After Hours</em></strong></a>. This is a shameless attempt to encourage listeners to our first show to subscribe to our second feed.</p><p> That <a href="https://80000hours.org/podcast/episodes/80k-after-hours-michelle-habiba-advice-for-younger-selves"><strong>second part</strong></a> covers:</p><p> • Whether just encouraging someone young to aspire to more than they currently are is one of the most impactful ways to spend half an hour<br> • How much impact the one-on-one team has, the biggest challenges they face as a group, and different paths they could have gone down<br> • Whether giving general advice is a doomed enterprise</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:24)</li><li>Cause prioritization (00:09:14)</li><li>Unexpected outcomes from 1-1 advice (00:18:10)</li><li>Making time for thinking about these things (00:22:28)</li><li>Balancing different priorities in life (00:26:54)</li><li>Gaps in the effective altruism space (00:32:06)</li><li>Plan change vignettes (00:37:49)</li><li>How large a role the 1-1 team is playing (00:49:04)</li><li>What about when our advice didn’t work out? (00:55:50)</li><li>The process of planning a career (00:59:05)</li><li>Why longtermism is hard (01:05:49)</li></ul><p><br> <em>Want to get free one-on-one advice from our team? We're here to help.</em></p><p> We’ve helped thousands of people formulate their plans and put them in touch with mentors.</p><p> We've expanded our ability to deliver one-on-one meetings so are keen to help more people than ever before. <strong><em>If you're a regular listener to the show we're especially likely to want to speak with you.</em></strong><em></em></p><p> <a href="https://80000hours.org/speak-with-us/?int_campaign=podcast__michelle-habiba"><strong><em>Learn about and apply for advising.</em></strong></a></p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 09 Mar 2022 17:39:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/081605d5/fd2e7f4a.mp3" length="46288315" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-KbffanVe2BkeNj4hLafDAzBHHGTY1dKAFcy-V9J6DU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzMv/MTY4MzU0NDcxMS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5786</itunes:duration>
      <itunes:summary>One of 80,000 Hours' main services is our free one-on-one careers advising, which we provide to around 1,000 people a year. Today we speak to two of our advisors, who have each spoken to hundreds of people -- including many regular listeners to this show -- about how they might be able to do more good while also having a highly motivating career. 

Before joining 80,000 Hours, Michelle Hutchinson completed a PhD in Philosophy at Oxford University and helped launch Oxford's Global Priorities Institute, while Habiba Islam studied politics, philosophy, and economics at Oxford University and qualified as a barrister. 

Links to learn more, summary and full transcript.

In this conversation, they cover many topics that recur in their advising calls, and what they've learned from watching advisees’ careers play out: 

• What they say when advisees want to help solve overpopulation 
• How to balance doing good against other priorities that people have for their lives 
• Why it's challenging to motivate yourself to focus on the long-term future of humanity, and how Michelle and Habiba do so nonetheless 
• How they use our latest guide to planning your career 
• Why you can specialise and take more risk if you're in a group 
• Gaps in the effective altruism community it would be really useful for people to fill 
• Stories of people who have spoken to 80,000 Hours and changed their career — and whether it went well or not 
• Why trying to have impact in multiple different ways can be a mistake 

The episode is split into two parts: the first section on The 80,000 Hours Podcast, and the second on our new show 80k After Hours. This is a shameless attempt to encourage listeners to our first show to subscribe to our second feed. 

That second part covers: 

• Whether just encouraging someone young to aspire to more than they currently are is one of the most impactful ways to spend half an hour 
• How much impact the one-on-one team has, the biggest challenges they face as a group, and different paths they could have gone down 
• Whether giving general advice is a doomed enterprise 

Get this second part by subscribing to our more experimental podcast on the world’s most pressing problems and how to solve them: type 80k After Hours into your podcasting app.

Want to get free one-on-one advice from our team? We're here to help. 
 
We’ve helped thousands of people formulate their plans and put them in touch with mentors. 

We've expanded our ability to deliver one-on-one meetings so are keen to help more people than ever before. If you're a regular listener to the show we're especially likely to want to speak with you. 

Learn about and apply for advising. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>One of 80,000 Hours' main services is our free one-on-one careers advising, which we provide to around 1,000 people a year. Today we speak to two of our advisors, who have each spoken to hundreds of people -- including many regular listeners to this show </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/081605d5/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/081605d5/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Introducing 80k After Hours</title>
      <itunes:title>Introducing 80k After Hours</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">0a6cbc16-98b2-11ec-a059-0ee1a17bfc41</guid>
      <link>https://share.transistor.fm/s/4928d0c7</link>
      <description>
        <![CDATA[Today we're launching a new podcast called <em><a href="https://80000hours.org/after-hours-podcast/?utm_campaign=podcast__80kah-podcastwords&amp;utm_source=80kah&amp;utm_medium=podcast"><b>80k After Hours</b></a></em>.<p> 

Like this show it’ll mostly still explore the best ways to do good — and some episodes will be even more laser-focused on careers than most original episodes.</p><p> 

But we’re also going to widen our scope, including things like how to solve pressing problems while also living a happy and fulfilling life, as well as releases that are just fun, entertaining or experimental.</p><p> 

It’ll feature:</p><p> 

</p><ul>
<li>Conversations between staff on <a href="https://80000hours.org/about/meet-the-team/">the 80,000 Hours team</a></li>
<li>More eclectic formats and topics — one episode could be a structured debate about 'human challenge trials', the next a staged reading of a play about the year 2750</li> 
<li>Niche content for specific audiences, such as high-school students, or active participants in the <a href="https://effectivealtruism.org/">effective altruism</a> community</li>
<li>Extras and outtakes from interviews on <a href="https://80000hours.org/podcast/">the original feed</a></li>
<li>80,000 Hours staff interviewed on other podcasts</li>
<li>Audio versions of our new <a href="https://80000hours.org/research/">articles and research</a></li>
</ul>

You can find it by searching for 80k After Hours in whatever podcasting app you use, or by going to   <em><a href="https://80000hours.org/after-hours-podcast/?utm_campaign=podcast__80kah-podcastwords&amp;utm_source=80kah&amp;utm_medium=podcast"><b>80000hours.org/after-hours-podcast</b></a></em>.]]>
      </description>
      <content:encoded>
        <![CDATA[Today we're launching a new podcast called <em><a href="https://80000hours.org/after-hours-podcast/?utm_campaign=podcast__80kah-podcastwords&amp;utm_source=80kah&amp;utm_medium=podcast"><b>80k After Hours</b></a></em>.<p> 

Like this show it’ll mostly still explore the best ways to do good — and some episodes will be even more laser-focused on careers than most original episodes.</p><p> 

But we’re also going to widen our scope, including things like how to solve pressing problems while also living a happy and fulfilling life, as well as releases that are just fun, entertaining or experimental.</p><p> 

It’ll feature:</p><p> 

</p><ul>
<li>Conversations between staff on <a href="https://80000hours.org/about/meet-the-team/">the 80,000 Hours team</a></li>
<li>More eclectic formats and topics — one episode could be a structured debate about 'human challenge trials', the next a staged reading of a play about the year 2750</li> 
<li>Niche content for specific audiences, such as high-school students, or active participants in the <a href="https://effectivealtruism.org/">effective altruism</a> community</li>
<li>Extras and outtakes from interviews on <a href="https://80000hours.org/podcast/">the original feed</a></li>
<li>80,000 Hours staff interviewed on other podcasts</li>
<li>Audio versions of our new <a href="https://80000hours.org/research/">articles and research</a></li>
</ul>

You can find it by searching for 80k After Hours in whatever podcasting app you use, or by going to   <em><a href="https://80000hours.org/after-hours-podcast/?utm_campaign=podcast__80kah-podcastwords&amp;utm_source=80kah&amp;utm_medium=podcast"><b>80000hours.org/after-hours-podcast</b></a></em>.]]>
      </content:encoded>
      <pubDate>Tue, 01 Mar 2022 18:04:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4928d0c7/8a4e5619.mp3" length="6482658" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/d7RObcaYNCajfe2n7l6HDMvFDk8h17FQtPMjkYTHuXQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzIv/MTY4MzU0NDcxMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>811</itunes:duration>
      <itunes:summary>Today we're launching a new podcast called 80k After Hours. 

Like this show it’ll mostly still explore the best ways to do good — and some episodes will be even more laser-focused on careers than most original episodes. 

But we’re also going to widen our scope, including things like how to solve pressing problems while also living a happy and fulfilling life, as well as releases that are just fun, entertaining or experimental. 

It’ll feature: 


Conversations between staff on the 80,000 Hours team
More eclectic formats and topics — one episode could be a structured debate about 'human challenge trials', the next a staged reading of a play about the year 2750 
Niche content for specific audiences, such as high-school students, or active participants in the effective altruism community
Extras and outtakes from interviews on the original feed
80,000 Hours staff interviewed on other podcasts
Audio versions of our new articles and research


You can find it by searching for 80k After Hours in whatever podcasting app you use, or by going to   80000hours.org/after-hours-podcast.</itunes:summary>
      <itunes:subtitle>Today we're launching a new podcast called 80k After Hours. 

Like this show it’ll mostly still explore the best ways to do good — and some episodes will be even more laser-focused on careers than most original episodes. 

But we’re also going to widen ou</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/4928d0c7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#121 – Matthew Yglesias on avoiding the pundit's fallacy and how much military intervention can be used for good</title>
      <itunes:title>#121 – Matthew Yglesias on avoiding the pundit's fallacy and how much military intervention can be used for good</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b81d31a8-8f4b-11ec-a809-1200ddd38e37</guid>
      <link>https://80000hours.org/podcast/episodes/matthew-yglesias-pundits-fallacy-military-intervention/</link>
      <description>
        <![CDATA[<p>If you read polls saying that the public supports a carbon tax, should you believe them? According to today's guest — journalist and blogger Matthew Yglesias — it's complicated, but probably not.</p><p> <a href="https://80k.link/MY"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Interpreting opinion polls about specific policies can be a challenge, and it's easy to trick yourself into believing what you want to believe. Matthew invented a term for a particular type of self-delusion called the 'pundit's fallacy': "the belief that what a politician needs to do to improve his or her political standing is do what the pundit wants substantively."</p><p> If we want to advocate not just for ideas that would be good if implemented, but ideas that have a real shot at getting implemented, we should do our best to understand public opinion as it really is.</p><p> The least trustworthy polls are published by think tanks and advocacy campaigns that would love to make their preferred policy seem popular. These surveys can be designed to nudge respondents toward the desired result — for example, by tinkering with question wording and order or shifting how participants are sampled. And if a poll produces the 'wrong answer', there's no need to publish it at all, so the 'publication bias' with these sorts of surveys is large.</p><p> Matthew says polling run by firms or researchers without any particular desired outcome can be taken more seriously. But the results that we ought to give by far the most weight are those from professional political campaigns trying to win votes and get their candidate elected because they have both the expertise to do polling properly, and a very strong incentive to understand what the public really thinks.</p><p> The problem is, campaigns run these expensive surveys because they think that having exclusive access to reliable information will give them a competitive advantage. As a result, they often don’t publish the findings, and instead use them to shape what their candidate says and does.</p><p> Journalists like Matthew can call up their contacts and get a summary from people they trust. But being unable to publish the polling itself, they're unlikely to be able to persuade sceptics.</p><p> When assessing what ideas are winners, one thing Matthew would like everyone to keep in mind is that politics is competitive, and politicians aren't (all) stupid. If advocating for your pet idea were a great way to win elections, someone would try it and win, and others would copy.</p><p> One other thing to check that's more reliable than polling is real-world experience. For example, voters may say they like a carbon tax on the phone — but the very liberal Washington State roundly rejected one in ballot initiatives in 2016 and 2018.</p><p> Of course you may want to advocate for what you think is best, even if it wouldn't pass a popular vote in the face of organised opposition. The public's ideas can shift, sometimes dramatically and unexpectedly. But at least you'll be going into the debate with your eyes wide open.</p><p> In this extensive conversation, host Rob Wiblin and Matthew also cover:</p><p> • How should a humanitarian think about US military interventions overseas?<br> • From an 'effective altruist' perspective, was the US wrong to withdraw from Afghanistan?<br> • Has NATO ultimately screwed over Ukrainians by misrepresenting the extent of its commitment to their independence?<br> • What philosopher does Matthew think is underrated?<br> • How big a risk is ubiquitous surveillance?<br> • What does Matthew think about wild animal suffering, anti-ageing research, and autonomous weapons?<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:05)</li><li>Autonomous weapons (00:04:42)</li><li>India and the US (00:07:25)</li><li>Evidence-backed interventions for reducing the harm done by racial prejudices (00:08:38)</li><li>Factory farming (00:10:44)</li><li>Wild animal suffering (00:12:41)</li><li>Vaccine development (00:15:20)</li><li>Anti-ageing research (00:16:27)</li><li>Should the US develop a semiconductor industry? (00:19:13)</li><li>What we should do about various existential risks (00:21:58)</li><li>What governments should do to stop the next pandemic (00:24:00)</li><li>Comets and supervolcanoes (00:31:30)</li><li>Nuclear weapons (00:34:25)</li><li>Advances in AI (00:35:46)</li><li>Surveillance systems (00:38:45)</li><li>How Matt thinks about public opinion research (00:43:22)</li><li>Issues with trusting public opinion polls (00:51:18)</li><li>The influence of prior beliefs (01:05:53)</li><li>Loss aversion (01:12:19)</li><li>Matt's take on military adventurism (01:18:54)</li><li>How military intervention looks as a humanitarian intervention (01:29:12)</li><li>Where Matt does favour military intervention (01:38:27)</li><li>Why smart people disagree (01:44:24)</li><li>The case for NATO taking an active stance in Ukraine (01:57:34)</li><li>One Billion Americans (02:08:02)</li><li>Matt’s views on the effective altruism community (02:11:46)</li><li>Matt’s views on the longtermist community (02:19:48)</li><li>Matt’s struggle to become more of a rationalist (02:22:42)</li><li>Megaprojects (02:26:20)</li><li>The impact of Matt’s work (02:32:28)</li><li>Matt’s philosophical views (02:47:58)</li><li>The value of formal education (02:56:59)</li><li>Worst thing Matt’s ever advocated for (03:02:25)</li><li>Rob’s outro (03:03:22)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If you read polls saying that the public supports a carbon tax, should you believe them? According to today's guest — journalist and blogger Matthew Yglesias — it's complicated, but probably not.</p><p> <a href="https://80k.link/MY"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Interpreting opinion polls about specific policies can be a challenge, and it's easy to trick yourself into believing what you want to believe. Matthew invented a term for a particular type of self-delusion called the 'pundit's fallacy': "the belief that what a politician needs to do to improve his or her political standing is do what the pundit wants substantively."</p><p> If we want to advocate not just for ideas that would be good if implemented, but ideas that have a real shot at getting implemented, we should do our best to understand public opinion as it really is.</p><p> The least trustworthy polls are published by think tanks and advocacy campaigns that would love to make their preferred policy seem popular. These surveys can be designed to nudge respondents toward the desired result — for example, by tinkering with question wording and order or shifting how participants are sampled. And if a poll produces the 'wrong answer', there's no need to publish it at all, so the 'publication bias' with these sorts of surveys is large.</p><p> Matthew says polling run by firms or researchers without any particular desired outcome can be taken more seriously. But the results that we ought to give by far the most weight are those from professional political campaigns trying to win votes and get their candidate elected because they have both the expertise to do polling properly, and a very strong incentive to understand what the public really thinks.</p><p> The problem is, campaigns run these expensive surveys because they think that having exclusive access to reliable information will give them a competitive advantage. As a result, they often don’t publish the findings, and instead use them to shape what their candidate says and does.</p><p> Journalists like Matthew can call up their contacts and get a summary from people they trust. But being unable to publish the polling itself, they're unlikely to be able to persuade sceptics.</p><p> When assessing what ideas are winners, one thing Matthew would like everyone to keep in mind is that politics is competitive, and politicians aren't (all) stupid. If advocating for your pet idea were a great way to win elections, someone would try it and win, and others would copy.</p><p> One other thing to check that's more reliable than polling is real-world experience. For example, voters may say they like a carbon tax on the phone — but the very liberal Washington State roundly rejected one in ballot initiatives in 2016 and 2018.</p><p> Of course you may want to advocate for what you think is best, even if it wouldn't pass a popular vote in the face of organised opposition. The public's ideas can shift, sometimes dramatically and unexpectedly. But at least you'll be going into the debate with your eyes wide open.</p><p> In this extensive conversation, host Rob Wiblin and Matthew also cover:</p><p> • How should a humanitarian think about US military interventions overseas?<br> • From an 'effective altruist' perspective, was the US wrong to withdraw from Afghanistan?<br> • Has NATO ultimately screwed over Ukrainians by misrepresenting the extent of its commitment to their independence?<br> • What philosopher does Matthew think is underrated?<br> • How big a risk is ubiquitous surveillance?<br> • What does Matthew think about wild animal suffering, anti-ageing research, and autonomous weapons?<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:05)</li><li>Autonomous weapons (00:04:42)</li><li>India and the US (00:07:25)</li><li>Evidence-backed interventions for reducing the harm done by racial prejudices (00:08:38)</li><li>Factory farming (00:10:44)</li><li>Wild animal suffering (00:12:41)</li><li>Vaccine development (00:15:20)</li><li>Anti-ageing research (00:16:27)</li><li>Should the US develop a semiconductor industry? (00:19:13)</li><li>What we should do about various existential risks (00:21:58)</li><li>What governments should do to stop the next pandemic (00:24:00)</li><li>Comets and supervolcanoes (00:31:30)</li><li>Nuclear weapons (00:34:25)</li><li>Advances in AI (00:35:46)</li><li>Surveillance systems (00:38:45)</li><li>How Matt thinks about public opinion research (00:43:22)</li><li>Issues with trusting public opinion polls (00:51:18)</li><li>The influence of prior beliefs (01:05:53)</li><li>Loss aversion (01:12:19)</li><li>Matt's take on military adventurism (01:18:54)</li><li>How military intervention looks as a humanitarian intervention (01:29:12)</li><li>Where Matt does favour military intervention (01:38:27)</li><li>Why smart people disagree (01:44:24)</li><li>The case for NATO taking an active stance in Ukraine (01:57:34)</li><li>One Billion Americans (02:08:02)</li><li>Matt’s views on the effective altruism community (02:11:46)</li><li>Matt’s views on the longtermist community (02:19:48)</li><li>Matt’s struggle to become more of a rationalist (02:22:42)</li><li>Megaprojects (02:26:20)</li><li>The impact of Matt’s work (02:32:28)</li><li>Matt’s philosophical views (02:47:58)</li><li>The value of formal education (02:56:59)</li><li>Worst thing Matt’s ever advocated for (03:02:25)</li><li>Rob’s outro (03:03:22)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 16 Feb 2022 17:23:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/eb4f3c20/a0a0fe89.mp3" length="88464586" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/BB24Uxka1-X_qQZAGd4CEIIi0lgyw3xS7gahf1LQlgc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzEv/MTY4MzU0NDcwOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11058</itunes:duration>
      <itunes:summary>If you read polls saying that the public supports a carbon tax, should you believe them? According to today's guest — journalist and blogger Matthew Yglesias — it's complicated, but probably not. 

Links to learn more, summary and full transcript.

Interpreting opinion polls about specific policies can be a challenge, and it's easy to trick yourself into believing what you want to believe. Matthew invented a term for a particular type of self-delusion called the 'pundit's fallacy': "the belief that what a politician needs to do to improve his or her political standing is do what the pundit wants substantively." 

If we want to advocate not just for ideas that would be good if implemented, but ideas that have a real shot at getting implemented, we should do our best to understand public opinion as it really is. 

The least trustworthy polls are published by think tanks and advocacy campaigns that would love to make their preferred policy seem popular. These surveys can be designed to nudge respondents toward the desired result — for example, by tinkering with question wording and order or shifting how participants are sampled. And if a poll produces the 'wrong answer', there's no need to publish it at all, so the 'publication bias' with these sorts of surveys is large. 

Matthew says polling run by firms or researchers without any particular desired outcome can be taken more seriously. But the results that we ought to give by far the most weight are those from professional political campaigns trying to win votes and get their candidate elected because they have both the expertise to do polling properly, and a very strong incentive to understand what the public really thinks. 

The problem is, campaigns run these expensive surveys because they think that having exclusive access to reliable information will give them a competitive advantage. As a result, they often don’t publish the findings, and instead use them to shape what their candidate says and does. 

Journalists like Matthew can call up their contacts and get a summary from people they trust. But being unable to publish the polling itself, they're unlikely to be able to persuade sceptics. 

When assessing what ideas are winners, one thing Matthew would like everyone to keep in mind is that politics is competitive, and politicians aren't (all) stupid. If advocating for your pet idea were a great way to win elections, someone would try it and win, and others would copy. 

One other thing to check that's more reliable than polling is real-world experience. For example, voters may say they like a carbon tax on the phone — but the very liberal Washington State roundly rejected one in ballot initiatives in 2016 and 2018. 

Of course you may want to advocate for what you think is best, even if it wouldn't pass a popular vote in the face of organised opposition. The public's ideas can shift, sometimes dramatically and unexpectedly. But at least you'll be going into the debate with your eyes wide open. 

In this extensive conversation, host Rob Wiblin and Matthew also cover: 

• How should a humanitarian think about US military interventions overseas? 
• From an 'effective altruist' perspective, was the US wrong to withdraw from Afghanistan? 
• Has NATO ultimately screwed over Ukrainians by misrepresenting the extent of its commitment to their independence? 
• What philosopher does Matthew think is underrated? 
• How big a risk is ubiquitous surveillance? 
• What does Matthew think about wild animal suffering, anti-ageing research, and autonomous weapons? 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If you read polls saying that the public supports a carbon tax, should you believe them? According to today's guest — journalist and blogger Matthew Yglesias — it's complicated, but probably not. 

Links to learn more, summary and full transcript.

In</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/eb4f3c20/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/eb4f3c20/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#120 – Audrey Tang on what we can learn from Taiwan’s experiments with how to do democracy</title>
      <itunes:title>#120 – Audrey Tang on what we can learn from Taiwan’s experiments with how to do democracy</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">bc335652-8477-11ec-8a7e-12010ffac149</guid>
      <link>https://80000hours.org/podcast/episodes/audrey-tang-what-we-can-learn-from-taiwan/</link>
      <description>
        <![CDATA[<p>In 2014 Taiwan was rocked by mass protests against a proposed trade agreement with China that was about to be agreed without the usual Parliamentary hearings. Students invaded and took over the Parliament. But rather than chant slogans, instead they livestreamed their own parliamentary debate over the trade deal, allowing volunteers to speak both in favour and against.</p><p>Instead of polarising the country more, this so-called 'Sunflower Student Movement' ultimately led to a bipartisan consensus that Taiwan should open up its government. That process has gradually made it one of the most communicative and interactive administrations anywhere in the world.</p><p>Today's guest — programming prodigy Audrey Tang — initially joined the student protests to help get their streaming infrastructure online. After the students got the official hearings they wanted and went home, she was invited to consult for the government. And when the government later changed hands, she was invited to work in the ministry herself.</p><p><a href="https://80k.link/AT"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> During six years as the country's 'Digital Minister' she has been helping Taiwan increase the flow of information between institutions and civil society and launched original experiments trying to make democracy itself work better.</p><p> That includes developing new tools to identify points of consensus between groups that mostly disagree, building social media platforms optimised for discussing policy issues, helping volunteers fight disinformation by making their own memes, and allowing the public to build their own alternatives to government websites whenever they don't like how they currently work.</p><p> As part of her ministerial role Audrey also sets aside time each week to help online volunteers working on government-related tech projects get the help they need. How does she decide who to help? She doesn't — that decision is made by members of an online community who upvote the projects they think are best.</p><p> According to Audrey, a more collaborative mentality among the country's leaders has helped increase public trust in government, and taught bureaucrats that they can (usually) trust the public in return.</p><p> Innovations in Taiwan may offer useful lessons to people who want to improve humanity's ability to make decisions and get along in large groups anywhere in the world. We cover:</p><p> • Why it makes sense to treat Facebook as a nightclub<br> • The value of having no reply button, and of getting more specific when you disagree<br> • Quadratic voting and funding<br> • Audrey’s experiences with the Sunflower Student Movement<br> • Technologies Audrey is most excited about<br> • Conservative anarchism<br> • What Audrey’s day-to-day work looks like<br> • Whether it’s ethical to eat oysters<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:04)</li><li>Global crisis of confidence in government (00:07:06)</li><li>Treating Facebook as a nightclub (00:10:55)</li><li>Polis (00:13:48)</li><li>The value of having no reply button (00:24:33)</li><li>The value of getting more specific (00:26:13)</li><li>Concerns with Polis (00:30:40)</li><li>Quadratic voting and funding (00:42:16)</li><li>Sunflower Student Movement (00:55:24)</li><li>Promising technologies (01:05:44)</li><li>Conservative anarchism (01:22:21)</li><li>What Audrey’s day-to-day work looks like (01:33:54)</li><li>Taiwanese politics (01:46:03)</li><li>G0v (01:50:09)</li><li>Rob’s outro (02:05:09)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In 2014 Taiwan was rocked by mass protests against a proposed trade agreement with China that was about to be agreed without the usual Parliamentary hearings. Students invaded and took over the Parliament. But rather than chant slogans, instead they livestreamed their own parliamentary debate over the trade deal, allowing volunteers to speak both in favour and against.</p><p>Instead of polarising the country more, this so-called 'Sunflower Student Movement' ultimately led to a bipartisan consensus that Taiwan should open up its government. That process has gradually made it one of the most communicative and interactive administrations anywhere in the world.</p><p>Today's guest — programming prodigy Audrey Tang — initially joined the student protests to help get their streaming infrastructure online. After the students got the official hearings they wanted and went home, she was invited to consult for the government. And when the government later changed hands, she was invited to work in the ministry herself.</p><p><a href="https://80k.link/AT"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> During six years as the country's 'Digital Minister' she has been helping Taiwan increase the flow of information between institutions and civil society and launched original experiments trying to make democracy itself work better.</p><p> That includes developing new tools to identify points of consensus between groups that mostly disagree, building social media platforms optimised for discussing policy issues, helping volunteers fight disinformation by making their own memes, and allowing the public to build their own alternatives to government websites whenever they don't like how they currently work.</p><p> As part of her ministerial role Audrey also sets aside time each week to help online volunteers working on government-related tech projects get the help they need. How does she decide who to help? She doesn't — that decision is made by members of an online community who upvote the projects they think are best.</p><p> According to Audrey, a more collaborative mentality among the country's leaders has helped increase public trust in government, and taught bureaucrats that they can (usually) trust the public in return.</p><p> Innovations in Taiwan may offer useful lessons to people who want to improve humanity's ability to make decisions and get along in large groups anywhere in the world. We cover:</p><p> • Why it makes sense to treat Facebook as a nightclub<br> • The value of having no reply button, and of getting more specific when you disagree<br> • Quadratic voting and funding<br> • Audrey’s experiences with the Sunflower Student Movement<br> • Technologies Audrey is most excited about<br> • Conservative anarchism<br> • What Audrey’s day-to-day work looks like<br> • Whether it’s ethical to eat oysters<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:04)</li><li>Global crisis of confidence in government (00:07:06)</li><li>Treating Facebook as a nightclub (00:10:55)</li><li>Polis (00:13:48)</li><li>The value of having no reply button (00:24:33)</li><li>The value of getting more specific (00:26:13)</li><li>Concerns with Polis (00:30:40)</li><li>Quadratic voting and funding (00:42:16)</li><li>Sunflower Student Movement (00:55:24)</li><li>Promising technologies (01:05:44)</li><li>Conservative anarchism (01:22:21)</li><li>What Audrey’s day-to-day work looks like (01:33:54)</li><li>Taiwanese politics (01:46:03)</li><li>G0v (01:50:09)</li><li>Rob’s outro (02:05:09)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 02 Feb 2022 22:45:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5996da09/ef45b8f4.mp3" length="60410601" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/h5uPC7SXne7Oo425xwhOlM0J9Sda7FMxbmr0rO5tP5E/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NzAv/MTY4MzU0NDcwOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7551</itunes:duration>
      <itunes:summary>In 2014 Taiwan was rocked by mass protests against a proposed trade agreement with China that was about to be agreed without the usual Parliamentary hearings. Students invaded and took over the Parliament. But rather than chant slogans, instead they livestreamed their own parliamentary debate over the trade deal, allowing volunteers to speak both in favour and against. 

Instead of polarising the country more, this so-called 'Sunflower Student Movement' ultimately led to a bipartisan consensus that Taiwan should open up its government. That process has gradually made it one of the most communicative and interactive administrations anywhere in the world. 

Today's guest — programming prodigy Audrey Tang — initially joined the student protests to help get their streaming infrastructure online. After the students got the official hearings they wanted and went home, she was invited to consult for the government. And when the government later changed hands, she was invited to work in the ministry herself. 

Links to learn more, summary and full transcript.

During six years as the country's 'Digital Minister' she has been helping Taiwan increase the flow of information between institutions and civil society and launched original experiments trying to make democracy itself work better. 

That includes developing new tools to identify points of consensus between groups that mostly disagree, building social media platforms optimised for discussing policy issues, helping volunteers fight disinformation by making their own memes, and allowing the public to build their own alternatives to government websites whenever they don't like how they currently work. 

As part of her ministerial role Audrey also sets aside time each week to help online volunteers working on government-related tech projects get the help they need. How does she decide who to help? She doesn't — that decision is made by members of an online community who upvote the projects they think are best. 

According to Audrey, a more collaborative mentality among the country's leaders has helped increase public trust in government, and taught bureaucrats that they can (usually) trust the public in return. 

Innovations in Taiwan may offer useful lessons to people who want to improve humanity's ability to make decisions and get along in large groups anywhere in the world. We cover: 

• Why it makes sense to treat Facebook as a nightclub 
• The value of having no reply button, and of getting more specific when you disagree 
• Quadratic voting and funding 
• Audrey’s experiences with the Sunflower Student Movement 
• Technologies Audrey is most excited about 
• Conservative anarchism 
• What Audrey’s day-to-day work looks like 
• Whether it’s ethical to eat oysters 
• And much more 

Check out two current job opportunities at 80,000 Hours: Advisor and Head of Job Board. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>In 2014 Taiwan was rocked by mass protests against a proposed trade agreement with China that was about to be agreed without the usual Parliamentary hearings. Students invaded and took over the Parliament. But rather than chant slogans, instead they lives</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/5996da09/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/5996da09/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#43 Classic episode - Daniel Ellsberg on the institutional insanity that maintains nuclear doomsday machines</title>
      <itunes:title>#43 Classic episode - Daniel Ellsberg on the institutional insanity that maintains nuclear doomsday machines</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4ace3368-786c-11ec-b80c-0e427baaad99</guid>
      <link>https://share.transistor.fm/s/e1152a3c</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in September 2018.</b><p>In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. </p><p>

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his book <a href="https://www.amazon.com/Doomsday-Machine-Confessions-Nuclear-Planner/dp/1608196704/">The Doomsday Machine: Confessions of a Nuclear War Planner</a> that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.</p><p>
<a href="https://80000hours.org/podcast/episodes/daniel-ellsberg-doomsday-machines/?utm_campaign=podcast__classic-daniel-ellsberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. </p><p>

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.</p><p>

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.</p><p>

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.</p><p>

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.</p><p>

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.</p><p>

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.</p><p>

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.</p><p>

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.</p><p>
Strategically, the setup is stupid. Ethically, it is monstrous.</p><p>
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?</p><p>

Daniel explores these questions eloquently and urgently in his book. Today we cover:</p><p>

• Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold<br>
• How well are secrets kept in the government?<br>
• What was the risk of the first atomic bomb test?<br>
• Do we have a reliable estimate of the magnitude of a ‘nuclear winter’?</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in September 2018.</b><p>In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. </p><p>

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his book <a href="https://www.amazon.com/Doomsday-Machine-Confessions-Nuclear-Planner/dp/1608196704/">The Doomsday Machine: Confessions of a Nuclear War Planner</a> that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.</p><p>
<a href="https://80000hours.org/podcast/episodes/daniel-ellsberg-doomsday-machines/?utm_campaign=podcast__classic-daniel-ellsberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. </p><p>

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.</p><p>

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.</p><p>

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.</p><p>

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.</p><p>

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.</p><p>

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.</p><p>

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.</p><p>

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.</p><p>
Strategically, the setup is stupid. Ethically, it is monstrous.</p><p>
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?</p><p>

Daniel explores these questions eloquently and urgently in his book. Today we cover:</p><p>

• Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold<br>
• How well are secrets kept in the government?<br>
• What was the risk of the first atomic bomb test?<br>
• Do we have a reliable estimate of the magnitude of a ‘nuclear winter’?</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 18 Jan 2022 17:21:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e1152a3c/e3c35242.mp3" length="74629087" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/MdO_eFi7AJu6xQJosTnnigNhX1QCuyUXqYfF4iSGaSw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Njkv/MTY4MzU0NDcwNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9328</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in September 2018.In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. 

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his book The Doomsday Machine: Confessions of a Nuclear War Planner that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.
Links to learn more, summary and full transcript.

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. 

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.
Strategically, the setup is stupid. Ethically, it is monstrous.
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?

Daniel explores these questions eloquently and urgently in his book. Today we cover:

• Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold
• How well are secrets kept in the government?
• What was the risk of the first atomic bomb test?
• Do we have a reliable estimate of the magnitude of a ‘nuclear winter’? 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in September 2018.In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanit</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#35 Classic episode - Tara Mac Aulay on the audacity to fix the world without asking permission</title>
      <itunes:title>#35 Classic episode - Tara Mac Aulay on the audacity to fix the world without asking permission</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4141d752-7266-11ec-8787-0e65e71ee8e9</guid>
      <link>https://share.transistor.fm/s/808e1a6d</link>
      <description>
        <![CDATA[<p><b>Rebroadcast: this episode was originally released in June 2018.</b><br></p><p>

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.</p><p>

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.</p><p>

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.</p><p>

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. </p><p>

<b><a href="https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__classic-tara-macaulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></b></p><p>

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.</p><p>

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.</p><p>

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article <a href="https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tara-mac-aulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">'Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:</p><p>

• Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.<br>
• How a student can save a hospital millions with a simple spreadsheet model.<br>
• The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.<br>
• What most people misunderstand about operations, and how to tell if you have what it takes.<br>
• And finally, operations jobs people should consider applying for.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><b>Rebroadcast: this episode was originally released in June 2018.</b><br></p><p>

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.</p><p>

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.</p><p>

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.</p><p>

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. </p><p>

<b><a href="https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__classic-tara-macaulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></b></p><p>

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.</p><p>

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.</p><p>

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article <a href="https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tara-mac-aulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">'Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:</p><p>

• Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.<br>
• How a student can save a hospital millions with a simple spreadsheet model.<br>
• The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.<br>
• What most people misunderstand about operations, and how to tell if you have what it takes.<br>
• And finally, operations jobs people should consider applying for.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 10 Jan 2022 23:17:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/808e1a6d/b0d22384.mp3" length="40107955" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/7gKDV0w0egUs3jGLuHGteaWKZKDo_z6NacBygTaXPoo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Njgv/MTY4MzU0NDcwNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5014</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in June 2018.

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. 

Links to learn more, summary and full transcript.

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article 'Why operations management is one of the biggest bottlenecks in effective altruism’, as well as:

• Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.
• How a student can save a hospital millions with a simple spreadsheet model.
• The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.
• What most people misunderstand about operations, and how to tell if you have what it takes.
• And finally, operations jobs people should consider applying for.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in June 2018.

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.

At 15 she took her first job - an entry-level position a</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#67 Classic episode – David Chalmers on the nature and ethics of consciousness</title>
      <itunes:title>#67 Classic episode – David Chalmers on the nature and ethics of consciousness</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">0722bfd6-6cea-11ec-8ae9-1280f8dbf6ab</guid>
      <link>https://share.transistor.fm/s/9a912693</link>
      <description>
        <![CDATA[<p><b>Rebroadcast: this episode was originally released in December 2019.</b><br></p><p>

What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. </p><p>

Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. </p><p>

The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': </p><p>

"Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?" </p><p>

Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. </p><p>

So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. </p><p>

<a href="https://80k.link/chalmers"><b>Links to learn more, summary and full transcript.</b></a> </p><p> 

Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. </p><p>

Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? </p><p>

Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. </p><p>

Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question <i>'what is consciousness?'</i> to the centre stage of philosophy with his 1996 book <i>'The Conscious Mind'</i>, which argued against then-dominant materialist theories of consciousness. </p><p>

This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. </p><p>

These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? </p><p>

Dave Chalmers is probably the best person on the planet to ask these questions, and Rob &amp; Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far.  </p><p>

<i>Get this episode by subscribing to our show on the world’s most pressing problems and how to solve them: search for 80,000 Hours in your podcasting app.</i> </p><p>

<i>The 80,000 Hours Podcast is produced by Keiran Harris.</i></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><b>Rebroadcast: this episode was originally released in December 2019.</b><br></p><p>

What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. </p><p>

Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. </p><p>

The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': </p><p>

"Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?" </p><p>

Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. </p><p>

So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. </p><p>

<a href="https://80k.link/chalmers"><b>Links to learn more, summary and full transcript.</b></a> </p><p> 

Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. </p><p>

Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? </p><p>

Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. </p><p>

Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question <i>'what is consciousness?'</i> to the centre stage of philosophy with his 1996 book <i>'The Conscious Mind'</i>, which argued against then-dominant materialist theories of consciousness. </p><p>

This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. </p><p>

These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? </p><p>

Dave Chalmers is probably the best person on the planet to ask these questions, and Rob &amp; Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far.  </p><p>

<i>Get this episode by subscribing to our show on the world’s most pressing problems and how to solve them: search for 80,000 Hours in your podcasting app.</i> </p><p>

<i>The 80,000 Hours Podcast is produced by Keiran Harris.</i></p>]]>
      </content:encoded>
      <pubDate>Mon, 03 Jan 2022 23:51:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/9a912693/b37008c7.mp3" length="135396205" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/p4zlCt-HAFSndTqPaZwp9meVvn7MOZDnWRwttViQ0xc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Njcv/MTY4MzU0NDcwNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>16925</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in December 2019.

What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. 

Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. 

The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': 

"Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?" 

Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. 

So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. 

Links to learn more, summary and full transcript.  

Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. 

Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? 

Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. 

Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question 'what is consciousness?' to the centre stage of philosophy with his 1996 book 'The Conscious Mind', which argued against then-dominant materialist theories of consciousness. 

This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. 

These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? 

Dave Chalmers is probably the best person on the planet to ask these questions, and Rob &amp;amp; Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far.  

Get this episode by subscribing to our show on the world’s most pressing problems and how to solve them: search for 80,000 Hours in your podcasting app. 

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in December 2019.

What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/9a912693/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#59 Classic episode - Cass Sunstein on how change happens, and why it's so often abrupt &amp; unpredictable</title>
      <itunes:title>#59 Classic episode - Cass Sunstein on how change happens, and why it's so often abrupt &amp; unpredictable</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1da13248-671c-11ec-a72f-12be2c01527f</guid>
      <link>https://share.transistor.fm/s/3c5d078a</link>
      <description>
        <![CDATA[<p><b>Rebroadcast: this episode was originally released in June 2019.</b><br></p><p>

It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.</p><p>

The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.</p><p>

In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.</p><p>

How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?</p><p>

Sunstein — co-author of <i>Nudge</i>, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book <i>How Change Happens</i>.</p><p>

He pulls together three phenomena which social scientists have studied in recent decades: <i>preference falsification</i>, <i>variable thresholds for action</i>, and <i>group polarisation</i>. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.</p><p><a href="https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/?utm_campaign=podcast__classic-cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.</p><p>

According to Sunstein, it's <i>"much, much easier"</i> to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.</p><p>

In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:</p><p>

• How much people misrepresent their views in democratic countries.<br>
• Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.<br>
• When is it justified to encourage your own group to polarise?<br>
• Sunstein's difficult experiences as a pioneer of animal rights law.<br>
• Whether activists can do better by spending half their resources on public opinion surveys.<br>
• Should people be more or less outspoken about their true views?<br>
• What might be the next social revolution to take off?<br>
• How can we learn about social movements that failed and disappeared?<br>
• How to find out what people really think.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the transcript on our site.</b></p><p>


<i>The 80,000 Hours Podcast is produced by Keiran Harris.</i></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><b>Rebroadcast: this episode was originally released in June 2019.</b><br></p><p>

It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.</p><p>

The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.</p><p>

In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.</p><p>

How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?</p><p>

Sunstein — co-author of <i>Nudge</i>, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book <i>How Change Happens</i>.</p><p>

He pulls together three phenomena which social scientists have studied in recent decades: <i>preference falsification</i>, <i>variable thresholds for action</i>, and <i>group polarisation</i>. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.</p><p><a href="https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/?utm_campaign=podcast__classic-cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.</p><p>

According to Sunstein, it's <i>"much, much easier"</i> to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.</p><p>

In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:</p><p>

• How much people misrepresent their views in democratic countries.<br>
• Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.<br>
• When is it justified to encourage your own group to polarise?<br>
• Sunstein's difficult experiences as a pioneer of animal rights law.<br>
• Whether activists can do better by spending half their resources on public opinion surveys.<br>
• Should people be more or less outspoken about their true views?<br>
• What might be the next social revolution to take off?<br>
• How can we learn about social movements that failed and disappeared?<br>
• How to find out what people really think.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the transcript on our site.</b></p><p>


<i>The 80,000 Hours Podcast is produced by Keiran Harris.</i></p>]]>
      </content:encoded>
      <pubDate>Mon, 27 Dec 2021 22:25:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3c5d078a/59fa7fc6.mp3" length="49474996" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/wAr5-N9r4YhAd1QJm3FY15-5NZYrpQjQmOtMp8aIKgk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjYv/MTY4MzU0NDcwNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6185</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in June 2019.

It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.

The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.

In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.

How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?

Sunstein — co-author of Nudge, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book How Change Happens.

He pulls together three phenomena which social scientists have studied in recent decades: preference falsification, variable thresholds for action, and group polarisation. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.Links to learn more, summary and full transcript.

In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.

According to Sunstein, it's "much, much easier" to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.

In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:

• How much people misrepresent their views in democratic countries.
• Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.
• When is it justified to encourage your own group to polarise?
• Sunstein's difficult experiences as a pioneer of animal rights law.
• Whether activists can do better by spending half their resources on public opinion surveys.
• Should people be more or less outspoken about their true views?
• What might be the next social revolution to take off?
• How can we learn about social movements that failed and disappeared?
• How to find out what people really think.

Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the transcript on our site.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in June 2019.

It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/3c5d078a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#119 – Andrew Yang on our very long-term future, and other topics most politicians won’t touch</title>
      <itunes:title>#119 – Andrew Yang on our very long-term future, and other topics most politicians won’t touch</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6d68b9ce-61b1-11ec-9f2f-128be3fde131</guid>
      <link>https://80000hours.org/podcast/episodes/andrew-yang-very-long-term-future/</link>
      <description>
        <![CDATA[<p>Andrew Yang — past presidential candidate, founder of the Forward Party, and leader of the 'Yang Gang' — is kind of a big deal, but is particularly popular among listeners to The 80,000 Hours Podcast.</p><p> Maybe that's because he's willing to embrace topics most politicians stay away from, like universal basic income, term limits for members of Congress, or what might happen when AI replaces whole industries.</p><p> <a href="https://80k.link/AY2"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> But even those topics are pretty vanilla compared to our usual fare on The 80,000 Hours Podcast. So we thought it’d be fun to throw Andrew some stranger or more niche questions we hadn't heard him comment on before, including:</p><p> 1. What would your ideal utopia in 500 years look like?<br> 2. Do we need more public optimism today?<br> 3. Is positively influencing the long-term future a key moral priority of our time?<br> 4. Should we invest far more to prevent low-probability risks?<br> 5. Should we think of future generations as an interest group that's disenfranchised by their inability to vote?<br> 6. The folks who worry that advanced AI is going to go off the rails and destroy us all... are they crazy, or a valuable insurance policy?<br> 7. Will people struggle to live fulfilling lives once AI systems remove the economic need to 'work'?<br> 8. Andrew is a huge proponent of ranked-choice voting. But what about 'approval voting' — where basically you just get to say “yea” or “nay” to every candidate that's running — which some experts prefer?<br> 9. What would Andrew do with a billion dollars to keep the US a democracy?<br> 10. What does Andrew think about the effective altruism community?<br> 11. What's one thing we should do to reduce the risk of nuclear war?<br> 12. Will Andrew's new political party get Trump elected by splitting the vote, the same way Nader got Bush elected back in 2000?</p><p> As it turns out, Rob and Andrew agree on a lot, so the episode is less a debate than a chat about ideas that aren’t mainstream yet... but might be one day. They also talk about:</p><p> • Andrew’s views on alternative meat<br> • Whether seniors have too much power in American society<br> • Andrew’s DC lobbying firm on behalf of humanity<br> • How the rest of the world could support the US<br> • The merits of 18-year term limits<br> • What technologies Andrew is most excited about<br> • How much the US should spend on foreign aid<br> • Persistence and prevalence of inflation in the US economy<br> • And plenty more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:38)</li><li>Andrew’s hopes for the year 2500 (00:03:10)</li><li>Tech over the next century (00:07:03)</li><li>Utopia for realists (00:10:41)</li><li>Most likely way humanity fails (00:12:43)</li><li>What Andrew would do with a billion dollars (00:14:44)</li><li>Approval voting vs. ranked-choice voting (00:19:51)</li><li>The worry that third party candidates could cause harm (00:21:12)</li><li>Investment in existential risk reduction (00:25:18)</li><li>Future generations as a disenfranchised interest group (00:30:37)</li><li>Humanity Forward (00:32:05)</li><li>Best way the rest of the world could support the US (00:37:17)</li><li>Recent advances in AI (00:39:56)</li><li>Artificial general intelligence (00:46:38)</li><li>The Windfall Clause (00:49:39)</li><li>The alignment problem (00:53:02)</li><li>18-year term limits (00:56:21)</li><li>Effective altruism and longtermism (01:00:44)</li><li>Persistence and prevalence of inflation in the US economy (01:01:25)</li><li>Downsides of policies Andrew advocates for (01:02:08)</li><li>What Andrew would have done differently with COVID (01:04:54)</li><li>Fighting for attention in the media (01:09:25)</li><li>Right ballpark level of foreign aid for the US (01:11:15)</li><li>Government science funding (01:11:58)</li><li>Nuclear weapons policy (01:15:06)</li><li>US-China relationship (01:16:20)</li><li>Human challenge trials (01:18:59)</li><li>Forecasting accuracy (01:20:17)</li><li>Upgrading public schools (01:21:41)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Andrew Yang — past presidential candidate, founder of the Forward Party, and leader of the 'Yang Gang' — is kind of a big deal, but is particularly popular among listeners to The 80,000 Hours Podcast.</p><p> Maybe that's because he's willing to embrace topics most politicians stay away from, like universal basic income, term limits for members of Congress, or what might happen when AI replaces whole industries.</p><p> <a href="https://80k.link/AY2"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> But even those topics are pretty vanilla compared to our usual fare on The 80,000 Hours Podcast. So we thought it’d be fun to throw Andrew some stranger or more niche questions we hadn't heard him comment on before, including:</p><p> 1. What would your ideal utopia in 500 years look like?<br> 2. Do we need more public optimism today?<br> 3. Is positively influencing the long-term future a key moral priority of our time?<br> 4. Should we invest far more to prevent low-probability risks?<br> 5. Should we think of future generations as an interest group that's disenfranchised by their inability to vote?<br> 6. The folks who worry that advanced AI is going to go off the rails and destroy us all... are they crazy, or a valuable insurance policy?<br> 7. Will people struggle to live fulfilling lives once AI systems remove the economic need to 'work'?<br> 8. Andrew is a huge proponent of ranked-choice voting. But what about 'approval voting' — where basically you just get to say “yea” or “nay” to every candidate that's running — which some experts prefer?<br> 9. What would Andrew do with a billion dollars to keep the US a democracy?<br> 10. What does Andrew think about the effective altruism community?<br> 11. What's one thing we should do to reduce the risk of nuclear war?<br> 12. Will Andrew's new political party get Trump elected by splitting the vote, the same way Nader got Bush elected back in 2000?</p><p> As it turns out, Rob and Andrew agree on a lot, so the episode is less a debate than a chat about ideas that aren’t mainstream yet... but might be one day. They also talk about:</p><p> • Andrew’s views on alternative meat<br> • Whether seniors have too much power in American society<br> • Andrew’s DC lobbying firm on behalf of humanity<br> • How the rest of the world could support the US<br> • The merits of 18-year term limits<br> • What technologies Andrew is most excited about<br> • How much the US should spend on foreign aid<br> • Persistence and prevalence of inflation in the US economy<br> • And plenty more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:38)</li><li>Andrew’s hopes for the year 2500 (00:03:10)</li><li>Tech over the next century (00:07:03)</li><li>Utopia for realists (00:10:41)</li><li>Most likely way humanity fails (00:12:43)</li><li>What Andrew would do with a billion dollars (00:14:44)</li><li>Approval voting vs. ranked-choice voting (00:19:51)</li><li>The worry that third party candidates could cause harm (00:21:12)</li><li>Investment in existential risk reduction (00:25:18)</li><li>Future generations as a disenfranchised interest group (00:30:37)</li><li>Humanity Forward (00:32:05)</li><li>Best way the rest of the world could support the US (00:37:17)</li><li>Recent advances in AI (00:39:56)</li><li>Artificial general intelligence (00:46:38)</li><li>The Windfall Clause (00:49:39)</li><li>The alignment problem (00:53:02)</li><li>18-year term limits (00:56:21)</li><li>Effective altruism and longtermism (01:00:44)</li><li>Persistence and prevalence of inflation in the US economy (01:01:25)</li><li>Downsides of policies Andrew advocates for (01:02:08)</li><li>What Andrew would have done differently with COVID (01:04:54)</li><li>Fighting for attention in the media (01:09:25)</li><li>Right ballpark level of foreign aid for the US (01:11:15)</li><li>Government science funding (01:11:58)</li><li>Nuclear weapons policy (01:15:06)</li><li>US-China relationship (01:16:20)</li><li>Human challenge trials (01:18:59)</li><li>Forecasting accuracy (01:20:17)</li><li>Upgrading public schools (01:21:41)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 20 Dec 2021 17:28:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0a8c43ce/c4ac9a7f.mp3" length="41258741" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/R0TPpnkxPjmeG2QBsPrMB93XLDvGMwFwLvqWWUMOvSM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjUv/MTY4MzU0NDcwMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5157</itunes:duration>
      <itunes:summary>Andrew Yang — past presidential candidate, founder of the Forward Party, and leader of the 'Yang Gang' — is kind of a big deal, but is particularly popular among listeners to The 80,000 Hours Podcast. 

Maybe that's because he's willing to embrace topics most politicians stay away from, like universal basic income, term limits for members of Congress, or what might happen when AI replaces whole industries. 

Links to learn more, summary and full transcript.

But even those topics are pretty vanilla compared to our usual fare on The 80,000 Hours Podcast. So we thought it’d be fun to throw Andrew some stranger or more niche questions we hadn't heard him comment on before, including:  

1. What would your ideal utopia in 500 years look like? 
2. Do we need more public optimism today? 
3. Is positively influencing the long-term future a key moral priority of our time? 
4. Should we invest far more to prevent low-probability risks? 
5. Should we think of future generations as an interest group that's disenfranchised by their inability to vote? 
6. The folks who worry that advanced AI is going to go off the rails and destroy us all... are they crazy, or a valuable insurance policy? 
7. Will people struggle to live fulfilling lives once AI systems remove the economic need to 'work'? 
8. Andrew is a huge proponent of ranked-choice voting. But what about 'approval voting' — where basically you just get to say “yea” or “nay” to every candidate that's running — which some experts prefer?  
9. What would Andrew do with a billion dollars to keep the US a democracy? 
10. What does Andrew think about the effective altruism community? 
11. What's one thing we should do to reduce the risk of nuclear war? 
12. Will Andrew's new political party get Trump elected by splitting the vote, the same way Nader got Bush elected back in 2000? 

As it turns out, Rob and Andrew agree on a lot, so the episode is less a debate than a chat about ideas that aren’t mainstream yet... but might be one day. They also talk about: 

• Andrew’s views on alternative meat 
• Whether seniors have too much power in American society 
• Andrew’s DC lobbying firm on behalf of humanity 
• How the rest of the world could support the US 
• The merits of 18-year term limits 
• What technologies Andrew is most excited about 
• How much the US should spend on foreign aid 
• Persistence and prevalence of inflation in the US economy 
• And plenty more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Andrew Yang — past presidential candidate, founder of the Forward Party, and leader of the 'Yang Gang' — is kind of a big deal, but is particularly popular among listeners to The 80,000 Hours Podcast. 

Maybe that's because he's willing to embrace topic</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/0a8c43ce/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/0a8c43ce/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#118 – Jaime Yassif on safeguarding bioscience to prevent catastrophic lab accidents and bioweapons development</title>
      <itunes:title>#118 – Jaime Yassif on safeguarding bioscience to prevent catastrophic lab accidents and bioweapons development</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c4256170-5c36-11ec-89f5-12be2c01527f</guid>
      <link>https://80000hours.org/podcast/episodes/jaime-yassif-safeguarding-bioscience/</link>
      <description>
        <![CDATA[<p>If a rich country were really committed to pursuing an active biological weapons program, there’s not much we could do to stop them. With enough money and persistence, they’d be able to buy equipment, and hire people to carry out the work.</p><p> But what we can do is intervene before they make that decision.</p><p> Today’s guest, Jaime Yassif — Senior Fellow for global biological policy and programs at the Nuclear Threat Initiative (NTI) — thinks that stopping states from wanting to pursue dangerous bioscience in the first place is one of our key lines of defence against global catastrophic biological risks (GCBRs).</p><p> <a href="https://80k.link/JY"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> It helps to understand why countries might consider developing biological weapons. Jaime says there are three main possible reasons:</p><p> 1. Fear of what their adversary might be up to<br> 2. Belief that they could gain a tactical or strategic advantage, with limited risk of getting caught<br> 3. Belief that even if they are caught, they are unlikely to be held accountable</p><p> In response, Jaime has developed a three-part recipe to create systems robust enough to meaningfully change the cost-benefit calculation.</p><p> The first is to substantially increase transparency. If countries aren’t confident about what their neighbours or adversaries are actually up to, misperceptions could lead to arms races that neither side desires. But if you know with confidence that no one around you is pursuing a biological weapons programme, you won’t feel motivated to pursue one yourself.</p><p> The second is to strengthen the capabilities of the United Nations’ system to investigate the origins of high-consequence biological events — whether naturally emerging, accidental or deliberate — and to make sure that the responsibility to figure out the source of bio-events of unknown origin doesn’t fall between the cracks of different existing mechanisms. The ability to quickly discover the source of emerging pandemics is important both for responding to them in real time and for deterring future bioweapons development or use.</p><p> And the third is meaningful accountability. States need to know that the consequences for getting caught in a deliberate attack are severe enough to make it a net negative in expectation to go down this road in the first place.</p><p> But having a good plan and actually implementing it are two very different things, and today’s episode focuses heavily on the practical steps we should be taking to influence both governments and international organisations, like the WHO and UN — and to help them maximise their effectiveness in guarding against catastrophic biological risks.</p><p> Jaime and Rob explore NTI’s current proposed plan for reducing global catastrophic biological risks, and discuss:</p><p> • The importance of reducing emerging biological risks associated with rapid technology advances<br> • How we can make it a lot harder for anyone to deliberately or accidentally produce or release a really dangerous pathogen<br> • The importance of having multiples theories of risk reduction<br> • Why Jaime’s more focused on prevention than response<br> • The history of the Biological Weapons Convention<br> • Jaime’s disagreements with the effective altruism community<br> • And much more</p><p> And if you might be interested in dedicating your career to reducing GCBRs, stick around to the end of the episode to get Jaime’s advice — including on how people outside of the US can best contribute, and how to compare career opportunities in academia vs think tanks, and nonprofits vs national governments vs international orgs.</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:32)</li><li>Categories of global catastrophic biological risks (00:05:24)</li><li>Disagreements with the effective altruism community (00:07:39)</li><li>Stopping the first person from getting infected (00:11:51)</li><li>Shaping intent (00:15:51)</li><li>Verification and the Biological Weapons Convention (00:25:31)</li><li>Attribution (00:37:15)</li><li>How to actually implement a new idea (00:50:54)</li><li>COVID-19: natural pandemic or lab leak? (00:53:31)</li><li>How much can we rely on traditional law enforcement to detect terrorists? (00:58:20)</li><li>Constraining capabilities (01:01:24)</li><li>The funding landscape (01:06:56)</li><li>Oversight committees (01:14:20)</li><li>Just winning the argument (01:20:17)</li><li>NTI’s vision (01:27:39)</li><li>Suppliers of goods and services (01:33:24)</li><li>Publishers (01:39:41)</li><li>Biggest weaknesses of NTI platform (01:42:29)</li><li>Careers (01:48:31)</li><li>How people outside of the US can best contribute (01:54:10)</li><li>Academia vs think tanks vs nonprofits vs government (01:59:21)</li><li>International cooperation (02:05:40)</li><li>Best things about living in the US, UK, China, and Israel (02:11:16)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If a rich country were really committed to pursuing an active biological weapons program, there’s not much we could do to stop them. With enough money and persistence, they’d be able to buy equipment, and hire people to carry out the work.</p><p> But what we can do is intervene before they make that decision.</p><p> Today’s guest, Jaime Yassif — Senior Fellow for global biological policy and programs at the Nuclear Threat Initiative (NTI) — thinks that stopping states from wanting to pursue dangerous bioscience in the first place is one of our key lines of defence against global catastrophic biological risks (GCBRs).</p><p> <a href="https://80k.link/JY"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> It helps to understand why countries might consider developing biological weapons. Jaime says there are three main possible reasons:</p><p> 1. Fear of what their adversary might be up to<br> 2. Belief that they could gain a tactical or strategic advantage, with limited risk of getting caught<br> 3. Belief that even if they are caught, they are unlikely to be held accountable</p><p> In response, Jaime has developed a three-part recipe to create systems robust enough to meaningfully change the cost-benefit calculation.</p><p> The first is to substantially increase transparency. If countries aren’t confident about what their neighbours or adversaries are actually up to, misperceptions could lead to arms races that neither side desires. But if you know with confidence that no one around you is pursuing a biological weapons programme, you won’t feel motivated to pursue one yourself.</p><p> The second is to strengthen the capabilities of the United Nations’ system to investigate the origins of high-consequence biological events — whether naturally emerging, accidental or deliberate — and to make sure that the responsibility to figure out the source of bio-events of unknown origin doesn’t fall between the cracks of different existing mechanisms. The ability to quickly discover the source of emerging pandemics is important both for responding to them in real time and for deterring future bioweapons development or use.</p><p> And the third is meaningful accountability. States need to know that the consequences for getting caught in a deliberate attack are severe enough to make it a net negative in expectation to go down this road in the first place.</p><p> But having a good plan and actually implementing it are two very different things, and today’s episode focuses heavily on the practical steps we should be taking to influence both governments and international organisations, like the WHO and UN — and to help them maximise their effectiveness in guarding against catastrophic biological risks.</p><p> Jaime and Rob explore NTI’s current proposed plan for reducing global catastrophic biological risks, and discuss:</p><p> • The importance of reducing emerging biological risks associated with rapid technology advances<br> • How we can make it a lot harder for anyone to deliberately or accidentally produce or release a really dangerous pathogen<br> • The importance of having multiples theories of risk reduction<br> • Why Jaime’s more focused on prevention than response<br> • The history of the Biological Weapons Convention<br> • Jaime’s disagreements with the effective altruism community<br> • And much more</p><p> And if you might be interested in dedicating your career to reducing GCBRs, stick around to the end of the episode to get Jaime’s advice — including on how people outside of the US can best contribute, and how to compare career opportunities in academia vs think tanks, and nonprofits vs national governments vs international orgs.</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:32)</li><li>Categories of global catastrophic biological risks (00:05:24)</li><li>Disagreements with the effective altruism community (00:07:39)</li><li>Stopping the first person from getting infected (00:11:51)</li><li>Shaping intent (00:15:51)</li><li>Verification and the Biological Weapons Convention (00:25:31)</li><li>Attribution (00:37:15)</li><li>How to actually implement a new idea (00:50:54)</li><li>COVID-19: natural pandemic or lab leak? (00:53:31)</li><li>How much can we rely on traditional law enforcement to detect terrorists? (00:58:20)</li><li>Constraining capabilities (01:01:24)</li><li>The funding landscape (01:06:56)</li><li>Oversight committees (01:14:20)</li><li>Just winning the argument (01:20:17)</li><li>NTI’s vision (01:27:39)</li><li>Suppliers of goods and services (01:33:24)</li><li>Publishers (01:39:41)</li><li>Biggest weaknesses of NTI platform (01:42:29)</li><li>Careers (01:48:31)</li><li>How people outside of the US can best contribute (01:54:10)</li><li>Academia vs think tanks vs nonprofits vs government (01:59:21)</li><li>International cooperation (02:05:40)</li><li>Best things about living in the US, UK, China, and Israel (02:11:16)</li></ul><p><br><em>Producer: Keiran Harris<br>Audio mastering: Ryan Kessler<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 13 Dec 2021 22:22:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f1e381ca/90234484.mp3" length="65118774" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Yd9uaecIcstkZf9TQ_PmqS1_fIIRsM0GqUblXrVBJjI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjQv/MTY4MzU0NDcwMS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8140</itunes:duration>
      <itunes:summary>If a rich country were really committed to pursuing an active biological weapons program, there’s not much we could do to stop them. With enough money and persistence, they’d be able to buy equipment, and hire people to carry out the work. 

But what we can do is intervene before they make that decision.  

Today’s guest, Jaime Yassif — Senior Fellow for global biological policy and programs at the Nuclear Threat Initiative (NTI) — thinks that stopping states from wanting to pursue dangerous bioscience in the first place is one of our key lines of defence against global catastrophic biological risks (GCBRs). 

Links to learn more, summary and full transcript.

It helps to understand why countries might consider developing biological weapons. Jaime says there are three main possible reasons:  

1. Fear of what their adversary might be up to 
2. Belief that they could gain a tactical or strategic advantage, with limited risk of getting caught 
3. Belief that even if they are caught, they are unlikely to be held accountable 

In response, Jaime has developed a three-part recipe to create systems robust enough to meaningfully change the cost-benefit calculation.  

The first is to substantially increase transparency. If countries aren’t confident about what their neighbours or adversaries are actually up to, misperceptions could lead to arms races that neither side desires. But if you know with confidence that no one around you is pursuing a biological weapons programme, you won’t feel motivated to pursue one yourself. 

The second is to strengthen the capabilities of the United Nations’ system to
investigate the origins of high-consequence biological events — whether naturally emerging, accidental or deliberate — and to make sure that the responsibility to figure out the source of bio-events of unknown origin doesn’t fall between the cracks of different existing mechanisms. The ability to quickly discover the source of emerging pandemics is important both for responding to them in real time and for deterring future bioweapons development or use. 

And the third is meaningful accountability. States need to know that the consequences for getting caught in a deliberate attack are severe enough to make it a net negative in expectation to go down this road in the first place. 

But having a good plan and actually implementing it are two very different things, and today’s episode focuses heavily on the practical steps we should be taking to influence both governments and international organisations, like the WHO and UN — and to help them maximise their effectiveness in guarding against catastrophic biological risks. 

Jaime and Rob explore NTI’s current proposed plan for reducing global catastrophic biological risks, and discuss:  

• The importance of reducing emerging biological risks associated with rapid technology advances 
• How we can make it a lot harder for anyone to deliberately or accidentally produce or release a really dangerous pathogen 
• The importance of having multiples theories of risk reduction 
• Why Jaime’s more focused on prevention than response 
• The history of the Biological Weapons Convention 
• Jaime’s disagreements with the effective altruism community  
• And much more 

And if you might be interested in dedicating your career to reducing GCBRs, stick around to the end of the episode to get Jaime’s advice — including on how people outside of the US can best contribute, and how to compare career opportunities in academia vs think tanks, and nonprofits vs national governments vs international orgs. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ryan Kessler
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If a rich country were really committed to pursuing an active biological weapons program, there’s not much we could do to stop them. With enough money and persistence, they’d be able to buy equipment, and hire people to carry out the work. 

But what we</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f1e381ca/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f1e381ca/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#117 – David Denkenberger on using paper mills and seaweed to feed everyone in a catastrophe, ft Sahil Shah</title>
      <itunes:title>#117 – David Denkenberger on using paper mills and seaweed to feed everyone in a catastrophe, ft Sahil Shah</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d2936b78-5158-11ec-8c89-0ebaf4bbbca7</guid>
      <link>https://80000hours.org/podcast/episodes/david-denkenberger-sahil-shah-using-paper-mills-and-seaweed-in-catastrophes/</link>
      <description>
        <![CDATA[<p>If there's a nuclear war followed by nuclear winter, and the sun is blocked out for years, most of us are going to starve, right? Well, currently, probably we would, because humanity hasn't done much to prevent it. But it turns out that an ounce of forethought might be enough for most people to get the calories they need to survive, even in a future as grim as that one.</p><p>Today's guest is engineering professor Dave Denkenberger, who co-founded the Alliance to Feed the Earth in Disasters (ALLFED), which has the goal of finding ways humanity might be able to feed itself for years without relying on the sun. Over the last seven years, Dave and his team have turned up options from the mundane, like mushrooms grown on rotting wood, to the bizarre, like bacteria that can eat natural gas or electricity itself.</p><p><a href="https://80k.link/DD"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> One option stands out as potentially able to feed billions: finding a way to eat wood ourselves. Even after a disaster, a huge amount of calories will be lying around, stored in wood and other plant cellulose. The trouble is that, even though cellulose is basically a lot of sugar molecules stuck together, humans can't eat wood.</p><p> But we do know how to turn wood into something people can eat. We can grind wood up in already existing paper mills, then mix the pulp with enzymes that break the cellulose into sugar and the hemicellulose into other sugars.</p><p> Another option that shows a lot of promise is seaweed. Buffered by the water around them, ocean life wouldn't be as affected by the lower temperatures resulting from the sun being obscured. Sea plants are also already used to growing in low light, because the water above them already shades them to some extent.</p><p> Dave points out that "there are several species of seaweed that can still grow 10% per day, even with the lower light levels in nuclear winter and lower temperatures. ... Not surprisingly, with that 10% growth per day, assuming we can scale up, we could actually get up to 160% of human calories in less than a year."</p><p> Of course it will be easier to scale up seaweed production if it's already a reasonably sized industry. At the end of the interview, we're joined by Sahil Shah, who is trying to expand seaweed production in the UK with his business Sustainable Seaweed.</p><p> While a diet of seaweed and trees turned into sugar might not seem that appealing, the team at ALLFED also thinks several perfectly normal crops could also make a big contribution to feeding the world, even in a truly catastrophic scenario. Those crops include potatoes, canola, and sugar beets, which are currently grown in cool low-light environments.</p><p> Many of these ideas could turn out to be misguided or impractical in real-world conditions, which is why Dave and ALLFED are raising money to test them out on the ground. They think it's essential to show these techniques can work so that should the worst happen, people turn their attention to producing more food rather than fighting one another over the small amount of food humanity has stockpiled.</p><p> In this conversation, Rob, Dave, and Sahil discuss the above, as well as:<br> • How much one can trust the sort of economic modelling ALLFED does<br> • Bacteria that turn natural gas or electricity into protein<br> • How to feed astronauts in space with nuclear power<br> • What individuals can do to prepare themselves for global catastrophes<br> • Whether we should worry about humanity running out of natural resources<br> • How David helped save $10 billion worth of electricity through energy efficiency standards<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:36)</li><li>Resilient foods recap (00:04:27)</li><li>Cost effectiveness recap (00:08:07)</li><li>Turning fiber or wood or cellulose into sugar (00:10:30)</li><li>Redirecting human-edible food away from animals (00:22:46)</li><li>Seaweed production (00:26:33)</li><li>Crops that can handle lower temperatures or lower light (00:35:24)</li><li>Greenhouses (00:40:51)</li><li>How much to trust this economic modeling (00:43:50)</li><li>Global cooperation (00:51:16)</li><li>People feeding themselves using these methods (00:57:15)</li><li>NASA and ALLFED (01:04:47)</li><li>Kinds of catastrophes (01:15:16)</li><li>Is New Zealand overrated? (01:25:35)</li><li>Should listeners be doing anything to prepare for possible disasters? (01:28:43)</li><li>Cost effectiveness of work on EMPs (01:30:43)</li><li>The future of ALLFED (01:33:34)</li><li>Opportunities at ALLFED (01:40:49)</li><li>Why Dave is optimistic around bigger-picture scarcity issues (01:46:58)</li><li>Energy return on energy invested (01:56:36)</li><li>Nitrogen and phosphorus (02:03:25)</li><li>Energy and food prices (02:07:18)</li><li>Sustainable Seaweed with Sahil Shah (02:21:44)</li><li>Locusts (02:38:33)</li><li>The effect of COVID on food supplies (02:44:01)</li><li>How much food prices would spike in a disaster (02:50:46)</li><li>How Dave helped to save ~$10 billion worth of energy (02:56:33)</li><li>What it’s like to live in Alaska (03:03:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If there's a nuclear war followed by nuclear winter, and the sun is blocked out for years, most of us are going to starve, right? Well, currently, probably we would, because humanity hasn't done much to prevent it. But it turns out that an ounce of forethought might be enough for most people to get the calories they need to survive, even in a future as grim as that one.</p><p>Today's guest is engineering professor Dave Denkenberger, who co-founded the Alliance to Feed the Earth in Disasters (ALLFED), which has the goal of finding ways humanity might be able to feed itself for years without relying on the sun. Over the last seven years, Dave and his team have turned up options from the mundane, like mushrooms grown on rotting wood, to the bizarre, like bacteria that can eat natural gas or electricity itself.</p><p><a href="https://80k.link/DD"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> One option stands out as potentially able to feed billions: finding a way to eat wood ourselves. Even after a disaster, a huge amount of calories will be lying around, stored in wood and other plant cellulose. The trouble is that, even though cellulose is basically a lot of sugar molecules stuck together, humans can't eat wood.</p><p> But we do know how to turn wood into something people can eat. We can grind wood up in already existing paper mills, then mix the pulp with enzymes that break the cellulose into sugar and the hemicellulose into other sugars.</p><p> Another option that shows a lot of promise is seaweed. Buffered by the water around them, ocean life wouldn't be as affected by the lower temperatures resulting from the sun being obscured. Sea plants are also already used to growing in low light, because the water above them already shades them to some extent.</p><p> Dave points out that "there are several species of seaweed that can still grow 10% per day, even with the lower light levels in nuclear winter and lower temperatures. ... Not surprisingly, with that 10% growth per day, assuming we can scale up, we could actually get up to 160% of human calories in less than a year."</p><p> Of course it will be easier to scale up seaweed production if it's already a reasonably sized industry. At the end of the interview, we're joined by Sahil Shah, who is trying to expand seaweed production in the UK with his business Sustainable Seaweed.</p><p> While a diet of seaweed and trees turned into sugar might not seem that appealing, the team at ALLFED also thinks several perfectly normal crops could also make a big contribution to feeding the world, even in a truly catastrophic scenario. Those crops include potatoes, canola, and sugar beets, which are currently grown in cool low-light environments.</p><p> Many of these ideas could turn out to be misguided or impractical in real-world conditions, which is why Dave and ALLFED are raising money to test them out on the ground. They think it's essential to show these techniques can work so that should the worst happen, people turn their attention to producing more food rather than fighting one another over the small amount of food humanity has stockpiled.</p><p> In this conversation, Rob, Dave, and Sahil discuss the above, as well as:<br> • How much one can trust the sort of economic modelling ALLFED does<br> • Bacteria that turn natural gas or electricity into protein<br> • How to feed astronauts in space with nuclear power<br> • What individuals can do to prepare themselves for global catastrophes<br> • Whether we should worry about humanity running out of natural resources<br> • How David helped save $10 billion worth of electricity through energy efficiency standards<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:36)</li><li>Resilient foods recap (00:04:27)</li><li>Cost effectiveness recap (00:08:07)</li><li>Turning fiber or wood or cellulose into sugar (00:10:30)</li><li>Redirecting human-edible food away from animals (00:22:46)</li><li>Seaweed production (00:26:33)</li><li>Crops that can handle lower temperatures or lower light (00:35:24)</li><li>Greenhouses (00:40:51)</li><li>How much to trust this economic modeling (00:43:50)</li><li>Global cooperation (00:51:16)</li><li>People feeding themselves using these methods (00:57:15)</li><li>NASA and ALLFED (01:04:47)</li><li>Kinds of catastrophes (01:15:16)</li><li>Is New Zealand overrated? (01:25:35)</li><li>Should listeners be doing anything to prepare for possible disasters? (01:28:43)</li><li>Cost effectiveness of work on EMPs (01:30:43)</li><li>The future of ALLFED (01:33:34)</li><li>Opportunities at ALLFED (01:40:49)</li><li>Why Dave is optimistic around bigger-picture scarcity issues (01:46:58)</li><li>Energy return on energy invested (01:56:36)</li><li>Nitrogen and phosphorus (02:03:25)</li><li>Energy and food prices (02:07:18)</li><li>Sustainable Seaweed with Sahil Shah (02:21:44)</li><li>Locusts (02:38:33)</li><li>The effect of COVID on food supplies (02:44:01)</li><li>How much food prices would spike in a disaster (02:50:46)</li><li>How Dave helped to save ~$10 billion worth of energy (02:56:33)</li><li>What it’s like to live in Alaska (03:03:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 29 Nov 2021 21:24:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/de2bf62e/2fb25310.mp3" length="90341885" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/QGJy3EJnJczJ9aArBll7YukrJZ68uNddQyrymXriFok/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjMv/MTY4MzU0NDY5OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11293</itunes:duration>
      <itunes:summary>If there's a nuclear war followed by nuclear winter, and the sun is blocked out for years, most of us are going to starve, right? Well, currently, probably we would, because humanity hasn't done much to prevent it. But it turns out that an ounce of forethought might be enough for most people to get the calories they need to survive, even in a future as grim as that one. 

Today's guest is engineering professor Dave Denkenberger, who co-founded the Alliance to Feed the Earth in Disasters (ALLFED), which has the goal of finding ways humanity might be able to feed itself for years without relying on the sun. Over the last seven years, Dave and his team have turned up options from the mundane, like mushrooms grown on rotting wood, to the bizarre, like bacteria that can eat natural gas or electricity itself. 

Links to learn more, summary and full transcript.

One option stands out as potentially able to feed billions: finding a way to eat wood ourselves. Even after a disaster, a huge amount of calories will be lying around, stored in wood and other plant cellulose. The trouble is that, even though cellulose is basically a lot of sugar molecules stuck together, humans can't eat wood. 

But we do know how to turn wood into something people can eat. We can grind wood up in already existing paper mills, then mix the pulp with enzymes that break the cellulose into sugar and the hemicellulose into other sugars. 

Another option that shows a lot of promise is seaweed. Buffered by the water around them, ocean life wouldn't be as affected by the lower temperatures resulting from the sun being obscured. Sea plants are also already used to growing in low light, because the water above them already shades them to some extent. 

Dave points out that "there are several species of seaweed that can still grow 10% per day, even with the lower light levels in nuclear winter and lower temperatures. ... Not surprisingly, with that 10% growth per day, assuming we can scale up, we could actually get up to 160% of human calories in less than a year." 

Of course it will be easier to scale up seaweed production if it's already a reasonably sized industry. At the end of the interview, we're joined by Sahil Shah, who is trying to expand seaweed production in the UK with his business Sustainable Seaweed. 

While a diet of seaweed and trees turned into sugar might not seem that appealing, the team at ALLFED also thinks several perfectly normal crops could also make a big contribution to feeding the world, even in a truly catastrophic scenario. Those crops include potatoes, canola, and sugar beets, which are currently grown in cool low-light environments. 

Many of these ideas could turn out to be misguided or impractical in real-world conditions, which is why Dave and ALLFED are raising money to test them out on the ground. They think it's essential to show these techniques can work so that should the worst happen, people turn their attention to producing more food rather than fighting one another over the small amount of food humanity has stockpiled.  

In this conversation, Rob, Dave, and Sahil discuss the above, as well as: 

• How much one can trust the sort of economic modelling ALLFED does 
• Bacteria that turn natural gas or electricity into protein 
• How to feed astronauts in space with nuclear power 
• What individuals can do to prepare themselves for global catastrophes 
• Whether we should worry about humanity running out of natural resources 
• How David helped save $10 billion worth of electricity through energy efficiency standards 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If there's a nuclear war followed by nuclear winter, and the sun is blocked out for years, most of us are going to starve, right? Well, currently, probably we would, because humanity hasn't done much to prevent it. But it turns out that an ounce of foreth</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/de2bf62e/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/de2bf62e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#116 – Luisa Rodriguez on why global catastrophes seem unlikely to kill us all</title>
      <itunes:title>#116 – Luisa Rodriguez on why global catastrophes seem unlikely to kill us all</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">5b79ef76-496a-11ec-ae64-124fed2af33d</guid>
      <link>https://80000hours.org/podcast/episodes/luisa-rodriguez-why-global-catastrophes-seem-unlikely-to-kill-us-all/</link>
      <description>
        <![CDATA[<p>If modern human civilisation collapsed — as a result of nuclear war, severe climate change, or a much worse pandemic than COVID-19 — billions of people might die.</p><p>That's terrible enough to contemplate. But what’s the probability that rather than recover, the survivors would falter and humanity would actually disappear for good?</p><p>It's an obvious enough question, but very few people have spent serious time looking into it -- possibly because it cuts across history, economics, and biology, among many other fields. There's no Disaster Apocalypse Studies department at any university, and governments have little incentive to plan for a future in which their country probably no longer even exists.</p><p>The person who may have spent the most time looking at this specific question is Luisa Rodriguez — who has conducted research at Rethink Priorities, Oxford University's Future of Humanity Institute, the Forethought Foundation, and now here, at 80,000 Hours.</p><p><a href="https://80k.link/LR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> She wrote a series of articles earnestly trying to foresee how likely humanity would be to recover and build back after a full-on civilisational collapse.</p><p> There are a couple of main stories people put forward for how a catastrophe like this would kill every single human on Earth — but Luisa doesn’t buy them.</p><p> <strong>Story 1</strong>: Nuclear war has led to nuclear winter. There's a 10-year period during which a lot of the world is really inhospitable to agriculture. The survivors just aren't able to figure out how to feed themselves in the time period, so everyone dies of starvation or cold.</p><p> <em>Why Luisa doesn’t buy it</em>:</p><p> Catastrophes will almost inevitably be non-uniform in their effects. If 80,000 people survive, they’re not all going to be in the same city — it would look more like groups of 5,000 in a bunch of different places.</p><p> People in some places will starve, but those in other places, such as New Zealand, will be able to fish, eat seaweed, grow potatoes, and find other sources of calories.</p><p> It’d be an incredibly unlucky coincidence if the survivors of a nuclear war -- likely spread out all over the world -- happened to all be affected by natural disasters or were all prohibitively far away from areas suitable for agriculture (which aren’t the same areas you’d expect to be attacked in a nuclear war).</p><p> <strong>Story 2</strong>: The catastrophe leads to hoarding and violence, and in addition to people being directly killed by the conflict, it distracts everyone so much from the key challenge of reestablishing agriculture that they simply fail. By the time they come to their senses, it’s too late -- they’ve used up too much of the resources they’d need to get agriculture going again.</p><p> <em>Why Luisa doesn’t buy it</em>:</p><p> We‘ve had lots of resource scarcity throughout history, and while we’ve seen examples of conflict petering out because basic needs aren’t being met, we’ve never seen the reverse.</p><p> And again, even if this happens in some places -- even if some groups fought each other until they literally ended up starving to death — it would be completely bizarre for it to happen to every group in the world. You just need one group of around 300 people to survive for them to be able to rebuild the species.</p><p> In this wide-ranging and free-flowing conversation, Luisa and Rob also cover:</p><p> • What the world might actually look like after one of these catastrophes<br> • The most valuable knowledge for survivors<br> • How fast populations could rebound<br> • ‘Boom and bust’ climate change scenarios<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:37)</li><li>Recovering from a serious collapse of civilization (00:11:41)</li><li>Existing literature (00:14:52)</li><li>Fiction (00:20:42)</li><li>Types of disasters (00:23:13)</li><li>What the world might look like after a catastrophe (00:29:09)</li><li>Nuclear winter (00:34:34)</li><li>Stuff that might stick around (00:38:58)</li><li>Grace period (00:42:39)</li><li>Examples of human ingenuity in tough situations (00:48:33)</li><li>The most valuable knowledge for survivors (00:57:23)</li><li>Would people really work together? (01:09:00)</li><li>Radiation (01:27:08)</li><li>Learning from the worst pandemics (01:31:40)</li><li>Learning from fallen civilizations (01:36:30)</li><li>Direct extinction (01:45:30)</li><li>Indirect extinction (02:01:53)</li><li>Rapid recovery vs. slow recovery (02:05:01)</li><li>Risk of culture shifting against science and tech (02:15:33)</li><li>Resource scarcity (02:23:07)</li><li>How fast could populations rebound (02:37:07)</li><li>Implications for what we ought to do right now (02:43:52)</li><li>How this work affected Luisa’s views (02:54:00)</li><li>Boom and bust climate change scenarios (02:57:06)</li><li>Stagnation and cold wars (03:01:18)</li><li>How Luisa met her biological father (03:18:23)</li><li>If Luisa had to change careers (03:40:38<strong>)</strong></li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If modern human civilisation collapsed — as a result of nuclear war, severe climate change, or a much worse pandemic than COVID-19 — billions of people might die.</p><p>That's terrible enough to contemplate. But what’s the probability that rather than recover, the survivors would falter and humanity would actually disappear for good?</p><p>It's an obvious enough question, but very few people have spent serious time looking into it -- possibly because it cuts across history, economics, and biology, among many other fields. There's no Disaster Apocalypse Studies department at any university, and governments have little incentive to plan for a future in which their country probably no longer even exists.</p><p>The person who may have spent the most time looking at this specific question is Luisa Rodriguez — who has conducted research at Rethink Priorities, Oxford University's Future of Humanity Institute, the Forethought Foundation, and now here, at 80,000 Hours.</p><p><a href="https://80k.link/LR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> She wrote a series of articles earnestly trying to foresee how likely humanity would be to recover and build back after a full-on civilisational collapse.</p><p> There are a couple of main stories people put forward for how a catastrophe like this would kill every single human on Earth — but Luisa doesn’t buy them.</p><p> <strong>Story 1</strong>: Nuclear war has led to nuclear winter. There's a 10-year period during which a lot of the world is really inhospitable to agriculture. The survivors just aren't able to figure out how to feed themselves in the time period, so everyone dies of starvation or cold.</p><p> <em>Why Luisa doesn’t buy it</em>:</p><p> Catastrophes will almost inevitably be non-uniform in their effects. If 80,000 people survive, they’re not all going to be in the same city — it would look more like groups of 5,000 in a bunch of different places.</p><p> People in some places will starve, but those in other places, such as New Zealand, will be able to fish, eat seaweed, grow potatoes, and find other sources of calories.</p><p> It’d be an incredibly unlucky coincidence if the survivors of a nuclear war -- likely spread out all over the world -- happened to all be affected by natural disasters or were all prohibitively far away from areas suitable for agriculture (which aren’t the same areas you’d expect to be attacked in a nuclear war).</p><p> <strong>Story 2</strong>: The catastrophe leads to hoarding and violence, and in addition to people being directly killed by the conflict, it distracts everyone so much from the key challenge of reestablishing agriculture that they simply fail. By the time they come to their senses, it’s too late -- they’ve used up too much of the resources they’d need to get agriculture going again.</p><p> <em>Why Luisa doesn’t buy it</em>:</p><p> We‘ve had lots of resource scarcity throughout history, and while we’ve seen examples of conflict petering out because basic needs aren’t being met, we’ve never seen the reverse.</p><p> And again, even if this happens in some places -- even if some groups fought each other until they literally ended up starving to death — it would be completely bizarre for it to happen to every group in the world. You just need one group of around 300 people to survive for them to be able to rebuild the species.</p><p> In this wide-ranging and free-flowing conversation, Luisa and Rob also cover:</p><p> • What the world might actually look like after one of these catastrophes<br> • The most valuable knowledge for survivors<br> • How fast populations could rebound<br> • ‘Boom and bust’ climate change scenarios<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:37)</li><li>Recovering from a serious collapse of civilization (00:11:41)</li><li>Existing literature (00:14:52)</li><li>Fiction (00:20:42)</li><li>Types of disasters (00:23:13)</li><li>What the world might look like after a catastrophe (00:29:09)</li><li>Nuclear winter (00:34:34)</li><li>Stuff that might stick around (00:38:58)</li><li>Grace period (00:42:39)</li><li>Examples of human ingenuity in tough situations (00:48:33)</li><li>The most valuable knowledge for survivors (00:57:23)</li><li>Would people really work together? (01:09:00)</li><li>Radiation (01:27:08)</li><li>Learning from the worst pandemics (01:31:40)</li><li>Learning from fallen civilizations (01:36:30)</li><li>Direct extinction (01:45:30)</li><li>Indirect extinction (02:01:53)</li><li>Rapid recovery vs. slow recovery (02:05:01)</li><li>Risk of culture shifting against science and tech (02:15:33)</li><li>Resource scarcity (02:23:07)</li><li>How fast could populations rebound (02:37:07)</li><li>Implications for what we ought to do right now (02:43:52)</li><li>How this work affected Luisa’s views (02:54:00)</li><li>Boom and bust climate change scenarios (02:57:06)</li><li>Stagnation and cold wars (03:01:18)</li><li>How Luisa met her biological father (03:18:23)</li><li>If Luisa had to change careers (03:40:38<strong>)</strong></li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 19 Nov 2021 20:53:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ed143ff0/11cc03d6.mp3" length="108356447" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Zrt4NyM9EmP5k7_dHj6CEHnpgmm_jB4KA7cv9t8Eqpg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjIv/MTY4MzU0NDY5OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13544</itunes:duration>
      <itunes:summary>If modern human civilisation collapsed — as a result of nuclear war, severe climate change, or a much worse pandemic than COVID-19 — billions of people might die. 

That's terrible enough to contemplate. But what’s the probability that rather than recover, the survivors would falter and humanity would actually disappear for good? 

It's an obvious enough question, but very few people have spent serious time looking into it -- possibly because it cuts across history, economics, and biology, among many other fields. There's no Disaster Apocalypse Studies department at any university, and governments have little incentive to plan for a future in which their country probably no longer even exists. 

The person who may have spent the most time looking at this specific question is Luisa Rodriguez — who has conducted research at Rethink Priorities, Oxford University's Future of Humanity Institute, the Forethought Foundation, and now here, at 80,000 Hours. 

Links to learn more, summary and full transcript. 

She wrote a series of articles earnestly trying to foresee how likely humanity would be to recover and build back after a full-on civilisational collapse. 

There are a couple of main stories people put forward for how a catastrophe like this would kill every single human on Earth — but Luisa doesn’t buy them. 

Story 1: Nuclear war has led to nuclear winter. There's a 10-year period during which a lot of the world is really inhospitable to agriculture. The survivors just aren't able to figure out how to feed themselves in the time period, so everyone dies of starvation or cold. 

Why Luisa doesn’t buy it:  

Catastrophes will almost inevitably be non-uniform in their effects. If 80,000 people survive, they’re not all going to be in the same city — it would look more like groups of 5,000 in a bunch of different places. 

People in some places will starve, but those in other places, such as New Zealand, will be able to fish, eat seaweed, grow potatoes, and find other sources of calories. 

It’d be an incredibly unlucky coincidence if the survivors of a nuclear war -- likely spread out all over the world -- happened to all be affected by natural disasters or were all prohibitively far away from areas suitable for agriculture (which aren’t the same areas you’d expect to be attacked in a nuclear war). 

Story 2: The catastrophe leads to hoarding and violence, and in addition to people being directly killed by the conflict, it distracts everyone so much from the key challenge of reestablishing agriculture that they simply fail. By the time they come to their senses, it’s too late -- they’ve used up too much of the resources they’d need to get agriculture going again. 

Why Luisa doesn’t buy it:  

We‘ve had lots of resource scarcity throughout history, and while we’ve seen examples of conflict petering out because basic needs aren’t being met, we’ve never seen the reverse.  

And again, even if this happens in some places -- even if some groups fought each other until they literally ended up starving to death — it would be completely bizarre for it to happen to every group in the world. You just need one group of around 300 people to survive for them to be able to rebuild the species. 

In this wide-ranging and free-flowing conversation, Luisa and Rob also cover: 

• What the world might actually look like after one of these catastrophes 
• The most valuable knowledge for survivors 
• How fast populations could rebound 
• ‘Boom and bust’ climate change scenarios 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>If modern human civilisation collapsed — as a result of nuclear war, severe climate change, or a much worse pandemic than COVID-19 — billions of people might die. 

That's terrible enough to contemplate. But what’s the probability that rather than recov</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/ed143ff0/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/ed143ff0/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#115 – David Wallace on the many-worlds theory of quantum mechanics and its implications</title>
      <itunes:title>#115 – David Wallace on the many-worlds theory of quantum mechanics and its implications</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c28c5f78-43f7-11ec-92bc-12cc53cc6b5b</guid>
      <link>https://80000hours.org/podcast/episodes/david-wallace-many-worlds-theory-of-quantum-mechanics/</link>
      <description>
        <![CDATA[<p>Quantum mechanics — our best theory of atoms, molecules, and the subatomic particles that make them up — underpins most of modern physics. But there are varying interpretations of what it means, all of them controversial in their own way.</p><p> Famously, quantum theory predicts that with the right setup, a cat can be made to be alive and dead at the same time. On the face of it, that sounds either meaningless or ridiculous.</p><p> According to today’s guest, David Wallace — professor at the University of Pittsburgh and one of the world's leading philosophers of physics — there are three broad ways experts react to this apparent dilemma:</p><p> 1. The theory must be wrong, and we need to change our philosophy to fix it.<br> 2. The theory must be wrong, and we need to change our physics to fix it.<br> 3. The theory is OK, and cats really can in some way be alive and dead simultaneously.</p><p> (David and Rob do their best to introduce quantum mechanics in the first 35 minutes of the episode, but it isn't the easiest thing to explain via audio alone. So if you need a refresher before jumping in, we recommend checking out our <a href="https://80k.link/DW"><strong>links to learn more, summary and full transcript.</strong></a>)</p><p> In 1955, physicist Hugh Everett bit the bullet on Option 3 and proposed Wallace's preferred solution to the puzzle: each time it's faced with a ‘quantum choice,’ the universe 'splits' into different worlds. Anything that has a probability greater than zero (from the perspective of quantum theory) happens in some branch — though more probable things happen in far more branches.</p><p> While not a consensus position, the ‘many-worlds’ approach is one of the top three most popular ways to make sense of what's going on, according to surveys of relevant experts.</p><p> Setting aside whether it's correct for a moment, one thing that's not often spelled out is what this approach would concretely imply if it were right.</p><p> <strong>Is there a world where Rob (the show's host) can roll a die a million times, and it comes up 6 every time?</strong></p><p> As David explains in this episode: absolutely, that’s completely possible — and if Rob rolled a die a million times, there would be a world like that.</p><p> <strong>Is there a world where Rob becomes president of the US?</strong></p><p> David thinks probably not. The things stopping Rob from becoming US president don’t seem down to random chance at the quantum level.</p><p> <strong>Is there a world where Rob deliberately murdered someone this morning?</strong></p><p> Only if he’s already predisposed to murder — becoming a different person in that way probably isn’t a matter of random fluctuations in our brains.</p><p> <strong>Is there a world where a horse-version of Rob hosts the 80,000 Horses Podcast?</strong></p><p> Well, due to the chance involved in evolution, it’s plausible that there are worlds where humans didn’t evolve, and intelligent horses have in some sense taken their place. And somewhere, fantastically distantly across the vast multiverse, there might even be a horse named Rob Wiblin who hosts a podcast, and who sounds remarkably like Rob. Though even then — it wouldn’t actually be Rob in the way we normally think of personal identity.</p><p> Rob and David also cover:</p><p> • If the many-worlds interpretation is right, should that change how we live our lives?<br> • Are our actions getting more (or less) important as the universe splits into finer and finer threads?<br> • Could we conceivably influence other branches of the multiverse?<br> • Alternatives to the many-worlds interpretation<br> • The practical value of physics today<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:15)</li><li>Introduction to quantum mechanics (00:08:10)</li><li>Why does quantum mechanics need an interpretation? (00:19:42)</li><li>Quantum mechanics in basic language (00:30:37)</li><li>Quantum field theory (00:33:13)</li><li>Different theories of quantum mechanics (00:38:49)</li><li>Many-worlds theory (00:43:14)</li><li>What stuff actually happens (00:52:09)</li><li>Can we count the worlds? (00:59:55)</li><li>Why anyone believes any of these (01:05:01)</li><li>Changing the physics (01:10:41)</li><li>Changing the philosophy (01:14:21)</li><li>Instrumentalism vs. realism (01:21:42)</li><li>Objections to many-worlds (01:35:26)</li><li>Why a consensus hasn’t emerged (01:50:59)</li><li>Practical implications of the many-worlds theory (01:57:11)</li><li>Are our actions getting more or less important? (02:04:21)</li><li>Does utility increase? (02:12:02)</li><li>Could we influence other branches? (02:17:01)</li><li>Should you do unpleasant things first? (02:19:52)</li><li>Progress in physics over the last 50 years (02:30:55)</li><li>Practical value of physics today (02:35:24)</li><li>Physics careers (02:43:56)</li><li>Subjective probabilities (02:48:39)</li><li>The philosophy of time (02:50:14)</li><li>David’s experience at Oxford (02:59:51) </li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel and Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Quantum mechanics — our best theory of atoms, molecules, and the subatomic particles that make them up — underpins most of modern physics. But there are varying interpretations of what it means, all of them controversial in their own way.</p><p> Famously, quantum theory predicts that with the right setup, a cat can be made to be alive and dead at the same time. On the face of it, that sounds either meaningless or ridiculous.</p><p> According to today’s guest, David Wallace — professor at the University of Pittsburgh and one of the world's leading philosophers of physics — there are three broad ways experts react to this apparent dilemma:</p><p> 1. The theory must be wrong, and we need to change our philosophy to fix it.<br> 2. The theory must be wrong, and we need to change our physics to fix it.<br> 3. The theory is OK, and cats really can in some way be alive and dead simultaneously.</p><p> (David and Rob do their best to introduce quantum mechanics in the first 35 minutes of the episode, but it isn't the easiest thing to explain via audio alone. So if you need a refresher before jumping in, we recommend checking out our <a href="https://80k.link/DW"><strong>links to learn more, summary and full transcript.</strong></a>)</p><p> In 1955, physicist Hugh Everett bit the bullet on Option 3 and proposed Wallace's preferred solution to the puzzle: each time it's faced with a ‘quantum choice,’ the universe 'splits' into different worlds. Anything that has a probability greater than zero (from the perspective of quantum theory) happens in some branch — though more probable things happen in far more branches.</p><p> While not a consensus position, the ‘many-worlds’ approach is one of the top three most popular ways to make sense of what's going on, according to surveys of relevant experts.</p><p> Setting aside whether it's correct for a moment, one thing that's not often spelled out is what this approach would concretely imply if it were right.</p><p> <strong>Is there a world where Rob (the show's host) can roll a die a million times, and it comes up 6 every time?</strong></p><p> As David explains in this episode: absolutely, that’s completely possible — and if Rob rolled a die a million times, there would be a world like that.</p><p> <strong>Is there a world where Rob becomes president of the US?</strong></p><p> David thinks probably not. The things stopping Rob from becoming US president don’t seem down to random chance at the quantum level.</p><p> <strong>Is there a world where Rob deliberately murdered someone this morning?</strong></p><p> Only if he’s already predisposed to murder — becoming a different person in that way probably isn’t a matter of random fluctuations in our brains.</p><p> <strong>Is there a world where a horse-version of Rob hosts the 80,000 Horses Podcast?</strong></p><p> Well, due to the chance involved in evolution, it’s plausible that there are worlds where humans didn’t evolve, and intelligent horses have in some sense taken their place. And somewhere, fantastically distantly across the vast multiverse, there might even be a horse named Rob Wiblin who hosts a podcast, and who sounds remarkably like Rob. Though even then — it wouldn’t actually be Rob in the way we normally think of personal identity.</p><p> Rob and David also cover:</p><p> • If the many-worlds interpretation is right, should that change how we live our lives?<br> • Are our actions getting more (or less) important as the universe splits into finer and finer threads?<br> • Could we conceivably influence other branches of the multiverse?<br> • Alternatives to the many-worlds interpretation<br> • The practical value of physics today<br> • And much more</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:15)</li><li>Introduction to quantum mechanics (00:08:10)</li><li>Why does quantum mechanics need an interpretation? (00:19:42)</li><li>Quantum mechanics in basic language (00:30:37)</li><li>Quantum field theory (00:33:13)</li><li>Different theories of quantum mechanics (00:38:49)</li><li>Many-worlds theory (00:43:14)</li><li>What stuff actually happens (00:52:09)</li><li>Can we count the worlds? (00:59:55)</li><li>Why anyone believes any of these (01:05:01)</li><li>Changing the physics (01:10:41)</li><li>Changing the philosophy (01:14:21)</li><li>Instrumentalism vs. realism (01:21:42)</li><li>Objections to many-worlds (01:35:26)</li><li>Why a consensus hasn’t emerged (01:50:59)</li><li>Practical implications of the many-worlds theory (01:57:11)</li><li>Are our actions getting more or less important? (02:04:21)</li><li>Does utility increase? (02:12:02)</li><li>Could we influence other branches? (02:17:01)</li><li>Should you do unpleasant things first? (02:19:52)</li><li>Progress in physics over the last 50 years (02:30:55)</li><li>Practical value of physics today (02:35:24)</li><li>Physics careers (02:43:56)</li><li>Subjective probabilities (02:48:39)</li><li>The philosophy of time (02:50:14)</li><li>David’s experience at Oxford (02:59:51) </li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel and Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 12 Nov 2021 20:55:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/23079bb3/b8238b34.mp3" length="91099470" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/wVb8GeD5FMeLte1dkccusxaU8_APVKy69SwIYAsy4FA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjEv/MTY4MzU0NDY5Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11387</itunes:duration>
      <itunes:summary>Quantum mechanics — our best theory of atoms, molecules, and the subatomic particles that make them up — underpins most of modern physics. But there are varying interpretations of what it means, all of them controversial in their own way. 

Famously, quantum theory predicts that with the right setup, a cat can be made to be alive and dead at the same time. On the face of it, that sounds either meaningless or ridiculous. 

According to today’s guest, David Wallace — professor at the University of Pittsburgh and one of the world's leading philosophers of physics — there are three broad ways experts react to this apparent dilemma: 

1. The theory must be wrong, and we need to change our philosophy to fix it.  
2. The theory must be wrong, and we need to change our physics to fix it. 
3. The theory is OK, and cats really can in some way be alive and dead simultaneously. 

(David and Rob do their best to introduce quantum mechanics in the first 35 minutes of the episode, but it isn't the easiest thing to explain via audio alone. So if you need a refresher before jumping in, we recommend checking out our links to learn more, summary and full transcript.)

In 1955, physicist Hugh Everett bit the bullet on Option 3 and proposed Wallace's preferred solution to the puzzle: each time it's faced with a ‘quantum choice,’ the universe 'splits' into different worlds. Anything that has a probability greater than zero (from the perspective of quantum theory) happens in some branch — though more probable things happen in far more branches. 

While not a consensus position, the ‘many-worlds’ approach is one of the top three most popular ways to make sense of what's going on, according to surveys of relevant experts. 

Setting aside whether it's correct for a moment, one thing that's not often spelled out is what this approach would concretely imply if it were right. 

Is there a world where Rob (the show's host) can roll a die a million times, and it comes up 6 every time? 

As David explains in this episode: absolutely, that’s completely possible  — and if Rob rolled a die a million times, there would be a world like that. 

Is there a world where Rob becomes president of the US? 

David thinks probably not. The things stopping Rob from becoming US president don’t seem down to random chance at the quantum level. 

Is there a world where Rob deliberately murdered someone this morning? 

Only if he’s already predisposed to murder — becoming a different person in that way probably isn’t a matter of random fluctuations in our brains. 

Is there a world where a horse-version of Rob hosts the 80,000 Horses Podcast? 

Well, due to the chance involved in evolution, it’s plausible that there are worlds where humans didn’t evolve, and intelligent horses have in some sense taken their place. And somewhere, fantastically distantly across the vast multiverse, there might even be a horse named Rob Wiblin who hosts a podcast, and who sounds remarkably like Rob. Though even then — it wouldn’t actually be Rob in the way we normally think of personal identity. 

Rob and David also cover: 

• If the many-worlds interpretation is right, should that change how we live our lives? 
• Are our actions getting more (or less) important as the universe splits into finer and finer threads? 
• Could we conceivably influence other branches of the multiverse? 
• Alternatives to the many-worlds interpretation 
• The practical value of physics today 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel and Katy Moore</itunes:summary>
      <itunes:subtitle>Quantum mechanics — our best theory of atoms, molecules, and the subatomic particles that make them up — underpins most of modern physics. But there are varying interpretations of what it means, all of them controversial in their own way. 

Famously, qu</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/23079bb3/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/23079bb3/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#114 – Maha Rehman on working with governments to rapidly deliver masks to millions of people</title>
      <itunes:title>#114 – Maha Rehman on working with governments to rapidly deliver masks to millions of people</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1ec6a6ca-336a-11ec-9d21-12f2ce1cd429</guid>
      <link>https://80000hours.org/podcast/episodes/maha-rehman-governments-masks-millions/</link>
      <description>
        <![CDATA[<p>It’s hard to believe, but until recently there had never been a large field trial that addressed these simple and obvious questions:</p><p>1. When ordinary people wear face masks, does it actually reduce the spread of respiratory diseases?<br>2. And if so, how do you get people to wear masks more often?</p><p>It turns out the first question is remarkably challenging to answer, but it's well worth doing nonetheless. Among other reasons, the first good trial of this prompted Maha Rehman — Policy Director at the Mahbub Ul Haq Research Centre — as well as a range of others to immediately use the findings to help tens of millions of people across South Asia, even before the results were public.</p><p><a href="https://80k.link/MR1"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The groundbreaking Bangladesh RCT that inspired her to take action found that:</p><p> • A 30% increase in mask wearing reduced total infections by 10%.<br> • The effect was more pronounced for surgical masks compared to cloth masks (plus ~50% effectiveness).<br> • Mask wearing also led to an increase in social distancing.<br> • Of all the incentives tested, the only thing that impacted mask wearing was their colour (people preferred blue over green, and red over purple!).</p><p> The research was done by social scientists at Yale, Berkeley, and Stanford, among others. It applied a program they called ‘NORM’ in half of 600 villages in which about 350,000 people lived. NORM has four components, which the researchers expected would work well for the general public:</p><p> N: no-cost distribution<br> O: offering information<br> R: reinforcing the message and the information in the field<br> M: modeling</p><p> Basically you make sure a community has enough masks and you tell them why it’s important to wear them. You also reinforce the message periodically in markets and mosques, and via role models and promoters in the community itself.</p><p> Tipped off that these positive findings were on the way, Maha took this program and rushed to put it into action in Lahore, Pakistan, a city with a population of about 13 million, before the Delta variant could sweep through the region.</p><p> Maha had already been doing a lot of data work on COVID policy over the past year, and that allowed her to quickly reach out to the relevant stakeholders — getting them interested and excited.</p><p> Governments aren’t exactly known for being super innovative, but in March and April Lahore was going through a very deadly third wave of COVID — so the commissioner quickly jumped on this approach, providing an endorsement as well as resources.</p><p> Together with the original researchers, Maha and her team at LUMS collected baseline data that allowed them to map the mask-wearing rate in every part of Lahore, in both markets and mosques. And then based on that data, they adapted the original rural-focused model to a very different urban setting.</p><p> The scale of this project was daunting, and in today’s episode Maha tells Rob all about the day-to-day experiences and stresses required to actually make it happen.</p><p> They also discuss:</p><p> • The challenges of data collection in this context<br> • Disasters and emergencies she had to respond to in the middle of the project<br> • What she learned from working closely with the Lahore Commissioner's Office<br> • How to get governments to provide you with large amounts of data for your research<br> • How she adapted from a more academic role to a ‘getting stuff done’ role<br> • How to reduce waste in government procurement<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:33)</li><li>Bangladesh RCT (00:06:24)</li><li>The NORM model (00:08:34)</li><li>Results of the experiment (00:10:46)</li><li>Experimental design (00:20:35)</li><li>Adapting the findings from Bangladesh to Lahore (00:23:55)</li><li>Collecting data (00:34:09)</li><li>Working with governments (00:38:38)</li><li>Coordination (00:44:53)</li><li>Disasters and emergencies (00:56:01)</li><li>Sending out masks to every single person in Lahore (00:59:15)</li><li>How Maha adapted to her role (01:07:17)</li><li>Logistic aptitude (01:11:45)</li><li>Disappointments (01:14:13)</li><li>Procurement RCT (01:16:51)</li><li>What we can learn (01:31:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>It’s hard to believe, but until recently there had never been a large field trial that addressed these simple and obvious questions:</p><p>1. When ordinary people wear face masks, does it actually reduce the spread of respiratory diseases?<br>2. And if so, how do you get people to wear masks more often?</p><p>It turns out the first question is remarkably challenging to answer, but it's well worth doing nonetheless. Among other reasons, the first good trial of this prompted Maha Rehman — Policy Director at the Mahbub Ul Haq Research Centre — as well as a range of others to immediately use the findings to help tens of millions of people across South Asia, even before the results were public.</p><p><a href="https://80k.link/MR1"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The groundbreaking Bangladesh RCT that inspired her to take action found that:</p><p> • A 30% increase in mask wearing reduced total infections by 10%.<br> • The effect was more pronounced for surgical masks compared to cloth masks (plus ~50% effectiveness).<br> • Mask wearing also led to an increase in social distancing.<br> • Of all the incentives tested, the only thing that impacted mask wearing was their colour (people preferred blue over green, and red over purple!).</p><p> The research was done by social scientists at Yale, Berkeley, and Stanford, among others. It applied a program they called ‘NORM’ in half of 600 villages in which about 350,000 people lived. NORM has four components, which the researchers expected would work well for the general public:</p><p> N: no-cost distribution<br> O: offering information<br> R: reinforcing the message and the information in the field<br> M: modeling</p><p> Basically you make sure a community has enough masks and you tell them why it’s important to wear them. You also reinforce the message periodically in markets and mosques, and via role models and promoters in the community itself.</p><p> Tipped off that these positive findings were on the way, Maha took this program and rushed to put it into action in Lahore, Pakistan, a city with a population of about 13 million, before the Delta variant could sweep through the region.</p><p> Maha had already been doing a lot of data work on COVID policy over the past year, and that allowed her to quickly reach out to the relevant stakeholders — getting them interested and excited.</p><p> Governments aren’t exactly known for being super innovative, but in March and April Lahore was going through a very deadly third wave of COVID — so the commissioner quickly jumped on this approach, providing an endorsement as well as resources.</p><p> Together with the original researchers, Maha and her team at LUMS collected baseline data that allowed them to map the mask-wearing rate in every part of Lahore, in both markets and mosques. And then based on that data, they adapted the original rural-focused model to a very different urban setting.</p><p> The scale of this project was daunting, and in today’s episode Maha tells Rob all about the day-to-day experiences and stresses required to actually make it happen.</p><p> They also discuss:</p><p> • The challenges of data collection in this context<br> • Disasters and emergencies she had to respond to in the middle of the project<br> • What she learned from working closely with the Lahore Commissioner's Office<br> • How to get governments to provide you with large amounts of data for your research<br> • How she adapted from a more academic role to a ‘getting stuff done’ role<br> • How to reduce waste in government procurement<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:33)</li><li>Bangladesh RCT (00:06:24)</li><li>The NORM model (00:08:34)</li><li>Results of the experiment (00:10:46)</li><li>Experimental design (00:20:35)</li><li>Adapting the findings from Bangladesh to Lahore (00:23:55)</li><li>Collecting data (00:34:09)</li><li>Working with governments (00:38:38)</li><li>Coordination (00:44:53)</li><li>Disasters and emergencies (00:56:01)</li><li>Sending out masks to every single person in Lahore (00:59:15)</li><li>How Maha adapted to her role (01:07:17)</li><li>Logistic aptitude (01:11:45)</li><li>Disappointments (01:14:13)</li><li>Procurement RCT (01:16:51)</li><li>What we can learn (01:31:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 22 Oct 2021 19:21:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/421b407f/8a0279ec.mp3" length="49398135" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Lr-LSzypY9sGeO0Lz9heYEmdmEvSVsIZPB67K_wAyJk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NjAv/MTY4MzU0NDY5NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6175</itunes:duration>
      <itunes:summary>It’s hard to believe, but until recently there had never been a large field trial that addressed these simple and obvious questions: 

1. When ordinary people wear face masks, does it actually reduce the spread of respiratory diseases? 
2. And if so, how do you get people to wear masks more often? 

It turns out the first question is remarkably challenging to answer, but it's well worth doing nonetheless. Among other reasons, the first good trial of this prompted Maha Rehman — Policy Director at the Mahbub Ul Haq Research Centre — as well as a range of others to immediately use the findings to help tens of millions of people across South Asia, even before the results were public. 

Links to learn more, summary and full transcript.

The groundbreaking Bangladesh RCT that inspired her to take action found that: 

• A 30% increase in mask wearing reduced total infections by 10%. 
• The effect was more pronounced for surgical masks compared to cloth masks (plus ~50% effectiveness). 
• Mask wearing also led to an increase in social distancing. 
• Of all the incentives tested, the only thing that impacted mask wearing was their colour (people preferred blue over green, and red over purple!). 

The research was done by social scientists at Yale, Berkeley, and Stanford, among others. It applied a program they called ‘NORM’ in half of 600 villages in which about 350,000 people lived. NORM has four components, which the researchers expected would work well for the general public:  

N: no-cost distribution 
O: offering information 
R: reinforcing the message and the information in the field 
M: modeling 

Basically you make sure a community has enough masks and you tell them why it’s important to wear them. You also reinforce the message periodically in markets and mosques, and via role models and promoters in the community itself. 

Tipped off that these positive findings were on the way, Maha took this program and rushed to put it into action in Lahore, Pakistan, a city with a population of about 13 million, before the Delta variant could sweep through the region. 

Maha had already been doing a lot of data work on COVID policy over the past year, and that allowed her to quickly reach out to the relevant stakeholders — getting them interested and excited. 

Governments aren’t exactly known for being super innovative, but in March and April Lahore was going through a very deadly third wave of COVID — so the commissioner quickly jumped on this approach, providing an endorsement as well as resources. 

Together with the original researchers, Maha and her team at LUMS collected baseline data that allowed them to map the mask-wearing rate in every part of Lahore, in both markets and mosques. And then based on that data, they adapted the original rural-focused model to a very different urban setting.  

The scale of this project was daunting, and in today’s episode Maha tells Rob all about the day-to-day experiences and stresses required to actually make it happen.  

They also discuss: 

• The challenges of data collection in this context 
• Disasters and emergencies she had to respond to in the middle of the project 
• What she learned from working closely with the Lahore Commissioner's Office 
• How to get governments to provide you with large amounts of data for your research 
• How she adapted from a more academic role to a ‘getting stuff done’ role 
• How to reduce waste in government procurement 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>It’s hard to believe, but until recently there had never been a large field trial that addressed these simple and obvious questions: 

1. When ordinary people wear face masks, does it actually reduce the spread of respiratory diseases? 
2. And if so, h</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/421b407f/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/421b407f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>We just put up a new compilation of ten core episodes of the show</title>
      <itunes:title>We just put up a new compilation of ten core episodes of the show</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4beb1536-3195-11ec-8871-12f2ce1cd429</guid>
      <link>https://share.transistor.fm/s/5278f24f</link>
      <description>
        <![CDATA[<p>We recently launched a new podcast feed that might be useful to you and people you know. </p><p>It's called <em><a href="https://80000hours.org/podcast/effective-altruism-ten-global-problems/?utm_campaign=podcast__tgp-podcastwords&amp;utm_source=tgp&amp;utm_medium=podcast"><b>Effective Altruism: Ten Global Problems</b></a>, </em>and it's a collection of ten top episodes of this show, selected to help listeners quickly get up to speed on ten pressing problems that the effective altruism community is working to solve.</p>

It's a companion to our other compilation <em><a href="https://80000hours.org/podcast/effective-altruism-an-introduction/"><b>Effective Altruism: An Introduction</b></a></em>, which explores the big picture debates within the community and how to set priorities in order to have the greatest impact.<p>These ten episodes cover:</p><ul>
<li>The cheapest ways to improve education in the developing world</li>
<li>How dangerous is climate change and what are the most effective ways to reduce it?</li>
<li>Using new technologies to prevent another disastrous pandemic</li>
<li>Ways to simultaneously reduce both police misconduct and crime</li>
<li>All the major approaches being taken to end factory farming</li>
<li>How advances in artificial intelligence could go very right or very wrong</li>
<li>Other big threats to the future of humanity — such as a nuclear war — and how can we make our species wiser and more resilient </li>
<li>One problem few even recognise as a problem at all</li>
</ul>

<p>The selection is ideal for people who are completely new to the effective altruist way of thinking, as well as those who are familiar with effective altruism but new to The 80,000 Hours Podcast.</p><p>If someone in your life wants to get an understanding of what 80,000 Hours or effective altruism are all about, and prefers to listen to things rather than read, this is a great resource to direct them to.</p><p>You can find it by searching for effective altruism in whatever podcasting app you use, or by going to <a href="https://80000hours.org/podcast/effective-altruism-ten-global-problems/?utm_campaign=podcast__tgp-podcastwords&amp;utm_source=tgp&amp;utm_medium=podcast"><b>80000hours.org/ten</b></a>.</p><p>We'd love to hear how you go listening to it yourself, or sharing it with others in your life. Get in touch by emailing podcast@80000hours.org.</p>
]]>
      </description>
      <content:encoded>
        <![CDATA[<p>We recently launched a new podcast feed that might be useful to you and people you know. </p><p>It's called <em><a href="https://80000hours.org/podcast/effective-altruism-ten-global-problems/?utm_campaign=podcast__tgp-podcastwords&amp;utm_source=tgp&amp;utm_medium=podcast"><b>Effective Altruism: Ten Global Problems</b></a>, </em>and it's a collection of ten top episodes of this show, selected to help listeners quickly get up to speed on ten pressing problems that the effective altruism community is working to solve.</p>

It's a companion to our other compilation <em><a href="https://80000hours.org/podcast/effective-altruism-an-introduction/"><b>Effective Altruism: An Introduction</b></a></em>, which explores the big picture debates within the community and how to set priorities in order to have the greatest impact.<p>These ten episodes cover:</p><ul>
<li>The cheapest ways to improve education in the developing world</li>
<li>How dangerous is climate change and what are the most effective ways to reduce it?</li>
<li>Using new technologies to prevent another disastrous pandemic</li>
<li>Ways to simultaneously reduce both police misconduct and crime</li>
<li>All the major approaches being taken to end factory farming</li>
<li>How advances in artificial intelligence could go very right or very wrong</li>
<li>Other big threats to the future of humanity — such as a nuclear war — and how can we make our species wiser and more resilient </li>
<li>One problem few even recognise as a problem at all</li>
</ul>

<p>The selection is ideal for people who are completely new to the effective altruist way of thinking, as well as those who are familiar with effective altruism but new to The 80,000 Hours Podcast.</p><p>If someone in your life wants to get an understanding of what 80,000 Hours or effective altruism are all about, and prefers to listen to things rather than read, this is a great resource to direct them to.</p><p>You can find it by searching for effective altruism in whatever podcasting app you use, or by going to <a href="https://80000hours.org/podcast/effective-altruism-ten-global-problems/?utm_campaign=podcast__tgp-podcastwords&amp;utm_source=tgp&amp;utm_medium=podcast"><b>80000hours.org/ten</b></a>.</p><p>We'd love to hear how you go listening to it yourself, or sharing it with others in your life. Get in touch by emailing podcast@80000hours.org.</p>
]]>
      </content:encoded>
      <pubDate>Wed, 20 Oct 2021 18:54:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5278f24f/3086d58e.mp3" length="1452304" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/qt4NtTQ93ECzvevdi2gwHrAo3PmwCmZuKmVu50FNycE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTkv/MTY4MzU0NDY5My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>182</itunes:duration>
      <itunes:summary>
We recently launched a new podcast feed that might be useful to you and people you know. It's called Effective Altruism: Ten Global Problems, and it's a collection of ten top episodes of this show, selected to help listeners quickly get up to speed on ten pressing problems that the effective altruism community is working to solve.

It's a companion to our other compilation Effective Altruism: An Introduction, which explores the big picture debates within the community and how to set priorities in order to have the greatest impact.These ten episodes cover:
The cheapest ways to improve education in the developing world
How dangerous is climate change and what are the most effective ways to reduce it?
Using new technologies to prevent another disastrous pandemic
Ways to simultaneously reduce both police misconduct and crime
All the major approaches being taken to end factory farming
How advances in artificial intelligence could go very right or very wrong
Other big threats to the future of humanity — such as a nuclear war — and how can we make our species wiser and more resilient 
One problem few even recognise as a problem at all


The selection is ideal for people who are completely new to the effective altruist way of thinking, as well as those who are familiar with effective altruism but new to The 80,000 Hours Podcast.If someone in your life wants to get an understanding of what 80,000 Hours or effective altruism are all about, and prefers to listen to things rather than read, this is a great resource to direct them to.You can find it by searching for effective altruism in whatever podcasting app you use, or by going to 80000hours.org/ten.We'd love to hear how you go listening to it yourself, or sharing it with others in your life. Get in touch by emailing podcast@80000hours.org.</itunes:summary>
      <itunes:subtitle>
We recently launched a new podcast feed that might be useful to you and people you know. It's called Effective Altruism: Ten Global Problems, and it's a collection of ten top episodes of this show, selected to help listeners quickly get up to speed on te</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/5278f24f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#113 – Varsha Venugopal on using gossip to help vaccinate every child in India</title>
      <itunes:title>#113 – Varsha Venugopal on using gossip to help vaccinate every child in India</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3114dc3a-3053-11ec-996b-125cacb61343</guid>
      <link>https://80000hours.org/podcast/episodes/varsha-venugopal-vaccinations-children-india/</link>
      <description>
        <![CDATA[<p>Our failure to make sure all kids globally get all of their basic vaccinations leads to 1.5 million child deaths every year.</p><p>According to today’s guest, Varsha Venugopal, for the great majority this has nothing to do with weird conspiracy theories or medical worries — in India 80% of undervaccinated children are already getting some shots. They just aren't getting all of them, for the tragically mundane reason that life can get in the way.</p><p><a href="https://80k.link/VV1"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> As Varsha says, we're all sometimes guilty of "valuing our present very differently from the way we value the future", leading to short-term thinking whether about getting vaccines or going to the gym.</p><p> So who should we call on to help fix this universal problem? The government, extended family, or maybe village elders?</p><p> Varsha says that research shows the most influential figures might actually be local gossips.</p><p> In 2018, Varsha heard about the ideas around effective altruism for the first time. By the end of 2019, she’d gone through Charity Entrepreneurship’s strategy incubation program, and quit her normal, stable job to co-found Suvita, a non-profit focused on improving the uptake of immunization in India, which focuses on two models:<br> 1. Sending SMS reminders directly to parents and carers<br> 2. Gossip</p><p> The first one is intuitive. You collect birth registers, digitize the paper records, process the data, and send out personalised SMS messages to hundreds of thousands of families. The effect size varies depending on the context but these messages usually increase vaccination rates by 8-18%.</p><p> The second approach is less intuitive and isn't yet entirely understood either.</p><p> Here’s what happens: Suvita calls up random households and asks, “if there were an event in town, who would be most likely to tell you about it?”</p><p> In over 90% of the cases, the households gave both the name and the phone number of a local ‘influencer’.</p><p> And when tracked down, more than 95% of the most frequently named 'influencers' agreed to become vaccination ambassadors. Those ambassadors then go on to share information about when and where to get vaccinations, in whatever way seems best to them.</p><p> When tested by a team of top academics at the Poverty Action Lab (J-PAL) it raised vaccination rates by 10 percentage points, or about 27%.</p><p> The advantage of SMS reminders is that they’re easier to scale up. But Varsha says the ambassador program isn’t actually that far from being a scalable model as well.</p><p> A phone call to get a name, another call to ask the influencer join, and boom — you might have just covered a whole village rather than just a single family.</p><p> Varsha says that Suvita has two major challenges on the horizon:<br> 1. Maintaining the same degree of oversight of their surveyors as they attempt to scale up the program, in order to ensure the program continues to work just as well<br> 2. Deciding between focusing on reaching a few more additional districts now vs. making longer term investments which could build up to a future exponential increase.</p><p> In this episode, Varsha and Rob talk about making these kinds of high-stakes, high-stress decisions, as well as:<br> • How Suvita got started, and their experience with Charity Entrepreneurship<br> • Weaknesses of the J-PAL studies<br> • The importance of co-founders<br> • Deciding how broad a program should be<br> • Varsha’s day-to-day experience<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:47)</li><li>The problem of undervaccinated kids (00:03:16)</li><li>Suvita (00:12:47)</li><li>Evidence on SMS reminders (00:20:30)</li><li>Gossip intervention (00:28:43)</li><li>Why parents aren’t already prioritizing vaccinations (00:38:29)</li><li>Weaknesses of studies (00:43:01)</li><li>Biggest challenges for Suvita (00:46:05)</li><li>Staff location (01:06:57)</li><li>Charity Entrepreneurship (01:14:37)</li><li>The importance of co-founders (01:23:23)</li><li>Deciding how broad a program should be (01:28:29)</li><li>Careers at Suvita (01:34:11)</li><li>Varsha’s advice (01:42:30)</li><li>Varsha’s day-to-day experience (01:56:19)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Our failure to make sure all kids globally get all of their basic vaccinations leads to 1.5 million child deaths every year.</p><p>According to today’s guest, Varsha Venugopal, for the great majority this has nothing to do with weird conspiracy theories or medical worries — in India 80% of undervaccinated children are already getting some shots. They just aren't getting all of them, for the tragically mundane reason that life can get in the way.</p><p><a href="https://80k.link/VV1"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> As Varsha says, we're all sometimes guilty of "valuing our present very differently from the way we value the future", leading to short-term thinking whether about getting vaccines or going to the gym.</p><p> So who should we call on to help fix this universal problem? The government, extended family, or maybe village elders?</p><p> Varsha says that research shows the most influential figures might actually be local gossips.</p><p> In 2018, Varsha heard about the ideas around effective altruism for the first time. By the end of 2019, she’d gone through Charity Entrepreneurship’s strategy incubation program, and quit her normal, stable job to co-found Suvita, a non-profit focused on improving the uptake of immunization in India, which focuses on two models:<br> 1. Sending SMS reminders directly to parents and carers<br> 2. Gossip</p><p> The first one is intuitive. You collect birth registers, digitize the paper records, process the data, and send out personalised SMS messages to hundreds of thousands of families. The effect size varies depending on the context but these messages usually increase vaccination rates by 8-18%.</p><p> The second approach is less intuitive and isn't yet entirely understood either.</p><p> Here’s what happens: Suvita calls up random households and asks, “if there were an event in town, who would be most likely to tell you about it?”</p><p> In over 90% of the cases, the households gave both the name and the phone number of a local ‘influencer’.</p><p> And when tracked down, more than 95% of the most frequently named 'influencers' agreed to become vaccination ambassadors. Those ambassadors then go on to share information about when and where to get vaccinations, in whatever way seems best to them.</p><p> When tested by a team of top academics at the Poverty Action Lab (J-PAL) it raised vaccination rates by 10 percentage points, or about 27%.</p><p> The advantage of SMS reminders is that they’re easier to scale up. But Varsha says the ambassador program isn’t actually that far from being a scalable model as well.</p><p> A phone call to get a name, another call to ask the influencer join, and boom — you might have just covered a whole village rather than just a single family.</p><p> Varsha says that Suvita has two major challenges on the horizon:<br> 1. Maintaining the same degree of oversight of their surveyors as they attempt to scale up the program, in order to ensure the program continues to work just as well<br> 2. Deciding between focusing on reaching a few more additional districts now vs. making longer term investments which could build up to a future exponential increase.</p><p> In this episode, Varsha and Rob talk about making these kinds of high-stakes, high-stress decisions, as well as:<br> • How Suvita got started, and their experience with Charity Entrepreneurship<br> • Weaknesses of the J-PAL studies<br> • The importance of co-founders<br> • Deciding how broad a program should be<br> • Varsha’s day-to-day experience<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:47)</li><li>The problem of undervaccinated kids (00:03:16)</li><li>Suvita (00:12:47)</li><li>Evidence on SMS reminders (00:20:30)</li><li>Gossip intervention (00:28:43)</li><li>Why parents aren’t already prioritizing vaccinations (00:38:29)</li><li>Weaknesses of studies (00:43:01)</li><li>Biggest challenges for Suvita (00:46:05)</li><li>Staff location (01:06:57)</li><li>Charity Entrepreneurship (01:14:37)</li><li>The importance of co-founders (01:23:23)</li><li>Deciding how broad a program should be (01:28:29)</li><li>Careers at Suvita (01:34:11)</li><li>Varsha’s advice (01:42:30)</li><li>Varsha’s day-to-day experience (01:56:19)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 18 Oct 2021 22:43:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4b02d2e5/8ab0ce63.mp3" length="60350292" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/_dRly-zrPl2Qjgn26bXL2zGPDwzguciv9eN7ku3LPdQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTgv/MTY4MzU0NDY5Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7544</itunes:duration>
      <itunes:summary>Our failure to make sure all kids globally get all of their basic vaccinations leads to 1.5 million child deaths every year. 

According to today’s guest, Varsha Venugopal, for the great majority this has nothing to do with weird conspiracy theories or medical worries — in India 80% of undervaccinated children are already getting some shots. They just aren't getting all of them, for the tragically mundane reason that life can get in the way. 

Links to learn more, summary and full transcript. 

As Varsha says, we're all sometimes guilty of "valuing our present very differently from the way we value the future", leading to short-term thinking whether about getting vaccines or going to the gym. 

So who should we call on to help fix this universal problem? The government, extended family, or maybe village elders? 

Varsha says that research shows the most influential figures might actually be local gossips.  

In 2018, Varsha heard about the ideas around effective altruism for the first time. By the end of 2019, she’d gone through Charity Entrepreneurship’s strategy incubation program, and quit her normal, stable job to co-found Suvita, a non-profit focused on improving the uptake of immunization in India, which focuses on two models: 

1. Sending SMS reminders directly to parents and carers  
2. Gossip  

The first one is intuitive. You collect birth registers, digitize the paper records, process the data, and send out personalised SMS messages to hundreds of thousands of families. The effect size varies depending on the context but these messages usually increase vaccination rates by 8-18%. 

The second approach is less intuitive and isn't yet entirely understood either. 

Here’s what happens: Suvita calls up random households and asks, “if there were an event in town, who would be most likely to tell you about it?” 

In over 90% of the cases, the households gave both the name and the phone number of a local ‘influencer’. 

And when tracked down, more than 95% of the most frequently named 'influencers' agreed to become vaccination ambassadors. Those ambassadors then go on to share information about when and where to get vaccinations, in whatever way seems best to them. 

When tested by a team of top academics at the Poverty Action Lab (J-PAL) it raised vaccination rates by 10 percentage points, or about 27%. 

The advantage of SMS reminders is that they’re easier to scale up. But Varsha says the ambassador program isn’t actually that far from being a scalable model as well.  

A phone call to get a name, another call to ask the influencer join, and boom — you might have just covered a whole village rather than just a single family. 
Varsha says that Suvita has two major challenges on the horizon: 
1. Maintaining the same degree of oversight of their surveyors as they attempt to scale up the program, in order to ensure the program continues to work just as well 
2. Deciding between focusing on reaching a few more additional districts now vs. making longer term investments which could build up to a future exponential increase. 

In this episode, Varsha and Rob talk about making these kinds of high-stakes, high-stress decisions, as well as: 

• How Suvita got started, and their experience with Charity Entrepreneurship 
• Weaknesses of the J-PAL studies 
• The importance of co-founders 
• Deciding how broad a program should be 
• Varsha’s day-to-day experience 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Our failure to make sure all kids globally get all of their basic vaccinations leads to 1.5 million child deaths every year. 

According to today’s guest, Varsha Venugopal, for the great majority this has nothing to do with weird conspiracy theories or </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/4b02d2e5/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/4b02d2e5/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#112 – Carl Shulman on the common-sense case for existential risk work and its practical implications</title>
      <itunes:title>#112 – Carl Shulman on the common-sense case for existential risk work and its practical implications</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">92016b9a-262f-11ec-a122-0eade52d5c0f</guid>
      <link>https://80000hours.org/podcast/episodes/carl-shulman-common-sense-case-existential-risks/</link>
      <description>
        <![CDATA[<p>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation.</p><p>But the policy of US government agencies is already to spend up to $4 million to save the life of a citizen, making the death of all Americans a $1,300,000,000,000,000 disaster.</p><p>According to Carl Shulman, research associate at Oxford University's Future of Humanity Institute, that means you don’t need any fancy philosophical arguments about the value or size of the future to justify working to reduce existential risk — it passes a mundane cost-benefit analysis whether or not you place any value on the long-term future.</p><p><a href="https://80k.link/CS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The key reason to make it a top priority is factual, not philosophical. That is, the risk of a disaster that kills billions of people alive today is alarmingly high, and it can be reduced at a reasonable cost. A back-of-the-envelope version of the argument runs:<br> • The US government is willing to pay up to $4 million (depending on the agency) to save the life of an American.<br> • So saving all US citizens at any given point in time would be worth $1,300 trillion.<br> • If you believe that the risk of human extinction over the next century is something like one in six (as Toby Ord suggests is a reasonable figure in his book <em>The Precipice</em>), then it would be worth the US government spending up to $2.2 trillion to reduce that risk by just 1%, in terms of American lives saved alone.<br> • Carl thinks it would cost a <em>lot</em> less than that to achieve a 1% risk reduction if the money were spent intelligently. So it easily passes a government cost-benefit test, with a very big benefit-to-cost ratio — likely over 1000:1 today.</p><p> This argument helped NASA get funding to scan the sky for any asteroids that might be on a collision course with Earth, and it was directly promoted by famous economists like Richard Posner, Larry Summers, and Cass Sunstein.</p><p> If the case is clear enough, why hasn't it already motivated a lot more spending or regulations to limit existential risks — enough to drive down what any additional efforts would achieve?</p><p> Carl thinks that one key barrier is that infrequent disasters are rarely politically salient. Research indicates that extra money is spent on flood defences in the years immediately following a massive flood — but as memories fade, that spending quickly dries up. Of course the annual probability of a disaster was the same the whole time; all that changed is what voters had on their minds.</p><p> Carl expects that all the reasons we didn’t adequately prepare for or respond to COVID-19 — with excess mortality over 15 million and costs well over $10 trillion — bite even harder when it comes to threats we've never faced before, such as engineered pandemics, risks from advanced artificial intelligence, and so on.</p><p> Today’s episode is in part our way of trying to improve this situation. In today’s wide-ranging conversation, Carl and Rob also cover:<br> • A few reasons Carl isn't excited by 'strong longtermism'<br> • How x-risk reduction compares to GiveWell recommendations<br> • Solutions for asteroids, comets, supervolcanoes, nuclear war, pandemics, and climate change<br> • The history of bioweapons<br> • Whether gain-of-function research is justifiable<br> • Successes and failures around COVID-19<br> • The history of existential risk<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:34)</li><li>A few reasons Carl isn't excited by strong longtermism (00:03:47)</li><li>Longtermism isn’t necessary for wanting to reduce big x-risks (00:08:21)</li><li>Why we don’t adequately prepare for disasters (00:11:16)</li><li>International programs to stop asteroids and comets (00:18:55)</li><li>Costs and political incentives around COVID (00:23:52)</li><li>How x-risk reduction compares to GiveWell recommendations (00:34:34)</li><li>Solutions for asteroids, comets, and supervolcanoes (00:50:22)</li><li>Solutions for climate change (00:54:15)</li><li>Solutions for nuclear weapons (01:02:18)</li><li>The history of bioweapons (01:22:41)</li><li>Gain-of-function research (01:34:22)</li><li>Solutions for bioweapons and natural pandemics (01:45:31)</li><li>Successes and failures around COVID-19 (01:58:26)</li><li>Who to trust going forward (02:09:09)</li><li>The history of existential risk (02:15:07)</li><li>The most compelling risks (02:24:59)</li><li>False alarms about big risks in the past (02:34:22)</li><li>Suspicious convergence around x-risk reduction (02:49:31)</li><li>How hard it would be to convince governments (02:57:59)</li><li>Defensive epistemology (03:04:34)</li><li>Hinge of history debate (03:16:01)</li><li>Technological progress can’t keep up for long (03:21:51)</li><li>Strongest argument against this being a really pivotal time (03:37:29)</li><li>How Carl unwinds (03:45:30)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation.</p><p>But the policy of US government agencies is already to spend up to $4 million to save the life of a citizen, making the death of all Americans a $1,300,000,000,000,000 disaster.</p><p>According to Carl Shulman, research associate at Oxford University's Future of Humanity Institute, that means you don’t need any fancy philosophical arguments about the value or size of the future to justify working to reduce existential risk — it passes a mundane cost-benefit analysis whether or not you place any value on the long-term future.</p><p><a href="https://80k.link/CS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> The key reason to make it a top priority is factual, not philosophical. That is, the risk of a disaster that kills billions of people alive today is alarmingly high, and it can be reduced at a reasonable cost. A back-of-the-envelope version of the argument runs:<br> • The US government is willing to pay up to $4 million (depending on the agency) to save the life of an American.<br> • So saving all US citizens at any given point in time would be worth $1,300 trillion.<br> • If you believe that the risk of human extinction over the next century is something like one in six (as Toby Ord suggests is a reasonable figure in his book <em>The Precipice</em>), then it would be worth the US government spending up to $2.2 trillion to reduce that risk by just 1%, in terms of American lives saved alone.<br> • Carl thinks it would cost a <em>lot</em> less than that to achieve a 1% risk reduction if the money were spent intelligently. So it easily passes a government cost-benefit test, with a very big benefit-to-cost ratio — likely over 1000:1 today.</p><p> This argument helped NASA get funding to scan the sky for any asteroids that might be on a collision course with Earth, and it was directly promoted by famous economists like Richard Posner, Larry Summers, and Cass Sunstein.</p><p> If the case is clear enough, why hasn't it already motivated a lot more spending or regulations to limit existential risks — enough to drive down what any additional efforts would achieve?</p><p> Carl thinks that one key barrier is that infrequent disasters are rarely politically salient. Research indicates that extra money is spent on flood defences in the years immediately following a massive flood — but as memories fade, that spending quickly dries up. Of course the annual probability of a disaster was the same the whole time; all that changed is what voters had on their minds.</p><p> Carl expects that all the reasons we didn’t adequately prepare for or respond to COVID-19 — with excess mortality over 15 million and costs well over $10 trillion — bite even harder when it comes to threats we've never faced before, such as engineered pandemics, risks from advanced artificial intelligence, and so on.</p><p> Today’s episode is in part our way of trying to improve this situation. In today’s wide-ranging conversation, Carl and Rob also cover:<br> • A few reasons Carl isn't excited by 'strong longtermism'<br> • How x-risk reduction compares to GiveWell recommendations<br> • Solutions for asteroids, comets, supervolcanoes, nuclear war, pandemics, and climate change<br> • The history of bioweapons<br> • Whether gain-of-function research is justifiable<br> • Successes and failures around COVID-19<br> • The history of existential risk<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:34)</li><li>A few reasons Carl isn't excited by strong longtermism (00:03:47)</li><li>Longtermism isn’t necessary for wanting to reduce big x-risks (00:08:21)</li><li>Why we don’t adequately prepare for disasters (00:11:16)</li><li>International programs to stop asteroids and comets (00:18:55)</li><li>Costs and political incentives around COVID (00:23:52)</li><li>How x-risk reduction compares to GiveWell recommendations (00:34:34)</li><li>Solutions for asteroids, comets, and supervolcanoes (00:50:22)</li><li>Solutions for climate change (00:54:15)</li><li>Solutions for nuclear weapons (01:02:18)</li><li>The history of bioweapons (01:22:41)</li><li>Gain-of-function research (01:34:22)</li><li>Solutions for bioweapons and natural pandemics (01:45:31)</li><li>Successes and failures around COVID-19 (01:58:26)</li><li>Who to trust going forward (02:09:09)</li><li>The history of existential risk (02:15:07)</li><li>The most compelling risks (02:24:59)</li><li>False alarms about big risks in the past (02:34:22)</li><li>Suspicious convergence around x-risk reduction (02:49:31)</li><li>How hard it would be to convince governments (02:57:59)</li><li>Defensive epistemology (03:04:34)</li><li>Hinge of history debate (03:16:01)</li><li>Technological progress can’t keep up for long (03:21:51)</li><li>Strongest argument against this being a really pivotal time (03:37:29)</li><li>How Carl unwinds (03:45:30)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Katy Moore</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 05 Oct 2021 23:12:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f06af970/c3e5071b.mp3" length="109762090" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/BSkDCwOOD_mnE_uZhpj5COPBVEt7oIdxTmM08aqKdd8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTcv/MTY4MzU0NDY4OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13720</itunes:duration>
      <itunes:summary>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation. 

But the policy of US government agencies is already to spend up to $4 million to save the life of a citizen, making the death of all Americans a $1,300,000,000,000,000 disaster. 

According to Carl Shulman, research associate at Oxford University's Future of Humanity Institute, that means you don’t need any fancy philosophical arguments about the value or size of the future to justify working to reduce existential risk — it passes a mundane cost-benefit analysis whether or not you place any value on the long-term future. 

Links to learn more, summary and full transcript.

The key reason to make it a top priority is factual, not philosophical. That is, the risk of a disaster that kills billions of people alive today is alarmingly high, and it can be reduced at a reasonable cost. A back-of-the-envelope version of the argument runs: 

• The US government is willing to pay up to $4 million (depending on the agency) to save the life of an American. 
• So saving all US citizens at any given point in time would be worth $1,300 trillion. 
• If you believe that the risk of human extinction over the next century is something like one in six (as Toby Ord suggests is a reasonable figure in his book The Precipice), then it would be worth the US government spending up to $2.2 trillion to reduce that risk by just 1%, in terms of American lives saved alone. 
• Carl thinks it would cost a lot less than that to achieve a 1% risk reduction if the money were spent intelligently. So it easily passes a government cost-benefit test, with a very big benefit-to-cost ratio — likely over 1000:1 today. 

This argument helped NASA get funding to scan the sky for any asteroids that might be on a collision course with Earth, and it was directly promoted by famous economists like Richard Posner, Larry Summers, and Cass Sunstein. 

If the case is clear enough, why hasn't it already motivated a lot more spending or regulations to limit existential risks — enough to drive down what any additional efforts would achieve? 

Carl thinks that one key barrier is that infrequent disasters are rarely politically salient. Research indicates that extra money is spent on flood defences in the years immediately following a massive flood — but as memories fade, that spending quickly dries up. Of course the annual probability of a disaster was the same the whole time; all that changed is what voters had on their minds. 

Carl expects that all the reasons we didn’t adequately prepare for or respond to COVID-19 — with excess mortality over 15 million and costs well over $10 trillion — bite even harder when it comes to threats we've never faced before, such as engineered pandemics, risks from advanced artificial intelligence, and so on. 

Today’s episode is in part our way of trying to improve this situation. In today’s wide-ranging conversation, Carl and Rob also cover: 

• A few reasons Carl isn't excited by 'strong longtermism' 
• How x-risk reduction compares to GiveWell recommendations 
• Solutions for asteroids, comets, supervolcanoes, nuclear war, pandemics, and climate change 
• The history of bioweapons 
• Whether gain-of-function research is justifiable 
• Successes and failures around COVID-19 
• The history of existential risk 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Katy Moore</itunes:summary>
      <itunes:subtitle>Preventing the apocalypse may sound like an idiosyncratic activity, and it sometimes is justified on exotic grounds, such as the potential for humanity to become a galaxy-spanning civilisation. 

But the policy of US government agencies is already to sp</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f06af970/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f06af970/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#111 – Mushtaq Khan on using institutional economics to predict effective government reforms</title>
      <itunes:title>#111 – Mushtaq Khan on using institutional economics to predict effective government reforms</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b55529a8-1188-11ec-bdba-0e1feffde4b1</guid>
      <link>https://80000hours.org/podcast/episodes/mushtaq-khan-institutional-economics/</link>
      <description>
        <![CDATA[<p>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines.</p><p> The resulting oil spills damage the environment and cause severe health problems, but the Nigerian government has continually failed in their attempts to stop this theft.</p><p> They send in the army, and the army gets corrupted. They send in enforcement agencies, and the enforcement agencies get corrupted. What’s happening here?</p><p> According to Mushtaq Khan, economics professor at SOAS University of London, this is a classic example of ‘networked corruption’. Everyone in the community is benefiting from the criminal enterprise — so much so that the locals would prefer civil war to following the law. It pays vastly better than other local jobs, hotels and restaurants have formed around it, and houses are even powered by the electricity generated from the oil.</p><p> <a href="https://80k.link/MK"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In today's episode, Mushtaq elaborates on the models he uses to understand these problems and make predictions he can test in the real world.</p><p> Some of the most important factors shaping the fate of nations are their structures of power: who is powerful, how they are organized, which interest groups can pull in favours with the government, and the constant push and pull between the country's rulers and its ruled. While traditional economic theory has relatively little to say about these topics, institutional economists like Mushtaq have a lot to say, and participate in lively debates about which of their competing ideas best explain the world around us.</p><p> The issues at stake are nothing less than why some countries are rich and others are poor, why some countries are mostly law abiding while others are not, and why some government programmes improve public welfare while others just enrich the well connected.</p><p> Mushtaq’s specialties are anti-corruption and industrial policy, where he believes mainstream theory and practice are largely misguided.</p><p> Mushtaq's rule of thumb is that when the locals most concerned with a specific issue are invested in preserving a status quo they're participating in, they almost always win out.</p><p> To actually reduce corruption, countries like his native Bangladesh have to follow the same gradual path the U.K. once did: find organizations that benefit from rule-abiding behaviour and are selfishly motivated to promote it, and help them police their peers.</p><p> Trying to impose a new way of doing things from the top down wasn't how Europe modernised, and it won't work elsewhere either.</p><p> In cases like oil theft in Nigeria, where no one wants to follow the rules, Mushtaq says corruption may be impossible to solve directly. Instead you have to play a long game, bringing in other employment opportunities, improving health services, and deploying alternative forms of energy — in the hope that one day this will give people a viable alternative to corruption.</p><p> In this extensive interview Rob and Mushtaq cover this and much more, including:</p><p> • How does one test theories like this?<br> • Why are companies in some poor countries so much less productive than their peers in rich countries?<br> • Have rich countries just legalized the corruption in their societies?<br> • What are the big live debates in institutional economics?<br> • Should poor countries protect their industries from foreign competition?<br> • How can listeners use these theories to predict which policies will work in their own countries?</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:55)</li><li>Institutional economics (00:15:37)</li><li>Anti-corruption policies (00:28:45)</li><li>Capabilities (00:34:51)</li><li>Why the market doesn’t solve the problem (00:42:29)</li><li>Industrial policy (00:46:11)</li><li>South Korea (01:01:31)</li><li>Chiang Kai-shek (01:16:01)</li><li>The logic of political survival (01:18:43)</li><li>Anti-corruption as a design of your policy (01:35:16)</li><li>Examples of anti-corruption programs with good prospects (01:45:17)</li><li>The importance of getting overseas influences (01:56:05)</li><li>Actually capturing the primary effect (02:03:26)</li><li>How less developed countries could successfully design subsidies (02:15:14)</li><li>What happens when horizontal policing isn't possible (02:26:34)</li><li>Rule of law &lt;--&gt; economic development (02:33:40)</li><li>Violence (02:38:31)</li><li>How this applies to developed countries (02:48:57)</li><li>Policies to help left-behind groups (02:55:39)</li><li>What to study (02:58:50)</li></ul><p><br> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines.</p><p> The resulting oil spills damage the environment and cause severe health problems, but the Nigerian government has continually failed in their attempts to stop this theft.</p><p> They send in the army, and the army gets corrupted. They send in enforcement agencies, and the enforcement agencies get corrupted. What’s happening here?</p><p> According to Mushtaq Khan, economics professor at SOAS University of London, this is a classic example of ‘networked corruption’. Everyone in the community is benefiting from the criminal enterprise — so much so that the locals would prefer civil war to following the law. It pays vastly better than other local jobs, hotels and restaurants have formed around it, and houses are even powered by the electricity generated from the oil.</p><p> <a href="https://80k.link/MK"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In today's episode, Mushtaq elaborates on the models he uses to understand these problems and make predictions he can test in the real world.</p><p> Some of the most important factors shaping the fate of nations are their structures of power: who is powerful, how they are organized, which interest groups can pull in favours with the government, and the constant push and pull between the country's rulers and its ruled. While traditional economic theory has relatively little to say about these topics, institutional economists like Mushtaq have a lot to say, and participate in lively debates about which of their competing ideas best explain the world around us.</p><p> The issues at stake are nothing less than why some countries are rich and others are poor, why some countries are mostly law abiding while others are not, and why some government programmes improve public welfare while others just enrich the well connected.</p><p> Mushtaq’s specialties are anti-corruption and industrial policy, where he believes mainstream theory and practice are largely misguided.</p><p> Mushtaq's rule of thumb is that when the locals most concerned with a specific issue are invested in preserving a status quo they're participating in, they almost always win out.</p><p> To actually reduce corruption, countries like his native Bangladesh have to follow the same gradual path the U.K. once did: find organizations that benefit from rule-abiding behaviour and are selfishly motivated to promote it, and help them police their peers.</p><p> Trying to impose a new way of doing things from the top down wasn't how Europe modernised, and it won't work elsewhere either.</p><p> In cases like oil theft in Nigeria, where no one wants to follow the rules, Mushtaq says corruption may be impossible to solve directly. Instead you have to play a long game, bringing in other employment opportunities, improving health services, and deploying alternative forms of energy — in the hope that one day this will give people a viable alternative to corruption.</p><p> In this extensive interview Rob and Mushtaq cover this and much more, including:</p><p> • How does one test theories like this?<br> • Why are companies in some poor countries so much less productive than their peers in rich countries?<br> • Have rich countries just legalized the corruption in their societies?<br> • What are the big live debates in institutional economics?<br> • Should poor countries protect their industries from foreign competition?<br> • How can listeners use these theories to predict which policies will work in their own countries?</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:55)</li><li>Institutional economics (00:15:37)</li><li>Anti-corruption policies (00:28:45)</li><li>Capabilities (00:34:51)</li><li>Why the market doesn’t solve the problem (00:42:29)</li><li>Industrial policy (00:46:11)</li><li>South Korea (01:01:31)</li><li>Chiang Kai-shek (01:16:01)</li><li>The logic of political survival (01:18:43)</li><li>Anti-corruption as a design of your policy (01:35:16)</li><li>Examples of anti-corruption programs with good prospects (01:45:17)</li><li>The importance of getting overseas influences (01:56:05)</li><li>Actually capturing the primary effect (02:03:26)</li><li>How less developed countries could successfully design subsidies (02:15:14)</li><li>What happens when horizontal policing isn't possible (02:26:34)</li><li>Rule of law &lt;--&gt; economic development (02:33:40)</li><li>Violence (02:38:31)</li><li>How this applies to developed countries (02:48:57)</li><li>Policies to help left-behind groups (02:55:39)</li><li>What to study (02:58:50)</li></ul><p><br> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 10 Sep 2021 11:38:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/7766d263/dc80138a.mp3" length="96208418" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2bmktZJyW4QL23mFFMaVXjlJYOMImVI3ptS2ejBj60U/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTYv/MTY4MzU0NDY4Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>12026</itunes:duration>
      <itunes:summary>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines. 

The resulting oil spills damage the environment and cause severe health problems, but the Nigerian government has continually failed in their attempts to stop this theft. 

They send in the army, and the army gets corrupted. They send in enforcement agencies, and the enforcement agencies get corrupted. What’s happening here? 

According to Mushtaq Khan, economics professor at SOAS University of London, this is a classic example of ‘networked corruption’. Everyone in the community is benefiting from the criminal enterprise — so much so that the locals would prefer civil war to following the law. It pays vastly better than other local jobs, hotels and restaurants have formed around it, and houses are even powered by the electricity generated from the oil. 

Links to learn more, summary and full transcript.

In today's episode, Mushtaq elaborates on the models he uses to understand these problems and make predictions he can test in the real world. 

Some of the most important factors shaping the fate of nations are their structures of power: who is powerful, how they are organized, which interest groups can pull in favours with the government, and the constant push and pull between the country's rulers and its ruled. While traditional economic theory has relatively little to say about these topics, institutional economists like Mushtaq have a lot to say, and participate in lively debates about which of their competing ideas best explain the world around us. 

The issues at stake are nothing less than why some countries are rich and others are poor, why some countries are mostly law abiding while others are not, and why some government programmes improve public welfare while others just enrich the well connected. 

Mushtaq’s specialties are anti-corruption and industrial policy, where he believes mainstream theory and practice are largely misguided. 

Mushtaq's rule of thumb is that when the locals most concerned with a specific issue are invested in preserving a status quo they're participating in, they almost always win out. 

To actually reduce corruption, countries like his native Bangladesh have to follow the same gradual path the U.K. once did: find organizations that benefit from rule-abiding behaviour and are selfishly motivated to promote it, and help them police their peers.  

Trying to impose a new way of doing things from the top down wasn't how Europe modernised, and it won't work elsewhere either.  

In cases like oil theft in Nigeria, where no one wants to follow the rules, Mushtaq says corruption may be impossible to solve directly. Instead you have to play a long game, bringing in other employment opportunities, improving health services, and deploying alternative forms of energy — in the hope that one day this will give people a viable alternative to corruption. 

In this extensive interview Rob and Mushtaq cover this and much more, including: 

• How does one test theories like this? 
• Why are companies in some poor countries so much less productive than their peers in rich countries? 
• Have rich countries just legalized the corruption in their societies? 
• What are the big live debates in institutional economics? 
• Should poor countries protect their industries from foreign competition? 
• How can listeners use these theories to predict which policies will work in their own countries? 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>If you’re living in the Niger Delta in Nigeria, your best bet at a high-paying career is probably ‘artisanal refining’ — or, in plain language, stealing oil from pipelines. 

The resulting oil spills damage the environment and cause severe health proble</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/7766d263/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/7766d263/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#110 – Holden Karnofsky on building aptitudes and kicking ass</title>
      <itunes:title>#110 – Holden Karnofsky on building aptitudes and kicking ass</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e34f7512-0688-11ec-9b8e-0e5a4ea1b54f</guid>
      <link>https://80000hours.org/podcast/episodes/holden-karnofsky-building-aptitudes-kicking-ass/</link>
      <description>
        <![CDATA[<p>Holden Karnofsky helped create two of the most influential organisations in the effective philanthropy world. So when he outlines a different perspective on career advice than the one we present at 80,000 Hours — we take it seriously.</p><p>Holden disagrees with us on a few specifics, but it's more than that: he prefers a different vibe when making career choices, especially early in one's career.</p><p><a href="https://80k.link/HKC"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> While he might ultimately recommend similar jobs to those we recommend at 80,000 Hours, the reasons are often different.</p><p> At 80,000 Hours we often talk about ‘paths’ to working on what we currently think of as the most pressing problems in the world. That’s partially because people seem to prefer the most concrete advice possible.</p><p> But Holden thinks a problem with that kind of advice is that it’s hard to take actions based on it if your job options don’t match well with your plan, and it’s hard to get a reliable signal about whether you're making the right choices. </p><p> How can you know you’ve chosen the right cause? How can you know the job you’re aiming for will be helpful to that cause? And what if you can’t get a job in this area at all?</p><p> Holden prefers to focus on ‘aptitudes’ that you can build in all sorts of different roles and cause areas, which can later be applied more directly.</p><p> Even if the current role doesn’t work out, or your career goes in wacky directions you’d never anticipated (like so many successful careers do), or you change your whole worldview — you’ll still have access to this aptitude.</p><p> So instead of trying to become a project manager at an effective altruism organisation, maybe you should just become great at project management. Instead of trying to become a researcher at a top AI lab, maybe you should just become great at digesting hard problems.</p><p> Who knows where these skills will end up being useful down the road?</p><p> Holden doesn’t think you should spend much time worrying about whether you’re having an impact in the first few years of your career — instead you should just focus on learning to kick ass at <em>something</em>, knowing that most of your impact is going to come decades into your career.</p><p> He thinks as long as you’ve gotten good at something, there will usually be a lot of ways that you can contribute to solving the biggest problems.</p><p> But Holden’s most important point, perhaps, is this: <strong>Be very careful about following career advice at all</strong>.</p><p> He points out that a career is such a personal thing that it’s very easy for the advice-giver to be oblivious to important factors having to do with your personality and unique situation.</p><p> He thinks it’s pretty hard for anyone to really have justified empirical beliefs about career choice, and that you should be very hesitant to make a radically different decision than you would have otherwise based on what some person (or website!) tells you to do.</p><p> Instead, he hopes conversations like these serve as a way of prompting discussion and raising points that you can apply your own personal judgment to.</p><p> That's why in the end he thinks people should look at their career decisions through his aptitude lens, the '80,000 Hours lens', and ideally several other frameworks as well. Because any one perspective risks missing something important.</p><p> Holden and Rob also cover:</p><p> • Ways to be helpful to longtermism outside of careers<br> • Why finding a new cause area might be overrated<br> • Historical events that deserve more attention<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>Holden’s current impressions on career choice for longtermists (00:02:34)</li><li>Aptitude-first vs. career path-first approaches (00:08:46)</li><li>How to tell if you’re on track (00:16:24)</li><li>Just try to kick ass in whatever (00:26:00)</li><li>When not to take the thing you're excited about (00:36:54)</li><li>Ways to be helpful to longtermism outside of careers (00:41:36)</li><li>Things 80,000 Hours might be doing wrong (00:44:31)</li><li>The state of longtermism (00:51:50)</li><li>Money pits (01:02:10)</li><li>Broad longtermism (01:06:56)</li><li>Cause X (01:21:33)</li><li>Open Philanthropy (01:24:23)</li><li>COVID and the biorisk portfolio (01:35:09)</li><li>Has the world gotten better? (01:51:16)</li><li>Historical events that deserve more attention (01:55:11)</li><li>Applied epistemology (02:10:55)</li><li>What Holden has learned from COVID (02:20:55)</li><li>What Holden has gotten wrong recently (02:32:59)</li><li>Having a kid (02:39:50)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Holden Karnofsky helped create two of the most influential organisations in the effective philanthropy world. So when he outlines a different perspective on career advice than the one we present at 80,000 Hours — we take it seriously.</p><p>Holden disagrees with us on a few specifics, but it's more than that: he prefers a different vibe when making career choices, especially early in one's career.</p><p><a href="https://80k.link/HKC"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> While he might ultimately recommend similar jobs to those we recommend at 80,000 Hours, the reasons are often different.</p><p> At 80,000 Hours we often talk about ‘paths’ to working on what we currently think of as the most pressing problems in the world. That’s partially because people seem to prefer the most concrete advice possible.</p><p> But Holden thinks a problem with that kind of advice is that it’s hard to take actions based on it if your job options don’t match well with your plan, and it’s hard to get a reliable signal about whether you're making the right choices. </p><p> How can you know you’ve chosen the right cause? How can you know the job you’re aiming for will be helpful to that cause? And what if you can’t get a job in this area at all?</p><p> Holden prefers to focus on ‘aptitudes’ that you can build in all sorts of different roles and cause areas, which can later be applied more directly.</p><p> Even if the current role doesn’t work out, or your career goes in wacky directions you’d never anticipated (like so many successful careers do), or you change your whole worldview — you’ll still have access to this aptitude.</p><p> So instead of trying to become a project manager at an effective altruism organisation, maybe you should just become great at project management. Instead of trying to become a researcher at a top AI lab, maybe you should just become great at digesting hard problems.</p><p> Who knows where these skills will end up being useful down the road?</p><p> Holden doesn’t think you should spend much time worrying about whether you’re having an impact in the first few years of your career — instead you should just focus on learning to kick ass at <em>something</em>, knowing that most of your impact is going to come decades into your career.</p><p> He thinks as long as you’ve gotten good at something, there will usually be a lot of ways that you can contribute to solving the biggest problems.</p><p> But Holden’s most important point, perhaps, is this: <strong>Be very careful about following career advice at all</strong>.</p><p> He points out that a career is such a personal thing that it’s very easy for the advice-giver to be oblivious to important factors having to do with your personality and unique situation.</p><p> He thinks it’s pretty hard for anyone to really have justified empirical beliefs about career choice, and that you should be very hesitant to make a radically different decision than you would have otherwise based on what some person (or website!) tells you to do.</p><p> Instead, he hopes conversations like these serve as a way of prompting discussion and raising points that you can apply your own personal judgment to.</p><p> That's why in the end he thinks people should look at their career decisions through his aptitude lens, the '80,000 Hours lens', and ideally several other frameworks as well. Because any one perspective risks missing something important.</p><p> Holden and Rob also cover:</p><p> • Ways to be helpful to longtermism outside of careers<br> • Why finding a new cause area might be overrated<br> • Historical events that deserve more attention<br> • And much more</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>Holden’s current impressions on career choice for longtermists (00:02:34)</li><li>Aptitude-first vs. career path-first approaches (00:08:46)</li><li>How to tell if you’re on track (00:16:24)</li><li>Just try to kick ass in whatever (00:26:00)</li><li>When not to take the thing you're excited about (00:36:54)</li><li>Ways to be helpful to longtermism outside of careers (00:41:36)</li><li>Things 80,000 Hours might be doing wrong (00:44:31)</li><li>The state of longtermism (00:51:50)</li><li>Money pits (01:02:10)</li><li>Broad longtermism (01:06:56)</li><li>Cause X (01:21:33)</li><li>Open Philanthropy (01:24:23)</li><li>COVID and the biorisk portfolio (01:35:09)</li><li>Has the world gotten better? (01:51:16)</li><li>Historical events that deserve more attention (01:55:11)</li><li>Applied epistemology (02:10:55)</li><li>What Holden has learned from COVID (02:20:55)</li><li>What Holden has gotten wrong recently (02:32:59)</li><li>Having a kid (02:39:50)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 26 Aug 2021 20:07:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/09705d11/cc8090c8.mp3" length="79726233" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/PP0bdR836usjV7JMfiaxB_RH-ojEaEomTmucwRDgEXI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTUv/MTY4MzU0NDY4Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9966</itunes:duration>
      <itunes:summary>Holden Karnofsky helped create two of the most influential organisations in the effective philanthropy world. So when he outlines a different perspective on career advice than the one we present at 80,000 Hours — we take it seriously. 

Holden disagrees with us on a few specifics, but it's more than that: he prefers a different vibe when making career choices, especially early in one's career. 

Links to learn more, summary and full transcript. 

While he might ultimately recommend similar jobs to those we recommend at 80,000 Hours, the reasons are often different. 

At 80,000 Hours we often talk about ‘paths’ to working on what we currently think of as the most pressing problems in the world. That’s partially because people seem to prefer the most concrete advice possible. 

But Holden thinks a problem with that kind of advice is that it’s hard to take actions based on it if your job options don’t match well with your plan, and it’s hard to get a reliable signal about whether you're making the right choices.   

How can you know you’ve chosen the right cause? How can you know the job you’re aiming for will be helpful to that cause? And what if you can’t get a job in this area at all? 

Holden prefers to focus on ‘aptitudes’ that you can build in all sorts of different roles and cause areas, which can later be applied more directly. 

Even if the current role doesn’t work out, or your career goes in wacky directions you’d never anticipated (like so many successful careers do), or you change your whole worldview — you’ll still have access to this aptitude. 

So instead of trying to become a project manager at an effective altruism organisation, maybe you should just become great at project management. Instead of trying to become a researcher at a top AI lab, maybe you should just become great at digesting hard problems. 

Who knows where these skills will end up being useful down the road?  

Holden doesn’t think you should spend much time worrying about whether you’re having an impact in the first few years of your career — instead you should just focus on learning to kick ass at something, knowing that most of your impact is going to come decades into your career.   

He thinks as long as you’ve gotten good at something, there will usually be a lot of ways that you can contribute to solving the biggest problems. 

But Holden’s most important point, perhaps, is this: Be very careful about following career advice at all. 

He points out that a career is such a personal thing that it’s very easy for the advice-giver to be oblivious to important factors having to do with your personality and unique situation.  

He thinks it’s pretty hard for anyone to really have justified empirical beliefs about career choice, and that you should be very hesitant to make a radically different decision than you would have otherwise based on what some person (or website!) tells you to do. 

Instead, he hopes conversations like these serve as a way of prompting discussion and raising points that you can apply your own personal judgment to. 

That's why in the end he thinks people should look at their career decisions through his aptitude lens, the '80,000 Hours lens', and ideally several other frameworks as well. Because any one perspective risks missing something important. 

Holden and Rob also cover: 

• Ways to be helpful to longtermism outside of careers 
• Why finding a new cause area might be overrated 
• Historical events that deserve more attention 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>Holden Karnofsky helped create two of the most influential organisations in the effective philanthropy world. So when he outlines a different perspective on career advice than the one we present at 80,000 Hours — we take it seriously. 

Holden disagrees</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/09705d11/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/09705d11/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#109 – Holden Karnofsky on the most important century</title>
      <itunes:title>#109 – Holden Karnofsky on the most important century</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">823977b6-0104-11ec-b1f5-12a30bca8743</guid>
      <link>https://share.transistor.fm/s/7c8f6ecb</link>
      <description>
        <![CDATA[<p>Will the future of humanity be wild, or boring? It's natural to think that if we're trying to be sober and measured, and predict what will really happen rather than spin an exciting story, it's more likely than not to be sort of... dull.</p><p> 

But there's also good reason to think that that is simply impossible. The idea that there's a boring future that's internally coherent is an illusion that comes from not inspecting those scenarios too closely.</p><p> 

At least that is what Holden Karnofsky — founder of charity evaluator GiveWell and foundation Open Philanthropy — argues in his new article series titled <a href="https://80k.link/MIC"><b>'The Most Important Century'</b></a>. He hopes to lay out part of the worldview that's driving the strategy and grantmaking of Open Philanthropy's longtermist team, and encourage more people to join his efforts to positively shape humanity's future.</p><p> 

<a href="https://80k.link/HK1"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

The bind is this. For the first 99% of human history the global economy (initially mostly food production) grew very slowly: under 0.1% a year. But since the industrial revolution around 1800, growth has exploded to over 2% a year.</p><p> 

To us in 2020 that sounds perfectly sensible and the natural order of things. But Holden points out that in fact it's not only unprecedented, it also can't continue for long.</p><p> 

The power of compounding increases means that to sustain 2% growth for just 10,000 years, 5% as long as humanity has already existed, would require us to turn every individual atom in the galaxy into an economy as large as the Earth's today. Not super likely.</p><p> 

So what are the options? First, maybe growth will slow and then stop. In that case we today live in the single miniscule slice in the history of life during which the world rapidly changed due to constant technological advances, before intelligent civilization permanently stagnated or even collapsed. What a wild time to be alive!</p><p> 

Alternatively, maybe growth will continue for thousands of years. In that case we are at the very beginning of what would necessarily have to become a stable galaxy-spanning civilization, harnessing the energy of entire stars among other feats of engineering. We would then stand among the first tiny sliver of all the quadrillions of intelligent beings who ever exist. What a wild time to be alive!</p><p> 

Isn't there another option where the future feels less remarkable and our current moment not so special?</p><p> 

While the full version of the argument above has a number of caveats, the short answer is 'not really'. We might be in a computer simulation and our galactic potential all an illusion, though that's hardly any less weird. And maybe the most exciting events won't happen for generations yet. But on a cosmic scale we'd still be living around the universe's most remarkable time.</p><p> 

Holden himself was very reluctant to buy into the idea that today’s civilization is in a strange and privileged position, but has ultimately concluded "all possible views about humanity's future are wild".</p><p> 

In the conversation Holden and Rob cover each part of the 'Most Important Century' series, including:</p><p> 

• The case that we live in an incredibly important time<br> 
• How achievable-seeming technology - in particular, mind uploading - could lead to unprecedented productivity, control of the environment, and more<br> 
• How economic growth is faster than it can be for all that much longer<br> 
• Forecasting transformative AI<br> 
• And the implications of living in the most important century</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Will the future of humanity be wild, or boring? It's natural to think that if we're trying to be sober and measured, and predict what will really happen rather than spin an exciting story, it's more likely than not to be sort of... dull.</p><p> 

But there's also good reason to think that that is simply impossible. The idea that there's a boring future that's internally coherent is an illusion that comes from not inspecting those scenarios too closely.</p><p> 

At least that is what Holden Karnofsky — founder of charity evaluator GiveWell and foundation Open Philanthropy — argues in his new article series titled <a href="https://80k.link/MIC"><b>'The Most Important Century'</b></a>. He hopes to lay out part of the worldview that's driving the strategy and grantmaking of Open Philanthropy's longtermist team, and encourage more people to join his efforts to positively shape humanity's future.</p><p> 

<a href="https://80k.link/HK1"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

The bind is this. For the first 99% of human history the global economy (initially mostly food production) grew very slowly: under 0.1% a year. But since the industrial revolution around 1800, growth has exploded to over 2% a year.</p><p> 

To us in 2020 that sounds perfectly sensible and the natural order of things. But Holden points out that in fact it's not only unprecedented, it also can't continue for long.</p><p> 

The power of compounding increases means that to sustain 2% growth for just 10,000 years, 5% as long as humanity has already existed, would require us to turn every individual atom in the galaxy into an economy as large as the Earth's today. Not super likely.</p><p> 

So what are the options? First, maybe growth will slow and then stop. In that case we today live in the single miniscule slice in the history of life during which the world rapidly changed due to constant technological advances, before intelligent civilization permanently stagnated or even collapsed. What a wild time to be alive!</p><p> 

Alternatively, maybe growth will continue for thousands of years. In that case we are at the very beginning of what would necessarily have to become a stable galaxy-spanning civilization, harnessing the energy of entire stars among other feats of engineering. We would then stand among the first tiny sliver of all the quadrillions of intelligent beings who ever exist. What a wild time to be alive!</p><p> 

Isn't there another option where the future feels less remarkable and our current moment not so special?</p><p> 

While the full version of the argument above has a number of caveats, the short answer is 'not really'. We might be in a computer simulation and our galactic potential all an illusion, though that's hardly any less weird. And maybe the most exciting events won't happen for generations yet. But on a cosmic scale we'd still be living around the universe's most remarkable time.</p><p> 

Holden himself was very reluctant to buy into the idea that today’s civilization is in a strange and privileged position, but has ultimately concluded "all possible views about humanity's future are wild".</p><p> 

In the conversation Holden and Rob cover each part of the 'Most Important Century' series, including:</p><p> 

• The case that we live in an incredibly important time<br> 
• How achievable-seeming technology - in particular, mind uploading - could lead to unprecedented productivity, control of the environment, and more<br> 
• How economic growth is faster than it can be for all that much longer<br> 
• Forecasting transformative AI<br> 
• And the implications of living in the most important century</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 19 Aug 2021 19:29:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/7c8f6ecb/4916fea0.mp3" length="66737343" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/L86NopPWEusTqnZ-TKY9h0024WJgbai_aOdsk7lzIDQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTQv/MTY4MzU0NDY4NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8342</itunes:duration>
      <itunes:summary>Will the future of humanity be wild, or boring? It's natural to think that if we're trying to be sober and measured, and predict what will really happen rather than spin an exciting story, it's more likely than not to be sort of... dull. 

But there's also good reason to think that that is simply impossible. The idea that there's a boring future that's internally coherent is an illusion that comes from not inspecting those scenarios too closely. 

At least that is what Holden Karnofsky — founder of charity evaluator GiveWell and foundation Open Philanthropy — argues in his new article series titled 'The Most Important Century'. He hopes to lay out part of the worldview that's driving the strategy and grantmaking of Open Philanthropy's longtermist team, and encourage more people to join his efforts to positively shape humanity's future. 

Links to learn more, summary and full transcript. 

The bind is this. For the first 99% of human history the global economy (initially mostly food production) grew very slowly: under 0.1% a year. But since the industrial revolution around 1800, growth has exploded to over 2% a year. 

To us in 2020 that sounds perfectly sensible and the natural order of things. But Holden points out that in fact it's not only unprecedented, it also can't continue for long. 

The power of compounding increases means that to sustain 2% growth for just 10,000 years, 5% as long as humanity has already existed, would require us to turn every individual atom in the galaxy into an economy as large as the Earth's today. Not super likely. 

So what are the options? First, maybe growth will slow and then stop. In that case we today live in the single miniscule slice in the history of life during which the world rapidly changed due to constant technological advances, before intelligent civilization permanently stagnated or even collapsed. What a wild time to be alive! 

Alternatively, maybe growth will continue for thousands of years. In that case we are at the very beginning of what would necessarily have to become a stable galaxy-spanning civilization, harnessing the energy of entire stars among other feats of engineering. We would then stand among the first tiny sliver of all the quadrillions of intelligent beings who ever exist. What a wild time to be alive! 

Isn't there another option where the future feels less remarkable and our current moment not so special? 

While the full version of the argument above has a number of caveats, the short answer is 'not really'. We might be in a computer simulation and our galactic potential all an illusion, though that's hardly any less weird. And maybe the most exciting events won't happen for generations yet. But on a cosmic scale we'd still be living around the universe's most remarkable time. 

Holden himself was very reluctant to buy into the idea that today’s civilization is in a strange and privileged position, but has ultimately concluded "all possible views about humanity's future are wild". 

In the conversation Holden and Rob cover each part of the 'Most Important Century' series, including: 

• The case that we live in an incredibly important time 
• How achievable-seeming technology - in particular, mind uploading - could lead to unprecedented productivity, control of the environment, and more 
• How economic growth is faster than it can be for all that much longer 
• Forecasting transformative AI 
• And the implications of living in the most important century 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>Will the future of humanity be wild, or boring? It's natural to think that if we're trying to be sober and measured, and predict what will really happen rather than spin an exciting story, it's more likely than not to be sort of... dull. 

But there's als</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/7c8f6ecb/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#108 – Chris Olah on working at top AI labs without an undergrad degree</title>
      <itunes:title>#108 – Chris Olah on working at top AI labs without an undergrad degree</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c58cd8fa-faa1-11eb-8fb2-0e087cd9aa13</guid>
      <link>https://share.transistor.fm/s/f86eb885</link>
      <description>
        <![CDATA[Chris Olah has had a fascinating and unconventional career path.<p>  

Most people who want to pursue a research career feel they need a degree to get taken seriously. But Chris not only doesn't have a PhD, but doesn’t even have an undergraduate degree. After dropping out of university to help defend an acquaintance who was facing bogus criminal charges, Chris started independently working on machine learning research, and eventually got an internship at Google Brain, a leading AI research group.</p><p>  

In this interview — a follow-up to <a href="https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/"><b>our episode on his technical work</b></a>  — we discuss what, if anything, can be learned from his unusual career path. Should more people pass on university and just throw themselves at solving a problem they care about? Or would it be foolhardy for others to try to copy a unique case like Chris’?</p><p> 

<a href="https://80000hours.org/podcast/episodes/chris-olah-unconventional-career-path/?utm_campaign=podcast__chris-olah&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

We also cover some of Chris' personal passions over the years, including his attempts to reduce what he calls 'research debt' by starting a new academic journal called Distill, focused just on explaining existing results unusually clearly.</p><p> 

As Chris explains, as fields develop they accumulate huge bodies of knowledge that researchers are meant to be familiar with before they start contributing themselves. But the weight of that existing knowledge — and the need to keep up with what everyone else is doing — can become crushing. It can take someone until their 30s or later to earn their stripes, and sometimes a field will split in two just to make it possible for anyone to stay on top of it.</p><p> 

If that were unavoidable it would be one thing, but Chris thinks we're nowhere near communicating existing knowledge as well as we could. Incrementally improving an explanation of a technical idea might take a single author weeks to do, but could go on to save a day for thousands, tens of thousands, or hundreds of thousands of students, if it becomes the best option available.</p><p> 

Despite that, academics have little incentive to produce outstanding explanations of complex ideas that can speed up the education of everyone coming up in their field. And some even see the process of deciphering bad explanations as a desirable right of passage all should pass through, just as they did.</p><p> 

So Chris tried his hand at chipping away at this problem — but concluded the nature of the problem wasn't quite what he originally thought. In this conversation we talk about that, as well as:</p><p> 

• Why highly thoughtful cold emails can be surprisingly effective, but average cold emails do little<br> 
• Strategies for growing as a researcher<br> 
• Thinking about research as a market<br> 
• How Chris thinks about writing outstanding explanations<br>  
• The concept of 'micromarriages' and ‘microbestfriendships’<br> 
• And much more.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Chris Olah has had a fascinating and unconventional career path.<p>  

Most people who want to pursue a research career feel they need a degree to get taken seriously. But Chris not only doesn't have a PhD, but doesn’t even have an undergraduate degree. After dropping out of university to help defend an acquaintance who was facing bogus criminal charges, Chris started independently working on machine learning research, and eventually got an internship at Google Brain, a leading AI research group.</p><p>  

In this interview — a follow-up to <a href="https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/"><b>our episode on his technical work</b></a>  — we discuss what, if anything, can be learned from his unusual career path. Should more people pass on university and just throw themselves at solving a problem they care about? Or would it be foolhardy for others to try to copy a unique case like Chris’?</p><p> 

<a href="https://80000hours.org/podcast/episodes/chris-olah-unconventional-career-path/?utm_campaign=podcast__chris-olah&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

We also cover some of Chris' personal passions over the years, including his attempts to reduce what he calls 'research debt' by starting a new academic journal called Distill, focused just on explaining existing results unusually clearly.</p><p> 

As Chris explains, as fields develop they accumulate huge bodies of knowledge that researchers are meant to be familiar with before they start contributing themselves. But the weight of that existing knowledge — and the need to keep up with what everyone else is doing — can become crushing. It can take someone until their 30s or later to earn their stripes, and sometimes a field will split in two just to make it possible for anyone to stay on top of it.</p><p> 

If that were unavoidable it would be one thing, but Chris thinks we're nowhere near communicating existing knowledge as well as we could. Incrementally improving an explanation of a technical idea might take a single author weeks to do, but could go on to save a day for thousands, tens of thousands, or hundreds of thousands of students, if it becomes the best option available.</p><p> 

Despite that, academics have little incentive to produce outstanding explanations of complex ideas that can speed up the education of everyone coming up in their field. And some even see the process of deciphering bad explanations as a desirable right of passage all should pass through, just as they did.</p><p> 

So Chris tried his hand at chipping away at this problem — but concluded the nature of the problem wasn't quite what he originally thought. In this conversation we talk about that, as well as:</p><p> 

• Why highly thoughtful cold emails can be surprisingly effective, but average cold emails do little<br> 
• Strategies for growing as a researcher<br> 
• Thinking about research as a market<br> 
• How Chris thinks about writing outstanding explanations<br>  
• The concept of 'micromarriages' and ‘microbestfriendships’<br> 
• And much more.</p><p> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 11 Aug 2021 13:18:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f86eb885/4f4781a2.mp3" length="44829316" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/jDLyNCLhduYBKFJ5Awudz2LBNGDS5gccGtW9-MqX_U4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTMv/MTY4MzU0NDY4NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5604</itunes:duration>
      <itunes:summary>Chris Olah has had a fascinating and unconventional career path.  

Most people who want to pursue a research career feel they need a degree to get taken seriously. But Chris not only doesn't have a PhD, but doesn’t even have an undergraduate degree. After dropping out of university to help defend an acquaintance who was facing bogus criminal charges, Chris started independently working on machine learning research, and eventually got an internship at Google Brain, a leading AI research group.  

In this interview — a follow-up to our episode on his technical work  — we discuss what, if anything, can be learned from his unusual career path. Should more people pass on university and just throw themselves at solving a problem they care about? Or would it be foolhardy for others to try to copy a unique case like Chris’? 

Links to learn more, summary and full transcript. 

We also cover some of Chris' personal passions over the years, including his attempts to reduce what he calls 'research debt' by starting a new academic journal called Distill, focused just on explaining existing results unusually clearly. 

As Chris explains, as fields develop they accumulate huge bodies of knowledge that researchers are meant to be familiar with before they start contributing themselves. But the weight of that existing knowledge — and the need to keep up with what everyone else is doing — can become crushing. It can take someone until their 30s or later to earn their stripes, and sometimes a field will split in two just to make it possible for anyone to stay on top of it. 

If that were unavoidable it would be one thing, but Chris thinks we're nowhere near communicating existing knowledge as well as we could. Incrementally improving an explanation of a technical idea might take a single author weeks to do, but could go on to save a day for thousands, tens of thousands, or hundreds of thousands of students, if it becomes the best option available. 

Despite that, academics have little incentive to produce outstanding explanations of complex ideas that can speed up the education of everyone coming up in their field. And some even see the process of deciphering bad explanations as a desirable right of passage all should pass through, just as they did. 

So Chris tried his hand at chipping away at this problem — but concluded the nature of the problem wasn't quite what he originally thought. In this conversation we talk about that, as well as: 

• Why highly thoughtful cold emails can be surprisingly effective, but average cold emails do little 
• Strategies for growing as a researcher 
• Thinking about research as a market 
• How Chris thinks about writing outstanding explanations  
• The concept of 'micromarriages' and ‘microbestfriendships’ 
• And much more. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>Chris Olah has had a fascinating and unconventional career path.  

Most people who want to pursue a research career feel they need a degree to get taken seriously. But Chris not only doesn't have a PhD, but doesn’t even have an undergraduate degree. Afte</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/f86eb885/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#107 – Chris Olah on what the hell is going on inside neural networks</title>
      <itunes:title>#107 – Chris Olah on what the hell is going on inside neural networks</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">cbba34b2-f54b-11eb-952d-12b0e8f11a13</guid>
      <link>https://share.transistor.fm/s/2422f8af</link>
      <description>
        <![CDATA[Big machine learning models can identify plant species better than any human, write passable essays, beat you at a game of Starcraft 2, figure out how a photo of Tobey Maguire and the word 'spider' are related, solve the 60-year-old 'protein folding problem', diagnose some diseases, play romantic matchmaker, write solid computer code, and offer questionable legal advice.<p> 

Humanity made these amazing and ever-improving tools. So how do our creations work? In short: we don't know.</p><p> 

Today's guest, Chris Olah, finds this both absurd and unacceptable. Over the last ten years he has been a leader in the effort to unravel what's really going on inside these black boxes. As part of that effort he helped create the famous DeepDream visualisations at Google Brain, reverse engineered the CLIP image classifier at OpenAI, and is now continuing his work at Anthropic, a new $100 million research company that tries to "co-develop the latest safety techniques alongside scaling of large ML models".</p><p> 

<a href="https://80k.link/CO"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Despite having a huge fan base thanks to his explanations of ML and tweets, today's episode is the first long interview Chris has ever given. It features his personal take on what we've learned so far about what ML algorithms are doing, and what's next for this research agenda at Anthropic.</p><p> 

His decade of work has borne substantial fruit, producing an approach for looking inside the mess of connections in a neural network and back out what functional role each piece is serving. Among other things, Chris and team found that every visual classifier seems to converge on a number of simple common elements in their early layers — elements so fundamental they may exist in our own visual cortex in some form.</p><p> 

They also found networks developing 'multimodal neurons' that would trigger in response to the presence of high-level concepts like 'romance', across both images and text, mimicking the famous 'Halle Berry neuron' from human neuroscience.</p><p> 

While reverse engineering how a mind works would make any top-ten list of the most valuable knowledge to pursue for its own sake, Chris's work is also of urgent practical importance. Machine learning models are already being deployed in medicine, business, the military, and the justice system, in ever more powerful roles. The competitive pressure to put them into action as soon as they can turn a profit is great, and only getting greater.</p><p> 

But if we don't know what these machines are doing, we can't be confident they'll continue to work the way we want as circumstances change. Before we hand an algorithm the proverbial nuclear codes, we should demand more assurance than "well, it's always worked fine so far".</p><p> 

But by peering inside neural networks and figuring out how to 'read their minds' we can potentially foresee future failures and prevent them before they happen. Artificial neural networks may even be a better way to study how our own minds work, given that, unlike a human brain, we can see everything that's happening inside them — and having been posed similar challenges, there's every reason to think evolution and 'gradient descent' often converge on similar solutions.</p><p> 

Among other things, Rob and Chris cover:</p><p> 

• Why Chris thinks it's necessary to work with the largest models<br> 
• What fundamental lessons we've learned about how neural networks (and perhaps humans) think<br> 
• How interpretability research might help make AI safer to deploy, and Chris’ response to skeptics<br> 
• Why there's such a fuss about 'scaling laws' and what they say about future AI progress</p><p>  

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Big machine learning models can identify plant species better than any human, write passable essays, beat you at a game of Starcraft 2, figure out how a photo of Tobey Maguire and the word 'spider' are related, solve the 60-year-old 'protein folding problem', diagnose some diseases, play romantic matchmaker, write solid computer code, and offer questionable legal advice.<p> 

Humanity made these amazing and ever-improving tools. So how do our creations work? In short: we don't know.</p><p> 

Today's guest, Chris Olah, finds this both absurd and unacceptable. Over the last ten years he has been a leader in the effort to unravel what's really going on inside these black boxes. As part of that effort he helped create the famous DeepDream visualisations at Google Brain, reverse engineered the CLIP image classifier at OpenAI, and is now continuing his work at Anthropic, a new $100 million research company that tries to "co-develop the latest safety techniques alongside scaling of large ML models".</p><p> 

<a href="https://80k.link/CO"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Despite having a huge fan base thanks to his explanations of ML and tweets, today's episode is the first long interview Chris has ever given. It features his personal take on what we've learned so far about what ML algorithms are doing, and what's next for this research agenda at Anthropic.</p><p> 

His decade of work has borne substantial fruit, producing an approach for looking inside the mess of connections in a neural network and back out what functional role each piece is serving. Among other things, Chris and team found that every visual classifier seems to converge on a number of simple common elements in their early layers — elements so fundamental they may exist in our own visual cortex in some form.</p><p> 

They also found networks developing 'multimodal neurons' that would trigger in response to the presence of high-level concepts like 'romance', across both images and text, mimicking the famous 'Halle Berry neuron' from human neuroscience.</p><p> 

While reverse engineering how a mind works would make any top-ten list of the most valuable knowledge to pursue for its own sake, Chris's work is also of urgent practical importance. Machine learning models are already being deployed in medicine, business, the military, and the justice system, in ever more powerful roles. The competitive pressure to put them into action as soon as they can turn a profit is great, and only getting greater.</p><p> 

But if we don't know what these machines are doing, we can't be confident they'll continue to work the way we want as circumstances change. Before we hand an algorithm the proverbial nuclear codes, we should demand more assurance than "well, it's always worked fine so far".</p><p> 

But by peering inside neural networks and figuring out how to 'read their minds' we can potentially foresee future failures and prevent them before they happen. Artificial neural networks may even be a better way to study how our own minds work, given that, unlike a human brain, we can see everything that's happening inside them — and having been posed similar challenges, there's every reason to think evolution and 'gradient descent' often converge on similar solutions.</p><p> 

Among other things, Rob and Chris cover:</p><p> 

• Why Chris thinks it's necessary to work with the largest models<br> 
• What fundamental lessons we've learned about how neural networks (and perhaps humans) think<br> 
• How interpretability research might help make AI safer to deploy, and Chris’ response to skeptics<br> 
• Why there's such a fuss about 'scaling laws' and what they say about future AI progress</p><p>  

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 04 Aug 2021 20:05:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/2422f8af/ac481e2a.mp3" length="90886011" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Td2Qagc-8CFDiJB0jlLheG9QiumalB7jCvYBkeZyVBk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTIv/MTY4MzU0NDY4My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11361</itunes:duration>
      <itunes:summary>Big machine learning models can identify plant species better than any human, write passable essays, beat you at a game of Starcraft 2, figure out how a photo of Tobey Maguire and the word 'spider' are related, solve the 60-year-old 'protein folding problem', diagnose some diseases, play romantic matchmaker, write solid computer code, and offer questionable legal advice. 

Humanity made these amazing and ever-improving tools. So how do our creations work? In short: we don't know. 

Today's guest, Chris Olah, finds this both absurd and unacceptable. Over the last ten years he has been a leader in the effort to unravel what's really going on inside these black boxes. As part of that effort he helped create the famous DeepDream visualisations at Google Brain, reverse engineered the CLIP image classifier at OpenAI, and is now continuing his work at Anthropic, a new $100 million research company that tries to "co-develop the latest safety techniques alongside scaling of large ML models". 

Links to learn more, summary and full transcript.

Despite having a huge fan base thanks to his explanations of ML and tweets, today's episode is the first long interview Chris has ever given. It features his personal take on what we've learned so far about what ML algorithms are doing, and what's next for this research agenda at Anthropic. 

His decade of work has borne substantial fruit, producing an approach for looking inside the mess of connections in a neural network and back out what functional role each piece is serving. Among other things, Chris and team found that every visual classifier seems to converge on a number of simple common elements in their early layers — elements so fundamental they may exist in our own visual cortex in some form. 

They also found networks developing 'multimodal neurons' that would trigger in response to the presence of high-level concepts like 'romance', across both images and text, mimicking the famous 'Halle Berry neuron' from human neuroscience. 

While reverse engineering how a mind works would make any top-ten list of the most valuable knowledge to pursue for its own sake, Chris's work is also of urgent practical importance. Machine learning models are already being deployed in medicine, business, the military, and the justice system, in ever more powerful roles. The competitive pressure to put them into action as soon as they can turn a profit is great, and only getting greater. 

But if we don't know what these machines are doing, we can't be confident they'll continue to work the way we want as circumstances change. Before we hand an algorithm the proverbial nuclear codes, we should demand more assurance than "well, it's always worked fine so far". 

But by peering inside neural networks and figuring out how to 'read their minds' we can potentially foresee future failures and prevent them before they happen. Artificial neural networks may even be a better way to study how our own minds work, given that, unlike a human brain, we can see everything that's happening inside them — and having been posed similar challenges, there's every reason to think evolution and 'gradient descent' often converge on similar solutions. 

Among other things, Rob and Chris cover: 

• Why Chris thinks it's necessary to work with the largest models 
• What fundamental lessons we've learned about how neural networks (and perhaps humans) think 
• How interpretability research might help make AI safer to deploy, and Chris’ response to skeptics 
• Why there's such a fuss about 'scaling laws' and what they say about future AI progress  

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>Big machine learning models can identify plant species better than any human, write passable essays, beat you at a game of Starcraft 2, figure out how a photo of Tobey Maguire and the word 'spider' are related, solve the 60-year-old 'protein folding probl</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/2422f8af/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#106 – Cal Newport on an industrial revolution for office work</title>
      <itunes:title>#106 – Cal Newport on an industrial revolution for office work</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3650ead4-ef7d-11eb-bd55-0e6d3e627b57</guid>
      <link>https://80000hours.org/podcast/episodes/cal-newport-industrial-revolution-for-office-work/</link>
      <description>
        <![CDATA[<p>If you wanted to start a university department from scratch, and attract as many superstar researchers as possible, what’s the most attractive perk you could offer?</p><p>How about just not needing an email address.</p><p>According to today's guest, Cal Newport — computer science professor and best-selling author of <a href="https://www.calnewport.com/books/a-world-without-email/"><strong><em>A World Without Email</em></strong></a> — it should seem obscene and absurd for a world-renowned vaccine researcher with decades of experience to spend a third of their time fielding requests from HR, building management, finance, and so on. Yet with offices organised the way they are today, nothing could be more natural.</p><p> <a href="https://80k.link/CN"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> But this isn’t just a problem at the elite level — this affects almost all of us. A typical U.S. office worker checks their email 80 times a day, once every six minutes on average. Data analysis by RescueTime found that a third of users checked email or Slack every three minutes or more, averaged over a full work day.</p><p> Each time that happens our focus is broken, killing our momentum on the knowledge work we're supposedly paid to do.</p><p> When we lament how much email and chat have reduced our focus and filled our days with anxiety and frenetic activity, we most naturally blame 'weakness of will'. If only we had the discipline to check Slack and email once a day, all would be well — or so the story goes.</p><p> Cal believes that line of thinking fundamentally misunderstands how we got to a place where knowledge workers can rarely find more than five consecutive minutes to spend doing just one thing.</p><p> Since the Industrial Revolution, a combination of technology and better organization have allowed the manufacturing industry to produce a hundred-fold as much with the same number of people.</p><p> Cal says that by comparison, it's not clear that specialised knowledge workers like scientists, authors, or senior managers are *any* more productive than they were 50 years ago. If the knowledge sector could achieve even a tiny fraction of what manufacturing has, and find a way to coordinate its work that raised productivity by just 1%, that would generate on the order of $100 billion globally each year.</p><p> Since the 1990s, when everyone got an email address and most lost their assistants, that lack of direction has led to what Cal calls the 'hyperactive hive mind': everyone sends emails and chats to everyone else, all through the day, whenever they need something.</p><p> Cal points out that this is so normal we don't even think of it as a way of organising work, but it is: it's what happens when management does nothing to enable teams to decide on a better way of organising themselves.</p><p> A few industries have made progress taming the 'hyperactive hive mind'. But on Cal's telling, this barely scratches the surface of the improvements that are possible within knowledge work. And reigning in the hyperactive hive mind won't just help people do higher quality work, it will free them from the 24/7 anxiety that there's someone somewhere they haven't gotten back to.</p><p> In this interview Cal and Rob also cover:<br> • Is this really one of the world's most pressing problems?<br> • The historical origins of the 'hyperactive hive mind'<br> • The harm caused by attention switching<br> • Who's working to solve the problem and how<br> • Cal's top productivity advice for high school students, university students, and early career workers<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:02)</li><li>The hyperactive hivemind (00:04:11)</li><li>Scale of the harm (00:08:40)</li><li>Is email making professors stupid? (00:22:09)</li><li>Why haven't we already made these changes? (00:29:38)</li><li>Do people actually prefer the hyperactive hivemind? (00:43:31)</li><li>Solutions (00:55:52)</li><li>Advocacy (01:10:47)</li><li>How to Be a High School Superstar (01:23:03)</li><li>How to Win at College (01:27:46)</li><li>So Good They Can't Ignore You (01:31:47)</li><li>Personal barriers (01:42:51)</li><li>George Marshall (01:47:11)</li><li>Rob’s outro (01:49:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If you wanted to start a university department from scratch, and attract as many superstar researchers as possible, what’s the most attractive perk you could offer?</p><p>How about just not needing an email address.</p><p>According to today's guest, Cal Newport — computer science professor and best-selling author of <a href="https://www.calnewport.com/books/a-world-without-email/"><strong><em>A World Without Email</em></strong></a> — it should seem obscene and absurd for a world-renowned vaccine researcher with decades of experience to spend a third of their time fielding requests from HR, building management, finance, and so on. Yet with offices organised the way they are today, nothing could be more natural.</p><p> <a href="https://80k.link/CN"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> But this isn’t just a problem at the elite level — this affects almost all of us. A typical U.S. office worker checks their email 80 times a day, once every six minutes on average. Data analysis by RescueTime found that a third of users checked email or Slack every three minutes or more, averaged over a full work day.</p><p> Each time that happens our focus is broken, killing our momentum on the knowledge work we're supposedly paid to do.</p><p> When we lament how much email and chat have reduced our focus and filled our days with anxiety and frenetic activity, we most naturally blame 'weakness of will'. If only we had the discipline to check Slack and email once a day, all would be well — or so the story goes.</p><p> Cal believes that line of thinking fundamentally misunderstands how we got to a place where knowledge workers can rarely find more than five consecutive minutes to spend doing just one thing.</p><p> Since the Industrial Revolution, a combination of technology and better organization have allowed the manufacturing industry to produce a hundred-fold as much with the same number of people.</p><p> Cal says that by comparison, it's not clear that specialised knowledge workers like scientists, authors, or senior managers are *any* more productive than they were 50 years ago. If the knowledge sector could achieve even a tiny fraction of what manufacturing has, and find a way to coordinate its work that raised productivity by just 1%, that would generate on the order of $100 billion globally each year.</p><p> Since the 1990s, when everyone got an email address and most lost their assistants, that lack of direction has led to what Cal calls the 'hyperactive hive mind': everyone sends emails and chats to everyone else, all through the day, whenever they need something.</p><p> Cal points out that this is so normal we don't even think of it as a way of organising work, but it is: it's what happens when management does nothing to enable teams to decide on a better way of organising themselves.</p><p> A few industries have made progress taming the 'hyperactive hive mind'. But on Cal's telling, this barely scratches the surface of the improvements that are possible within knowledge work. And reigning in the hyperactive hive mind won't just help people do higher quality work, it will free them from the 24/7 anxiety that there's someone somewhere they haven't gotten back to.</p><p> In this interview Cal and Rob also cover:<br> • Is this really one of the world's most pressing problems?<br> • The historical origins of the 'hyperactive hive mind'<br> • The harm caused by attention switching<br> • Who's working to solve the problem and how<br> • Cal's top productivity advice for high school students, university students, and early career workers<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:02)</li><li>The hyperactive hivemind (00:04:11)</li><li>Scale of the harm (00:08:40)</li><li>Is email making professors stupid? (00:22:09)</li><li>Why haven't we already made these changes? (00:29:38)</li><li>Do people actually prefer the hyperactive hivemind? (00:43:31)</li><li>Solutions (00:55:52)</li><li>Advocacy (01:10:47)</li><li>How to Be a High School Superstar (01:23:03)</li><li>How to Win at College (01:27:46)</li><li>So Good They Can't Ignore You (01:31:47)</li><li>Personal barriers (01:42:51)</li><li>George Marshall (01:47:11)</li><li>Rob’s outro (01:49:18)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 28 Jul 2021 17:02:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5e41ac95/286ec37e.mp3" length="54458497" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/vewBbbWqea16boQRkaj9CRPqiKjak4oOTqJMrDLYEiQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTEv/MTY4MzU0NDY4Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6807</itunes:duration>
      <itunes:summary>If you wanted to start a university department from scratch, and attract as many superstar researchers as possible, what’s the most attractive perk you could offer? 

How about just not needing an email address. 

According to today's guest, Cal Newport — computer science professor and best-selling author of A World Without Email — it should seem obscene and absurd for a world-renowned vaccine researcher with decades of experience to spend a third of their time fielding requests from HR, building management, finance, and so on. Yet with offices organised the way they are today, nothing could be more natural. 

Links to learn more, summary and full transcript.

But this isn’t just a problem at the elite level — this affects almost all of us. A typical U.S. office worker checks their email 80 times a day, once every six minutes on average. Data analysis by RescueTime found that a third of users checked email or Slack every three minutes or more, averaged over a full work day. 

Each time that happens our focus is broken, killing our momentum on the knowledge work we're supposedly paid to do. 

When we lament how much email and chat have reduced our focus and filled our days with anxiety and frenetic activity, we most naturally blame 'weakness of will'. If only we had the discipline to check Slack and email once a day, all would be well — or so the story goes. 

Cal believes that line of thinking fundamentally misunderstands how we got to a place where knowledge workers can rarely find more than five consecutive minutes to spend doing just one thing. 

Since the Industrial Revolution, a combination of technology and better organization have allowed the manufacturing industry to produce a hundred-fold as much with the same number of people. 

Cal says that by comparison, it's not clear that specialised knowledge workers like scientists, authors, or senior managers are *any* more productive than they were 50 years ago. If the knowledge sector could achieve even a tiny fraction of what manufacturing has, and find a way to coordinate its work that raised productivity by just 1%, that would generate on the order of $100 billion globally each year. 

Since the 1990s, when everyone got an email address and most lost their assistants, that lack of direction has led to what Cal calls the 'hyperactive hive mind': everyone sends emails and chats to everyone else, all through the day, whenever they need something. 

Cal points out that this is so normal we don't even think of it as a way of organising work, but it is: it's what happens when management does nothing to enable teams to decide on a better way of organising themselves.  

A few industries have made progress taming the 'hyperactive hive mind'. But on Cal's telling, this barely scratches the surface of the improvements that are possible within knowledge work. And reigning in the hyperactive hive mind won't just help people do higher quality work, it will free them from the 24/7 anxiety that there's someone somewhere they haven't gotten back to. 

In this interview Cal and Rob also cover: 

• Is this really one of the world's most pressing problems? 
• The historical origins of the 'hyperactive hive mind' 
• The harm caused by attention switching 
• Who's working to solve the problem and how 
• Cal's top productivity advice for high school students, university students, and early career workers 
• And much more 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>If you wanted to start a university department from scratch, and attract as many superstar researchers as possible, what’s the most attractive perk you could offer? 

How about just not needing an email address. 

According to today's guest, Cal Newpo</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/5e41ac95/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/5e41ac95/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#105 – Alexander Berger on improving global health and wellbeing in clear and direct ways</title>
      <itunes:title>#105 – Alexander Berger on improving global health and wellbeing in clear and direct ways</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c513e092-e270-11eb-a68c-122891a7c55f</guid>
      <link>https://share.transistor.fm/s/ae629035</link>
      <description>
        <![CDATA[The effective altruist research community tries to identify the highest impact things people can do to improve the world. Unsurprisingly, given the difficulty of such a massive and open-ended project, very different schools of thought have arisen about how to do the most good.<p>

Today's guest, Alexander Berger, leads Open Philanthropy's 'Global Health and Wellbeing' programme, where he oversees around $175 million in grants each year, and ultimately aspires to disburse billions in the most impactful ways he and his team can identify.</p><p>

This programme is the flagship effort representing one major effective altruist approach: try to improve the health and wellbeing of humans and animals that are alive today, in clearly identifiable ways, applying an especially analytical and empirical mindset.</p><p>

<a href="https://80k.link/ABerger"><b>Links to learn more, summary, Open Phil jobs, and full transcript.</b></a></p><p>

The programme makes grants to tackle easily-prevented illnesses among the world's poorest people, offer cash to people living in extreme poverty, prevent cruelty to billions of farm animals, advance biomedical science, and improve criminal justice and immigration policy in the United States.</p><p>

Open Philanthropy's researchers rely on empirical information to guide their decisions where it's available, and where it's not, they aim to maximise expected benefits to recipients through careful analysis of the gains different projects would offer and their relative likelihoods of success.</p><p>

This 'global health and wellbeing' approach — sometimes referred to as 'neartermism' — contrasts with another big school of thought in effective altruism, known as 'longtermism', which aims to direct the long-term future of humanity and its descendants in a positive direction. Longtermism bets that while it's harder to figure out how to benefit future generations than people alive today, the total number of people who might live in the future is far greater than the number alive today, and this gain in scale more than offsets that lower tractability.</p><p>

The debate between these two very different theories of how to best improve the world has been one of the most significant within effective altruist research since its inception. Alexander first joined the influential charity evaluator GiveWell in 2011, and since then has conducted research alongside top thinkers on global health and wellbeing and longtermism alike, ultimately deciding to dedicate his efforts to improving the world today in identifiable ways.</p><p>

In this conversation Alexander advocates for that choice, explaining the case in favour of adopting the 'global health and wellbeing' mindset, while going through the arguments for the longtermist approach that he finds most and least convincing.</p><p>

Rob and Alexander also tackle:</p><p>

• Why it should be legal to sell your kidney, and why Alexander donated his to a total stranger<br>
• Why it's shockingly hard to find ways to give away large amounts of money that are more cost effective than distributing anti-malaria bed nets<br>
• How much you gain from working with tight feedback loops<br>
• Open Philanthropy's biggest wins<br>
• Why Open Philanthropy engages in 'worldview diversification' by having both a global health and wellbeing programme and a longtermist programme as well<br>
• Whether funding science and political advocacy is a good way to have more social impact<br>
• Whether our effects on future generations are predictable or unforeseeable<br>
• What problems the global health and wellbeing team works to solve and why<br>
• Opportunities to work at Open Philanthropy</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[The effective altruist research community tries to identify the highest impact things people can do to improve the world. Unsurprisingly, given the difficulty of such a massive and open-ended project, very different schools of thought have arisen about how to do the most good.<p>

Today's guest, Alexander Berger, leads Open Philanthropy's 'Global Health and Wellbeing' programme, where he oversees around $175 million in grants each year, and ultimately aspires to disburse billions in the most impactful ways he and his team can identify.</p><p>

This programme is the flagship effort representing one major effective altruist approach: try to improve the health and wellbeing of humans and animals that are alive today, in clearly identifiable ways, applying an especially analytical and empirical mindset.</p><p>

<a href="https://80k.link/ABerger"><b>Links to learn more, summary, Open Phil jobs, and full transcript.</b></a></p><p>

The programme makes grants to tackle easily-prevented illnesses among the world's poorest people, offer cash to people living in extreme poverty, prevent cruelty to billions of farm animals, advance biomedical science, and improve criminal justice and immigration policy in the United States.</p><p>

Open Philanthropy's researchers rely on empirical information to guide their decisions where it's available, and where it's not, they aim to maximise expected benefits to recipients through careful analysis of the gains different projects would offer and their relative likelihoods of success.</p><p>

This 'global health and wellbeing' approach — sometimes referred to as 'neartermism' — contrasts with another big school of thought in effective altruism, known as 'longtermism', which aims to direct the long-term future of humanity and its descendants in a positive direction. Longtermism bets that while it's harder to figure out how to benefit future generations than people alive today, the total number of people who might live in the future is far greater than the number alive today, and this gain in scale more than offsets that lower tractability.</p><p>

The debate between these two very different theories of how to best improve the world has been one of the most significant within effective altruist research since its inception. Alexander first joined the influential charity evaluator GiveWell in 2011, and since then has conducted research alongside top thinkers on global health and wellbeing and longtermism alike, ultimately deciding to dedicate his efforts to improving the world today in identifiable ways.</p><p>

In this conversation Alexander advocates for that choice, explaining the case in favour of adopting the 'global health and wellbeing' mindset, while going through the arguments for the longtermist approach that he finds most and least convincing.</p><p>

Rob and Alexander also tackle:</p><p>

• Why it should be legal to sell your kidney, and why Alexander donated his to a total stranger<br>
• Why it's shockingly hard to find ways to give away large amounts of money that are more cost effective than distributing anti-malaria bed nets<br>
• How much you gain from working with tight feedback loops<br>
• Open Philanthropy's biggest wins<br>
• Why Open Philanthropy engages in 'worldview diversification' by having both a global health and wellbeing programme and a longtermist programme as well<br>
• Whether funding science and political advocacy is a good way to have more social impact<br>
• Whether our effects on future generations are predictable or unforeseeable<br>
• What problems the global health and wellbeing team works to solve and why<br>
• Opportunities to work at Open Philanthropy</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>Producer: Keiran Harris<br>
Audio mastering: Ben Cordell<br>
Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 12 Jul 2021 15:18:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ae629035/2f4458e6.mp3" length="83963541" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/8AiWHDexxzkhkhhnuzx6qsN3e0vcswX-38JrnhQK5Jg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NTAv/MTY4MzU0NDY4MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10472</itunes:duration>
      <itunes:summary>The effective altruist research community tries to identify the highest impact things people can do to improve the world. Unsurprisingly, given the difficulty of such a massive and open-ended project, very different schools of thought have arisen about how to do the most good.

Today's guest, Alexander Berger, leads Open Philanthropy's 'Global Health and Wellbeing' programme, where he oversees around $175 million in grants each year, and ultimately aspires to disburse billions in the most impactful ways he and his team can identify.

This programme is the flagship effort representing one major effective altruist approach: try to improve the health and wellbeing of humans and animals that are alive today, in clearly identifiable ways, applying an especially analytical and empirical mindset.

Links to learn more, summary, Open Phil jobs, and full transcript.

The programme makes grants to tackle easily-prevented illnesses among the world's poorest people, offer cash to people living in extreme poverty, prevent cruelty to billions of farm animals, advance biomedical science, and improve criminal justice and immigration policy in the United States.

Open Philanthropy's researchers rely on empirical information to guide their decisions where it's available, and where it's not, they aim to maximise expected benefits to recipients through careful analysis of the gains different projects would offer and their relative likelihoods of success.

This 'global health and wellbeing' approach — sometimes referred to as 'neartermism' — contrasts with another big school of thought in effective altruism, known as 'longtermism', which aims to direct the long-term future of humanity and its descendants in a positive direction. Longtermism bets that while it's harder to figure out how to benefit future generations than people alive today, the total number of people who might live in the future is far greater than the number alive today, and this gain in scale more than offsets that lower tractability.

The debate between these two very different theories of how to best improve the world has been one of the most significant within effective altruist research since its inception. Alexander first joined the influential charity evaluator GiveWell in 2011, and since then has conducted research alongside top thinkers on global health and wellbeing and longtermism alike, ultimately deciding to dedicate his efforts to improving the world today in identifiable ways.

In this conversation Alexander advocates for that choice, explaining the case in favour of adopting the 'global health and wellbeing' mindset, while going through the arguments for the longtermist approach that he finds most and least convincing.

Rob and Alexander also tackle:

• Why it should be legal to sell your kidney, and why Alexander donated his to a total stranger
• Why it's shockingly hard to find ways to give away large amounts of money that are more cost effective than distributing anti-malaria bed nets
• How much you gain from working with tight feedback loops
• Open Philanthropy's biggest wins
• Why Open Philanthropy engages in 'worldview diversification' by having both a global health and wellbeing programme and a longtermist programme as well
• Whether funding science and political advocacy is a good way to have more social impact
• Whether our effects on future generations are predictable or unforeseeable
• What problems the global health and wellbeing team works to solve and why
• Opportunities to work at Open Philanthropy

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcriptions: Sofia Davis-Fogel</itunes:summary>
      <itunes:subtitle>The effective altruist research community tries to identify the highest impact things people can do to improve the world. Unsurprisingly, given the difficulty of such a massive and open-ended project, very different schools of thought have arisen about ho</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/ae629035/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#104 – Pardis Sabeti on the Sentinel system for detecting and stopping pandemics</title>
      <itunes:title>#104 – Pardis Sabeti on the Sentinel system for detecting and stopping pandemics</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">09e6bbe4-d926-11eb-a71a-123d942c1d61</guid>
      <link>https://80000hours.org/podcast/episodes/pardis-sabeti-sentinel/</link>
      <description>
        <![CDATA[<p>When the first person with COVID-19 went to see a doctor in Wuhan, nobody could tell that it wasn’t a familiar disease like the flu — that we were dealing with something new.</p><p> How much death and destruction could we have avoided if we'd had a hero who could? That's what the last Assistant Secretary of Defense Andy Weber asked on the show <a href="https://80000hours.org/podcast/episodes/andy-weber-rendering-bioweapons-obsolete/"><strong>back in March.</strong></a></p><p> Today’s guest Pardis Sabeti is a professor at Harvard, fought Ebola on the ground in Africa during the 2014 outbreak, runs her own lab, co-founded a company that produces next-level testing, and is even the lead singer of a rock band. If anyone is going to be that hero in the next pandemic — it just might be her.</p><p> <a href="https://80k.link/PS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> She is a co-author of the SENTINEL proposal, a practical system for detecting new diseases quickly, using an escalating series of three novel diagnostic techniques.</p><p> The first method, called SHERLOCK, uses CRISPR gene editing to detect familiar viruses in a simple, inexpensive filter paper test, using non-invasive samples.</p><p> If SHERLOCK draws a blank, we escalate to the second step, CARMEN, an advanced version of SHERLOCK that uses microfluidics and CRISPR to simultaneously detect hundreds of viruses and viral strains. More expensive, but far more comprehensive.</p><p> If neither SHERLOCK nor CARMEN detects a known pathogen, it's time to pull out the big gun: metagenomic sequencing. More expensive still, but sequencing all the DNA in a patient sample lets you identify and track every virus — known and unknown — in a sample.</p><p> If Pardis and her team succeeds, our future pandemic potential patient zero may:</p><p> 1. Go to the hospital with flu-like symptoms, and immediately be tested using SHERLOCK — which will come back negative<br> 2. Take the CARMEN test for a much broader range of illnesses — which will also come back negative<br> 3. Their sample will be sent for metagenomic sequencing, which will reveal that they're carrying a new virus we'll have to contend with<br> 4. At all levels, information will be recorded in a cloud-based data system that shares data in real time; the hospital will be alerted and told to quarantine the patient<br> 5. The world will be able to react weeks — or even months — faster, potentially saving millions of lives</p><p> It's a wonderful vision, and one humanity is ready to test out. But there are all sorts of practical questions, such as:</p><p> • How do you scale these technologies, including to remote and rural areas?<br> • Will doctors everywhere be able to operate them?<br> • Who will pay for it?<br> • How do you maintain the public’s trust and protect against misuse of sequencing data?<br> • How do you avoid drowning in the data the system produces?</p><p> In this conversation Pardis and Rob address all those questions, as well as:</p><p> • Pardis’ history with trying to control emerging contagious diseases<br> • The potential of mRNA vaccines<br> • Other emerging technologies<br> • How to best educate people about pandemics<br> • The pros and cons of gain-of-function research<br> • Turning mistakes into exercises you can learn from<br> • Overcoming enormous life challenges<br> • Why it’s so important to work with people you can laugh with<br> • And much more</p><p>Chapters:</p><ul><li>The interview begins (00:01:40)</li><li>Trying to control emerging contagious diseases (00:04:36)</li><li>SENTINEL (00:15:31)</li><li>SHERLOCK (00:25:09)</li><li>CARMEN (00:36:32)</li><li>Metagenomic sequencing (00:51:53)</li><li>How useful these technologies could be (01:02:35)</li><li>How this technology could apply to the US (01:06:41)</li><li>Failure modes for this technology (01:18:34)</li><li>Funding (01:27:06)</li><li>mRNA vaccines (01:31:14)</li><li>Other emerging technologies (01:34:45)</li><li>Operation Outbreak (01:41:07)</li><li>COVID (01:49:16)</li><li>Gain-of-function research (01:57:34)</li><li>Career advice (02:01:47)</li><li>Overcoming big challenges (02:10:23)</li></ul><p><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>When the first person with COVID-19 went to see a doctor in Wuhan, nobody could tell that it wasn’t a familiar disease like the flu — that we were dealing with something new.</p><p> How much death and destruction could we have avoided if we'd had a hero who could? That's what the last Assistant Secretary of Defense Andy Weber asked on the show <a href="https://80000hours.org/podcast/episodes/andy-weber-rendering-bioweapons-obsolete/"><strong>back in March.</strong></a></p><p> Today’s guest Pardis Sabeti is a professor at Harvard, fought Ebola on the ground in Africa during the 2014 outbreak, runs her own lab, co-founded a company that produces next-level testing, and is even the lead singer of a rock band. If anyone is going to be that hero in the next pandemic — it just might be her.</p><p> <a href="https://80k.link/PS"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> She is a co-author of the SENTINEL proposal, a practical system for detecting new diseases quickly, using an escalating series of three novel diagnostic techniques.</p><p> The first method, called SHERLOCK, uses CRISPR gene editing to detect familiar viruses in a simple, inexpensive filter paper test, using non-invasive samples.</p><p> If SHERLOCK draws a blank, we escalate to the second step, CARMEN, an advanced version of SHERLOCK that uses microfluidics and CRISPR to simultaneously detect hundreds of viruses and viral strains. More expensive, but far more comprehensive.</p><p> If neither SHERLOCK nor CARMEN detects a known pathogen, it's time to pull out the big gun: metagenomic sequencing. More expensive still, but sequencing all the DNA in a patient sample lets you identify and track every virus — known and unknown — in a sample.</p><p> If Pardis and her team succeeds, our future pandemic potential patient zero may:</p><p> 1. Go to the hospital with flu-like symptoms, and immediately be tested using SHERLOCK — which will come back negative<br> 2. Take the CARMEN test for a much broader range of illnesses — which will also come back negative<br> 3. Their sample will be sent for metagenomic sequencing, which will reveal that they're carrying a new virus we'll have to contend with<br> 4. At all levels, information will be recorded in a cloud-based data system that shares data in real time; the hospital will be alerted and told to quarantine the patient<br> 5. The world will be able to react weeks — or even months — faster, potentially saving millions of lives</p><p> It's a wonderful vision, and one humanity is ready to test out. But there are all sorts of practical questions, such as:</p><p> • How do you scale these technologies, including to remote and rural areas?<br> • Will doctors everywhere be able to operate them?<br> • Who will pay for it?<br> • How do you maintain the public’s trust and protect against misuse of sequencing data?<br> • How do you avoid drowning in the data the system produces?</p><p> In this conversation Pardis and Rob address all those questions, as well as:</p><p> • Pardis’ history with trying to control emerging contagious diseases<br> • The potential of mRNA vaccines<br> • Other emerging technologies<br> • How to best educate people about pandemics<br> • The pros and cons of gain-of-function research<br> • Turning mistakes into exercises you can learn from<br> • Overcoming enormous life challenges<br> • Why it’s so important to work with people you can laugh with<br> • And much more</p><p>Chapters:</p><ul><li>The interview begins (00:01:40)</li><li>Trying to control emerging contagious diseases (00:04:36)</li><li>SENTINEL (00:15:31)</li><li>SHERLOCK (00:25:09)</li><li>CARMEN (00:36:32)</li><li>Metagenomic sequencing (00:51:53)</li><li>How useful these technologies could be (01:02:35)</li><li>How this technology could apply to the US (01:06:41)</li><li>Failure modes for this technology (01:18:34)</li><li>Funding (01:27:06)</li><li>mRNA vaccines (01:31:14)</li><li>Other emerging technologies (01:34:45)</li><li>Operation Outbreak (01:41:07)</li><li>COVID (01:49:16)</li><li>Gain-of-function research (01:57:34)</li><li>Career advice (02:01:47)</li><li>Overcoming big challenges (02:10:23)</li></ul><p><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 29 Jun 2021 22:52:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/bb75c1f2/a5483e9b.mp3" length="68120042" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/gvS0OxTZteQDomElJBsP5X6TCweJ7_B0idTlDRSyxBk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDkv/MTY4MzU0NDY4MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8458</itunes:duration>
      <itunes:summary>When the first person with COVID-19 went to see a doctor in Wuhan, nobody could tell that it wasn’t a familiar disease like the flu — that we were dealing with something new. 

How much death and destruction could we have avoided if we'd had a hero who could? That's what the last Assistant Secretary of Defense Andy Weber asked on the show back in March.

Today’s guest Pardis Sabeti is a professor at Harvard, fought Ebola on the ground in Africa during the 2014 outbreak, runs her own lab, co-founded a company that produces next-level testing, and is even the lead singer of a rock band. If anyone is going to be that hero in the next pandemic — it just might be her. 

Links to learn more, summary and full transcript. 

She is a co-author of the SENTINEL proposal, a practical system for detecting new diseases quickly, using an escalating series of three novel diagnostic techniques. 

The first method, called SHERLOCK, uses CRISPR gene editing to detect familiar viruses in a simple, inexpensive filter paper test, using non-invasive samples.  

If SHERLOCK draws a blank, we escalate to the second step, CARMEN, an advanced version of SHERLOCK that uses microfluidics and CRISPR to simultaneously detect hundreds of viruses and viral strains. More expensive, but far more comprehensive. 

If neither SHERLOCK nor CARMEN detects a known pathogen, it's time to pull out the big gun: metagenomic sequencing. More expensive still, but sequencing all the DNA in a patient sample lets you identify and track every virus — known and unknown — in a sample. 

If Pardis and her team succeeds, our future pandemic potential patient zero may:  

1. Go to the hospital with flu-like symptoms, and immediately be tested using SHERLOCK — which will come back negative 
2. Take the CARMEN test for a much broader range of illnesses — which will also come back negative 
3. Their sample will be sent for metagenomic sequencing, which will reveal that they're carrying a new virus we'll have to contend with 
4. At all levels, information will be recorded in a cloud-based data system that shares data in real time; the hospital will be alerted and told to quarantine the patient 
5. The world will be able to react weeks — or even months — faster, potentially saving millions of lives 

It's a wonderful vision, and one humanity is ready to test out. But there are all sorts of practical questions, such as: 

• How do you scale these technologies, including to remote and rural areas? 
• Will doctors everywhere be able to operate them? 
• Who will pay for it? 
• How do you maintain the public’s trust and protect against misuse of sequencing data? 
• How do you avoid drowning in the data the system produces? 

In this conversation Pardis and Rob address all those questions, as well as: 

• Pardis’ history with trying to control emerging contagious diseases 
• The potential of mRNA vaccines 
• Other emerging technologies 
• How to best educate people about pandemics 
• The pros and cons of gain-of-function research 
• Turning mistakes into exercises you can learn from 
• Overcoming enormous life challenges 
• Why it’s so important to work with people you can laugh with 
• And much more 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>When the first person with COVID-19 went to see a doctor in Wuhan, nobody could tell that it wasn’t a familiar disease like the flu — that we were dealing with something new. 

How much death and destruction could we have avoided if we'd had a hero who </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/bb75c1f2/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/bb75c1f2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#103 – Max Roser on building the world's best source of COVID-19 data at Our World in Data</title>
      <itunes:title>#103 – Max Roser on building the world's best source of COVID-19 data at Our World in Data</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">99a71e1e-d2a4-11eb-8582-0e637cffa82d</guid>
      <link>https://80k.link/MR</link>
      <description>
        <![CDATA[<p>History is filled with stories of great people stepping up in times of crisis. Presidents averting wars; soldiers leading troops away from certain death; data scientists sleeping on the office floor to launch <a href="https://ourworldindata.org/coronavirus"><strong>a new webpage</strong></a> a few days sooner.</p><p> That last one is barely a joke — by our lights, people like today’s guest Max Roser should be viewed with similar admiration by historians of COVID-19.</p><p> <a href="https://80k.link/MR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Max runs <a href="https://ourworldindata.org/"><strong>Our World in Data</strong></a>, a small education nonprofit which began the pandemic with just six staff. But since last February his team has supplied essential COVID statistics to over 130 million users — among them <em>BBC</em>, <em>The Financial Times</em>, <em>The New York Times</em>, the OECD, the World Bank, the IMF, Donald Trump, Tedros Adhanom, and Dr. Anthony Fauci, just to name a few.</p><p> An economist at Oxford University, Max Roser founded Our World in Data as a small side project in 2011 and has led it since, including through the wild ride of 2020. In today's interview Max explains how he and his team realized that if <em>they</em> didn't start making COVID data accessible and easy to make sense of, it wasn't clear when <em>anyone</em> would.</p><p> Our World in Data wasn't naturally set up to become the world's go-to source for COVID updates. Up until then their specialty had been long articles explaining century-length trends in metrics like life expectancy — to the point that their graphing software was only set up to present yearly data.</p><p> But the team eventually realized that the World Health Organization was publishing numbers that flatly contradicted themselves, most of the press was embarrassingly out of its depth, and countries were posting case data as images buried deep in their sites where nobody would find them. Even worse, nobody was reporting or compiling how many tests different countries were doing, rendering all those case figures largely meaningless.</p><p> Trying to make sense of the pandemic was a time-consuming nightmare. If you were leading a national COVID response, learning what other countries were doing and whether it was working would take weeks of study — and that meant, with the walls falling in around you, it simply wasn't going to happen. Ministries of health around the world were flying blind.</p><p> Disbelief ultimately turned to determination, and the Our World in Data team committed to do whatever had to be done to fix the situation. Overnight their software was quickly redesigned to handle daily data, and for the next few months Max and colleagues like Edouard Mathieu and Hannah Ritchie did little but sleep and compile COVID data.</p><p> In this episode Max tells the story of how Our World in Data ran into a huge gap that never should have been there in the first place — and how they had to do it all again in December 2020 when, eleven months into the pandemic, there was nobody to compile global vaccination statistics.</p><p> We also talk about:</p><p> • Our World in Data's early struggles to get funding<br> • Why government agencies are so bad at presenting data<br> • Which agencies did a good job during the COVID pandemic (shout out to the European CDC)<br> • How much impact Our World in Data has by helping people understand the world<br> • How to deal with the unreliability of development statistics<br> • Why research shouldn't be published as a PDF<br> • Why academia under-incentivises data collection<br> • The history of war<br> • And much more</p><p>Chapters:<br> • Rob’s intro (00:00:00)<br>• The interview begins (00:01:41)<br>• Our World In Data (00:04:46)<br>• How OWID became a leader on COVID-19 information (00:11:45)<br>• COVID-19 gaps that OWID filled (00:27:45)<br>• Incentives that make it so hard to get good data (00:31:20)<br>• OWID funding (00:39:53)<br>• What it was like to be so successful (00:42:11)<br>• Vaccination data set (00:45:43)<br>• Improving the vaccine rollout (00:52:44)<br>• Who did well (00:58:08)<br>• Global sanity (01:00:57)<br>• How high-impact is this work? (01:04:43)<br>• Does this work get you anywhere in the academic system? (01:12:48)<br>• Other projects Max admires in this space (01:20:05)<br>• Data reliability and availability (01:30:49)<br>• Bringing together knowledge and presentation (01:39:26)<br>• History of war (01:49:17)<br>• Careers at OWID (02:01:15)<br>• How OWID prioritise topics (02:12:30)<br>• Rob's outro (02:21:02) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ryan Kessler.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>History is filled with stories of great people stepping up in times of crisis. Presidents averting wars; soldiers leading troops away from certain death; data scientists sleeping on the office floor to launch <a href="https://ourworldindata.org/coronavirus"><strong>a new webpage</strong></a> a few days sooner.</p><p> That last one is barely a joke — by our lights, people like today’s guest Max Roser should be viewed with similar admiration by historians of COVID-19.</p><p> <a href="https://80k.link/MR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Max runs <a href="https://ourworldindata.org/"><strong>Our World in Data</strong></a>, a small education nonprofit which began the pandemic with just six staff. But since last February his team has supplied essential COVID statistics to over 130 million users — among them <em>BBC</em>, <em>The Financial Times</em>, <em>The New York Times</em>, the OECD, the World Bank, the IMF, Donald Trump, Tedros Adhanom, and Dr. Anthony Fauci, just to name a few.</p><p> An economist at Oxford University, Max Roser founded Our World in Data as a small side project in 2011 and has led it since, including through the wild ride of 2020. In today's interview Max explains how he and his team realized that if <em>they</em> didn't start making COVID data accessible and easy to make sense of, it wasn't clear when <em>anyone</em> would.</p><p> Our World in Data wasn't naturally set up to become the world's go-to source for COVID updates. Up until then their specialty had been long articles explaining century-length trends in metrics like life expectancy — to the point that their graphing software was only set up to present yearly data.</p><p> But the team eventually realized that the World Health Organization was publishing numbers that flatly contradicted themselves, most of the press was embarrassingly out of its depth, and countries were posting case data as images buried deep in their sites where nobody would find them. Even worse, nobody was reporting or compiling how many tests different countries were doing, rendering all those case figures largely meaningless.</p><p> Trying to make sense of the pandemic was a time-consuming nightmare. If you were leading a national COVID response, learning what other countries were doing and whether it was working would take weeks of study — and that meant, with the walls falling in around you, it simply wasn't going to happen. Ministries of health around the world were flying blind.</p><p> Disbelief ultimately turned to determination, and the Our World in Data team committed to do whatever had to be done to fix the situation. Overnight their software was quickly redesigned to handle daily data, and for the next few months Max and colleagues like Edouard Mathieu and Hannah Ritchie did little but sleep and compile COVID data.</p><p> In this episode Max tells the story of how Our World in Data ran into a huge gap that never should have been there in the first place — and how they had to do it all again in December 2020 when, eleven months into the pandemic, there was nobody to compile global vaccination statistics.</p><p> We also talk about:</p><p> • Our World in Data's early struggles to get funding<br> • Why government agencies are so bad at presenting data<br> • Which agencies did a good job during the COVID pandemic (shout out to the European CDC)<br> • How much impact Our World in Data has by helping people understand the world<br> • How to deal with the unreliability of development statistics<br> • Why research shouldn't be published as a PDF<br> • Why academia under-incentivises data collection<br> • The history of war<br> • And much more</p><p>Chapters:<br> • Rob’s intro (00:00:00)<br>• The interview begins (00:01:41)<br>• Our World In Data (00:04:46)<br>• How OWID became a leader on COVID-19 information (00:11:45)<br>• COVID-19 gaps that OWID filled (00:27:45)<br>• Incentives that make it so hard to get good data (00:31:20)<br>• OWID funding (00:39:53)<br>• What it was like to be so successful (00:42:11)<br>• Vaccination data set (00:45:43)<br>• Improving the vaccine rollout (00:52:44)<br>• Who did well (00:58:08)<br>• Global sanity (01:00:57)<br>• How high-impact is this work? (01:04:43)<br>• Does this work get you anywhere in the academic system? (01:12:48)<br>• Other projects Max admires in this space (01:20:05)<br>• Data reliability and availability (01:30:49)<br>• Bringing together knowledge and presentation (01:39:26)<br>• History of war (01:49:17)<br>• Careers at OWID (02:01:15)<br>• How OWID prioritise topics (02:12:30)<br>• Rob's outro (02:21:02) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ryan Kessler.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 21 Jun 2021 16:13:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8733f51b/317ab353.mp3" length="68858745" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/PVtT8BRo-Wdxf9X8mZmuGLE5CcOtJAgoCaeLXlGNiIc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDgv/MTY4MzU0NDY3OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8545</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>History is filled with stories of great people stepping up in times of crisis. Presidents averting wars; soldiers leading troops away from certain death; data scientists sleeping on the office floor to launch <a href="https://ourworldindata.org/coronavirus"><strong>a new webpage</strong></a> a few days sooner.</p><p> That last one is barely a joke — by our lights, people like today’s guest Max Roser should be viewed with similar admiration by historians of COVID-19.</p><p> <a href="https://80k.link/MR"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Max runs <a href="https://ourworldindata.org/"><strong>Our World in Data</strong></a>, a small education nonprofit which began the pandemic with just six staff. But since last February his team has supplied essential COVID statistics to over 130 million users — among them <em>BBC</em>, <em>The Financial Times</em>, <em>The New York Times</em>, the OECD, the World Bank, the IMF, Donald Trump, Tedros Adhanom, and Dr. Anthony Fauci, just to name a few.</p><p> An economist at Oxford University, Max Roser founded Our World in Data as a small side project in 2011 and has led it since, including through the wild ride of 2020. In today's interview Max explains how he and his team realized that if <em>they</em> didn't start making COVID data accessible and easy to make sense of, it wasn't clear when <em>anyone</em> would.</p><p> Our World in Data wasn't naturally set up to become the world's go-to source for COVID updates. Up until then their specialty had been long articles explaining century-length trends in metrics like life expectancy — to the point that their graphing software was only set up to present yearly data.</p><p> But the team eventually realized that the World Health Organization was publishing numbers that flatly contradicted themselves, most of the press was embarrassingly out of its depth, and countries were posting case data as images buried deep in their sites where nobody would find them. Even worse, nobody was reporting or compiling how many tests different countries were doing, rendering all those case figures largely meaningless.</p><p> Trying to make sense of the pandemic was a time-consuming nightmare. If you were leading a national COVID response, learning what other countries were doing and whether it was working would take weeks of study — and that meant, with the walls falling in around you, it simply wasn't going to happen. Ministries of health around the world were flying blind.</p><p> Disbelief ultimately turned to determination, and the Our World in Data team committed to do whatever had to be done to fix the situation. Overnight their software was quickly redesigned to handle daily data, and for the next few months Max and colleagues like Edouard Mathieu and Hannah Ritchie did little but sleep and compile COVID data.</p><p> In this episode Max tells the story of how Our World in Data ran into a huge gap that never should have been there in the first place — and how they had to do it all again in December 2020 when, eleven months into the pandemic, there was nobody to compile global vaccination statistics.</p><p> We also talk about:</p><p> • Our World in Data's early struggles to get funding<br> • Why government agencies are so bad at presenting data<br> • Which agencies did a good job during the COVID pandemic (shout out to the European CDC)<br> • How much impact Our World in Data has by helping people understand the world<br> • How to deal with the unreliability of development statistics<br> • Why research shouldn't be published as a PDF<br> • Why academia under-incentivises data collection<br> • The history of war<br> • And much more</p><p>Chapters:<br> • Rob’s intro (00:00:00)<br>• The interview begins (00:01:41)<br>• Our World In Data (00:04:46)<br>• How OWID became a leader on COVID-19 information (00:11:45)<br>• COVID-19 gaps that OWID filled (00:27:45)<br>• Incentives that make it so hard to get good data (00:31:20)<br>• OWID funding (00:39:53)<br>• What it was like to be so successful (00:42:11)<br>• Vaccination data set (00:45:43)<br>• Improving the vaccine rollout (00:52:44)<br>• Who did well (00:58:08)<br>• Global sanity (01:00:57)<br>• How high-impact is this work? (01:04:43)<br>• Does this work get you anywhere in the academic system? (01:12:48)<br>• Other projects Max admires in this space (01:20:05)<br>• Data reliability and availability (01:30:49)<br>• Bringing together knowledge and presentation (01:39:26)<br>• History of war (01:49:17)<br>• Careers at OWID (02:01:15)<br>• How OWID prioritise topics (02:12:30)<br>• Rob's outro (02:21:02) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ryan Kessler.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/8733f51b/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/8733f51b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#102 – Tom Moynihan on why prior generations missed some of the biggest priorities of all</title>
      <itunes:title>#102 – Tom Moynihan on why prior generations missed some of the biggest priorities of all</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c2eeec02-cafc-11eb-b350-0e927b750a65</guid>
      <link>https://80000hours.org/podcast/episodes/tom-moynihan-prior-generations/</link>
      <description>
        <![CDATA[<p>It can be tough to get people to truly care about reducing existential risks today. But spare a thought for the longtermist of the 17th century: they were surrounded by people who thought extinction was literally impossible.</p><p> Today’s guest Tom Moynihan, intellectual historian and author of the book <a href="https://80k.link/XR"><strong><em>X-Risk: How Humanity Discovered Its Own Extinction</em></strong></a>, says that until the 18th century, almost everyone — including early atheists — couldn’t imagine that humanity or life could simply disappear because of an act of nature.</p><p> <a href="https://80k.link/TM"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This is largely because of the prevalence of the ‘principle of plenitude’, which Tom defines as saying:</p><p> “Whatever can happen will happen. In its stronger form it says whatever can happen will happen reliably and recurrently. And in its strongest form it says that all that can happen is happening right now. And that's the way things will be forever.”</p><p> This has the implication that if humanity ever disappeared for some reason, then it would have to reappear. So why would you ever worry about extinction?</p><p> Here are 4 more commonly held beliefs from generations past that Tom shares in the interview:</p><p> • <strong>All regions of matter that can be populated will be populated</strong>: In other words, there are aliens on every planet, because it would be a massive waste of real estate if all of them were just inorganic masses, where nothing interesting was going on. This also led to the idea that <strong>if you dug deep into the Earth, you’d potentially find thriving societies.</strong><br> • <strong>Aliens were human-like, and shared the same values as us</strong>: they would have the same moral beliefs, and the same aesthetic beliefs. The idea that aliens might be very different from us only arrived in the 20th century.<br> • <strong>Fossils were rocks that had gotten a bit too big for their britches and were trying to act like animals</strong>: they couldn’t actually move, so becoming an imprint of an animal was the next best thing.<br> • <strong>All future generations were contained in miniature form, Russian-doll style, in the sperm of the first man</strong>: preformation was the idea that within the ovule or the sperm of an animal is contained its offspring in miniature form, and the French philosopher Malebranche said, well, if one is contained in the other one, then surely that goes on forever.</p><p> And here are another three that weren’t held widely, but were proposed by scholars and taken seriously:</p><p> • <strong>Life preceded the existence of rocks</strong>: Living things, like clams and mollusks, came first, and they extruded the earth.<br> • <strong>No idea can be wrong</strong>: Nothing we can say about the world is wrong in a strong sense, because at some point in the future or the past, it has been true.<br> • <strong>Maybe we were living before the Trojan War</strong>: Aristotle said that we might actually be living <em>before</em> Troy, because it — like every other event — will repeat at some future date. And he said that actually, the set of possibilities might be so narrow that it might be safer to say that we actually live before Troy.</p><p> But Tom tries to be magnanimous when faced with these incredibly misguided worldviews.</p><p> In this nearly four-hour long interview, Tom and Rob cover all of these ideas, as well as:</p><p> • How we know people really believed such things<br> • How we moved on from these theories<br> • How future intellectual historians might view our beliefs today<br> • The distinction between ‘apocalypse’ and ‘extinction’<br> • Utopias and dystopias<br> • Big ideas that haven’t flowed through into all relevant fields yet<br> • Intellectual history as a possible high-impact career<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:45)</li><li>Principle of Plenitude (00:04:02)</li><li>How do we know they really believed this? (00:13:20)</li><li>Religious conceptions of time (00:24:01)</li><li>How to react to wacky old ideas (00:29:18)</li><li>The Copernican revolution (00:36:55)</li><li>Fossils (00:42:30)</li><li>How we got past these theories (00:51:19)</li><li>Intellectual history (01:01:45)</li><li>Future historians looking back to today (01:13:11)</li><li>Could plenitude actually be true? (01:27:38)</li><li>What is vs. what ought to be (01:36:43)</li><li>Apocalypse vs. extinction (01:45:56)</li><li>The history of probability (02:00:52)</li><li>Utopias and dystopias (02:12:11)</li><li>How Tom has changed his mind since writing the book (02:28:58)</li><li>Are we making progress? (02:35:00)</li><li>Big ideas that haven’t flowed through to all relevant fields yet (02:52:07)</li><li>Failed predictions (02:59:01)</li><li>Intellectual history as high-impact career (03:06:56)</li><li>Communicating progress (03:15:07)</li><li>What careers in history actually look like (03:23:03)</li><li>Tom’s next major project (03:43:06)</li><li>One of the funniest things past generations believed (03:51:50)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>It can be tough to get people to truly care about reducing existential risks today. But spare a thought for the longtermist of the 17th century: they were surrounded by people who thought extinction was literally impossible.</p><p> Today’s guest Tom Moynihan, intellectual historian and author of the book <a href="https://80k.link/XR"><strong><em>X-Risk: How Humanity Discovered Its Own Extinction</em></strong></a>, says that until the 18th century, almost everyone — including early atheists — couldn’t imagine that humanity or life could simply disappear because of an act of nature.</p><p> <a href="https://80k.link/TM"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This is largely because of the prevalence of the ‘principle of plenitude’, which Tom defines as saying:</p><p> “Whatever can happen will happen. In its stronger form it says whatever can happen will happen reliably and recurrently. And in its strongest form it says that all that can happen is happening right now. And that's the way things will be forever.”</p><p> This has the implication that if humanity ever disappeared for some reason, then it would have to reappear. So why would you ever worry about extinction?</p><p> Here are 4 more commonly held beliefs from generations past that Tom shares in the interview:</p><p> • <strong>All regions of matter that can be populated will be populated</strong>: In other words, there are aliens on every planet, because it would be a massive waste of real estate if all of them were just inorganic masses, where nothing interesting was going on. This also led to the idea that <strong>if you dug deep into the Earth, you’d potentially find thriving societies.</strong><br> • <strong>Aliens were human-like, and shared the same values as us</strong>: they would have the same moral beliefs, and the same aesthetic beliefs. The idea that aliens might be very different from us only arrived in the 20th century.<br> • <strong>Fossils were rocks that had gotten a bit too big for their britches and were trying to act like animals</strong>: they couldn’t actually move, so becoming an imprint of an animal was the next best thing.<br> • <strong>All future generations were contained in miniature form, Russian-doll style, in the sperm of the first man</strong>: preformation was the idea that within the ovule or the sperm of an animal is contained its offspring in miniature form, and the French philosopher Malebranche said, well, if one is contained in the other one, then surely that goes on forever.</p><p> And here are another three that weren’t held widely, but were proposed by scholars and taken seriously:</p><p> • <strong>Life preceded the existence of rocks</strong>: Living things, like clams and mollusks, came first, and they extruded the earth.<br> • <strong>No idea can be wrong</strong>: Nothing we can say about the world is wrong in a strong sense, because at some point in the future or the past, it has been true.<br> • <strong>Maybe we were living before the Trojan War</strong>: Aristotle said that we might actually be living <em>before</em> Troy, because it — like every other event — will repeat at some future date. And he said that actually, the set of possibilities might be so narrow that it might be safer to say that we actually live before Troy.</p><p> But Tom tries to be magnanimous when faced with these incredibly misguided worldviews.</p><p> In this nearly four-hour long interview, Tom and Rob cover all of these ideas, as well as:</p><p> • How we know people really believed such things<br> • How we moved on from these theories<br> • How future intellectual historians might view our beliefs today<br> • The distinction between ‘apocalypse’ and ‘extinction’<br> • Utopias and dystopias<br> • Big ideas that haven’t flowed through into all relevant fields yet<br> • Intellectual history as a possible high-impact career<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:45)</li><li>Principle of Plenitude (00:04:02)</li><li>How do we know they really believed this? (00:13:20)</li><li>Religious conceptions of time (00:24:01)</li><li>How to react to wacky old ideas (00:29:18)</li><li>The Copernican revolution (00:36:55)</li><li>Fossils (00:42:30)</li><li>How we got past these theories (00:51:19)</li><li>Intellectual history (01:01:45)</li><li>Future historians looking back to today (01:13:11)</li><li>Could plenitude actually be true? (01:27:38)</li><li>What is vs. what ought to be (01:36:43)</li><li>Apocalypse vs. extinction (01:45:56)</li><li>The history of probability (02:00:52)</li><li>Utopias and dystopias (02:12:11)</li><li>How Tom has changed his mind since writing the book (02:28:58)</li><li>Are we making progress? (02:35:00)</li><li>Big ideas that haven’t flowed through to all relevant fields yet (02:52:07)</li><li>Failed predictions (02:59:01)</li><li>Intellectual history as high-impact career (03:06:56)</li><li>Communicating progress (03:15:07)</li><li>What careers in history actually look like (03:23:03)</li><li>Tom’s next major project (03:43:06)</li><li>One of the funniest things past generations believed (03:51:50)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 11 Jun 2021 22:41:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/133a5130/2c11da5e.mp3" length="113989724" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/P65QRXbYXGvA_nJ0r3-CKNOEpFKdKkPee65H1GD0h8k/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDcv/MTY4MzU0NDY3OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>14204</itunes:duration>
      <itunes:summary>It can be tough to get people to truly care about reducing existential risks today. But spare a thought for the longtermist of the 17th century: they were surrounded by people who thought extinction was literally impossible. 

Today’s guest Tom Moynihan, intellectual historian and author of the book X-Risk: How Humanity Discovered Its Own Extinction, says that until the 18th century, almost everyone — including early atheists — couldn’t imagine that humanity or life could simply disappear because of an act of nature.  

Links to learn more, summary and full transcript.

This is largely because of the prevalence of the ‘principle of plenitude’, which Tom defines as saying: 

“Whatever can happen will happen. In its stronger form it says whatever can happen will happen reliably and recurrently. And in its strongest form it says that all that can happen is happening right now. And that's the way things will be forever.” 

This has the implication that if humanity ever disappeared for some reason, then it would have to reappear. So why would you ever worry about extinction? 

Here are 4 more commonly held beliefs from generations past that Tom shares in the interview: 

•  All regions of matter that can be populated will be populated: In other words, there are aliens on every planet, because it would be a massive waste of real estate if all of them were just inorganic masses, where nothing interesting was going on. This also led to the idea that if you dug deep into the Earth, you’d potentially find thriving societies. 
•  Aliens were human-like, and shared the same values as us: they would have the same moral beliefs, and the same aesthetic beliefs. The idea that aliens might be very different from us only arrived in the 20th century. 
•  Fossils were rocks that had gotten a bit too big for their britches and were trying to act like animals: they couldn’t actually move, so becoming an imprint of an animal was the next best thing. 
•  All future generations were contained in miniature form, Russian-doll style, in the sperm of the first man: preformation was the idea that within the ovule or the sperm of an animal is contained its offspring in miniature form, and the French philosopher Malebranche said, well, if one is contained in the other one, then surely that goes on forever. 

And here are another three that weren’t held widely, but were proposed by scholars and taken seriously: 

•  Life preceded the existence of rocks: Living things, like clams and mollusks, came first, and they extruded the earth. 
•  No idea can be wrong: Nothing we can say about the world is wrong in a strong sense, because at some point in the future or the past, it has been true. 
•  Maybe we were living before the Trojan War: Aristotle said that we might actually be living before Troy, because it — like every other event — will repeat at some future date. And he said that actually, the set of possibilities might be so narrow that it might be safer to say that we actually live before Troy. 

But Tom tries to be magnanimous when faced with these incredibly misguided worldviews.  

In this nearly four-hour long interview, Tom and Rob cover all of these ideas, as well as: 

•  How we know people really believed such things 
•  How we moved on from these theories 
•  How future intellectual historians might view our beliefs today 
•  The distinction between ‘apocalypse’ and ‘extinction’ 
•  Utopias and dystopias 
•  Big ideas that haven’t flowed through into all relevant fields yet 
•  Intellectual history as a possible high-impact career 
•  And much more 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>It can be tough to get people to truly care about reducing existential risks today. But spare a thought for the longtermist of the 17th century: they were surrounded by people who thought extinction was literally impossible. 

Today’s guest Tom Moynihan</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/133a5130/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/133a5130/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#101 – Robert Wright on using cognitive empathy to save the world</title>
      <itunes:title>#101 – Robert Wright on using cognitive empathy to save the world</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">0300e122-bf38-11eb-ae51-0e4ca44c7a7b</guid>
      <link>https://share.transistor.fm/s/c2f465d2</link>
      <description>
        <![CDATA[<p>In 2003, Saddam Hussein refused to let Iraqi weapons scientists leave the country to be interrogated. Given the overwhelming domestic support for an invasion at the time, most key figures in the U.S. took that as confirmation that he had something to hide — probably an active WMD program.</p><p> 

But what about alternative explanations? Maybe those scientists knew about past crimes. Or maybe they’d defect. Or maybe giving in to that kind of demand would have humiliated Hussein in the eyes of enemies like Iran and Saudi Arabia.</p><p> 

According to today’s guest Robert Wright, host of the popular podcast The Wright Show, these are the kinds of things that might have come up if people were willing to look at things from Saddam Hussein’s perspective.</p><p> 

<a href="https://80k.link/BW"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

He calls this ‘cognitive empathy’. It's not feeling-your-pain-type empathy — it's just trying to understand how another person thinks.</p><p>  

He says if you pitched this kind of thing back in 2003 you’d be shouted down as a 'Saddam apologist' — and he thinks the same is true today when it comes to regimes in China, Russia, Iran, and North Korea.</p><p> 

The two Roberts in today’s episode — Bob Wright and Rob Wiblin — agree that removing this taboo against perspective taking, even with people you consider truly evil, could potentially significantly improve discourse around international relations.</p><p>  

They feel that if we could spread the meme that if you’re able to understand what dictators are thinking and calculating, based on their country’s history and interests, it seems like we’d be less likely to make terrible foreign policy errors.</p><p> 

But how do you actually do that?</p><p> 

Bob’s new 
<a href="https://nonzero.org/post/apocalypse-aversion-project"><i>‘Apocalypse Aversion Project’</i></a> is focused on creating the necessary conditions for solving non-zero-sum global coordination problems, something most people are already on board with.</p><p> 

And in particular he thinks that might come from enough individuals “transcending the psychology of tribalism”. He doesn’t just mean rage and hatred and violence, he’s also talking about cognitive biases.</p><p> 

Bob makes the striking claim that if enough people in the U.S. had been able to combine perspective taking with mindfulness — the ability to notice and identify thoughts as they arise — then the U.S. might have even been able to avoid the invasion of Iraq.</p><p> 

Rob pushes back on how realistic this approach really is, asking questions like:</p><p> 

• Haven’t people been trying to do this since the beginning of time?<br> 
• Is there a great novel angle that will change how a lot of people think and behave?<br> 
• Wouldn’t it be better to focus on a much narrower task, like getting more mindfulness and meditation and reflectiveness among the U.S. foreign policy elite?</p><p> 

But despite the differences in approaches, Bob has a lot of common ground with 80,000 Hours — and the result is a fun back-and-forth about the best ways to achieve shared goals.</p><p> 

Bob starts by questioning Rob about effective altruism, and they go on to cover a bunch of other topics, such as:</p><p> 

• Specific risks like climate change and new technologies<br> 
• How to achieve social cohesion<br> 
• The pros and cons of society-wide surveillance<br> 
• How Rob got into effective altruism</p><p> 

If you're interested to hear more of Bob's interviews you can subscribe to <a href="https://podcasts.apple.com/us/podcast/the-wright-show/id505824847"><i>The Wright Show</i></a> anywhere you're getting this one. You can also watch videos of this and all his other episodes on  <a href="https://bloggingheads.tv">Bloggingheads.tv</a>.</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In 2003, Saddam Hussein refused to let Iraqi weapons scientists leave the country to be interrogated. Given the overwhelming domestic support for an invasion at the time, most key figures in the U.S. took that as confirmation that he had something to hide — probably an active WMD program.</p><p> 

But what about alternative explanations? Maybe those scientists knew about past crimes. Or maybe they’d defect. Or maybe giving in to that kind of demand would have humiliated Hussein in the eyes of enemies like Iran and Saudi Arabia.</p><p> 

According to today’s guest Robert Wright, host of the popular podcast The Wright Show, these are the kinds of things that might have come up if people were willing to look at things from Saddam Hussein’s perspective.</p><p> 

<a href="https://80k.link/BW"><b>Links to learn more, summary and full transcript.</b></a></p><p> 

He calls this ‘cognitive empathy’. It's not feeling-your-pain-type empathy — it's just trying to understand how another person thinks.</p><p>  

He says if you pitched this kind of thing back in 2003 you’d be shouted down as a 'Saddam apologist' — and he thinks the same is true today when it comes to regimes in China, Russia, Iran, and North Korea.</p><p> 

The two Roberts in today’s episode — Bob Wright and Rob Wiblin — agree that removing this taboo against perspective taking, even with people you consider truly evil, could potentially significantly improve discourse around international relations.</p><p>  

They feel that if we could spread the meme that if you’re able to understand what dictators are thinking and calculating, based on their country’s history and interests, it seems like we’d be less likely to make terrible foreign policy errors.</p><p> 

But how do you actually do that?</p><p> 

Bob’s new 
<a href="https://nonzero.org/post/apocalypse-aversion-project"><i>‘Apocalypse Aversion Project’</i></a> is focused on creating the necessary conditions for solving non-zero-sum global coordination problems, something most people are already on board with.</p><p> 

And in particular he thinks that might come from enough individuals “transcending the psychology of tribalism”. He doesn’t just mean rage and hatred and violence, he’s also talking about cognitive biases.</p><p> 

Bob makes the striking claim that if enough people in the U.S. had been able to combine perspective taking with mindfulness — the ability to notice and identify thoughts as they arise — then the U.S. might have even been able to avoid the invasion of Iraq.</p><p> 

Rob pushes back on how realistic this approach really is, asking questions like:</p><p> 

• Haven’t people been trying to do this since the beginning of time?<br> 
• Is there a great novel angle that will change how a lot of people think and behave?<br> 
• Wouldn’t it be better to focus on a much narrower task, like getting more mindfulness and meditation and reflectiveness among the U.S. foreign policy elite?</p><p> 

But despite the differences in approaches, Bob has a lot of common ground with 80,000 Hours — and the result is a fun back-and-forth about the best ways to achieve shared goals.</p><p> 

Bob starts by questioning Rob about effective altruism, and they go on to cover a bunch of other topics, such as:</p><p> 

• Specific risks like climate change and new technologies<br> 
• How to achieve social cohesion<br> 
• The pros and cons of society-wide surveillance<br> 
• How Rob got into effective altruism</p><p> 

If you're interested to hear more of Bob's interviews you can subscribe to <a href="https://podcasts.apple.com/us/podcast/the-wright-show/id505824847"><i>The Wright Show</i></a> anywhere you're getting this one. You can also watch videos of this and all his other episodes on  <a href="https://bloggingheads.tv">Bloggingheads.tv</a>.</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 28 May 2021 00:57:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c2f465d2/2b749b79.mp3" length="46277167" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/pi7423ZJlbdfviMvRg3nKpQRyiyK79OOV6G7Ab0jmJg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDYv/MTY4MzU0NDY3Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5760</itunes:duration>
      <itunes:summary>In 2003, Saddam Hussein refused to let Iraqi weapons scientists leave the country to be interrogated. Given the overwhelming domestic support for an invasion at the time, most key figures in the U.S. took that as confirmation that he had something to hide — probably an active WMD program. 

But what about alternative explanations? Maybe those scientists knew about past crimes. Or maybe they’d defect. Or maybe giving in to that kind of demand would have humiliated Hussein in the eyes of enemies like Iran and Saudi Arabia. 

According to today’s guest Robert Wright, host of the popular podcast The Wright Show, these are the kinds of things that might have come up if people were willing to look at things from Saddam Hussein’s perspective. 

Links to learn more, summary and full transcript. 

He calls this ‘cognitive empathy’. It's not feeling-your-pain-type empathy — it's just trying to understand how another person thinks.  

He says if you pitched this kind of thing back in 2003 you’d be shouted down as a 'Saddam apologist' — and he thinks the same is true today when it comes to regimes in China, Russia, Iran, and North Korea. 

The two Roberts in today’s episode — Bob Wright and Rob Wiblin — agree that removing this taboo against perspective taking, even with people you consider truly evil, could potentially significantly improve discourse around international relations.  

They feel that if we could spread the meme that if you’re able to understand what dictators are thinking and calculating, based on their country’s history and interests, it seems like we’d be less likely to make terrible foreign policy errors. 

But how do you actually do that? 

Bob’s new 
‘Apocalypse Aversion Project’ is focused on creating the necessary conditions for solving non-zero-sum global coordination problems, something most people are already on board with. 

And in particular he thinks that might come from enough individuals “transcending the psychology of tribalism”. He doesn’t just mean rage and hatred and violence, he’s also talking about cognitive biases. 

Bob makes the striking claim that if enough people in the U.S. had been able to combine perspective taking with mindfulness — the ability to notice and identify thoughts as they arise — then the U.S. might have even been able to avoid the invasion of Iraq. 

Rob pushes back on how realistic this approach really is, asking questions like: 

• Haven’t people been trying to do this since the beginning of time? 
• Is there a great novel angle that will change how a lot of people think and behave? 
• Wouldn’t it be better to focus on a much narrower task, like getting more mindfulness and meditation and reflectiveness among the U.S. foreign policy elite? 

But despite the differences in approaches, Bob has a lot of common ground with 80,000 Hours — and the result is a fun back-and-forth about the best ways to achieve shared goals. 

Bob starts by questioning Rob about effective altruism, and they go on to cover a bunch of other topics, such as: 

• Specific risks like climate change and new technologies 
• How to achieve social cohesion 
• The pros and cons of society-wide surveillance 
• How Rob got into effective altruism 

If you're interested to hear more of Bob's interviews you can subscribe to The Wright Show anywhere you're getting this one. You can also watch videos of this and all his other episodes on  Bloggingheads.tv. 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>In 2003, Saddam Hussein refused to let Iraqi weapons scientists leave the country to be interrogated. Given the overwhelming domestic support for an invasion at the time, most key figures in the U.S. took that as confirmation that he had something to hide</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/c2f465d2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#100 – Having a successful career with depression, anxiety and imposter syndrome</title>
      <itunes:title>#100 – Having a successful career with depression, anxiety and imposter syndrome</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">20c67e60-b8c1-11eb-8bb6-12eedf9b25a5</guid>
      <link>https://share.transistor.fm/s/3ca7e464</link>
      <description>
        <![CDATA[Today's episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!).<p> 

The producer of this show, Keiran Harris, interviewed our mutual colleague Howie about the major ways that mental illness has affected his life and career. While depression, anxiety, ADHD and other problems are extremely common, it's rare for people to offer detailed insight into their thoughts and struggles — and even rarer for someone as perceptive as Howie to do so.</p><p> 

<a href="https://80k.link/MI"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The first half of this conversation is a searingly honest account of Howie’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today.</p><p> 

The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort.</p><p> 

Howie tries his best to be a corrective to this misguided attitude and pragmatically focus on what actually matters — doing whatever will help you get better.</p><p> 

Mental illness is one of the things that most often trips up people who could otherwise enjoy flourishing careers and have a large social impact, so we think this could plausibly be one of our more valuable episodes.</p><p> 

Howie and Keiran basically treated it like a private conversation, with the understanding that it may be too sensitive to release. But, after getting some really positive feedback, they’ve decided to share it with the world.</p><p> 

We hope that the episode will:</p><p> 

1. Help people realise that they have a shot at making a difference in the future, even if they’re experiencing (or have experienced in the past) mental illness, self doubt, imposter syndrome, or other personal obstacles.</p><p> 


2. Give insight into what it's like in the head of one person with depression, anxiety, and imposter syndrome, including the specific thought patterns they experience on typical days and more extreme days. In addition to being interesting for its own sake, this might make it easier for people to understand the experiences of family members, friends, and colleagues — and know how to react more helpfully.</p><p> 

So we think this episode will be valuable for:</p><p>  

• People who have experienced mental health problems or might in future;<br>  
• People who have had troubles with stress, anxiety, low mood, low self esteem, and similar issues, even if their experience isn’t well described as ‘mental illness’;<br>  
• People who have never experienced these problems but want to learn about what it's like, so they can better relate to and assist family, friends or colleagues who do.</p><p> 

In other words, we think this episode could be worthwhile for almost everybody.</p><p> 

Just a heads up that this conversation gets pretty intense at times, and includes references to self-harm and suicidal thoughts.</p><p> 

If you don’t want to hear the most intense section, you can skip the chapter called ‘Disaster’ (44–57mins). And if you’d rather avoid almost all of these references, you could skip straight to the chapter called ‘80,000 Hours’ (1hr 11mins).</p><p> 

<i>If you're feeling suicidal or have thoughts of harming yourself right now, there are suicide hotlines at <a href="https://80k.link/SP"><b>National Suicide Prevention Lifeline</b></a> in the U.S. (800-273-8255) and <a href="https://80k.link/S"><b>Samaritans</b></a> in the U.K. (116 123).</i></p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today's episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!).<p> 

The producer of this show, Keiran Harris, interviewed our mutual colleague Howie about the major ways that mental illness has affected his life and career. While depression, anxiety, ADHD and other problems are extremely common, it's rare for people to offer detailed insight into their thoughts and struggles — and even rarer for someone as perceptive as Howie to do so.</p><p> 

<a href="https://80k.link/MI"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The first half of this conversation is a searingly honest account of Howie’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today.</p><p> 

The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort.</p><p> 

Howie tries his best to be a corrective to this misguided attitude and pragmatically focus on what actually matters — doing whatever will help you get better.</p><p> 

Mental illness is one of the things that most often trips up people who could otherwise enjoy flourishing careers and have a large social impact, so we think this could plausibly be one of our more valuable episodes.</p><p> 

Howie and Keiran basically treated it like a private conversation, with the understanding that it may be too sensitive to release. But, after getting some really positive feedback, they’ve decided to share it with the world.</p><p> 

We hope that the episode will:</p><p> 

1. Help people realise that they have a shot at making a difference in the future, even if they’re experiencing (or have experienced in the past) mental illness, self doubt, imposter syndrome, or other personal obstacles.</p><p> 


2. Give insight into what it's like in the head of one person with depression, anxiety, and imposter syndrome, including the specific thought patterns they experience on typical days and more extreme days. In addition to being interesting for its own sake, this might make it easier for people to understand the experiences of family members, friends, and colleagues — and know how to react more helpfully.</p><p> 

So we think this episode will be valuable for:</p><p>  

• People who have experienced mental health problems or might in future;<br>  
• People who have had troubles with stress, anxiety, low mood, low self esteem, and similar issues, even if their experience isn’t well described as ‘mental illness’;<br>  
• People who have never experienced these problems but want to learn about what it's like, so they can better relate to and assist family, friends or colleagues who do.</p><p> 

In other words, we think this episode could be worthwhile for almost everybody.</p><p> 

Just a heads up that this conversation gets pretty intense at times, and includes references to self-harm and suicidal thoughts.</p><p> 

If you don’t want to hear the most intense section, you can skip the chapter called ‘Disaster’ (44–57mins). And if you’d rather avoid almost all of these references, you could skip straight to the chapter called ‘80,000 Hours’ (1hr 11mins).</p><p> 

<i>If you're feeling suicidal or have thoughts of harming yourself right now, there are suicide hotlines at <a href="https://80k.link/SP"><b>National Suicide Prevention Lifeline</b></a> in the U.S. (800-273-8255) and <a href="https://80k.link/S"><b>Samaritans</b></a> in the U.K. (116 123).</i></p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 19 May 2021 17:55:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3ca7e464/1dc3d7f4.mp3" length="82440237" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/biRol4I49lPMAOLTTAqZQzZLqK50zhvHiuqfHL97pXI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDUv/MTY4MzU0NDY3Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10281</itunes:duration>
      <itunes:summary>Today's episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!). 

The producer of this show, Keiran Harris, interviewed our mutual colleague Howie about the major ways that mental illness has affected his life and career. While depression, anxiety, ADHD and other problems are extremely common, it's rare for people to offer detailed insight into their thoughts and struggles — and even rarer for someone as perceptive as Howie to do so. 

Links to learn more, summary and full transcript.

The first half of this conversation is a searingly honest account of Howie’s story, including losing a job he loved due to a depressed episode, what it was like to be basically out of commission for over a year, how he got back on his feet, and the things he still finds difficult today. 

The second half covers Howie’s advice. Conventional wisdom on mental health can be really focused on cultivating willpower — telling depressed people that the virtuous thing to do is to start exercising, improve their diet, get their sleep in check, and generally fix all their problems before turning to therapy and medication as some sort of last resort. 

Howie tries his best to be a corrective to this misguided attitude and pragmatically focus on what actually matters — doing whatever will help you get better. 

Mental illness is one of the things that most often trips up people who could otherwise enjoy flourishing careers and have a large social impact, so we think this could plausibly be one of our more valuable episodes. 

Howie and Keiran basically treated it like a private conversation, with the understanding that it may be too sensitive to release. But, after getting some really positive feedback, they’ve decided to share it with the world. 

We hope that the episode will: 

1. Help people realise that they have a shot at making a difference in the future, even if they’re experiencing (or have experienced in the past) mental illness, self doubt, imposter syndrome, or other personal obstacles. 


2. Give insight into what it's like in the head of one person with depression, anxiety, and imposter syndrome, including the specific thought patterns they experience on typical days and more extreme days. In addition to being interesting for its own sake, this might make it easier for people to understand the experiences of family members, friends, and colleagues — and know how to react more helpfully. 

So we think this episode will be valuable for:  

• People who have experienced mental health problems or might in future;  
• People who have had troubles with stress, anxiety, low mood, low self esteem, and similar issues, even if their experience isn’t well described as ‘mental illness’;  
• People who have never experienced these problems but want to learn about what it's like, so they can better relate to and assist family, friends or colleagues who do. 

In other words, we think this episode could be worthwhile for almost everybody. 

Just a heads up that this conversation gets pretty intense at times, and includes references to self-harm and suicidal thoughts. 

If you don’t want to hear the most intense section, you can skip the chapter called ‘Disaster’ (44–57mins). And if you’d rather avoid almost all of these references, you could skip straight to the chapter called ‘80,000 Hours’ (1hr 11mins). 

If you're feeling suicidal or have thoughts of harming yourself right now, there are suicide hotlines at National Suicide Prevention Lifeline in the U.S. (800-273-8255) and Samaritans in the U.K. (116 123). 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>Today's episode is one of the most remarkable and really, unique, pieces of content we’ve ever produced (and I can say that because I had almost nothing to do with making it!). 

The producer of this show, Keiran Harris, interviewed our mutual colleague H</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/3ca7e464/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#99 – Leah Garcés on turning adversaries into allies to change the chicken industry</title>
      <itunes:title>#99 – Leah Garcés on turning adversaries into allies to change the chicken industry</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">760ac1a6-b434-11eb-a2b5-1239abf1ce19</guid>
      <link>https://80000hours.org/podcast/episodes/leah-garces-chicken-industry/</link>
      <description>
        <![CDATA[<p>For a chance to prevent enormous amounts of suffering, would you be brave enough to drive five hours to a remote location to meet a man who seems likely to be your enemy, knowing that it might be an ambush?</p><p>Today’s guest — Leah Garcés — was.</p><p>That man was a chicken farmer named Craig Watts, and that ambush never happened. Instead, Leah and Craig forged a friendship and a partnership focused on reducing suffering on factory farms.</p><p>Leah, now president of Mercy For Animals (MFA), tried for years to get access to a chicken farm to document the horrors she knew were happening behind closed doors. It made sense that no one would let her in — why would the evil chicken farmers behind these atrocities ever be willing to help her take them down?</p><p>But after sitting with Craig on his living room floor for hours and listening to his story, she discovered that he wasn’t evil at all — in fact he was just stuck in a cycle he couldn’t escape, forced to use methods he didn’t endorse.</p><p><a href="https://80k.link/L"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Most chicken farmers have enormous debts they are constantly struggling to pay off, make very little money, and have to work in terrible conditions — their main activity most days is finding and killing the sick chickens in their flock. Craig was one of very few farmers close to finally paying off his debts, which made him slightly less vulnerable to retaliation. That opened up the possibility for him to work with Leah.</p><p> Craig let Leah openly film inside the chicken houses, and shared highly confidential documents about the antibiotics put into the feed. That led to a viral video, and a <em>New York Times</em> story. The villain of that video was Jim Perdue, CEO of one of the biggest meat companies in the world. They show him saying, "Farmers are happy. Chickens are happy. There's a lot of space. They're clean." And then they show the grim reality.</p><p> For years, Perdue wouldn’t speak to Leah. But remarkably, when they actually met in person, she again managed to forge a meaningful relationship with a natural adversary. She was able to put aside her utter contempt for the chicken industry and see Craig and Jim as people, not cartoonish villains.</p><p> Leah believes that you need to be willing to sit down with anyone who has the power to solve a problem that you don’t — recognising them as human beings with a lifetime of complicated decisions behind their actions. And she stresses that finding or making a connection is really important. In the case of Jim Perdue, it was the fact they both had adopted children. Because of this, they were able to forget that they were supposed to be enemies in that moment, and build some trust.</p><p> The other lesson that Leah highlights is that you need to look for win-wins and start there, rather than starting with disagreements. With Craig Watts, instead of opening with “How do I end his job”, she thought, “How can I find him a better job?” If you find solutions where everybody wins, you don’t need to spend resources fighting the former enemy. They’ll come to you.</p><p> It turns out that conditions in chicken houses are perfect for growing hemp or mushrooms, so MFA have started their ‘Transfarmation project’ to help farmers like Craig escape from the prison of factory farming by converting their production from animals to plants. To convince farmers to leave behind a life of producing suffering, all you need to do is find them something better — which for many of them is almost anything else.</p><p> Leah and Rob also talk about:<br> • Why conditions for farmers are so bad<br> • The benefits of creating a public ranking, and scoring companies against each other<br> • The difficulty of enforcing corporate pledges<br> • And much more</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>The interview begins (00:01:06)</li><li>Grilled (00:06:25)</li><li>Why are conditions for farmers so bad? (00:18:31)</li><li>Lessons for others focused on social reform (00:25:04)</li><li>Driving up the price of factory farmed meat (00:31:18)</li><li>Mercy For Animals (00:50:08)</li><li>The importance of building on past work (00:56:27)</li><li>Farm sanctuaries (01:06:11)</li><li>Important weaknesses of MFA (01:09:44)</li><li>Farmed Animal Opportunity Index (01:12:54)</li><li>Latin America (01:20:49)</li><li>Enforcing corporate pledges (01:27:21)</li><li>The Transfarmation project (01:35:25)</li><li>Disagreements with others in the animal welfare movement (01:45:59)</li><li>How has the animal welfare movement evolved? (01:51:52)</li><li>Careers (02:03:32)</li><li>Ending factory farming (02:05:57)</li><li>Leah’s career (02:13:02)</li><li>Mental health challenges (02:20:40)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>For a chance to prevent enormous amounts of suffering, would you be brave enough to drive five hours to a remote location to meet a man who seems likely to be your enemy, knowing that it might be an ambush?</p><p>Today’s guest — Leah Garcés — was.</p><p>That man was a chicken farmer named Craig Watts, and that ambush never happened. Instead, Leah and Craig forged a friendship and a partnership focused on reducing suffering on factory farms.</p><p>Leah, now president of Mercy For Animals (MFA), tried for years to get access to a chicken farm to document the horrors she knew were happening behind closed doors. It made sense that no one would let her in — why would the evil chicken farmers behind these atrocities ever be willing to help her take them down?</p><p>But after sitting with Craig on his living room floor for hours and listening to his story, she discovered that he wasn’t evil at all — in fact he was just stuck in a cycle he couldn’t escape, forced to use methods he didn’t endorse.</p><p><a href="https://80k.link/L"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Most chicken farmers have enormous debts they are constantly struggling to pay off, make very little money, and have to work in terrible conditions — their main activity most days is finding and killing the sick chickens in their flock. Craig was one of very few farmers close to finally paying off his debts, which made him slightly less vulnerable to retaliation. That opened up the possibility for him to work with Leah.</p><p> Craig let Leah openly film inside the chicken houses, and shared highly confidential documents about the antibiotics put into the feed. That led to a viral video, and a <em>New York Times</em> story. The villain of that video was Jim Perdue, CEO of one of the biggest meat companies in the world. They show him saying, "Farmers are happy. Chickens are happy. There's a lot of space. They're clean." And then they show the grim reality.</p><p> For years, Perdue wouldn’t speak to Leah. But remarkably, when they actually met in person, she again managed to forge a meaningful relationship with a natural adversary. She was able to put aside her utter contempt for the chicken industry and see Craig and Jim as people, not cartoonish villains.</p><p> Leah believes that you need to be willing to sit down with anyone who has the power to solve a problem that you don’t — recognising them as human beings with a lifetime of complicated decisions behind their actions. And she stresses that finding or making a connection is really important. In the case of Jim Perdue, it was the fact they both had adopted children. Because of this, they were able to forget that they were supposed to be enemies in that moment, and build some trust.</p><p> The other lesson that Leah highlights is that you need to look for win-wins and start there, rather than starting with disagreements. With Craig Watts, instead of opening with “How do I end his job”, she thought, “How can I find him a better job?” If you find solutions where everybody wins, you don’t need to spend resources fighting the former enemy. They’ll come to you.</p><p> It turns out that conditions in chicken houses are perfect for growing hemp or mushrooms, so MFA have started their ‘Transfarmation project’ to help farmers like Craig escape from the prison of factory farming by converting their production from animals to plants. To convince farmers to leave behind a life of producing suffering, all you need to do is find them something better — which for many of them is almost anything else.</p><p> Leah and Rob also talk about:<br> • Why conditions for farmers are so bad<br> • The benefits of creating a public ranking, and scoring companies against each other<br> • The difficulty of enforcing corporate pledges<br> • And much more</p><p>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>The interview begins (00:01:06)</li><li>Grilled (00:06:25)</li><li>Why are conditions for farmers so bad? (00:18:31)</li><li>Lessons for others focused on social reform (00:25:04)</li><li>Driving up the price of factory farmed meat (00:31:18)</li><li>Mercy For Animals (00:50:08)</li><li>The importance of building on past work (00:56:27)</li><li>Farm sanctuaries (01:06:11)</li><li>Important weaknesses of MFA (01:09:44)</li><li>Farmed Animal Opportunity Index (01:12:54)</li><li>Latin America (01:20:49)</li><li>Enforcing corporate pledges (01:27:21)</li><li>The Transfarmation project (01:35:25)</li><li>Disagreements with others in the animal welfare movement (01:45:59)</li><li>How has the animal welfare movement evolved? (01:51:52)</li><li>Careers (02:03:32)</li><li>Ending factory farming (02:05:57)</li><li>Leah’s career (02:13:02)</li><li>Mental health challenges (02:20:40)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Sofia Davis-Fogel</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 13 May 2021 23:12:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/dc6d001a/55d39081.mp3" length="71880628" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/pAgHdcf5c51DBso3WbTmTedQ2WYlpCMW-82fz6pFPvw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDQv/MTY4MzU0NDY3NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8764</itunes:duration>
      <itunes:summary>For a chance to prevent enormous amounts of suffering, would you be brave enough to drive five hours to a remote location to meet a man who seems likely to be your enemy, knowing that it might be an ambush? 

Today’s guest — Leah Garcés — was. 

That man was a chicken farmer named Craig Watts, and that ambush never happened. Instead, Leah and Craig forged a friendship and a partnership focused on reducing suffering on factory farms. 

Leah, now president of Mercy For Animals (MFA), tried for years to get access to a chicken farm to document the horrors she knew were happening behind closed doors. It made sense that no one would let her in — why would the evil chicken farmers behind these atrocities ever be willing to help her take them down?  

But after sitting with Craig on his living room floor for hours and listening to his story, she discovered that he wasn’t evil at all — in fact he was just stuck in a cycle he couldn’t escape, forced to use methods he didn’t endorse.  

Links to learn more, summary and full transcript. 

Most chicken farmers have enormous debts they are constantly struggling to pay off, make very little money, and have to work in terrible conditions — their main activity most days is finding and killing the sick chickens in their flock. Craig was one of very few farmers close to finally paying off his debts, which made him slightly less vulnerable to retaliation. That opened up the possibility for him to work with Leah. 

Craig let Leah openly film inside the chicken houses, and shared highly confidential documents about the antibiotics put into the feed. That led to a viral video, and a New York Times story. The villain of that video was Jim Perdue, CEO of one of the biggest meat companies in the world. They show him saying, "Farmers are happy. Chickens are happy. There's a lot of space. They're clean." And then they show the grim reality. 

For years, Perdue wouldn’t speak to Leah. But remarkably, when they actually met in person, she again managed to forge a meaningful relationship with a natural adversary. She was able to put aside her utter contempt for the chicken industry and see Craig and Jim as people, not cartoonish villains.  

Leah believes that you need to be willing to sit down with anyone who has the power to solve a problem that you don’t — recognising them as human beings with a lifetime of complicated decisions behind their actions. And she stresses that finding or making a connection is really important. In the case of Jim Perdue, it was the fact they both had adopted children. Because of this, they were able to forget that they were supposed to be enemies in that moment, and build some trust.  

The other lesson that Leah highlights is that you need to look for win-wins and start there, rather than starting with disagreements. With Craig Watts, instead of opening with “How do I end his job”, she thought, “How can I find him a better job?” If you find solutions where everybody wins, you don’t need to spend resources fighting the former enemy. They’ll come to you. 

It turns out that conditions in chicken houses are perfect for growing hemp or mushrooms, so MFA have started their ‘Transfarmation project’ to help farmers like Craig escape from the prison of factory farming by converting their production from animals to plants. To convince farmers to leave behind a life of producing suffering, all you need to do is find them something better — which for many of them is almost anything else. 

Leah and Rob also talk about: 

• Why conditions for farmers are so bad 
• The benefits of creating a public ranking, and scoring companies against each other 
• The difficulty of enforcing corporate pledges 
• And much more 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>For a chance to prevent enormous amounts of suffering, would you be brave enough to drive five hours to a remote location to meet a man who seems likely to be your enemy, knowing that it might be an ambush? 

Today’s guest — Leah Garcés — was. 

That </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/dc6d001a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/dc6d001a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#98 – Christian Tarsney on future bias and a possible solution to moral fanaticism</title>
      <itunes:title>#98 – Christian Tarsney on future bias and a possible solution to moral fanaticism</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4efdbe1a-addb-11eb-b18b-0e63f81bf809</guid>
      <link>https://80000hours.org/podcast/episodes/christian-tarsney-future-bias-fanaticism/</link>
      <description>
        <![CDATA[<p>Imagine that you’re in the hospital for surgery. This kind of procedure is always safe, and always successful — but it can take anywhere from one to ten hours. You can’t be knocked out for the operation, but because it’s so painful — you’ll be given a drug that makes you forget the experience.</p><p> You wake up, not remembering going to sleep. You ask the nurse if you’ve had the operation yet. They look at the foot of your bed, and see two different charts for two patients. They say “Well, you’re one of these two — but I’m not sure which one. One of them had an operation yesterday that lasted ten hours. The other is set to have a one-hour operation later today.”</p><p> So it’s either true that you already suffered for ten hours, or true that you’re about to suffer for one hour.</p><p> Which patient would you rather be?</p><p> Most people would be relieved to find out they’d already had the operation. Normally we prefer less pain rather than more pain, but in this case, we prefer ten times more pain — just because the pain would be in the past rather than the future.</p><p> Christian Tarsney, a philosopher at Oxford University's Global Priorities Institute, has written a couple of papers about this ‘future bias’ — that is, that people seem to care more about their future experiences than about their past experiences.</p><p> <a href="https://80k.link/GHZ"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> That probably sounds perfectly normal to you. But do we actually have good reasons to prefer to have our positive experiences in the future, and our negative experiences in the past?</p><p> One of Christian’s experiments found that when you ask people to imagine hypothetical scenarios where they can affect their own past experiences, they care about those experiences more — which suggests that our inability to affect the past is one reason why we feel mostly indifferent to it.</p><p> But he points out that if that was the main reason, then we should also be indifferent to inevitable future experiences — if you know for sure that something bad is going to happen to you tomorrow, you shouldn't care about it. But if you found out you simply had to have a horribly painful operation tomorrow, it’s probably all you’d care about!</p><p> Another explanation for future bias is that we have this intuition that time is like a videotape, where the things that haven't played yet are still on the way.</p><p> If your future experiences really are ahead of you rather than behind you, that makes it rational to care more about the future than the past. But Christian says that, even though he shares this intuition, it’s actually very hard to make the case for time having a direction. It’s a live debate that’s playing out in the philosophy of time, as well as in physics.</p><p> For Christian, there are two big practical implications of these past, present, and future ethical comparison cases.</p><p> The first is for altruists: If we care about whether current people’s goals are realised, then maybe we should care about the realisation of people's past goals, including the goals of people who are now dead.</p><p> The second is more personal: If we can’t actually justify caring more about the future than the past, should we really worry about death any more than we worry about all the years we spent not existing before we were born?</p><p> Christian and Rob also cover several other big topics, including:</p><p> • A possible solution to moral fanaticism<br> • How much of humanity's resources we should spend on improving the long-term future<br> • How large the expected value of the continued existence of Earth-originating civilization might be<br> • How we should respond to uncertainty about the state of the world<br> • The state of global priorities research<br> • And much more</p><p>Chapters:<br> • Rob’s intro (00:00:00)<br>• The interview begins (00:01:20)<br>• Future bias (00:04:33)<br>• Philosophy of time (00:11:17)<br>• Money pumping (00:18:53)<br>• Time travel (00:21:22)<br>• Decision theory (00:24:36)<br>• Eternalism (00:32:32)<br>• Fanaticism (00:38:33)<br>• Stochastic dominance (00:52:11)<br>• Background uncertainty (00:56:27)<br>• Epistemic worries about longtermism (01:12:44)<br>• Best arguments against working on existential risk reduction (01:32:34)<br>• The scope of longtermism (01:41:12)<br>• The value of the future (01:50:09)<br>• Moral uncertainty (01:57:25)<br>• The Berry paradox (02:35:00)<br>• Competitive debating (02:28:34)<br>• The state of global priorities research (02:21:33)<br>• Christian’s personal priorities (02:17:27) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ryan Kessler.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Imagine that you’re in the hospital for surgery. This kind of procedure is always safe, and always successful — but it can take anywhere from one to ten hours. You can’t be knocked out for the operation, but because it’s so painful — you’ll be given a drug that makes you forget the experience.</p><p> You wake up, not remembering going to sleep. You ask the nurse if you’ve had the operation yet. They look at the foot of your bed, and see two different charts for two patients. They say “Well, you’re one of these two — but I’m not sure which one. One of them had an operation yesterday that lasted ten hours. The other is set to have a one-hour operation later today.”</p><p> So it’s either true that you already suffered for ten hours, or true that you’re about to suffer for one hour.</p><p> Which patient would you rather be?</p><p> Most people would be relieved to find out they’d already had the operation. Normally we prefer less pain rather than more pain, but in this case, we prefer ten times more pain — just because the pain would be in the past rather than the future.</p><p> Christian Tarsney, a philosopher at Oxford University's Global Priorities Institute, has written a couple of papers about this ‘future bias’ — that is, that people seem to care more about their future experiences than about their past experiences.</p><p> <a href="https://80k.link/GHZ"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> That probably sounds perfectly normal to you. But do we actually have good reasons to prefer to have our positive experiences in the future, and our negative experiences in the past?</p><p> One of Christian’s experiments found that when you ask people to imagine hypothetical scenarios where they can affect their own past experiences, they care about those experiences more — which suggests that our inability to affect the past is one reason why we feel mostly indifferent to it.</p><p> But he points out that if that was the main reason, then we should also be indifferent to inevitable future experiences — if you know for sure that something bad is going to happen to you tomorrow, you shouldn't care about it. But if you found out you simply had to have a horribly painful operation tomorrow, it’s probably all you’d care about!</p><p> Another explanation for future bias is that we have this intuition that time is like a videotape, where the things that haven't played yet are still on the way.</p><p> If your future experiences really are ahead of you rather than behind you, that makes it rational to care more about the future than the past. But Christian says that, even though he shares this intuition, it’s actually very hard to make the case for time having a direction. It’s a live debate that’s playing out in the philosophy of time, as well as in physics.</p><p> For Christian, there are two big practical implications of these past, present, and future ethical comparison cases.</p><p> The first is for altruists: If we care about whether current people’s goals are realised, then maybe we should care about the realisation of people's past goals, including the goals of people who are now dead.</p><p> The second is more personal: If we can’t actually justify caring more about the future than the past, should we really worry about death any more than we worry about all the years we spent not existing before we were born?</p><p> Christian and Rob also cover several other big topics, including:</p><p> • A possible solution to moral fanaticism<br> • How much of humanity's resources we should spend on improving the long-term future<br> • How large the expected value of the continued existence of Earth-originating civilization might be<br> • How we should respond to uncertainty about the state of the world<br> • The state of global priorities research<br> • And much more</p><p>Chapters:<br> • Rob’s intro (00:00:00)<br>• The interview begins (00:01:20)<br>• Future bias (00:04:33)<br>• Philosophy of time (00:11:17)<br>• Money pumping (00:18:53)<br>• Time travel (00:21:22)<br>• Decision theory (00:24:36)<br>• Eternalism (00:32:32)<br>• Fanaticism (00:38:33)<br>• Stochastic dominance (00:52:11)<br>• Background uncertainty (00:56:27)<br>• Epistemic worries about longtermism (01:12:44)<br>• Best arguments against working on existential risk reduction (01:32:34)<br>• The scope of longtermism (01:41:12)<br>• The value of the future (01:50:09)<br>• Moral uncertainty (01:57:25)<br>• The Berry paradox (02:35:00)<br>• Competitive debating (02:28:34)<br>• The state of global priorities research (02:21:33)<br>• Christian’s personal priorities (02:17:27) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ryan Kessler.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 05 May 2021 20:16:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/35210752/ac719515.mp3" length="76235165" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/7wHZjmuh1FVnzNrLUdAdR51zaiGF9nAX1IjEM_YV1JM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDMv/MTY4MzU0NDY3Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9502</itunes:duration>
      <itunes:summary>Imagine that you’re in the hospital for surgery. This kind of procedure is always safe, and always successful — but it can take anywhere from one to ten hours. You can’t be knocked out for the operation, but because it’s so painful — you’ll be given a drug that makes you forget the experience. 

You wake up, not remembering going to sleep. You ask the nurse if you’ve had the operation yet. They look at the foot of your bed, and see two different charts for two patients. They say “Well, you’re one of these two — but I’m not sure which one. One of them had an operation yesterday that lasted ten hours. The other is set to have a one-hour operation later today.”  

So it’s either true that you already suffered for ten hours, or true that you’re about to suffer for one hour. 

Which patient would you rather be? 

Most people would be relieved to find out they’d already had the operation. Normally we prefer less pain rather than more pain, but in this case, we prefer ten times more pain — just because the pain would be in the past rather than the future. 

Christian Tarsney, a philosopher at Oxford University's Global Priorities Institute, has written a couple of papers about this ‘future bias’ — that is, that people seem to care more about their future experiences than about their past experiences. 

Links to learn more, summary and full transcript. 

That probably sounds perfectly normal to you. But do we actually have good reasons to prefer to have our positive experiences in the future, and our negative experiences in the past? 

One of Christian’s experiments found that when you ask people to imagine hypothetical scenarios where they can affect their own past experiences, they care about those experiences more — which suggests that our inability to affect the past is one reason why we feel mostly indifferent to it. 

But he points out that if that was the main reason, then we should also be indifferent to inevitable future experiences — if you know for sure that something bad is going to happen to you tomorrow, you shouldn't care about it. But if you found out you simply had to have a horribly painful operation tomorrow, it’s probably all you’d care about! 

Another explanation for future bias is that we have this intuition that time is like a videotape, where the things that haven't played yet are still on the way. 

If your future experiences really are ahead of you rather than behind you, that makes it rational to care more about the future than the past. But Christian says that, even though he shares this intuition, it’s actually very hard to make the case for time having a direction. It’s a live debate that’s playing out in the philosophy of time, as well as in physics. 

For Christian, there are two big practical implications of these past, present, and future ethical comparison cases.  

The first is for altruists: If we care about whether current people’s goals are realised, then maybe we should care about the realisation of people's past goals, including the goals of people who are now dead. 

The second is more personal: If we can’t actually justify caring more about the future than the past, should we really worry about death any more than we worry about all the years we spent not existing before we were born? 

Christian and Rob also cover several other big topics, including: 

• A possible solution to moral fanaticism 
• How much of humanity's resources we should spend on improving the long-term future 
• How large the expected value of the continued existence of Earth-originating civilization might be 
• How we should respond to uncertainty about the state of the world 
• The state of global priorities research 
• And much more 

Producer: Keiran Harris.
Audio mastering: Ryan Kessler.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>Imagine that you’re in the hospital for surgery. This kind of procedure is always safe, and always successful — but it can take anywhere from one to ten hours. You can’t be knocked out for the operation, but because it’s so painful — you’ll be given a dru</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/35210752/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/35210752/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#97 – Mike Berkowitz on keeping the US a liberal democratic country</title>
      <itunes:title>#97 – Mike Berkowitz on keeping the US a liberal democratic country</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a6ba1d2a-a218-11eb-8b1c-127b05a2e32f</guid>
      <link>https://80000hours.org/podcast/episodes/mike-berkowitz-preserving-us-democracy/</link>
      <description>
        <![CDATA[<p>Donald Trump’s attempt to overturn the results of the 2020 election split the Republican party. There were those who went along with it — 147 members of Congress raised objections to the official certification of electoral votes — but there were others who refused. These included Brad Raffensperger and Brian Kemp in Georgia, and Vice President Mike Pence.</p><p>Although one could say that the latter Republicans showed great courage, the key to the split may lie less in differences of moral character or commitment to democracy, and more in what was being asked of them. Trump wanted the first group to break norms, but he wanted the second group to break the law.</p><p>And while norms were indeed shattered, laws were upheld.</p><p>Today’s guest, Mike Berkowitz, executive director of the Democracy Funders Network, points out a problem we came to realize throughout the Trump presidency: So many of the things that we thought were laws were actually just customs.</p><p><a href="https://80000hours.org/podcast/episodes/mike-berkowitz-preserving-us-democracy/?utm_campaign=podcast__mike-berkowitz&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> So once you have leaders who don’t buy into those customs — like, say, that a president shouldn’t tell the Department of Justice who it should and shouldn’t be prosecuting — there’s nothing preventing said customs from being violated.</p><p> And what happens if current laws change?</p><p> A recent Georgia bill took away some of the powers of Georgia's Secretary of State — Brad Raffensberger. Mike thinks that's clearly retribution for Raffensperger's refusal to overturn the 2020 election results. But he also thinks it means that the next time someone tries to overturn the results of the election, they could get much farther than Trump did in 2020.</p><p> In this interview Mike covers what he thinks are the three most important levers to push on to preserve liberal democracy in the United States:<br> 1. Reforming the political system, by e.g. introducing new voting methods<br> 2. Revitalizing local journalism<br> 3. Reducing partisan hatred within the United States</p><p> Mike says that American democracy, like democracy elsewhere in the world, is not an inevitability. The U.S. has institutions that are really important for the functioning of democracy, but they don't automatically protect themselves — they need people to stand up and protect them.</p><p> In addition to the changes listed above, Mike also thinks that we need to harden more norms into laws, such that individuals have fewer opportunities to undermine the system.</p><p> And inasmuch as laws provided the foundation for the likes of Raffensperger, Kemp, and Pence to exhibit political courage, if we can succeed in creating and maintaining the right laws — we may see many others following their lead.</p><p> As Founding Father James Madison put it: “If men were angels, no government would be necessary. If angels were to govern men, neither external nor internal controls on government would be necessary.”</p><p> Mike and Rob also talk about:<br> • What sorts of terrible scenarios we should actually be worried about, i.e. the difference between being overly alarmist and properly alarmist<br> • How to reduce perverse incentives for political actors, including those to overturn election results<br> • The best opportunities for donations in this space<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:01)</li><li>What we should actually be worried about (00:05:03)</li><li>January 6th, 2021 (00:11:03)</li><li>Trump’s defeat (00:16:44)</li><li>Improving incentives for representatives (00:30:55)</li><li>Signs of a loss of confidence in American democratic institutions (00:44:58)</li><li>Most valuable political reforms (00:54:39)</li><li>Revitalising local journalism (01:08:07)</li><li>Reducing partisan hatred (01:21:53)</li><li>Should workplaces be political? (01:31:40)</li><li>Mistakes of the left (01:36:50)</li><li>Risk of overestimating the problem (01:39:56)</li><li>Charitable giving (01:48:13)</li><li>How to shortlist projects (01:56:42)</li><li>Speaking to Republicans (02:04:15)</li><li>Patriots &amp; Pragmatists and The Democracy Funders Network (02:12:51)</li><li>Rob’s outro (02:32:58)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Donald Trump’s attempt to overturn the results of the 2020 election split the Republican party. There were those who went along with it — 147 members of Congress raised objections to the official certification of electoral votes — but there were others who refused. These included Brad Raffensperger and Brian Kemp in Georgia, and Vice President Mike Pence.</p><p>Although one could say that the latter Republicans showed great courage, the key to the split may lie less in differences of moral character or commitment to democracy, and more in what was being asked of them. Trump wanted the first group to break norms, but he wanted the second group to break the law.</p><p>And while norms were indeed shattered, laws were upheld.</p><p>Today’s guest, Mike Berkowitz, executive director of the Democracy Funders Network, points out a problem we came to realize throughout the Trump presidency: So many of the things that we thought were laws were actually just customs.</p><p><a href="https://80000hours.org/podcast/episodes/mike-berkowitz-preserving-us-democracy/?utm_campaign=podcast__mike-berkowitz&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> So once you have leaders who don’t buy into those customs — like, say, that a president shouldn’t tell the Department of Justice who it should and shouldn’t be prosecuting — there’s nothing preventing said customs from being violated.</p><p> And what happens if current laws change?</p><p> A recent Georgia bill took away some of the powers of Georgia's Secretary of State — Brad Raffensberger. Mike thinks that's clearly retribution for Raffensperger's refusal to overturn the 2020 election results. But he also thinks it means that the next time someone tries to overturn the results of the election, they could get much farther than Trump did in 2020.</p><p> In this interview Mike covers what he thinks are the three most important levers to push on to preserve liberal democracy in the United States:<br> 1. Reforming the political system, by e.g. introducing new voting methods<br> 2. Revitalizing local journalism<br> 3. Reducing partisan hatred within the United States</p><p> Mike says that American democracy, like democracy elsewhere in the world, is not an inevitability. The U.S. has institutions that are really important for the functioning of democracy, but they don't automatically protect themselves — they need people to stand up and protect them.</p><p> In addition to the changes listed above, Mike also thinks that we need to harden more norms into laws, such that individuals have fewer opportunities to undermine the system.</p><p> And inasmuch as laws provided the foundation for the likes of Raffensperger, Kemp, and Pence to exhibit political courage, if we can succeed in creating and maintaining the right laws — we may see many others following their lead.</p><p> As Founding Father James Madison put it: “If men were angels, no government would be necessary. If angels were to govern men, neither external nor internal controls on government would be necessary.”</p><p> Mike and Rob also talk about:<br> • What sorts of terrible scenarios we should actually be worried about, i.e. the difference between being overly alarmist and properly alarmist<br> • How to reduce perverse incentives for political actors, including those to overturn election results<br> • The best opportunities for donations in this space<br> • And much more</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:01)</li><li>What we should actually be worried about (00:05:03)</li><li>January 6th, 2021 (00:11:03)</li><li>Trump’s defeat (00:16:44)</li><li>Improving incentives for representatives (00:30:55)</li><li>Signs of a loss of confidence in American democratic institutions (00:44:58)</li><li>Most valuable political reforms (00:54:39)</li><li>Revitalising local journalism (01:08:07)</li><li>Reducing partisan hatred (01:21:53)</li><li>Should workplaces be political? (01:31:40)</li><li>Mistakes of the left (01:36:50)</li><li>Risk of overestimating the problem (01:39:56)</li><li>Charitable giving (01:48:13)</li><li>How to shortlist projects (01:56:42)</li><li>Speaking to Republicans (02:04:15)</li><li>Patriots &amp; Pragmatists and The Democracy Funders Network (02:12:51)</li><li>Rob’s outro (02:32:58)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 20 Apr 2021 21:03:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f5dfb83d/186b2b14.mp3" length="75271480" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Jsuv3U3epUao36ewMsQrCqUxYTFfzsgoaM_cn5KkRTw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDIv/MTY4MzU0NDY3MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9370</itunes:duration>
      <itunes:summary>Donald Trump’s attempt to overturn the results of the 2020 election split the Republican party. There were those who went along with it — 147 members of Congress raised objections to the official certification of electoral votes — but there were others who refused. These included Brad Raffensperger and Brian Kemp in Georgia, and Vice President Mike Pence. 

Although one could say that the latter Republicans showed great courage, the key to the split may lie less in differences of moral character or commitment to democracy, and more in what was being asked of them. Trump wanted the first group to break norms, but he wanted the second group to break the law. 

And while norms were indeed shattered, laws were upheld. 

Today’s guest Mike Berkowitz, executive director of the Democracy Funders Network, points out a problem we came to realize throughout the Trump presidency: So many of the things that we thought were laws were actually just customs.  

Links to learn more, summary and full transcript.

So once you have leaders who don’t buy into those customs — like, say, that a president shouldn’t tell the Department of Justice who it should and shouldn’t be prosecuting — there’s nothing preventing said customs from being violated. 

And what happens if current laws change? 

A recent Georgia bill took away some of the powers of Georgia's Secretary of State — Brad Raffensberger. Mike thinks that's clearly retribution for Raffensperger's refusal to overturn the 2020 election results. But he also thinks it means that the next time someone tries to overturn the results of the election, they could get much farther than Trump did in 2020. 

In this interview Mike covers what he thinks are the three most important levers to push on to preserve liberal democracy in the United States: 

1. Reforming the political system, by e.g. introducing new voting methods 
2. Revitalizing local journalism 
3. Reducing partisan hatred within the United States 

Mike says that American democracy, like democracy elsewhere in the world, is not an inevitability. The U.S. has institutions that are really important for the functioning of democracy, but they don't automatically protect themselves — they need people to stand up and protect them.  

In addition to the changes listed above, Mike also thinks that we need to harden more norms into laws, such that individuals have fewer opportunities to undermine the system.  

And inasmuch as laws provided the foundation for the likes of Raffensperger, Kemp, and Pence to exhibit political courage, if we can succeed in creating and maintaining the right laws — we may see many others following their lead.      

As Founding Father James Madison put it: “If men were angels, no government would be necessary. If angels were to govern men, neither external nor internal controls on government would be necessary.” 

Mike and Rob also talk about: 

• What sorts of terrible scenarios we should actually be worried about, i.e. the difference between being overly alarmist and properly alarmist 
• How to reduce perverse incentives for political actors, including those to overturn election results  
• The best opportunities for donations in this space 
• And much more 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>Donald Trump’s attempt to overturn the results of the 2020 election split the Republican party. There were those who went along with it — 147 members of Congress raised objections to the official certification of electoral votes — but there were others wh</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f5dfb83d/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f5dfb83d/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>The ten episodes of this show you should listen to first</title>
      <itunes:title>The ten episodes of this show you should listen to first</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ab3773d8-9dff-11eb-b608-123b0689cb43</guid>
      <link>https://share.transistor.fm/s/0bd520ff</link>
      <description>
        <![CDATA[Today we're launching a new podcast feed that might be useful to you and people you know.<p> 

It's called '<a href="https://80000hours.org/podcast/effective-altruism-an-introduction/?utm_campaign=podcast__eaai-podcastwords&amp;utm_source=eaai&amp;utm_medium=podcast"><i>Effective Altruism: An Introduction</i></a>', and it's a carefully chosen selection of ten episodes of this show, with various new intros and outros to guide folks through them.</p><p> 

Basically, as the number of episodes of this show has grown, it has become less and less practical to ask new subscribers to go back and listen through most of our archives.</p><p> 

So naturally new subscribers want to know... what should I listen to first? What episodes will help me make sense of effective altruist thinking and get the most out of new episodes?</p><p> 

We hope that '<a href="https://80000hours.org/podcast/effective-altruism-an-introduction/?utm_campaign=podcast__eaai-podcastwords&amp;utm_source=eaai&amp;utm_medium=podcast"><i>Effective Altruism: An Introduction</i></a>' will fill in that gap.</p><p>  

Across the ten episodes, we cover what effective altruism at its core really is, what folks who are tackling a number of well-known problem areas are up to and why, some more unusual and speculative problems, and how we and the rest of the team here try to think through difficult questions as clearly as possible.</p><p> 

Like 80,000 Hours itself, the selection leans towards a focus on longtermism, though other perspectives are covered as well.</p><p> 

Another gap it might fill is in helping you recommend the show to people, or suggest a way to learn more about effective altruist style thinking to people who are curious about it.</p><p> 

If someone in your life wants to get an understanding of what 80,000 Hours or effective altruism are all about, and prefers to listen to things rather than read, this is a great resource to direct them to.</p><p> 

You can find it by searching for effective altruism in your podcasting app, or by going to <a href="https://80000hours.org/podcast/effective-altruism-an-introduction/?utm_campaign=podcast__eaai-podcastwords&amp;utm_source=eaai&amp;utm_medium=podcast"><i>80000hours.org/intro</i></a>.</p><p> 

We'd love to hear how you go listening to it yourself, or sharing it with others in your life.
Get in touch by emailing podcast@80000hours.org.</p><p></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today we're launching a new podcast feed that might be useful to you and people you know.<p> 

It's called '<a href="https://80000hours.org/podcast/effective-altruism-an-introduction/?utm_campaign=podcast__eaai-podcastwords&amp;utm_source=eaai&amp;utm_medium=podcast"><i>Effective Altruism: An Introduction</i></a>', and it's a carefully chosen selection of ten episodes of this show, with various new intros and outros to guide folks through them.</p><p> 

Basically, as the number of episodes of this show has grown, it has become less and less practical to ask new subscribers to go back and listen through most of our archives.</p><p> 

So naturally new subscribers want to know... what should I listen to first? What episodes will help me make sense of effective altruist thinking and get the most out of new episodes?</p><p> 

We hope that '<a href="https://80000hours.org/podcast/effective-altruism-an-introduction/?utm_campaign=podcast__eaai-podcastwords&amp;utm_source=eaai&amp;utm_medium=podcast"><i>Effective Altruism: An Introduction</i></a>' will fill in that gap.</p><p>  

Across the ten episodes, we cover what effective altruism at its core really is, what folks who are tackling a number of well-known problem areas are up to and why, some more unusual and speculative problems, and how we and the rest of the team here try to think through difficult questions as clearly as possible.</p><p> 

Like 80,000 Hours itself, the selection leans towards a focus on longtermism, though other perspectives are covered as well.</p><p> 

Another gap it might fill is in helping you recommend the show to people, or suggest a way to learn more about effective altruist style thinking to people who are curious about it.</p><p> 

If someone in your life wants to get an understanding of what 80,000 Hours or effective altruism are all about, and prefers to listen to things rather than read, this is a great resource to direct them to.</p><p> 

You can find it by searching for effective altruism in your podcasting app, or by going to <a href="https://80000hours.org/podcast/effective-altruism-an-introduction/?utm_campaign=podcast__eaai-podcastwords&amp;utm_source=eaai&amp;utm_medium=podcast"><i>80000hours.org/intro</i></a>.</p><p> 

We'd love to hear how you go listening to it yourself, or sharing it with others in your life.
Get in touch by emailing podcast@80000hours.org.</p><p></p>]]>
      </content:encoded>
      <pubDate>Thu, 15 Apr 2021 16:50:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0bd520ff/8b5ccbca.mp3" length="1545591" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-1ChMRfPDEU15KXY19j9wqKQQnr6CW-hqL8jBaHaegY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDEv/MTY4MzU0NDY3MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>183</itunes:duration>
      <itunes:summary>Today we're launching a new podcast feed that might be useful to you and people you know. 

It's called 'Effective Altruism: An Introduction', and it's a carefully chosen selection of ten episodes of this show, with various new intros and outros to guide folks through them. 

Basically, as the number of episodes of this show has grown, it has become less and less practical to ask new subscribers to go back and listen through most of our archives. 

So naturally new subscribers want to know... what should I listen to first? What episodes will help me make sense of effective altruist thinking and get the most out of new episodes? 

We hope that 'Effective Altruism: An Introduction' will fill in that gap.  

Across the ten episodes, we cover what effective altruism at its core really is, what folks who are tackling a number of well-known problem areas are up to and why, some more unusual and speculative problems, and how we and the rest of the team here try to think through difficult questions as clearly as possible. 

Like 80,000 Hours itself, the selection leans towards a focus on longtermism, though other perspectives are covered as well. 

Another gap it might fill is in helping you recommend the show to people, or suggest a way to learn more about effective altruist style thinking to people who are curious about it. 

If someone in your life wants to get an understanding of what 80,000 Hours or effective altruism are all about, and prefers to listen to things rather than read, this is a great resource to direct them to. 

You can find it by searching for effective altruism in your podcasting app, or by going to 80000hours.org/intro. 

We'd love to hear how you go listening to it yourself, or sharing it with others in your life.
Get in touch by emailing podcast@80000hours.org.</itunes:summary>
      <itunes:subtitle>Today we're launching a new podcast feed that might be useful to you and people you know. 

It's called 'Effective Altruism: An Introduction', and it's a carefully chosen selection of ten episodes of this show, with various new intros and outros to guide </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#96 – Nina Schick on disinformation and the rise of synthetic media</title>
      <itunes:title>#96 – Nina Schick on disinformation and the rise of synthetic media</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c407db7a-9713-11eb-a3bd-120bbe6f1c87</guid>
      <link>https://80000hours.org/podcast/episodes/nina-schick-disinformation-synthetic-media</link>
      <description>
        <![CDATA[<p>You might have heard fears like this in the last few years: What if Donald Trump was woken up in the middle of the night and shown a fake video — indistinguishable from a real one — in which Kim Jong Un announced an imminent nuclear strike on the U.S.?</p><p>Today’s guest Nina Schick, author of <a href="https://ninaschick.org/deepfakes/"><strong><em>Deepfakes: The Coming Infocalypse</em></strong></a>, thinks these concerns were the result of hysterical reporting, and that the barriers to entry in terms of making a very sophisticated ‘deepfake’ video today are a lot higher than people think.</p><p> But she also says that by the end of the decade, YouTubers will be able to produce the kind of content that's currently only accessible to Hollywood studios. So is it just a matter of time until we’ll be right to be terrified of this stuff?</p><p> <a href="https://80000hours.org/podcast/episodes/nina-schick-disinformation-synthetic-media/?utm_campaign=podcast__nina-schick&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Nina thinks the problem of misinformation and disinformation might be roughly as important as climate change, because as she says: “Everything exists within this information ecosystem, it encompasses everything.” We haven’t done enough research to properly weigh in on that ourselves, but Rob did present Nina with some early objections, such as:</p><p> • Won’t people quickly learn that audio and video can be faked, and so will only take them seriously if they come from a trusted source?<br> • If photoshop didn’t lead to total chaos, why should this be any different?</p><p> But the grim reality is that if you wrote “I believe that the world will end on April 6, 2022” and pasted it next to a photo of Albert Einstein — a lot of people <em>would</em> believe it was a genuine quote. And Nina thinks that flawless synthetic videos will represent a significant jump in our ability to deceive.</p><p> She also points out that the direct impact of fake videos is just one side of the issue. In a world where all media can be faked, everything can be denied.</p><p> Consider Trump’s infamous <em>Access Hollywood</em> tape. If that happened in 2020 instead of 2016, he would have almost certainly claimed it was fake — and that claim wouldn’t be obviously ridiculous. Malignant politicians everywhere could plausibly deny footage of them receiving a bribe, or ordering a massacre. What happens if in every criminal trial, a suspect caught on camera can just look at the jury and say “that video is fake”?</p><p> Nina says that undeniably, this technology is going to give bad actors a lot of scope for not having accountability for their actions.</p><p> As we try to inoculate people against being tricked by synthetic media, we risk corroding their trust in all authentic media too. And Nina asks: If you can't agree on any set of objective facts or norms on which to start your debate, how on earth do you even run a society?</p><p> Nina and Rob also talk about a bunch of other topics, including:</p><p> • The history of disinformation, and groups who sow disinformation professionally<br> • How deepfake pornography is used to attack and silence women activitists<br> • The key differences between how this technology interacts with liberal democracies vs. authoritarian regimes<br> • Whether we should make it illegal to make a deepfake of someone without their permission<br> • And the coolest positive uses of this technology</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:28)</li><li>Deepfakes (00:05:49)</li><li>The influence of synthetic media today (00:17:20)</li><li>The history of misinformation and disinformation (00:28:13)</li><li>Text vs. video (00:34:05)</li><li>Privacy (00:40:17)</li><li>Deepfake pornography (00:49:05)</li><li>Russia and other bad actors (00:58:38)</li><li>2016 vs. 2020 US elections (01:13:44)</li><li>Authoritarian regimes vs. liberal democracies (01:24:08)</li><li>Law reforms (01:31:52)</li><li>Positive uses (01:37:04)</li><li>Technical solutions (01:40:56)</li><li>Careers (01:52:30)</li><li>Rob’s outro (01:58:27)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>You might have heard fears like this in the last few years: What if Donald Trump was woken up in the middle of the night and shown a fake video — indistinguishable from a real one — in which Kim Jong Un announced an imminent nuclear strike on the U.S.?</p><p>Today’s guest Nina Schick, author of <a href="https://ninaschick.org/deepfakes/"><strong><em>Deepfakes: The Coming Infocalypse</em></strong></a>, thinks these concerns were the result of hysterical reporting, and that the barriers to entry in terms of making a very sophisticated ‘deepfake’ video today are a lot higher than people think.</p><p> But she also says that by the end of the decade, YouTubers will be able to produce the kind of content that's currently only accessible to Hollywood studios. So is it just a matter of time until we’ll be right to be terrified of this stuff?</p><p> <a href="https://80000hours.org/podcast/episodes/nina-schick-disinformation-synthetic-media/?utm_campaign=podcast__nina-schick&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Nina thinks the problem of misinformation and disinformation might be roughly as important as climate change, because as she says: “Everything exists within this information ecosystem, it encompasses everything.” We haven’t done enough research to properly weigh in on that ourselves, but Rob did present Nina with some early objections, such as:</p><p> • Won’t people quickly learn that audio and video can be faked, and so will only take them seriously if they come from a trusted source?<br> • If photoshop didn’t lead to total chaos, why should this be any different?</p><p> But the grim reality is that if you wrote “I believe that the world will end on April 6, 2022” and pasted it next to a photo of Albert Einstein — a lot of people <em>would</em> believe it was a genuine quote. And Nina thinks that flawless synthetic videos will represent a significant jump in our ability to deceive.</p><p> She also points out that the direct impact of fake videos is just one side of the issue. In a world where all media can be faked, everything can be denied.</p><p> Consider Trump’s infamous <em>Access Hollywood</em> tape. If that happened in 2020 instead of 2016, he would have almost certainly claimed it was fake — and that claim wouldn’t be obviously ridiculous. Malignant politicians everywhere could plausibly deny footage of them receiving a bribe, or ordering a massacre. What happens if in every criminal trial, a suspect caught on camera can just look at the jury and say “that video is fake”?</p><p> Nina says that undeniably, this technology is going to give bad actors a lot of scope for not having accountability for their actions.</p><p> As we try to inoculate people against being tricked by synthetic media, we risk corroding their trust in all authentic media too. And Nina asks: If you can't agree on any set of objective facts or norms on which to start your debate, how on earth do you even run a society?</p><p> Nina and Rob also talk about a bunch of other topics, including:</p><p> • The history of disinformation, and groups who sow disinformation professionally<br> • How deepfake pornography is used to attack and silence women activitists<br> • The key differences between how this technology interacts with liberal democracies vs. authoritarian regimes<br> • Whether we should make it illegal to make a deepfake of someone without their permission<br> • And the coolest positive uses of this technology</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:28)</li><li>Deepfakes (00:05:49)</li><li>The influence of synthetic media today (00:17:20)</li><li>The history of misinformation and disinformation (00:28:13)</li><li>Text vs. video (00:34:05)</li><li>Privacy (00:40:17)</li><li>Deepfake pornography (00:49:05)</li><li>Russia and other bad actors (00:58:38)</li><li>2016 vs. 2020 US elections (01:13:44)</li><li>Authoritarian regimes vs. liberal democracies (01:24:08)</li><li>Law reforms (01:31:52)</li><li>Positive uses (01:37:04)</li><li>Technical solutions (01:40:56)</li><li>Careers (01:52:30)</li><li>Rob’s outro (01:58:27)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 06 Apr 2021 21:02:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/fb838703/909738b3.mp3" length="57929454" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Vn8Th4ZWv2p3zu9lU-wllbGV4egsyCjY7ORhhE-Oq9I/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4NDAv/MTY4MzU0NDY2OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7204</itunes:duration>
      <itunes:summary>You might have heard fears like this in the last few years: What if Donald Trump was woken up in the middle of the night and shown a fake video —  indistinguishable from a real one — in which Kim Jong Un announced an imminent nuclear strike on the U.S.? 

Today’s guest Nina Schick, author of Deepfakes: The Coming Infocalypse, thinks these concerns were the result of hysterical reporting, and that the barriers to entry in terms of making a very sophisticated ‘deepfake’ video today are a lot higher than people think. 

But she also says that by the end of the decade, YouTubers will be able to produce the kind of content that's currently only accessible to Hollywood studios. So is it just a matter of time until we’ll be right to be terrified of this stuff? 

Links to learn more, summary and full transcript.

Nina thinks the problem of misinformation and disinformation might be roughly as important as climate change, because as she says: “Everything exists within this information ecosystem, it encompasses everything.” We haven’t done enough research to properly weigh in on that ourselves, but Rob did present Nina with some early objections, such as:  

• Won’t people quickly learn that audio and video can be faked, and so will only take them seriously if they come from a trusted source?  
• If photoshop didn’t lead to total chaos, why should this be any different?  

But the grim reality is that if you wrote “I believe that the world will end on April 6, 2022” and pasted it next to a photo of Albert Einstein — a lot of people would believe it was a genuine quote. And Nina thinks that flawless synthetic videos will represent a significant jump in our ability to deceive. 

She also points out that the direct impact of fake videos is just one side of the issue. In a world where all media can be faked, everything can be denied. 

Consider Trump’s infamous Access Hollywood tape. If that happened in 2020 instead of 2016, he would have almost certainly claimed it was fake — and that claim wouldn’t be obviously ridiculous. Malignant politicians everywhere could plausibly deny footage of them receiving a bribe, or ordering a massacre. What happens if in every criminal trial, a suspect caught on camera can just look at the jury and say “that video is fake”? 

Nina says that undeniably, this technology is going to give bad actors a lot of scope for not having accountability for their actions.  

As we try to inoculate people against being tricked by synthetic media, we risk corroding their trust in all authentic media too. And Nina asks: If you can't agree on any set of objective facts or norms on which to start your debate, how on earth do you even run a society? 

Nina and Rob also talk about a bunch of other topics, including: 

• The history of disinformation, and groups who sow disinformation professionally 
• How deepfake pornography is used to attack and silence women activitists 
• The key differences between how this technology interacts with liberal democracies vs. authoritarian regimes 
• Whether we should make it illegal to make a deepfake of someone without their permission 
• And the coolest positive uses of this technology  

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>You might have heard fears like this in the last few years: What if Donald Trump was woken up in the middle of the night and shown a fake video —  indistinguishable from a real one — in which Kim Jong Un announced an imminent nuclear strike on the U.S.? </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/fb838703/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/fb838703/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#95 – Kelly Wanser on whether to deliberately intervene in the climate</title>
      <itunes:title>#95 – Kelly Wanser on whether to deliberately intervene in the climate</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2bd9afd6-8e67-11eb-aeb5-123dfcf2b33d</guid>
      <link>https://80000hours.org/podcast/episodes/kelly-wanser-climate-interventions/</link>
      <description>
        <![CDATA[<p>How long do you think it’ll be before we’re able to bend the weather to our will? A massive rainmaking program in China, efforts to seed new oases in the Arabian peninsula, or chemically induce snow for skiers in Colorado.</p><p> 100 years? 50 years? 20?</p><p> Those who know how to write a teaser hook for a podcast episode will have correctly guessed that all these things are already happening today. And the techniques being used could be turned to managing climate change as well.</p><p> Today’s guest, Kelly Wanser, founded SilverLining — a nonprofit organization that advocates research into climate interventions, such as seeding or brightening clouds, to ensure that we maintain a safe climate.</p><p> <a href="https://80k.link/KW"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Kelly says that current climate projections, even if we do everything right from here on out, imply that two degrees of global warming are now unavoidable. And the same scientists who made those projections fear the flow-through effect that warming could have.</p><p> Since our best case scenario may already be too dangerous, SilverLining focuses on ways that we could intervene quickly in the climate if things get especially grim — their research serving as a kind of insurance policy.</p><p> After considering everything from mirrors in space, to shiny objects on the ocean, to materials on the Arctic, their scientists concluded that the most promising approach was leveraging one of the ways that the Earth already regulates its temperature — the reflection of sunlight off particles and clouds in the atmosphere.</p><p> Cloud brightening is a climate control approach that uses the spraying of a fine mist of sea water into clouds to make them 'whiter' so they reflect even more sunlight back into space.</p><p> These ‘streaks’ in clouds are already created by ships because the particulates from their diesel engines inadvertently make clouds a bit brighter.</p><p> Kelly says that scientists estimate that we're already lowering the global temperature this way by 0.5–1.1ºC, without even intending to.</p><p> While fossil fuel particulates are terrible for human health, they think we could replicate this effect by simply spraying sea water up into clouds. But so far there hasn't been funding to measure how much temperature change you get for a given amount of spray.</p><p> And we won't want to dive into these methods head first because the atmosphere is a complex system we can't yet properly model, and there are many things to check first. For instance, chemicals that reflect light from the upper atmosphere might totally change wind patterns in the stratosphere. Or they might not — for all the discussion of global warming the climate is surprisingly understudied.</p><p> The public tends to be skeptical of climate interventions, otherwise known as geoengineering, so in this episode we cover a range of possible objections, such as:</p><p> • It being riskier than doing nothing<br> • That it will inevitably be dangerously political<br> • And the risk of the 'double catastrophe', where a pandemic stops our climate interventions and temperatures sky-rocket at the worst time.</p><p> Kelly and Rob also talk about:</p><p> • The many climate interventions that are already happening<br> • The most promising ideas in the field<br> • And whether people would be more accepting if we found ways to intervene that had nothing to do with making the world a better place.</p><p>Chapters:<br>• Rob’s intro (00:00:00)<br>• The interview begins (00:01:37)<br>• Existing climate interventions (00:06:44)<br>• Most promising ideas (00:16:23)<br>• Doing good by accident (00:28:39)<br>• Objections to this approach (00:31:16)<br>• How much could countries do individually? (00:47:19)<br>• Government funding (00:50:08)<br>• Is global coordination possible? (00:53:01)<br>• Malicious use (00:57:07)<br>• Careers and SilverLining (01:04:03)<br>• Rob’s outro (01:23:34) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>How long do you think it’ll be before we’re able to bend the weather to our will? A massive rainmaking program in China, efforts to seed new oases in the Arabian peninsula, or chemically induce snow for skiers in Colorado.</p><p> 100 years? 50 years? 20?</p><p> Those who know how to write a teaser hook for a podcast episode will have correctly guessed that all these things are already happening today. And the techniques being used could be turned to managing climate change as well.</p><p> Today’s guest, Kelly Wanser, founded SilverLining — a nonprofit organization that advocates research into climate interventions, such as seeding or brightening clouds, to ensure that we maintain a safe climate.</p><p> <a href="https://80k.link/KW"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Kelly says that current climate projections, even if we do everything right from here on out, imply that two degrees of global warming are now unavoidable. And the same scientists who made those projections fear the flow-through effect that warming could have.</p><p> Since our best case scenario may already be too dangerous, SilverLining focuses on ways that we could intervene quickly in the climate if things get especially grim — their research serving as a kind of insurance policy.</p><p> After considering everything from mirrors in space, to shiny objects on the ocean, to materials on the Arctic, their scientists concluded that the most promising approach was leveraging one of the ways that the Earth already regulates its temperature — the reflection of sunlight off particles and clouds in the atmosphere.</p><p> Cloud brightening is a climate control approach that uses the spraying of a fine mist of sea water into clouds to make them 'whiter' so they reflect even more sunlight back into space.</p><p> These ‘streaks’ in clouds are already created by ships because the particulates from their diesel engines inadvertently make clouds a bit brighter.</p><p> Kelly says that scientists estimate that we're already lowering the global temperature this way by 0.5–1.1ºC, without even intending to.</p><p> While fossil fuel particulates are terrible for human health, they think we could replicate this effect by simply spraying sea water up into clouds. But so far there hasn't been funding to measure how much temperature change you get for a given amount of spray.</p><p> And we won't want to dive into these methods head first because the atmosphere is a complex system we can't yet properly model, and there are many things to check first. For instance, chemicals that reflect light from the upper atmosphere might totally change wind patterns in the stratosphere. Or they might not — for all the discussion of global warming the climate is surprisingly understudied.</p><p> The public tends to be skeptical of climate interventions, otherwise known as geoengineering, so in this episode we cover a range of possible objections, such as:</p><p> • It being riskier than doing nothing<br> • That it will inevitably be dangerously political<br> • And the risk of the 'double catastrophe', where a pandemic stops our climate interventions and temperatures sky-rocket at the worst time.</p><p> Kelly and Rob also talk about:</p><p> • The many climate interventions that are already happening<br> • The most promising ideas in the field<br> • And whether people would be more accepting if we found ways to intervene that had nothing to do with making the world a better place.</p><p>Chapters:<br>• Rob’s intro (00:00:00)<br>• The interview begins (00:01:37)<br>• Existing climate interventions (00:06:44)<br>• Most promising ideas (00:16:23)<br>• Doing good by accident (00:28:39)<br>• Objections to this approach (00:31:16)<br>• How much could countries do individually? (00:47:19)<br>• Government funding (00:50:08)<br>• Is global coordination possible? (00:53:01)<br>• Malicious use (00:57:07)<br>• Careers and SilverLining (01:04:03)<br>• Rob’s outro (01:23:34) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 26 Mar 2021 19:21:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/1cb0c03f/6bcf3b68.mp3" length="42993734" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/EqEYeaT5sU4Li2If2tZHVQaB8nNtBYI5NUVztgs2WjU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Mzkv/MTY4MzU0NDY2OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5048</itunes:duration>
      <itunes:summary>How long do you think it’ll be before we’re able to bend the weather to our will? A massive rainmaking program in China, efforts to seed new oases in the Arabian peninsula, or chemically induce snow for skiers in Colorado. 

100 years? 50 years? 20? 

Those who know how to write a teaser hook for a podcast episode will have correctly guessed that all these things are already happening today. And the techniques being used could be turned to managing climate change as well. 

Today’s guest, Kelly Wanser, founded SilverLining — a nonprofit organization that advocates research into climate interventions, such as seeding or brightening clouds, to ensure that we maintain a safe climate. 

Links to learn more, summary and full transcript.

Kelly says that current climate projections, even if we do everything right from here on out, imply that two degrees of global warming are now unavoidable. And the same scientists who made those projections fear the flow-through effect that warming could have.  

Since our best case scenario may already be too dangerous, SilverLining focuses on ways that we could intervene quickly in the climate if things get especially grim — their research serving as a kind of insurance policy. 

After considering everything from mirrors in space, to shiny objects on the ocean, to materials on the Arctic, their scientists concluded that the most promising approach was leveraging one of the ways that the Earth already regulates its temperature — the reflection of sunlight off particles and clouds in the atmosphere. 

Cloud brightening is a climate control approach that uses the spraying of a fine mist of sea water into clouds to make them 'whiter' so they reflect even more sunlight back into space. 

These ‘streaks’ in clouds are already created by ships because the particulates from their diesel engines inadvertently make clouds a bit brighter. 

Kelly says that scientists estimate that we're already lowering the global temperature this way by 0.5–1.1ºC, without even intending to. 

While fossil fuel particulates are terrible for human health, they think we could replicate this effect by simply spraying sea water up into clouds. But so far there hasn't been funding to measure how much temperature change you get for a given amount of spray. 

And we won't want to dive into these methods head first because the atmosphere is a complex system we can't yet properly model, and there are many things to check first. For instance, chemicals that reflect light from the upper atmosphere might totally change wind patterns in the stratosphere. Or they might not — for all the discussion of global warming the climate is surprisingly understudied. 

The public tends to be skeptical of climate interventions, otherwise known as geoengineering, so in this episode we cover a range of possible objections, such as:  

• It being riskier than doing nothing 
• That it will inevitably be dangerously political 
• And the risk of the 'double catastrophe', where a pandemic stops our climate interventions and temperatures sky-rocket at the worst time. 

Kelly and Rob also talk about: 

• The many climate interventions that are already happening 
• The most promising ideas in the field 
• And whether people would be more accepting if we found ways to intervene that had nothing to do with making the world a better place. 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>How long do you think it’ll be before we’re able to bend the weather to our will? A massive rainmaking program in China, efforts to seed new oases in the Arabian peninsula, or chemically induce snow for skiers in Colorado. 

100 years? 50 years? 20? 
</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/1cb0c03f/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/1cb0c03f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#94 – Ezra Klein on aligning journalism, politics, and what matters most</title>
      <itunes:title>#94 – Ezra Klein on aligning journalism, politics, and what matters most</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">028e6d10-88f4-11eb-87d3-12176196d185</guid>
      <link>https://share.transistor.fm/s/d750fb50</link>
      <description>
        <![CDATA[How many words in U.S. newspapers have been spilled on tax policy in the past five years? And how many words on CRISPR? Or meat alternatives? Or how AI may soon automate the majority of jobs?<p> 

When people look back on this era, is the interesting thing going to have been fights over whether or not the top marginal tax rate was 39.5% or 35.4%, or is it going to be that human beings started to take control of human evolution; that we stood on the brink of eliminating immeasurable levels of suffering on factory farms; and that for the first time the average American might become financially comfortable and unemployed simultaneously?</p><p> 

Today’s guest is Ezra Klein, one of the most prominent journalists in the world. Ezra thinks that pressing issues are neglected largely because there's little pre-existing infrastructure to push them.</p><p> 

<a href="https://80k.link/EK"><b>Links to learn more, summary and full transcript.</b></a></p><p>

He points out that for a long time taxes have been considered hugely important in D.C. political circles — and maybe once they were. But either way, the result is that there are a lot of congressional committees, think tanks, and experts that have focused on taxes for decades and continue to produce a steady stream of papers, articles, and opinions for journalists they know to cover (often these are journalists hired to write specifically about tax policy).</p><p> 

To Ezra (and to us, and to many others) AI seems obviously more important than marginal changes in taxation over the next 10 or 15 years — yet there's very little infrastructure for thinking about it. There isn't a committee in Congress that primarily deals with AI, and no one has a dedicated AI position in the executive branch of the U.S. Government; nor are big AI think tanks in D.C. producing weekly articles for journalists they know to report on.</p><p> 

All of this generates a strong 'path dependence' that can lock the media in to covering less important topics despite having no intention to do so.</p><p> 

According to Ezra, the hardest thing to do in journalism — as the leader of a publication, or even to some degree just as a writer — is to maintain your own sense of what’s important, and not just be swept along in the tide of what “the industry / the narrative / the conversation has decided is important."</p><p> 

One reason Ezra created the Future Perfect vertical at <i>Vox</i> is that as he began to learn about effective altruism, he thought: "This is a framework for thinking about importance that could offer a different lens that we could use in journalism. It could help us order things differently.”</p><p>  

Ezra says there is an audience for the stuff that we’d consider most important here at 80,000 Hours. It’s broadly believed that nobody will read articles on animal suffering, but Ezra says that his experience at <i>Vox</i> shows these stories actually do really well — and that many of the things that the effective altruist community cares a lot about are “...like catnip for readers.”</p><p> 

Ezra’s bottom line for fellow journalists is that if something important is happening in the world and you can't make the audience interested in it, that is your failure — never the audience's failure.</p><p>  

But is that really true? In today’s episode we explore that claim, as well as:</p><p>  

• How many hours of news the average person should consume<br> 
• Where the progressive movement is failing to live up to its values<br> 
• Why Ezra thinks 'price gouging' is a bad idea<br> 
• Where the FDA has failed on rapid at-home testing for COVID-19<br> 
• Whether we should be more worried about tail-risk scenarios<br> 
• And his biggest critiques of the effective altruism community</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[How many words in U.S. newspapers have been spilled on tax policy in the past five years? And how many words on CRISPR? Or meat alternatives? Or how AI may soon automate the majority of jobs?<p> 

When people look back on this era, is the interesting thing going to have been fights over whether or not the top marginal tax rate was 39.5% or 35.4%, or is it going to be that human beings started to take control of human evolution; that we stood on the brink of eliminating immeasurable levels of suffering on factory farms; and that for the first time the average American might become financially comfortable and unemployed simultaneously?</p><p> 

Today’s guest is Ezra Klein, one of the most prominent journalists in the world. Ezra thinks that pressing issues are neglected largely because there's little pre-existing infrastructure to push them.</p><p> 

<a href="https://80k.link/EK"><b>Links to learn more, summary and full transcript.</b></a></p><p>

He points out that for a long time taxes have been considered hugely important in D.C. political circles — and maybe once they were. But either way, the result is that there are a lot of congressional committees, think tanks, and experts that have focused on taxes for decades and continue to produce a steady stream of papers, articles, and opinions for journalists they know to cover (often these are journalists hired to write specifically about tax policy).</p><p> 

To Ezra (and to us, and to many others) AI seems obviously more important than marginal changes in taxation over the next 10 or 15 years — yet there's very little infrastructure for thinking about it. There isn't a committee in Congress that primarily deals with AI, and no one has a dedicated AI position in the executive branch of the U.S. Government; nor are big AI think tanks in D.C. producing weekly articles for journalists they know to report on.</p><p> 

All of this generates a strong 'path dependence' that can lock the media in to covering less important topics despite having no intention to do so.</p><p> 

According to Ezra, the hardest thing to do in journalism — as the leader of a publication, or even to some degree just as a writer — is to maintain your own sense of what’s important, and not just be swept along in the tide of what “the industry / the narrative / the conversation has decided is important."</p><p> 

One reason Ezra created the Future Perfect vertical at <i>Vox</i> is that as he began to learn about effective altruism, he thought: "This is a framework for thinking about importance that could offer a different lens that we could use in journalism. It could help us order things differently.”</p><p>  

Ezra says there is an audience for the stuff that we’d consider most important here at 80,000 Hours. It’s broadly believed that nobody will read articles on animal suffering, but Ezra says that his experience at <i>Vox</i> shows these stories actually do really well — and that many of the things that the effective altruist community cares a lot about are “...like catnip for readers.”</p><p> 

Ezra’s bottom line for fellow journalists is that if something important is happening in the world and you can't make the audience interested in it, that is your failure — never the audience's failure.</p><p>  

But is that really true? In today’s episode we explore that claim, as well as:</p><p>  

• How many hours of news the average person should consume<br> 
• Where the progressive movement is failing to live up to its values<br> 
• Why Ezra thinks 'price gouging' is a bad idea<br> 
• Where the FDA has failed on rapid at-home testing for COVID-19<br> 
• Whether we should be more worried about tail-risk scenarios<br> 
• And his biggest critiques of the effective altruism community</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Sat, 20 Mar 2021 21:00:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d750fb50/8a2fea25.mp3" length="50977382" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ukwp_xWydXkr7MT6buLwf6AdtgwC0ER8OaD9vdFHoPU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Mzgv/MTY4MzU0NDY2Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6321</itunes:duration>
      <itunes:summary>How many words in U.S. newspapers have been spilled on tax policy in the past five years? And how many words on CRISPR? Or meat alternatives? Or how AI may soon automate the majority of jobs? 

When people look back on this era, is the interesting thing going to have been fights over whether or not the top marginal tax rate was 39.5% or 35.4%, or is it going to be that human beings started to take control of human evolution; that we stood on the brink of eliminating immeasurable levels of suffering on factory farms; and that for the first time the average American might become financially comfortable and unemployed simultaneously? 

Today’s guest is Ezra Klein, one of the most prominent journalists in the world. Ezra thinks that pressing issues are neglected largely because there's little pre-existing infrastructure to push them. 

Links to learn more, summary and full transcript.

He points out that for a long time taxes have been considered hugely important in D.C. political circles — and maybe once they were. But either way, the result is that there are a lot of congressional committees, think tanks, and experts that have focused on taxes for decades and continue to produce a steady stream of papers, articles, and opinions for journalists they know to cover (often these are journalists hired to write specifically about tax policy). 

To Ezra (and to us, and to many others) AI seems obviously more important than marginal changes in taxation over the next 10 or 15 years — yet there's very little infrastructure for thinking about it. There isn't a committee in Congress that primarily deals with AI, and no one has a dedicated AI position in the executive branch of the U.S. Government; nor are big AI think tanks in D.C. producing weekly articles for journalists they know to report on. 

All of this generates a strong 'path dependence' that can lock the media in to covering less important topics despite having no intention to do so. 

According to Ezra, the hardest thing to do in journalism — as the leader of a publication, or even to some degree just as a writer — is to maintain your own sense of what’s important, and not just be swept along in the tide of what “the industry / the narrative / the conversation has decided is important." 

One reason Ezra created the Future Perfect vertical at Vox is that as he began to learn about effective altruism, he thought: "This is a framework for thinking about importance that could offer a different lens that we could use in journalism. It could help us order things differently.”  

Ezra says there is an audience for the stuff that we’d consider most important here at 80,000 Hours. It’s broadly believed that nobody will read articles on animal suffering, but Ezra says that his experience at Vox shows these stories actually do really well — and that many of the things that the effective altruist community cares a lot about are “...like catnip for readers.” 

Ezra’s bottom line for fellow journalists is that if something important is happening in the world and you can't make the audience interested in it, that is your failure — never the audience's failure.  

But is that really true? In today’s episode we explore that claim, as well as:  

• How many hours of news the average person should consume 
• Where the progressive movement is failing to live up to its values 
• Why Ezra thinks 'price gouging' is a bad idea 
• Where the FDA has failed on rapid at-home testing for COVID-19 
• Whether we should be more worried about tail-risk scenarios 
• And his biggest critiques of the effective altruism community 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>How many words in U.S. newspapers have been spilled on tax policy in the past five years? And how many words on CRISPR? Or meat alternatives? Or how AI may soon automate the majority of jobs? 

When people look back on this era, is the interesting thing g</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/d750fb50/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#93 – Andy Weber on rendering bioweapons obsolete &amp; ending the new nuclear arms race</title>
      <itunes:title>#93 – Andy Weber on rendering bioweapons obsolete &amp; ending the new nuclear arms race</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c2714b5a-8360-11eb-8211-0ebea9bab453</guid>
      <link>https://share.transistor.fm/s/0c965419</link>
      <description>
        <![CDATA[COVID-19 has provided a vivid reminder of the power of biological threats. But the threat doesn't come from natural sources alone. Weaponized contagious diseases — which were abandoned by the United States, but developed in large numbers by the Soviet Union, right up until its collapse — have the potential to spread globally and kill just as many as an all-out nuclear war.<p> 

For five years today’s guest — Andy Weber — was the US Assistant Secretary of Defense responsible for biological and other weapons of mass destruction. While people primarily associate the Pentagon with waging wars, including most within the Pentagon itself, Andy is quick to point out that you can't have national security if your population remains at grave risk from natural and lab-created diseases.</p><p> 

Andy's current mission is to spread the word that while bioweapons are terrifying, scientific advances also leave them on the verge of becoming an outdated technology.</p><p> 

<a href="https://80k.link/AW"><b>Links to learn more, summary and full transcript.</b></a></p><p>

He thinks there is an overwhelming case to increase our investment in two new technologies that could dramatically reduce the risk of bioweapons and end natural pandemics in the process.</p><p> 

First, advances in genetic <b>sequencing technology</b> allow direct, real-time analysis of DNA or RNA fragments collected from the environment. You sample widely, and if you start seeing DNA sequences that you don't recognise — that sets off an alarm.</p><p> 

Andy says that while desktop sequencers may be expensive enough that they're only in hospitals today, they're rapidly getting smaller, cheaper, and easier to use. In fact DNA sequencing has recently experienced the most dramatic cost decrease of any technology, declining by a factor of 10,000 since 2007. It's only a matter of time before they're cheap enough to put in every home.</p><p>  

The second major breakthrough comes from <b>mRNA vaccines</b>, which are today being used to end the COVID pandemic. The wonder of mRNA vaccines is that they can instruct our cells to make any random protein we choose — and trigger a protective immune response from the body.</p><p> 

By using the sequencing technology above, we can quickly get the genetic code that matches the surface proteins of any new pathogen, and switch that code into the mRNA vaccines we're already making. Making a new vaccine would become less like manufacturing a new iPhone and more like printing a new book — you use the same printing press and just change the words.</p><p> 

So long as we kept enough capacity to manufacture and deliver mRNA vaccines on hand, a whole country could in principle be vaccinated against a new disease in months.</p><p> 

In tandem these technologies could make advanced bioweapons a threat of the past. And in the process contagious disease could be brought under control like never before.</p><p> 

Andy has always been pretty open and honest, but his retirement last year has allowed him to stop worrying about being seen to speak for the Department of Defense, or for the president of the United States – and we were able to get his forthright views on a bunch of interesting other topics, such as:</p><p> 

• The chances that COVID-19 escaped from a research facility<br> 
• Whether a US president can really truly launch nuclear weapons unilaterally<br> 
• What he thinks should be the top priorities for the Biden administration<br> 
• The time he and colleagues found 600kg of unsecured, highly enriched uranium sitting around in a barely secured facility in Kazakhstan, and eventually transported it to the United States<br>  
• And much more.</p><p> 

<a href="https://80k.link/WMJ">Job opportunity: Executive Assistant to Will MacAskill</a></p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[COVID-19 has provided a vivid reminder of the power of biological threats. But the threat doesn't come from natural sources alone. Weaponized contagious diseases — which were abandoned by the United States, but developed in large numbers by the Soviet Union, right up until its collapse — have the potential to spread globally and kill just as many as an all-out nuclear war.<p> 

For five years today’s guest — Andy Weber — was the US Assistant Secretary of Defense responsible for biological and other weapons of mass destruction. While people primarily associate the Pentagon with waging wars, including most within the Pentagon itself, Andy is quick to point out that you can't have national security if your population remains at grave risk from natural and lab-created diseases.</p><p> 

Andy's current mission is to spread the word that while bioweapons are terrifying, scientific advances also leave them on the verge of becoming an outdated technology.</p><p> 

<a href="https://80k.link/AW"><b>Links to learn more, summary and full transcript.</b></a></p><p>

He thinks there is an overwhelming case to increase our investment in two new technologies that could dramatically reduce the risk of bioweapons and end natural pandemics in the process.</p><p> 

First, advances in genetic <b>sequencing technology</b> allow direct, real-time analysis of DNA or RNA fragments collected from the environment. You sample widely, and if you start seeing DNA sequences that you don't recognise — that sets off an alarm.</p><p> 

Andy says that while desktop sequencers may be expensive enough that they're only in hospitals today, they're rapidly getting smaller, cheaper, and easier to use. In fact DNA sequencing has recently experienced the most dramatic cost decrease of any technology, declining by a factor of 10,000 since 2007. It's only a matter of time before they're cheap enough to put in every home.</p><p>  

The second major breakthrough comes from <b>mRNA vaccines</b>, which are today being used to end the COVID pandemic. The wonder of mRNA vaccines is that they can instruct our cells to make any random protein we choose — and trigger a protective immune response from the body.</p><p> 

By using the sequencing technology above, we can quickly get the genetic code that matches the surface proteins of any new pathogen, and switch that code into the mRNA vaccines we're already making. Making a new vaccine would become less like manufacturing a new iPhone and more like printing a new book — you use the same printing press and just change the words.</p><p> 

So long as we kept enough capacity to manufacture and deliver mRNA vaccines on hand, a whole country could in principle be vaccinated against a new disease in months.</p><p> 

In tandem these technologies could make advanced bioweapons a threat of the past. And in the process contagious disease could be brought under control like never before.</p><p> 

Andy has always been pretty open and honest, but his retirement last year has allowed him to stop worrying about being seen to speak for the Department of Defense, or for the president of the United States – and we were able to get his forthright views on a bunch of interesting other topics, such as:</p><p> 

• The chances that COVID-19 escaped from a research facility<br> 
• Whether a US president can really truly launch nuclear weapons unilaterally<br> 
• What he thinks should be the top priorities for the Biden administration<br> 
• The time he and colleagues found 600kg of unsecured, highly enriched uranium sitting around in a barely secured facility in Kazakhstan, and eventually transported it to the United States<br>  
• And much more.</p><p> 

<a href="https://80k.link/WMJ">Job opportunity: Executive Assistant to Will MacAskill</a></p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 12 Mar 2021 22:54:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0c965419/929e53db.mp3" length="55132990" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/XfLyP-6jOFTq18YQNYuoK9fjCe1UJMRvS9jyraYI85Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Mzcv/MTY4MzU0NDY2NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6861</itunes:duration>
      <itunes:summary>COVID-19 has provided a vivid reminder of the power of biological threats. But the threat doesn't come from natural sources alone. Weaponized contagious diseases — which were abandoned by the United States, but developed in large numbers by the Soviet Union, right up until its collapse — have the potential to spread globally and kill just as many as an all-out nuclear war. 

For five years today’s guest — Andy Weber — was the US Assistant Secretary of Defense responsible for biological and other weapons of mass destruction. While people primarily associate the Pentagon with waging wars, including most within the Pentagon itself, Andy is quick to point out that you can't have national security if your population remains at grave risk from natural and lab-created diseases. 

Andy's current mission is to spread the word that while bioweapons are terrifying, scientific advances also leave them on the verge of becoming an outdated technology. 

Links to learn more, summary and full transcript.

He thinks there is an overwhelming case to increase our investment in two new technologies that could dramatically reduce the risk of bioweapons and end natural pandemics in the process. 

First, advances in genetic sequencing technology allow direct, real-time analysis of DNA or RNA fragments collected from the environment. You sample widely, and if you start seeing DNA sequences that you don't recognise — that sets off an alarm. 

Andy says that while desktop sequencers may be expensive enough that they're only in hospitals today, they're rapidly getting smaller, cheaper, and easier to use. In fact DNA sequencing has recently experienced the most dramatic cost decrease of any technology, declining by a factor of 10,000 since 2007. It's only a matter of time before they're cheap enough to put in every home.  

The second major breakthrough comes from mRNA vaccines, which are today being used to end the COVID pandemic. The wonder of mRNA vaccines is that they can instruct our cells to make any random protein we choose — and trigger a protective immune response from the body. 

By using the sequencing technology above, we can quickly get the genetic code that matches the surface proteins of any new pathogen, and switch that code into the mRNA vaccines we're already making. Making a new vaccine would become less like manufacturing a new iPhone and more like printing a new book — you use the same printing press and just change the words. 

So long as we kept enough capacity to manufacture and deliver mRNA vaccines on hand, a whole country could in principle be vaccinated against a new disease in months. 

In tandem these technologies could make advanced bioweapons a threat of the past. And in the process contagious disease could be brought under control like never before. 

Andy has always been pretty open and honest, but his retirement last year has allowed him to stop worrying about being seen to speak for the Department of Defense, or for the president of the United States – and we were able to get his forthright views on a bunch of interesting other topics, such as: 

• The chances that COVID-19 escaped from a research facility 
• Whether a US president can really truly launch nuclear weapons unilaterally 
• What he thinks should be the top priorities for the Biden administration 
• The time he and colleagues found 600kg of unsecured, highly enriched uranium sitting around in a barely secured facility in Kazakhstan, and eventually transported it to the United States  
• And much more. 

Job opportunity: Executive Assistant to Will MacAskill 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>COVID-19 has provided a vivid reminder of the power of biological threats. But the threat doesn't come from natural sources alone. Weaponized contagious diseases — which were abandoned by the United States, but developed in large numbers by the Soviet Uni</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/0c965419/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#92 – Brian Christian on the alignment problem</title>
      <itunes:title>#92 – Brian Christian on the alignment problem</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">bf1e0c00-7df1-11eb-b7b8-12045045fbe3</guid>
      <link>https://share.transistor.fm/s/ef2413c6</link>
      <description>
        <![CDATA[Brian Christian is a bestselling author with a particular knack for accurately communicating difficult or technical ideas from both mathematics and computer science.<p>  

Listeners loved <a href="https://80000hours.org/podcast/episodes/brian-christian-algorithms-to-live-by/">our episode</a> about his book <a href="https://brianchristian.org/algorithms-to-live-by/"><i>Algorithms to Live By</i></a> — so when the team read his new book, <a href="https://brianchristian.org/the-alignment-problem/"><i>The Alignment Problem</i></a>, and found it to be an insightful and comprehensive review of the state of the research into making advanced AI useful and reliably safe, getting him back on the show was a no-brainer.</p><p> 

Brian has so much of substance to say this episode will likely be of interest to people who know a lot about AI as well as those who know a little, and of interest to people who are nervous about where AI is going as well as those who aren't nervous at all.</p><p> 

<a href="https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/?utm_campaign=podcast__brian-christian-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Here’s a tease of 10 Hollywood-worthy stories from the episode:</p><p> 

• <b>The Riddle of Dopamine</b>: The development of reinforcement learning solves a long-standing mystery of how humans are able to learn from their experience.<br> 
• <b>ALVINN</b>: A student teaches a military vehicle to drive between Pittsburgh and Lake Erie, without intervention, in the early 1990s, using a computer with a tenth the processing capacity of an Apple Watch.<br> 
• <b>Couch Potato</b>: An agent trained to be curious is stopped in its quest to navigate a maze by a paralysing TV screen.<br> 
• <b>Pitts &amp; McCulloch</b>: A homeless teenager and his foster father figure invent the idea of the neural net.<br> 
• <b>Tree Senility</b>: Agents become so good at living in trees to escape predators that they forget how to leave, starve, and die.<br> 
• <b>The Danish Bicycle</b>: A reinforcement learning agent figures out that it can better achieve its goal by riding in circles as quickly as possible than reaching its purported destination.<br> 
• <b>Montezuma's Revenge</b>: By 2015 a reinforcement learner can play 60 different Atari games — the majority impossibly well — but can’t score a single point on one game humans find tediously simple.<br> 
• <b>Curious Pong</b>: Two novelty-seeking agents, forced to play Pong against one another, create increasingly extreme rallies.<br> 
• <b>AlphaGo Zero</b>: A computer program becomes superhuman at Chess and Go in under a day by attempting to imitate itself.<br> 
• <b>Robot Gymnasts</b>: Over the course of an hour, humans teach robots to do perfect backflips just by telling them which of 2 random actions look more like a backflip.</p><p> 

We also cover:</p><p> 

• How reinforcement learning actually works, and some of its key achievements and failures<br> 
• How a lack of curiosity can cause AIs to fail to be able to do basic things<br> 
• The pitfalls of getting AI to imitate how we ourselves behave<br> 
• The benefits of getting AI to infer what we must be trying to achieve<br> 
• Why it’s good for agents to be uncertain about what they're doing<br> 
• Why Brian isn’t that worried about explicit deception<br> 
• The interviewees Brian most agrees with, and most disagrees with<br> 
• Developments since Brian finished the manuscript<br> 
• The effective altruism and AI safety communities<br> 
• And much more</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Brian Christian is a bestselling author with a particular knack for accurately communicating difficult or technical ideas from both mathematics and computer science.<p>  

Listeners loved <a href="https://80000hours.org/podcast/episodes/brian-christian-algorithms-to-live-by/">our episode</a> about his book <a href="https://brianchristian.org/algorithms-to-live-by/"><i>Algorithms to Live By</i></a> — so when the team read his new book, <a href="https://brianchristian.org/the-alignment-problem/"><i>The Alignment Problem</i></a>, and found it to be an insightful and comprehensive review of the state of the research into making advanced AI useful and reliably safe, getting him back on the show was a no-brainer.</p><p> 

Brian has so much of substance to say this episode will likely be of interest to people who know a lot about AI as well as those who know a little, and of interest to people who are nervous about where AI is going as well as those who aren't nervous at all.</p><p> 

<a href="https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/?utm_campaign=podcast__brian-christian-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Here’s a tease of 10 Hollywood-worthy stories from the episode:</p><p> 

• <b>The Riddle of Dopamine</b>: The development of reinforcement learning solves a long-standing mystery of how humans are able to learn from their experience.<br> 
• <b>ALVINN</b>: A student teaches a military vehicle to drive between Pittsburgh and Lake Erie, without intervention, in the early 1990s, using a computer with a tenth the processing capacity of an Apple Watch.<br> 
• <b>Couch Potato</b>: An agent trained to be curious is stopped in its quest to navigate a maze by a paralysing TV screen.<br> 
• <b>Pitts &amp; McCulloch</b>: A homeless teenager and his foster father figure invent the idea of the neural net.<br> 
• <b>Tree Senility</b>: Agents become so good at living in trees to escape predators that they forget how to leave, starve, and die.<br> 
• <b>The Danish Bicycle</b>: A reinforcement learning agent figures out that it can better achieve its goal by riding in circles as quickly as possible than reaching its purported destination.<br> 
• <b>Montezuma's Revenge</b>: By 2015 a reinforcement learner can play 60 different Atari games — the majority impossibly well — but can’t score a single point on one game humans find tediously simple.<br> 
• <b>Curious Pong</b>: Two novelty-seeking agents, forced to play Pong against one another, create increasingly extreme rallies.<br> 
• <b>AlphaGo Zero</b>: A computer program becomes superhuman at Chess and Go in under a day by attempting to imitate itself.<br> 
• <b>Robot Gymnasts</b>: Over the course of an hour, humans teach robots to do perfect backflips just by telling them which of 2 random actions look more like a backflip.</p><p> 

We also cover:</p><p> 

• How reinforcement learning actually works, and some of its key achievements and failures<br> 
• How a lack of curiosity can cause AIs to fail to be able to do basic things<br> 
• The pitfalls of getting AI to imitate how we ourselves behave<br> 
• The benefits of getting AI to infer what we must be trying to achieve<br> 
• Why it’s good for agents to be uncertain about what they're doing<br> 
• Why Brian isn’t that worried about explicit deception<br> 
• The interviewees Brian most agrees with, and most disagrees with<br> 
• Developments since Brian finished the manuscript<br> 
• The effective altruism and AI safety communities<br> 
• And much more</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 05 Mar 2021 20:59:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ef2413c6/f0f38e04.mp3" length="84938418" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/zEpY866AcSjNyPGn744baqWw2wKE0ivPpRu7HI45YLg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzYv/MTY4MzU0NDY2NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10546</itunes:duration>
      <itunes:summary>Brian Christian is a bestselling author with a particular knack for accurately communicating difficult or technical ideas from both mathematics and computer science.  

Listeners loved our episode about his book Algorithms to Live By — so when the team read his new book, The Alignment Problem, and found it to be an insightful and comprehensive review of the state of the research into making advanced AI useful and reliably safe, getting him back on the show was a no-brainer. 

Brian has so much of substance to say this episode will likely be of interest to people who know a lot about AI as well as those who know a little, and of interest to people who are nervous about where AI is going as well as those who aren't nervous at all. 

Links to learn more, summary and full transcript.

Here’s a tease of 10 Hollywood-worthy stories from the episode: 

• The Riddle of Dopamine: The development of reinforcement learning solves a long-standing mystery of how humans are able to learn from their experience. 
• ALVINN: A student teaches a military vehicle to drive between Pittsburgh and Lake Erie, without intervention, in the early 1990s, using a computer with a tenth the processing capacity of an Apple Watch. 
• Couch Potato: An agent trained to be curious is stopped in its quest to navigate a maze by a paralysing TV screen. 
• Pitts &amp;amp; McCulloch: A homeless teenager and his foster father figure invent the idea of the neural net. 
• Tree Senility: Agents become so good at living in trees to escape predators that they forget how to leave, starve, and die. 
• The Danish Bicycle: A reinforcement learning agent figures out that it can better achieve its goal by riding in circles as quickly as possible than reaching its purported destination. 
• Montezuma's Revenge: By 2015 a reinforcement learner can play 60 different Atari games — the majority impossibly well — but can’t score a single point on one game humans find tediously simple. 
• Curious Pong: Two novelty-seeking agents, forced to play Pong against one another, create increasingly extreme rallies. 
• AlphaGo Zero: A computer program becomes superhuman at Chess and Go in under a day by attempting to imitate itself. 
• Robot Gymnasts: Over the course of an hour, humans teach robots to do perfect backflips just by telling them which of 2 random actions look more like a backflip. 

We also cover: 

• How reinforcement learning actually works, and some of its key achievements and failures 
• How a lack of curiosity can cause AIs to fail to be able to do basic things 
• The pitfalls of getting AI to imitate how we ourselves behave 
• The benefits of getting AI to infer what we must be trying to achieve 
• Why it’s good for agents to be uncertain about what they're doing 
• Why Brian isn’t that worried about explicit deception 
• The interviewees Brian most agrees with, and most disagrees with 
• Developments since Brian finished the manuscript 
• The effective altruism and AI safety communities 
• And much more 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>Brian Christian is a bestselling author with a particular knack for accurately communicating difficult or technical ideas from both mathematics and computer science.  

Listeners loved our episode about his book Algorithms to Live By — so when the team re</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/ef2413c6/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#91 – Lewis Bollard on big wins against factory farming and how they happened</title>
      <itunes:title>#91 – Lewis Bollard on big wins against factory farming and how they happened</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c0f3f600-6f89-11eb-bab6-0e139608b13b</guid>
      <link>https://80000hours.org/podcast/episodes/lewis-bollard-big-wins-against-factory-farming/</link>
      <description>
        <![CDATA[<p>I suspect today's guest, Lewis Bollard, might be the single best person in the world to interview to get an overview of all the methods that might be effective for putting an end to factory farming and what broader lessons we can learn from the experiences of people working to end cruelty in animal agriculture.</p><p> That's why I interviewed him <a href="https://80000hours.org/podcast/episodes/lewis-bollard-end-factory-farming/"><strong>back in 2017</strong></a>, and it's why I've come back for an updated second dose four years later.</p><p> That conversation became a touchstone resource for anyone wanting to understand why people might decide to focus their altruism on farmed animal welfare, what those people are up to, and why.</p><p> Lewis leads Open Philanthropy’s strategy for <a href="https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare"><strong>farm animal welfare</strong></a>, and since he joined in 2015 they’ve disbursed about $130 million in grants to nonprofits as part of this program.</p><p> This episode certainly isn't only for vegetarians or people whose primary focus is animal welfare. The farmed animal welfare movement has had a lot of big wins over the last five years, and many of the lessons animal activists and plant-based meat entrepreneurs have learned are of much broader interest.</p><p> <a href="https://80000hours.org/podcast/episodes/lewis-bollard-big-wins-against-factory-farming/?utm_campaign=podcast__lewis-bollard-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Some of those include:</p><p> • <strong>Between 2019 and 2020, Beyond Meat's cost of goods sold fell from about $4.50 a pound to $3.50 a pound.</strong> Will plant-based meat or clean meat displace animal meat, and if so when? How quickly can it reach price parity?<br> • <strong>One study reported that philosophy students reduced their meat consumption by 13% after going through a course on the ethics of factory farming.</strong> But do studies like this replicate? And what happens several months later?<br> • <strong>One survey showed that 33% of people supported a ban on animal farming.</strong> Should we take such findings seriously? Or is it as informative as the study which showed that 38% of Americans believe that Ted Cruz might be the Zodiac killer?<br> • <strong>Costco, the second largest retailer in the U.S., is now over 95% cage-free.</strong> Why have they done that years before they had to? And can ethical individuals within these companies make a real difference?</p><p> We also cover:</p><p> • Switzerland’s ballot measure on eliminating factory farming<br> • What a Biden administration could mean for reducing animal suffering<br> • How chicken is cheaper than peanuts<br> • The biggest recent wins for farmed animals<br> • Things that haven’t gone to plan in animal advocacy<br> • Political opportunities for farmed animal advocates in Europe<br> • How the US is behind Brazil and Israel on animal welfare standards<br> • The value of increasing media coverage of factory farming<br> • The state of the animal welfare movement<br> • And much more</p><p> If you’d like an introduction to the nature of the problem and why Lewis is working on it, in addition to our <a href="https://80000hours.org/podcast/episodes/lewis-bollard-end-factory-farming/"><strong>2017 interview with Lewis</strong></a>, you could check out this <a href="https://www.openphilanthropy.org/research/cause-reports/treatment-animals-industrial-agriculture"><strong>2013 cause report from Open Philanthropy</strong></a>.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:37)</li><li>Biggest recent wins for farmed animals (00:06:13)</li><li>How to lower the price of plant-based meat (00:24:57)</li><li>Documentaries for farmed animals (00:37:05)</li><li>Political opportunities (00:43:07)</li><li>Do we know how to get people to reduce their meat consumption? (00:45:03)</li><li>The fraction of Americans who don’t eat meat (00:52:17)</li><li>Surprising number of people who support a ban on animal farming (00:57:57)</li><li>What we’ve learned over the past four years (01:02:48)</li><li>Things that haven’t gone to plan (01:26:30)</li><li>Animal advocacy in emerging countries (01:34:44)</li><li>Fish, crustaceans, and wild animals (01:40:28)</li><li>Open Philanthropy grants (01:47:43)</li><li>Audience questions (01:59:29)</li><li>The elimination of slavery (02:10:03)</li><li>Careers (02:15:52)</li></ul><p><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>I suspect today's guest, Lewis Bollard, might be the single best person in the world to interview to get an overview of all the methods that might be effective for putting an end to factory farming and what broader lessons we can learn from the experiences of people working to end cruelty in animal agriculture.</p><p> That's why I interviewed him <a href="https://80000hours.org/podcast/episodes/lewis-bollard-end-factory-farming/"><strong>back in 2017</strong></a>, and it's why I've come back for an updated second dose four years later.</p><p> That conversation became a touchstone resource for anyone wanting to understand why people might decide to focus their altruism on farmed animal welfare, what those people are up to, and why.</p><p> Lewis leads Open Philanthropy’s strategy for <a href="https://www.openphilanthropy.org/focus/us-policy/farm-animal-welfare"><strong>farm animal welfare</strong></a>, and since he joined in 2015 they’ve disbursed about $130 million in grants to nonprofits as part of this program.</p><p> This episode certainly isn't only for vegetarians or people whose primary focus is animal welfare. The farmed animal welfare movement has had a lot of big wins over the last five years, and many of the lessons animal activists and plant-based meat entrepreneurs have learned are of much broader interest.</p><p> <a href="https://80000hours.org/podcast/episodes/lewis-bollard-big-wins-against-factory-farming/?utm_campaign=podcast__lewis-bollard-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Some of those include:</p><p> • <strong>Between 2019 and 2020, Beyond Meat's cost of goods sold fell from about $4.50 a pound to $3.50 a pound.</strong> Will plant-based meat or clean meat displace animal meat, and if so when? How quickly can it reach price parity?<br> • <strong>One study reported that philosophy students reduced their meat consumption by 13% after going through a course on the ethics of factory farming.</strong> But do studies like this replicate? And what happens several months later?<br> • <strong>One survey showed that 33% of people supported a ban on animal farming.</strong> Should we take such findings seriously? Or is it as informative as the study which showed that 38% of Americans believe that Ted Cruz might be the Zodiac killer?<br> • <strong>Costco, the second largest retailer in the U.S., is now over 95% cage-free.</strong> Why have they done that years before they had to? And can ethical individuals within these companies make a real difference?</p><p> We also cover:</p><p> • Switzerland’s ballot measure on eliminating factory farming<br> • What a Biden administration could mean for reducing animal suffering<br> • How chicken is cheaper than peanuts<br> • The biggest recent wins for farmed animals<br> • Things that haven’t gone to plan in animal advocacy<br> • Political opportunities for farmed animal advocates in Europe<br> • How the US is behind Brazil and Israel on animal welfare standards<br> • The value of increasing media coverage of factory farming<br> • The state of the animal welfare movement<br> • And much more</p><p> If you’d like an introduction to the nature of the problem and why Lewis is working on it, in addition to our <a href="https://80000hours.org/podcast/episodes/lewis-bollard-end-factory-farming/"><strong>2017 interview with Lewis</strong></a>, you could check out this <a href="https://www.openphilanthropy.org/research/cause-reports/treatment-animals-industrial-agriculture"><strong>2013 cause report from Open Philanthropy</strong></a>.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:37)</li><li>Biggest recent wins for farmed animals (00:06:13)</li><li>How to lower the price of plant-based meat (00:24:57)</li><li>Documentaries for farmed animals (00:37:05)</li><li>Political opportunities (00:43:07)</li><li>Do we know how to get people to reduce their meat consumption? (00:45:03)</li><li>The fraction of Americans who don’t eat meat (00:52:17)</li><li>Surprising number of people who support a ban on animal farming (00:57:57)</li><li>What we’ve learned over the past four years (01:02:48)</li><li>Things that haven’t gone to plan (01:26:30)</li><li>Animal advocacy in emerging countries (01:34:44)</li><li>Fish, crustaceans, and wild animals (01:40:28)</li><li>Open Philanthropy grants (01:47:43)</li><li>Audience questions (01:59:29)</li><li>The elimination of slavery (02:10:03)</li><li>Careers (02:15:52)</li></ul><p><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 15 Feb 2021 16:37:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4cf49759/fb41bb75.mp3" length="73750315" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/CX5M-BkwDek4JQnc5o7nGx7C2gbXE-cPtmIeaAiP5LQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzUv/MTY4MzU0NDY2My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9197</itunes:duration>
      <itunes:summary>I suspect today's guest, Lewis Bollard, might be the single best person in the world to interview to get an overview of all the methods that might be effective for putting an end to factory farming and what broader lessons we can learn from the experiences of people working to end cruelty in animal agriculture. 

That's why I interviewed him back in 2017, and it's why I've come back for an updated second dose four years later. 

That conversation became a touchstone resource for anyone wanting to understand why people might decide to focus their altruism on farmed animal welfare, what those people are up to, and why. 

Lewis leads Open Philanthropy’s strategy for farm animal welfare, and since he joined in 2015 they’ve disbursed about $130 million in grants to nonprofits as part of this program. 

This episode certainly isn't only for vegetarians or people whose primary focus is animal welfare. The farmed animal welfare movement has had a lot of big wins over the last five years, and many of the lessons animal activists and plant-based meat entrepreneurs have learned are of much broader interest. 

Links to learn more, summary and full transcript.

Some of those include: 

• Between 2019 and 2020, Beyond Meat's cost of goods sold fell from about $4.50 a pound to $3.50 a pound. Will plant-based meat or clean meat displace animal meat, and if so when? How quickly can it reach price parity? 
• One study reported that philosophy students reduced their meat consumption by 13% after going through a course on the ethics of factory farming. But do studies like this replicate? And what happens several months later? 
• One survey showed that 33% of people supported a ban on animal farming. Should we take such findings seriously? Or is it as informative as the study which showed that 38% of Americans believe that Ted Cruz might be the Zodiac killer? 
• Costco, the second largest retailer in the U.S., is now over 95% cage-free. Why have they done that years before they had to? And can ethical individuals within these companies make a real difference? 

We also cover: 

• Switzerland’s ballot measure on eliminating factory farming 
• What a Biden administration could mean for reducing animal suffering 
• How chicken is cheaper than peanuts 
• The biggest recent wins for farmed animals 
• Things that haven’t gone to plan in animal advocacy 
• Political opportunities for farmed animal advocates in Europe 
• How the US is behind Brazil and Israel on animal welfare standards 
• The value of increasing media coverage of factory farming 
• The state of the animal welfare movement 
• And much more 

If you’d like an introduction to the nature of the problem and why Lewis is working on it, in addition to our 2017 interview with Lewis, you could check out this 2013 cause report from Open Philanthropy. 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>I suspect today's guest, Lewis Bollard, might be the single best person in the world to interview to get an overview of all the methods that might be effective for putting an end to factory farming and what broader lessons we can learn from the experience</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/4cf49759/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/4cf49759/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Rob Wiblin on how he ended up the way he is</title>
      <itunes:title>Rob Wiblin on how he ended up the way he is</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d868cd5c-65b0-11eb-848d-12b9b5736719</guid>
      <link>https://share.transistor.fm/s/04cc94fa</link>
      <description>
        <![CDATA[<p>This is a crosspost of an episode of the <a href="https://mishasaul.com/conversations/"><i>Eureka Podcast</i></a>.</p><p> 

The interviewer is Misha Saul, a childhood friend of Rob's, who he has known for over 20 years. While it's not an episode of our own show, we decided to share it with subscribers because it's fun, and because it touches on personal topics that we don't usually cover on the show.</p><p> 

Rob and Misha cover:</p><p> 

• How Rob's parents shaped who he is (if indeed they did)<br> 
• Their shared teenage obsession with philosophy, which eventually led to Rob working at 80,000 Hours<br> 
• How their politics were shaped by growing up in the 90s<br> 
• How talking to Rob helped Misha develop his own very different worldview<br> 
• Why The Lord of the Rings movies have held up so well<br> 
• What was it like being an exchange student in Spain, and was learning Spanish a mistake?<br> 
• Marriage and kids<br> 
• Institutional decline and historical analogies for the US in 2021<br> 
• Making fun of teachers<br> 
• Should we stop eating animals?</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>This is a crosspost of an episode of the <a href="https://mishasaul.com/conversations/"><i>Eureka Podcast</i></a>.</p><p> 

The interviewer is Misha Saul, a childhood friend of Rob's, who he has known for over 20 years. While it's not an episode of our own show, we decided to share it with subscribers because it's fun, and because it touches on personal topics that we don't usually cover on the show.</p><p> 

Rob and Misha cover:</p><p> 

• How Rob's parents shaped who he is (if indeed they did)<br> 
• Their shared teenage obsession with philosophy, which eventually led to Rob working at 80,000 Hours<br> 
• How their politics were shaped by growing up in the 90s<br> 
• How talking to Rob helped Misha develop his own very different worldview<br> 
• Why The Lord of the Rings movies have held up so well<br> 
• What was it like being an exchange student in Spain, and was learning Spanish a mistake?<br> 
• Marriage and kids<br> 
• Institutional decline and historical analogies for the US in 2021<br> 
• Making fun of teachers<br> 
• Should we stop eating animals?</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 03 Feb 2021 16:53:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/04cc94fa/8ca6ede7.mp3" length="56931705" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/yd1VK0sLXX6GhAtRGe67yR_O6VO1UdBBXhF34agUpUk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzQv/MTY4MzU0NDY2Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7077</itunes:duration>
      <itunes:summary>This is a crosspost of an episode of the Eureka Podcast. 

The interviewer is Misha Saul, a childhood friend of Rob's, who he has known for over 20 years. While it's not an episode of our own show, we decided to share it with subscribers because it's fun, and because it touches on personal topics that we don't usually cover on the show. 

Rob and Misha cover: 

• How Rob's parents shaped who he is (if indeed they did) 
• Their shared teenage obsession with philosophy, which eventually led to Rob working at 80,000 Hours 
• How their politics were shaped by growing up in the 90s 
• How talking to Rob helped Misha develop his own very different worldview 
• Why The Lord of the Rings movies have held up so well 
• What was it like being an exchange student in Spain, and was learning Spanish a mistake? 
• Marriage and kids 
• Institutional decline and historical analogies for the US in 2021 
• Making fun of teachers 
• Should we stop eating animals? 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.</itunes:summary>
      <itunes:subtitle>This is a crosspost of an episode of the Eureka Podcast. 

The interviewer is Misha Saul, a childhood friend of Rob's, who he has known for over 20 years. While it's not an episode of our own show, we decided to share it with subscribers because it's fun,</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#90 – Ajeya Cotra on worldview diversification and how big the future could be</title>
      <itunes:title>#90 – Ajeya Cotra on worldview diversification and how big the future could be</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1731b200-5b6c-11eb-9152-12aceaf157c3</guid>
      <link>https://share.transistor.fm/s/51e6f8f7</link>
      <description>
        <![CDATA[You wake up in a mysterious box, and hear the booming voice of God:<p>  

<i>“I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it.</i></p><p> 

If it came up tails, I made ten billion boxes, labeled 1 through 10 billion — also with one human in each box.</p><p> 

To get into heaven, you have to answer this correctly: Which way did the coin land?”</p><p> 

You think briefly, and decide you should bet your eternal soul on tails. The fact that you woke up at all seems like pretty good evidence that you’re in the big world — if the coin landed tails, way more people should be having an experience just like yours.</p><p> 

But then you get up, walk outside, and look at the number on your box.</p><p> 

‘3’. Huh. Now you don’t know what to believe.</p><p>  

If God made 10 billion boxes, surely it's much more likely that you would have seen a number like 7,346,678,928?</p><p> 

In today's interview, Ajeya Cotra — a senior research analyst at Open Philanthropy — explains why this thought experiment from the niche of philosophy known as 'anthropic reasoning' could be relevant for figuring out where we should direct our charitable giving.</p><p> 

<a href="https://80k.link/ACP">Links to learn more, summary and full transcript.</a></p><p> 

Some thinkers both inside and outside Open Philanthropy believe that philanthropic giving should be guided by 'longtermism' — the idea that we can do the most good if we focus primarily on the impact our actions will have on the long-term future.</p><p>  

Ajeya thinks that for that notion to make sense, there needs to be a good chance we can settle other planets and solar systems and build a society that's both very large relative to what's possible on Earth and, by virtue of being so spread out, able to protect itself from extinction for a very long time.</p><p> 

But imagine that humanity has two possible futures ahead of it: Either we’re going to have a huge future like that, in which trillions of people ultimately exist, or we’re going to wipe ourselves out quite soon, thereby ensuring that only around 100 billion people ever get to live.</p><p> 

If there are eventually going to be 1,000 trillion humans, what should we think of the fact that we seemingly find ourselves so early in history? Being among the first 100 billion humans, as we are, is equivalent to walking outside and seeing a three on your box. Suspicious! If the future will have many trillions of people, the odds of us appearing so strangely early are very low indeed.</p><p> 

If we accept the analogy, maybe we can be confident that humanity is at a high risk of extinction based on this so-called 'doomsday argument' alone.</p><p> 

If that’s true, maybe we should put more of our resources into avoiding apparent extinction threats like nuclear war and pandemics. But on the other hand, maybe the argument shows we're incredibly unlikely to achieve a long and stable future no matter what we do, and we should forget the long term and just focus on the here and now instead.</p><p> 

There are many critics of this theoretical ‘doomsday argument’, and it may be the case that it logically doesn't work. This is why Ajeya spent time investigating it, with the goal of ultimately making better philanthropic grants.</p><p>  

In this conversation, Ajeya and Rob discuss both the doomsday argument and the challenge Open Phil faces striking a balance between taking big ideas seriously, and not going all in on philosophical arguments that may turn out to be barking up the wrong tree entirely.</p><p> 

They also discuss:</p><p> 

• Which worldviews Open Phil finds most plausible, and how it balances them<br> 
• How hard it is to get to other solar systems<br> 
• The 'simulation argument'<br> 
• When transformative AI might actually arrive<br> 
• And much more</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[You wake up in a mysterious box, and hear the booming voice of God:<p>  

<i>“I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it.</i></p><p> 

If it came up tails, I made ten billion boxes, labeled 1 through 10 billion — also with one human in each box.</p><p> 

To get into heaven, you have to answer this correctly: Which way did the coin land?”</p><p> 

You think briefly, and decide you should bet your eternal soul on tails. The fact that you woke up at all seems like pretty good evidence that you’re in the big world — if the coin landed tails, way more people should be having an experience just like yours.</p><p> 

But then you get up, walk outside, and look at the number on your box.</p><p> 

‘3’. Huh. Now you don’t know what to believe.</p><p>  

If God made 10 billion boxes, surely it's much more likely that you would have seen a number like 7,346,678,928?</p><p> 

In today's interview, Ajeya Cotra — a senior research analyst at Open Philanthropy — explains why this thought experiment from the niche of philosophy known as 'anthropic reasoning' could be relevant for figuring out where we should direct our charitable giving.</p><p> 

<a href="https://80k.link/ACP">Links to learn more, summary and full transcript.</a></p><p> 

Some thinkers both inside and outside Open Philanthropy believe that philanthropic giving should be guided by 'longtermism' — the idea that we can do the most good if we focus primarily on the impact our actions will have on the long-term future.</p><p>  

Ajeya thinks that for that notion to make sense, there needs to be a good chance we can settle other planets and solar systems and build a society that's both very large relative to what's possible on Earth and, by virtue of being so spread out, able to protect itself from extinction for a very long time.</p><p> 

But imagine that humanity has two possible futures ahead of it: Either we’re going to have a huge future like that, in which trillions of people ultimately exist, or we’re going to wipe ourselves out quite soon, thereby ensuring that only around 100 billion people ever get to live.</p><p> 

If there are eventually going to be 1,000 trillion humans, what should we think of the fact that we seemingly find ourselves so early in history? Being among the first 100 billion humans, as we are, is equivalent to walking outside and seeing a three on your box. Suspicious! If the future will have many trillions of people, the odds of us appearing so strangely early are very low indeed.</p><p> 

If we accept the analogy, maybe we can be confident that humanity is at a high risk of extinction based on this so-called 'doomsday argument' alone.</p><p> 

If that’s true, maybe we should put more of our resources into avoiding apparent extinction threats like nuclear war and pandemics. But on the other hand, maybe the argument shows we're incredibly unlikely to achieve a long and stable future no matter what we do, and we should forget the long term and just focus on the here and now instead.</p><p> 

There are many critics of this theoretical ‘doomsday argument’, and it may be the case that it logically doesn't work. This is why Ajeya spent time investigating it, with the goal of ultimately making better philanthropic grants.</p><p>  

In this conversation, Ajeya and Rob discuss both the doomsday argument and the challenge Open Phil faces striking a balance between taking big ideas seriously, and not going all in on philosophical arguments that may turn out to be barking up the wrong tree entirely.</p><p> 

They also discuss:</p><p> 

• Which worldviews Open Phil finds most plausible, and how it balances them<br> 
• How hard it is to get to other solar systems<br> 
• The 'simulation argument'<br> 
• When transformative AI might actually arrive<br> 
• And much more</p><p> 

<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 21 Jan 2021 00:18:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/51e6f8f7/fcc4bfc7.mp3" length="86266325" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/GFgwdyLh0XTdiAypAvACtbKuy7N2JW6-esdWQbQKtU4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzMv/MTY4MzU0NDY2MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10745</itunes:duration>
      <itunes:summary>You wake up in a mysterious box, and hear the booming voice of God:  

“I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it. 

If it came up tails, I made ten billion boxes, labeled 1 through 10 billion — also with one human in each box. 

To get into heaven, you have to answer this correctly: Which way did the coin land?” 

You think briefly, and decide you should bet your eternal soul on tails. The fact that you woke up at all seems like pretty good evidence that you’re in the big world — if the coin landed tails, way more people should be having an experience just like yours. 

But then you get up, walk outside, and look at the number on your box. 

‘3’. Huh. Now you don’t know what to believe.  

If God made 10 billion boxes, surely it's much more likely that you would have seen a number like 7,346,678,928? 

In today's interview, Ajeya Cotra — a senior research analyst at Open Philanthropy — explains why this thought experiment from the niche of philosophy known as 'anthropic reasoning' could be relevant for figuring out where we should direct our charitable giving. 

Links to learn more, summary and full transcript. 

Some thinkers both inside and outside Open Philanthropy believe that philanthropic giving should be guided by 'longtermism' — the idea that we can do the most good if we focus primarily on the impact our actions will have on the long-term future.  

Ajeya thinks that for that notion to make sense, there needs to be a good chance we can settle other planets and solar systems and build a society that's both very large relative to what's possible on Earth and, by virtue of being so spread out, able to protect itself from extinction for a very long time. 

But imagine that humanity has two possible futures ahead of it: Either we’re going to have a huge future like that, in which trillions of people ultimately exist, or we’re going to wipe ourselves out quite soon, thereby ensuring that only around 100 billion people ever get to live. 

If there are eventually going to be 1,000 trillion humans, what should we think of the fact that we seemingly find ourselves so early in history? Being among the first 100 billion humans, as we are, is equivalent to walking outside and seeing a three on your box. Suspicious! If the future will have many trillions of people, the odds of us appearing so strangely early are very low indeed. 

If we accept the analogy, maybe we can be confident that humanity is at a high risk of extinction based on this so-called 'doomsday argument' alone. 

If that’s true, maybe we should put more of our resources into avoiding apparent extinction threats like nuclear war and pandemics. But on the other hand, maybe the argument shows we're incredibly unlikely to achieve a long and stable future no matter what we do, and we should forget the long term and just focus on the here and now instead. 

There are many critics of this theoretical ‘doomsday argument’, and it may be the case that it logically doesn't work. This is why Ajeya spent time investigating it, with the goal of ultimately making better philanthropic grants.  

In this conversation, Ajeya and Rob discuss both the doomsday argument and the challenge Open Phil faces striking a balance between taking big ideas seriously, and not going all in on philosophical arguments that may turn out to be barking up the wrong tree entirely. 

They also discuss: 

• Which worldviews Open Phil finds most plausible, and how it balances them 
• How hard it is to get to other solar systems 
• The 'simulation argument' 
• When transformative AI might actually arrive 
• And much more 

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>You wake up in a mysterious box, and hear the booming voice of God:  

“I just flipped a coin. If it came up heads, I made ten boxes, labeled 1 through 10 — each of which has a human in it. 

If it came up tails, I made ten billion boxes, labeled 1 throug</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/51e6f8f7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Rob Wiblin on self-improvement and research ethics</title>
      <itunes:title>Rob Wiblin on self-improvement and research ethics</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">53955fca-55c3-11eb-98d7-0e2a47f5f259</guid>
      <link>https://share.transistor.fm/s/26b137d2</link>
      <description>
        <![CDATA[<p>This is a crosspost of an episode of the <i>Clearer Thinking Podcast:</i> <a href="https://clearerthinkingpodcast.com/?ep=022"><b>022: Self-Improvement and Research Ethics with Rob Wiblin.</b></a> </p><p> 

Rob chats with Spencer Greenberg, who has been an audience favourite in episodes <b><a href="https://80000hours.org/podcast/episodes/spencer-greenberg-social-science/">11</a></b> and <b><a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/">39</a></b> of the 80,000 Hours Podcast, and has now created this show of his own.</p><p> 

Among other things they cover:</p><p> 

• Is trying to become a better person a good strategy for self-improvement<br> 
• Why Rob thinks many people could achieve much more by finding themselves a line manager<br> 
• Why interviews on this show are so damn long<br> 
• Is it complicated to figure out what human beings value, or actually simpler than it seems<br> 
• Why Rob thinks research ethics and institutional review boards are causing immense harm<br> 
• Where prediction markets might be failing today and how to tell</p><p> 

If you like this go ahead and subscribe to Spencer's show by searching for Clearer Thinking in your podcasting app.</p><p> 

In particular, you might want to check out Spencer’s conversation with another 80,000 Hours researcher: <a href="https://clearerthinkingpodcast.com/?ep=008"><b>008: Life Experiments and Philosophical Thinking with Arden Koehler</b>. </a></p><p> 

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>This is a crosspost of an episode of the <i>Clearer Thinking Podcast:</i> <a href="https://clearerthinkingpodcast.com/?ep=022"><b>022: Self-Improvement and Research Ethics with Rob Wiblin.</b></a> </p><p> 

Rob chats with Spencer Greenberg, who has been an audience favourite in episodes <b><a href="https://80000hours.org/podcast/episodes/spencer-greenberg-social-science/">11</a></b> and <b><a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/">39</a></b> of the 80,000 Hours Podcast, and has now created this show of his own.</p><p> 

Among other things they cover:</p><p> 

• Is trying to become a better person a good strategy for self-improvement<br> 
• Why Rob thinks many people could achieve much more by finding themselves a line manager<br> 
• Why interviews on this show are so damn long<br> 
• Is it complicated to figure out what human beings value, or actually simpler than it seems<br> 
• Why Rob thinks research ethics and institutional review boards are causing immense harm<br> 
• Where prediction markets might be failing today and how to tell</p><p> 

If you like this go ahead and subscribe to Spencer's show by searching for Clearer Thinking in your podcasting app.</p><p> 

In particular, you might want to check out Spencer’s conversation with another 80,000 Hours researcher: <a href="https://clearerthinkingpodcast.com/?ep=008"><b>008: Life Experiments and Philosophical Thinking with Arden Koehler</b>. </a></p><p> 

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 13 Jan 2021 18:08:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/26b137d2/1395266e.mp3" length="72455380" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/7856m2zbobov0vH-xfhTKjkbFobS8bToWd4Q_TdCqEo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzIv/MTY4MzU0NDY2MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9037</itunes:duration>
      <itunes:summary>This is a crosspost of an episode of the Clearer Thinking Podcast: 022: Self-Improvement and Research Ethics with Rob Wiblin.  

Rob chats with Spencer Greenberg, who has been an audience favourite in episodes 11 and 39 of the 80,000 Hours Podcast, and has now created this show of his own. 

Among other things they cover: 

• Is trying to become a better person a good strategy for self-improvement 
• Why Rob thinks many people could achieve much more by finding themselves a line manager 
• Why interviews on this show are so damn long 
• Is it complicated to figure out what human beings value, or actually simpler than it seems 
• Why Rob thinks research ethics and institutional review boards are causing immense harm 
• Where prediction markets might be failing today and how to tell 

If you like this go ahead and subscribe to Spencer's show by searching for Clearer Thinking in your podcasting app. 

In particular, you might want to check out Spencer’s conversation with another 80,000 Hours researcher: 008: Life Experiments and Philosophical Thinking with Arden Koehler.  

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>This is a crosspost of an episode of the Clearer Thinking Podcast: 022: Self-Improvement and Research Ethics with Rob Wiblin.  

Rob chats with Spencer Greenberg, who has been an audience favourite in episodes 11 and 39 of the 80,000 Hours Podcast, and ha</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#73 - Phil Trammell on patient philanthropy and waiting to do good [re-release]</title>
      <itunes:title>#73 - Phil Trammell on patient philanthropy and waiting to do good [re-release]</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">098846fc-4fa3-11eb-a826-12a45adb4289</guid>
      <link>https://share.transistor.fm/s/6dcbf8f1</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in March 2020. </b><p>

To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. </p><p>

If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. </p><p>

This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to <a href="https://80k.link/PP"><b>investigate the case for and against</b></a> so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. </p><p>

He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. </p><p>

<i>Does the COVID-19 emergency mean we should actually use resources right now? See Phil's <a href="https://80k.link/PTC"><b>first thoughts on this question here.</b></a></i> </p><p>

• <a href="https://80k.link/ptpp"><b>Links to learn more, summary and full transcript.</b></a> <br>
• <a href="https://80k.link/PP"><b>Latest version of Phil’s paper on the topic.</b></a></p><p>

What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? </p><p>

And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. </p><p>

Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? </p><p>

Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? </p><p>

Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. </p><p>

Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. </p><p>

Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my colleague Howie Lempel, we try to answer that, and also discuss: </p><p>

• Historical attempts at patient philanthropy <br>
• Should we have a mixed strategy, where some altruists are patient and others impatient? <br>
• Which causes most need money now? <br>
• What is the research frontier here? <br>
• What does this all mean for what listeners should do differently? </p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the transcript linked above.</b> </p><p>

<em>Producer: Keiran Harris.</em> <br>
<em>Audio mastering: Ben Cordell.</em> <br>
<em>Transcripts: Zakee Ulhaq.</em> <br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in March 2020. </b><p>

To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. </p><p>

If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. </p><p>

This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to <a href="https://80k.link/PP"><b>investigate the case for and against</b></a> so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. </p><p>

He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. </p><p>

<i>Does the COVID-19 emergency mean we should actually use resources right now? See Phil's <a href="https://80k.link/PTC"><b>first thoughts on this question here.</b></a></i> </p><p>

• <a href="https://80k.link/ptpp"><b>Links to learn more, summary and full transcript.</b></a> <br>
• <a href="https://80k.link/PP"><b>Latest version of Phil’s paper on the topic.</b></a></p><p>

What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? </p><p>

And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. </p><p>

Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? </p><p>

Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? </p><p>

Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. </p><p>

Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. </p><p>

Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my colleague Howie Lempel, we try to answer that, and also discuss: </p><p>

• Historical attempts at patient philanthropy <br>
• Should we have a mixed strategy, where some altruists are patient and others impatient? <br>
• Which causes most need money now? <br>
• What is the research frontier here? <br>
• What does this all mean for what listeners should do differently? </p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the transcript linked above.</b> </p><p>

<em>Producer: Keiran Harris.</em> <br>
<em>Audio mastering: Ben Cordell.</em> <br>
<em>Transcripts: Zakee Ulhaq.</em> <br></p>]]>
      </content:encoded>
      <pubDate>Thu, 07 Jan 2021 19:57:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6dcbf8f1/592d60cf.mp3" length="77781614" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/GEOPKfvy8htGZKCNVlSm3xRl2TIGDIxEWq6GJts7vAw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzEv/MTY4MzU0NDY1OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9666</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in March 2020. 

To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. 

If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. 

This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to investigate the case for and against so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. 

He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. 

Does the COVID-19 emergency mean we should actually use resources right now? See Phil's first thoughts on this question here. 

• Links to learn more, summary and full transcript. 
• Latest version of Phil’s paper on the topic.

What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? 

And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. 

Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? 

Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? 

Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. 

Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. 

Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my colleague Howie Lempel, we try to answer that, and also discuss: 

• Historical attempts at patient philanthropy 
• Should we have a mixed strategy, where some altruists are patient and others impatient? 
• Which causes most need money now? 
• What is the research frontier here? 
• What does this all mean for what listeners should do differently? 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the transcript linked above. 

Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcripts: Zakee Ulhaq. </itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in March 2020. 

To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. 

If you took $1,000 you were going to donate and instead put it in</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/6dcbf8f1/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#75 – Michelle Hutchinson on what people most often ask 80,000 Hours [re-release]</title>
      <itunes:title>#75 – Michelle Hutchinson on what people most often ask 80,000 Hours [re-release]</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">dfa2aaaa-4abe-11eb-8391-12f3bd7a62a9</guid>
      <link>https://share.transistor.fm/s/467a3e51</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in April 2020. <br></b>

<p>Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. </p><p>

I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's <em>Global Priorities Institute</em>, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. </p><p>So we thought it would be useful to discuss some on the show for everyone to hear. </p><p>

• <a href="https://80k.link/mh2"><b>Links to learn more, summary and full transcript.</b></a><br>
• <a href="https://80k.link/mh-jb"><b>See over 500 vacancies on our job board.</b></a><br>
• <a href="https://80k.link/mh-a"><b>Apply for one-on-one career advising.</b></a></p><p>

Among other common topics, we cover: </p><p>

• Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. <br>
• That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. <br>
• Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. <br>
• Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. <br>
• The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. </p><p>

I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. </p><p>

If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: </p><p>

1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. <br>
2. Who don’t yet have close connections with people working at effective altruist organisations. <br>
3. Who aren’t strongly locationally constrained. </p><p>

If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. </p><p>

Also in this episode: </p><p>

• I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. <br>
• Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. <br>
• I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. </p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b> </p><p>


<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq.</em> <br>

</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in April 2020. <br></b>

<p>Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. </p><p>

I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's <em>Global Priorities Institute</em>, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. </p><p>So we thought it would be useful to discuss some on the show for everyone to hear. </p><p>

• <a href="https://80k.link/mh2"><b>Links to learn more, summary and full transcript.</b></a><br>
• <a href="https://80k.link/mh-jb"><b>See over 500 vacancies on our job board.</b></a><br>
• <a href="https://80k.link/mh-a"><b>Apply for one-on-one career advising.</b></a></p><p>

Among other common topics, we cover: </p><p>

• Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. <br>
• That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. <br>
• Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. <br>
• Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. <br>
• The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. </p><p>

I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. </p><p>

If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: </p><p>

1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. <br>
2. Who don’t yet have close connections with people working at effective altruist organisations. <br>
3. Who aren’t strongly locationally constrained. </p><p>

If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. </p><p>

Also in this episode: </p><p>

• I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. <br>
• Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. <br>
• I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. </p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b> </p><p>


<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq.</em> <br>

</p>]]>
      </content:encoded>
      <pubDate>Wed, 30 Dec 2020 17:00:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/467a3e51/e94393bc.mp3" length="65507720" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/SuUbKLsjyRh4asrXtN0iYXTY52MDvEVqAH1hrJ72PKo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MzAv/MTY4MzU0NDY1OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8090</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in April 2020. 

Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. 

I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's Global Priorities Institute, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. So we thought it would be useful to discuss some on the show for everyone to hear. 

• Links to learn more, summary and full transcript.
• See over 500 vacancies on our job board.
• Apply for one-on-one career advising.

Among other common topics, we cover: 

• Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. 
• That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. 
• Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. 
• Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. 
• The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. 

I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. 

If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: 

1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. 
2. Who don’t yet have close connections with people working at effective altruist organisations. 
3. Who aren’t strongly locationally constrained. 

If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. 

Also in this episode: 

• I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. 
• Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. 
• I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq. </itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in April 2020. 

Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible pa</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/467a3e51/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#89 – Owen Cotton-Barratt on epistemic systems and layers of defense against potential global catastrophes</title>
      <itunes:title>#89 – Owen Cotton-Barratt on epistemic systems and layers of defense against potential global catastrophes</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">dd8baee8-3fa0-11eb-b793-0ef1e97cab95</guid>
      <link>https://80000hours.org/podcast/episodes/owen-cotton-barratt-epistemic-systems/</link>
      <description>
        <![CDATA[<p>From one point of view academia forms one big 'epistemic' system — a process which directs attention, generates ideas, and judges which are good. Traditional print media is another such system, and we can think of society as a whole as a huge epistemic system, made up of these and many other subsystems.</p><p> How these systems absorb, process, combine and organise information will have a big impact on what humanity as a whole ends up doing with itself — in fact, at a broad level it basically entirely determines the direction of the future.</p><p> With that in mind, today’s guest Owen Cotton-Barratt has founded the <a href="https://www.fhi.ox.ac.uk/rsp/#applying"><em>Research Scholars Programme</em></a> (RSP) at the Future of Humanity Institute at Oxford University, which gives early-stage researchers leeway to try to understand how the world works.</p><p> <a href="https://80000hours.org/podcast/episodes/owen-cotton-barratt-epistemic-systems/?utm_campaign=podcast__owen-cotton-barratt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Instead of you having to pay for a masters degree, the RSP pays *you* to spend significant amounts of time thinking about high-level questions, like "What is important to do?” and “How can I usefully contribute?"</p><p> Participants get to practice their research skills, while also thinking about research as a process and how research communities can function as epistemic systems that plug into the rest of society as productively as possible.</p><p> The programme attracts people with several years of experience who are looking to take their existing knowledge — whether that’s in physics, medicine, policy work, or something else — and apply it to what they determine to be the most important topics.</p><p> It also attracts people without much experience, but who have a lot of ideas. If you went directly into a PhD programme, you might have to narrow your focus quickly. But the RSP gives you time to explore the possibilities, and to figure out the answer to the question “What’s the topic that really matters, and that I’d be happy to spend several years of my life on?”</p><p> Owen thinks one of the most useful things about the two-year programme is being around other people — other RSP participants, as well as other researchers at the Future of Humanity Institute — who are trying to think seriously about where our civilisation is headed and how to have a positive impact on this trajectory.</p><p> Instead of being isolated in a PhD, you’re surrounded by folks with similar goals who can push back on your ideas and point out where you’re making mistakes. Saving years not pursuing an unproductive path could mean that you will ultimately have a much bigger impact with your career.</p><p> <a href="https://www.fhi.ox.ac.uk/rsp/#applying"><em>RSP applications</em></a><strong> are set to open in the Spring of 2021 — but Owen thinks it’s helpful for people to think about it in advance.</strong></p><p> In today’s episode, Arden and Owen mostly talk about Owen’s own research. They cover:</p><p> • Extinction risk classification and reduction strategies<br> • Preventing small disasters from becoming large disasters<br> • How likely we are to go from being in a collapsed state to going extinct<br> • What most people should do if longtermism is true<br> • Advice for mathematically-minded people<br> • And much more</p><p> <strong>Chapters:<br></strong> • Rob’s intro (00:00:00)<br>• The interview begins (00:02:22)<br>• Extinction risk classification and reduction strategies (00:06:02)<br>• Defense layers (00:16:37)<br>• Preventing small disasters from becoming large disasters (00:23:31)<br>• Risk factors (00:38:57)<br>• How likely are we to go from being in a collapsed state to going extinct? (00:48:02)<br>• Estimating total levels of existential risk (00:54:35)<br>• Everyday longtermism (01:01:35)<br>• What should most people do if longtermism is true? (01:12:18)<br>• 80,000 Hours’ issue with promoting career paths (01:24:12)<br>• The existential risk of making a lot of really bad decisions (01:29:27)<br>• What should longtermists do differently today (01:39:08)<br>• Biggest concerns with this framework (01:51:28)<br>• Research careers (02:04:04)<br>• Being a mathematician (02:13:33)<br>• Advice for mathematically minded people (02:24:30)<br>• Rob’s outro (02:37:32) </p><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcript: Zakee Ulhaq</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>From one point of view academia forms one big 'epistemic' system — a process which directs attention, generates ideas, and judges which are good. Traditional print media is another such system, and we can think of society as a whole as a huge epistemic system, made up of these and many other subsystems.</p><p> How these systems absorb, process, combine and organise information will have a big impact on what humanity as a whole ends up doing with itself — in fact, at a broad level it basically entirely determines the direction of the future.</p><p> With that in mind, today’s guest Owen Cotton-Barratt has founded the <a href="https://www.fhi.ox.ac.uk/rsp/#applying"><em>Research Scholars Programme</em></a> (RSP) at the Future of Humanity Institute at Oxford University, which gives early-stage researchers leeway to try to understand how the world works.</p><p> <a href="https://80000hours.org/podcast/episodes/owen-cotton-barratt-epistemic-systems/?utm_campaign=podcast__owen-cotton-barratt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Instead of you having to pay for a masters degree, the RSP pays *you* to spend significant amounts of time thinking about high-level questions, like "What is important to do?” and “How can I usefully contribute?"</p><p> Participants get to practice their research skills, while also thinking about research as a process and how research communities can function as epistemic systems that plug into the rest of society as productively as possible.</p><p> The programme attracts people with several years of experience who are looking to take their existing knowledge — whether that’s in physics, medicine, policy work, or something else — and apply it to what they determine to be the most important topics.</p><p> It also attracts people without much experience, but who have a lot of ideas. If you went directly into a PhD programme, you might have to narrow your focus quickly. But the RSP gives you time to explore the possibilities, and to figure out the answer to the question “What’s the topic that really matters, and that I’d be happy to spend several years of my life on?”</p><p> Owen thinks one of the most useful things about the two-year programme is being around other people — other RSP participants, as well as other researchers at the Future of Humanity Institute — who are trying to think seriously about where our civilisation is headed and how to have a positive impact on this trajectory.</p><p> Instead of being isolated in a PhD, you’re surrounded by folks with similar goals who can push back on your ideas and point out where you’re making mistakes. Saving years not pursuing an unproductive path could mean that you will ultimately have a much bigger impact with your career.</p><p> <a href="https://www.fhi.ox.ac.uk/rsp/#applying"><em>RSP applications</em></a><strong> are set to open in the Spring of 2021 — but Owen thinks it’s helpful for people to think about it in advance.</strong></p><p> In today’s episode, Arden and Owen mostly talk about Owen’s own research. They cover:</p><p> • Extinction risk classification and reduction strategies<br> • Preventing small disasters from becoming large disasters<br> • How likely we are to go from being in a collapsed state to going extinct<br> • What most people should do if longtermism is true<br> • Advice for mathematically-minded people<br> • And much more</p><p> <strong>Chapters:<br></strong> • Rob’s intro (00:00:00)<br>• The interview begins (00:02:22)<br>• Extinction risk classification and reduction strategies (00:06:02)<br>• Defense layers (00:16:37)<br>• Preventing small disasters from becoming large disasters (00:23:31)<br>• Risk factors (00:38:57)<br>• How likely are we to go from being in a collapsed state to going extinct? (00:48:02)<br>• Estimating total levels of existential risk (00:54:35)<br>• Everyday longtermism (01:01:35)<br>• What should most people do if longtermism is true? (01:12:18)<br>• 80,000 Hours’ issue with promoting career paths (01:24:12)<br>• The existential risk of making a lot of really bad decisions (01:29:27)<br>• What should longtermists do differently today (01:39:08)<br>• Biggest concerns with this framework (01:51:28)<br>• Research careers (02:04:04)<br>• Being a mathematician (02:13:33)<br>• Advice for mathematically minded people (02:24:30)<br>• Rob’s outro (02:37:32) </p><p> <em>Producer: Keiran Harris<br> Audio mastering: Ben Cordell<br> Transcript: Zakee Ulhaq</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 17 Dec 2020 17:03:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f2cce34a/ca9d1f3a.mp3" length="76329349" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/jG2SOA8dLi9iy0YP5hZj0T9k3YSwGdfpkRU4i221ZBE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Mjkv/MTY4MzU0NDY1Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9492</itunes:duration>
      <itunes:summary>From one point of view academia forms one big  'epistemic' system — a process which directs attention, generates ideas, and judges which are good. Traditional print media is another such system, and we can think of society as a whole as a huge epistemic system, made up of these and many other subsystems. 

How these systems absorb, process, combine and organise information will have a big impact on what humanity as a whole ends up doing with itself — in fact, at a broad level it basically entirely determines the direction of the future. 

With that in mind, today’s guest Owen Cotton-Barratt has founded the Research Scholars Programme (RSP) at the Future of Humanity Institute at Oxford University, which gives early-stage researchers leeway to try to understand how the world works. 

Links to learn more, summary and full transcript.

Instead of you having to pay for a masters degree, the RSP pays *you* to spend significant amounts of time thinking about high-level questions, like "What is important to do?” and “How can I usefully contribute?" 

Participants get to practice their research skills, while also thinking about research as a process and how research communities can function as epistemic systems that plug into the rest of society as productively as possible. 

The programme attracts people with several years of experience who are looking to take their existing knowledge — whether that’s in physics, medicine, policy work, or something else — and apply it to what they determine to be the most important topics. 

It also attracts people without much experience, but who have a lot of ideas. If you went directly into a PhD programme, you might have to narrow your focus quickly. But the RSP gives you time to explore the possibilities, and to figure out the answer to the question “What’s the topic that really matters, and that I’d be happy to spend several years of my life on?” 

Owen thinks one of the most useful things about the two-year programme is being around other people — other RSP participants, as well as other researchers at the Future of Humanity Institute — who are trying to think seriously about where our civilisation is headed and how to have a positive impact on this trajectory. 

Instead of being isolated in a PhD, you’re surrounded by folks with similar goals who can push back on your ideas and point out where you’re making mistakes. Saving years not pursuing an unproductive path could mean that you will ultimately have a much bigger impact with your career. 

RSP applications  are set to open in the Spring of 2021 — but Owen thinks it’s helpful for people to think about it in advance.

In today’s episode, Arden and Owen mostly talk about Owen’s own research. They cover:  


• Extinction risk classification and reduction strategies 
• Preventing small disasters from becoming large disasters 
• How likely we are to go from being in a collapsed state to going extinct 
• What most people should do if longtermism is true 
• Advice for mathematically-minded people 
• And much more  

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris
Audio mastering: Ben Cordell
Transcript: Zakee Ulhaq</itunes:summary>
      <itunes:subtitle>From one point of view academia forms one big  'epistemic' system — a process which directs attention, generates ideas, and judges which are good. Traditional print media is another such system, and we can think of society as a whole as a huge epistemic s</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f2cce34a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/f2cce34a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#88 – Tristan Harris on the need to change the incentives of social media companies</title>
      <itunes:title>#88 – Tristan Harris on the need to change the incentives of social media companies</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">9610b7f4-359e-11eb-83cc-1248d901e74b</guid>
      <link>https://80000hours.org/podcast/episodes/tristan-harris-changing-incentives-social-media/</link>
      <description>
        <![CDATA[<p>In its first 28 days on Netflix, the documentary <em>The Social Dilemma</em> — about the possible harms being caused by social media and other technology products — was seen by 38 million households in about 190 countries and in 30 languages.</p><p> Over the last ten years, the idea that Facebook, Twitter, and YouTube are degrading political discourse and grabbing and monetizing our attention in an alarming way has gone mainstream to such an extent that it's hard to remember how recently it was a fringe view.</p><p> It feels intuitively true that our attention spans are shortening, we’re spending more time alone, we’re less productive, there’s more polarization and radicalization, and that we have less trust in our fellow citizens, due to having less of a shared basis of reality.</p><p> But while it all feels plausible, how strong is the evidence that it's true? In the past, people have worried about every new technological development — often in ways that seem foolish in retrospect. Socrates famously feared that being able to write things down would ruin our memory.</p><p> At the same time, historians think that the printing press probably generated religious wars across Europe, and that the radio helped Hitler and Stalin maintain power by giving them and them alone the ability to spread propaganda across the whole of Germany and the USSR. Fears about new technologies aren't always misguided.</p><p> Tristan Harris, leader of the Center for Humane Technology, and <a href="https://www.humanetech.com/podcast">co-host of the Your Undivided Attention podcast</a>, is arguably the most prominent person working on reducing the harms of social media, and he was happy to engage with Rob’s good-faith critiques.</p><p> • <a href="https://80k.link/TH2"><strong>Links to learn more, summary and full transcript.</strong></a><br> • FYI, the <a href="https://www.surveymonkey.co.uk/r/EAS80K2">2020 Effective Altruism Survey</a> is closing soon: <em>https://www.surveymonkey.co.uk/r/EAS80K2</em></p><p> Tristan and Rob provide a thorough exploration of the merits of possible concrete solutions – something <em>The Social Dilemma</em> didn’t really address.</p><p> Given that these companies are mostly trying to design their products in the way that makes them the most money, how can we get that incentive to align with what's in our interests as users and citizens?</p><p> One way is to encourage a shift to a subscription model.</p><p> One claim in <em>The Social Dilemma</em> is that the machine learning algorithms on these sites try to shift what you believe and what you enjoy in order to make it easier to predict what content recommendations will keep you on the site.</p><p> But if you paid a yearly fee to Facebook in lieu of seeing ads, their incentive would shift towards making you as satisfied as possible with their service — even if that meant using it for five minutes a day rather than 50.</p><p> Despite all the negatives, Tristan doesn’t want us to abandon the technologies he's concerned about. He asks us to imagine a social media environment designed to regularly bring our attention back to what each of us can do to improve our lives and the world.</p><p> Just as we can focus on the positives of nuclear power while remaining vigilant about the threat of nuclear weapons, we could embrace social media and recommendation algorithms as the largest mass-coordination engine we've ever had — tools that could educate and organise people better than anything that has come before.</p><p> The tricky and open question is how to get there.</p><p> Rob and Tristan also discuss:</p><p> • Justified concerns vs. moral panics<br> • The effect of social media on politics in the US and developing countries<br> • Tips for individuals</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:36)</li><li>Center for Humane Technology (00:04:53)</li><li>Critics (00:08:19)</li><li>The Social Dilemma (00:13:20)</li><li>Three categories of harm (00:20:31)</li><li>Justified concerns vs. moral panics (00:30:23)</li><li>The messy real world vs. an imagined idealised world (00:38:20)</li><li>The persuasion apocalypse (00:47:46)</li><li>Revolt of the Public (00:56:48)</li><li>Global effects (01:02:44)</li><li>US politics (01:13:32)</li><li>Potential solutions (01:20:59)</li><li>Unintended consequences (01:42:57)</li><li>Win-win changes (01:50:47)</li><li>Big wins over the last 5 or 10 years (01:59:10)</li><li>The subscription model (02:02:28)</li><li>Tips for individuals (02:14:05)</li><li>The current state of the research (02:22:37)</li><li>Careers (02:26:36)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In its first 28 days on Netflix, the documentary <em>The Social Dilemma</em> — about the possible harms being caused by social media and other technology products — was seen by 38 million households in about 190 countries and in 30 languages.</p><p> Over the last ten years, the idea that Facebook, Twitter, and YouTube are degrading political discourse and grabbing and monetizing our attention in an alarming way has gone mainstream to such an extent that it's hard to remember how recently it was a fringe view.</p><p> It feels intuitively true that our attention spans are shortening, we’re spending more time alone, we’re less productive, there’s more polarization and radicalization, and that we have less trust in our fellow citizens, due to having less of a shared basis of reality.</p><p> But while it all feels plausible, how strong is the evidence that it's true? In the past, people have worried about every new technological development — often in ways that seem foolish in retrospect. Socrates famously feared that being able to write things down would ruin our memory.</p><p> At the same time, historians think that the printing press probably generated religious wars across Europe, and that the radio helped Hitler and Stalin maintain power by giving them and them alone the ability to spread propaganda across the whole of Germany and the USSR. Fears about new technologies aren't always misguided.</p><p> Tristan Harris, leader of the Center for Humane Technology, and <a href="https://www.humanetech.com/podcast">co-host of the Your Undivided Attention podcast</a>, is arguably the most prominent person working on reducing the harms of social media, and he was happy to engage with Rob’s good-faith critiques.</p><p> • <a href="https://80k.link/TH2"><strong>Links to learn more, summary and full transcript.</strong></a><br> • FYI, the <a href="https://www.surveymonkey.co.uk/r/EAS80K2">2020 Effective Altruism Survey</a> is closing soon: <em>https://www.surveymonkey.co.uk/r/EAS80K2</em></p><p> Tristan and Rob provide a thorough exploration of the merits of possible concrete solutions – something <em>The Social Dilemma</em> didn’t really address.</p><p> Given that these companies are mostly trying to design their products in the way that makes them the most money, how can we get that incentive to align with what's in our interests as users and citizens?</p><p> One way is to encourage a shift to a subscription model.</p><p> One claim in <em>The Social Dilemma</em> is that the machine learning algorithms on these sites try to shift what you believe and what you enjoy in order to make it easier to predict what content recommendations will keep you on the site.</p><p> But if you paid a yearly fee to Facebook in lieu of seeing ads, their incentive would shift towards making you as satisfied as possible with their service — even if that meant using it for five minutes a day rather than 50.</p><p> Despite all the negatives, Tristan doesn’t want us to abandon the technologies he's concerned about. He asks us to imagine a social media environment designed to regularly bring our attention back to what each of us can do to improve our lives and the world.</p><p> Just as we can focus on the positives of nuclear power while remaining vigilant about the threat of nuclear weapons, we could embrace social media and recommendation algorithms as the largest mass-coordination engine we've ever had — tools that could educate and organise people better than anything that has come before.</p><p> The tricky and open question is how to get there.</p><p> Rob and Tristan also discuss:</p><p> • Justified concerns vs. moral panics<br> • The effect of social media on politics in the US and developing countries<br> • Tips for individuals</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:36)</li><li>Center for Humane Technology (00:04:53)</li><li>Critics (00:08:19)</li><li>The Social Dilemma (00:13:20)</li><li>Three categories of harm (00:20:31)</li><li>Justified concerns vs. moral panics (00:30:23)</li><li>The messy real world vs. an imagined idealised world (00:38:20)</li><li>The persuasion apocalypse (00:47:46)</li><li>Revolt of the Public (00:56:48)</li><li>Global effects (01:02:44)</li><li>US politics (01:13:32)</li><li>Potential solutions (01:20:59)</li><li>Unintended consequences (01:42:57)</li><li>Win-win changes (01:50:47)</li><li>Big wins over the last 5 or 10 years (01:59:10)</li><li>The subscription model (02:02:28)</li><li>Tips for individuals (02:14:05)</li><li>The current state of the research (02:22:37)</li><li>Careers (02:26:36)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Sofia Davis-Fogel.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 03 Dec 2020 20:18:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6e4dbd51/c7030e04.mp3" length="74942866" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/hD7lqTa6LF9U6-2A3AvfCqcZPkXIp2PvatZXt8jPT7k/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Mjgv/MTY4MzU0NDY1NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9339</itunes:duration>
      <itunes:summary>In its first 28 days on Netflix, the documentary The Social Dilemma — about the possible harms being caused by social media and other technology products — was seen by 38 million households in about 190 countries and in 30 languages.  

Over the last ten years, the idea that Facebook, Twitter, and YouTube are degrading political discourse and grabbing and monetizing our attention in an alarming way has gone mainstream to such an extent that it's hard to remember how recently it was a fringe view.

It feels intuitively true that our attention spans are shortening, we’re spending more time alone, we’re less productive, there’s more polarization and radicalization, and that we have less trust in our fellow citizens, due to having less of a shared basis of reality.  

But while it all feels plausible, how strong is the evidence that it's true? In the past, people have worried about every new technological development — often in ways that seem foolish in retrospect. Socrates famously feared that being able to write things down would ruin our memory.

At the same time, historians think that the printing press probably generated religious wars across Europe, and that the radio helped Hitler and Stalin maintain power by giving them and them alone the ability to spread propaganda across the whole of Germany and the USSR. Fears about new technologies aren't always misguided.

Tristan Harris, leader of the Center for Humane Technology, and co-host of the Your Undivided Attention podcast, is arguably the most prominent person working on reducing the harms of social media, and he was happy to engage with Rob’s good-faith critiques.  

• Links to learn more, summary and full transcript.
• FYI, the 2020 Effective Altruism Survey is closing soon: https://www.surveymonkey.co.uk/r/EAS80K2

Tristan and Rob provide a thorough exploration of the merits of possible concrete solutions – something The Social Dilemma didn’t really address.

Given that these companies are mostly trying to design their products in the way that makes them the most money, how can we get that incentive to align with what's in our interests as users and citizens?  

One way is to encourage a shift to a subscription model.

One claim in The Social Dilemma is that the machine learning algorithms on these sites try to shift what you believe and what you enjoy in order to make it easier to predict what content recommendations will keep you on the site.

But if you paid a yearly fee to Facebook in lieu of seeing ads, their incentive would shift towards making you as satisfied as possible with their service — even if that meant using it for five minutes a day rather than 50.

Despite all the negatives, Tristan doesn’t want us to abandon the technologies he's concerned about. He asks us to imagine a social media environment designed to regularly bring our attention back to what each of us can do to improve our lives and the world.

Just as we can focus on the positives of nuclear power while remaining vigilant about the threat of nuclear weapons, we could embrace social media and recommendation algorithms as the largest mass-coordination engine we've ever had — tools that could educate and organise people better than anything that has come before.

The tricky and open question is how to get there.  

Rob and Tristan also discuss:

• Justified concerns vs. moral panics
• The effect of social media on politics in the US and developing countries
• Tips for individuals

Get this episode by subscribing: type 80,000 Hours into your podcasting app.

Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Sofia Davis-Fogel.</itunes:summary>
      <itunes:subtitle>In its first 28 days on Netflix, the documentary The Social Dilemma — about the possible harms being caused by social media and other technology products — was seen by 38 million households in about 190 countries and in 30 languages.  

Over the last te</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/6e4dbd51/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/6e4dbd51/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Benjamin Todd on what the effective altruism community most needs (80k team chat #4)</title>
      <itunes:title>Benjamin Todd on what the effective altruism community most needs (80k team chat #4)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e3334564-251e-11eb-9665-128b29bb2ed3</guid>
      <link>https://share.transistor.fm/s/0fa7f5b0</link>
      <description>
        <![CDATA[<p>In the <b><a href="https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/">last '80k team chat'</a></b> with Ben Todd and Arden Koehler, we discussed what effective altruism is and isn't, and how to argue for it. In this episode we turn now to what the effective altruism community most needs.</p><p>  

• <a href="https://80000hours.org/podcast/episodes/ben-todd-on-what-effective-altruism-most-needs/?utm_campaign=podcast__ben-todd3&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript</b></a><br>
• The <b><a href="https://www.surveymonkey.co.uk/r/EAS80K2">2020 Effective Altruism Survey</a></b> just opened. If you're involved with the effective altruism community, or sympathetic to its ideas, it's would be wonderful if you could fill it out: <i>https://www.surveymonkey.co.uk/r/EAS80K2</i></p><p> 

According to Ben, we can think of the effective altruism movement as having gone through several stages, categorised by what kind of resource has been most able to unlock more progress on important issues (i.e. by what's the 'bottleneck'). Plausibly, these stages are common for other social movements as well.</p><p> 

• <em>Needing money</em>: In the first stage, when effective altruism was just getting going, more money (to do things like pay staff and put on events) was the main bottleneck to making progress.<br> 
• <em>Needing talent</em>: In the second stage, we especially needed more talented people being willing to work on whatever seemed most pressing.<br> 
• <em>Needing specific skills and capacity</em>: In the third stage, which Ben thinks we're in now, the main bottlenecks are organizational capacity, infrastructure, and management to help train people up, as well as specialist skills that people can put to work now.</p><p> 

What's next? Perhaps needing coordination -- the ability to make sure people keep working efficiently and effectively together as the community grows.</p><p> 

Ben and I also cover the career implications of those stages, as well as the ability to save money and the possibility that someone else would do your job in your absence.</p><p> 

If you’d like to learn more about these topics, you should check out a couple of articles on our site:</p><p> 

•  <b><a href="https://80000hours.org/2018/11/clarifying-talent-gaps/">Think twice before talking about ‘talent gaps’ – clarifying nine misconceptions</a></b><br> 
•  <b><a href="https://80000hours.org/2019/08/how-replaceable-are-top-candidates-in-large-hiring-rounds/">How replaceable are the top candidates in large hiring rounds? Why the answer flips depending on the distribution of applicant ability</a></b></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In the <b><a href="https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/">last '80k team chat'</a></b> with Ben Todd and Arden Koehler, we discussed what effective altruism is and isn't, and how to argue for it. In this episode we turn now to what the effective altruism community most needs.</p><p>  

• <a href="https://80000hours.org/podcast/episodes/ben-todd-on-what-effective-altruism-most-needs/?utm_campaign=podcast__ben-todd3&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript</b></a><br>
• The <b><a href="https://www.surveymonkey.co.uk/r/EAS80K2">2020 Effective Altruism Survey</a></b> just opened. If you're involved with the effective altruism community, or sympathetic to its ideas, it's would be wonderful if you could fill it out: <i>https://www.surveymonkey.co.uk/r/EAS80K2</i></p><p> 

According to Ben, we can think of the effective altruism movement as having gone through several stages, categorised by what kind of resource has been most able to unlock more progress on important issues (i.e. by what's the 'bottleneck'). Plausibly, these stages are common for other social movements as well.</p><p> 

• <em>Needing money</em>: In the first stage, when effective altruism was just getting going, more money (to do things like pay staff and put on events) was the main bottleneck to making progress.<br> 
• <em>Needing talent</em>: In the second stage, we especially needed more talented people being willing to work on whatever seemed most pressing.<br> 
• <em>Needing specific skills and capacity</em>: In the third stage, which Ben thinks we're in now, the main bottlenecks are organizational capacity, infrastructure, and management to help train people up, as well as specialist skills that people can put to work now.</p><p> 

What's next? Perhaps needing coordination -- the ability to make sure people keep working efficiently and effectively together as the community grows.</p><p> 

Ben and I also cover the career implications of those stages, as well as the ability to save money and the possibility that someone else would do your job in your absence.</p><p> 

If you’d like to learn more about these topics, you should check out a couple of articles on our site:</p><p> 

•  <b><a href="https://80000hours.org/2018/11/clarifying-talent-gaps/">Think twice before talking about ‘talent gaps’ – clarifying nine misconceptions</a></b><br> 
•  <b><a href="https://80000hours.org/2019/08/how-replaceable-are-top-candidates-in-large-hiring-rounds/">How replaceable are the top candidates in large hiring rounds? Why the answer flips depending on the distribution of applicant ability</a></b></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 12 Nov 2020 22:26:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0fa7f5b0/cea4e204.mp3" length="46531796" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/P2KYsLhQ9o_x7zmn8yzjS5M7hB6CusIzUb1ih1VPllU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4Mjcv/MTY4MzU0NDY1NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5121</itunes:duration>
      <itunes:summary>In the last '80k team chat' with Ben Todd and Arden Koehler, we discussed what effective altruism is and isn't, and how to argue for it. In this episode we turn now to what the effective altruism community most needs.  

• Links to learn more, summary and full transcript
• The 2020 Effective Altruism Survey just opened. If you're involved with the effective altruism community, or sympathetic to its ideas, it's would be wonderful if you could fill it out: https://www.surveymonkey.co.uk/r/EAS80K2 

According to Ben, we can think of the effective altruism movement as having gone through several stages, categorised by what kind of resource has been most able to unlock more progress on important issues (i.e. by what's the 'bottleneck'). Plausibly, these stages are common for other social movements as well. 

• Needing money: In the first stage, when effective altruism was just getting going, more money (to do things like pay staff and put on events) was the main bottleneck to making progress. 
• Needing talent: In the second stage, we especially needed more talented people being willing to work on whatever seemed most pressing. 
• Needing specific skills and capacity: In the third stage, which Ben thinks we're in now, the main bottlenecks are organizational capacity, infrastructure, and management to help train people up, as well as specialist skills that people can put to work now. 

What's next? Perhaps needing coordination -- the ability to make sure people keep working efficiently and effectively together as the community grows. 

Ben and I also cover the career implications of those stages, as well as the ability to save money and the possibility that someone else would do your job in your absence. 

If you’d like to learn more about these topics, you should check out a couple of articles on our site: 

•  Think twice before talking about ‘talent gaps’ – clarifying nine misconceptions 
•  How replaceable are the top candidates in large hiring rounds? Why the answer flips depending on the distribution of applicant ability 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>In the last '80k team chat' with Ben Todd and Arden Koehler, we discussed what effective altruism is and isn't, and how to argue for it. In this episode we turn now to what the effective altruism community most needs.  

• Links to learn more, summary and</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/0fa7f5b0/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#87 – Russ Roberts on whether it's more effective to help strangers, or people you know</title>
      <itunes:title>#87 – Russ Roberts on whether it's more effective to help strangers, or people you know</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c8081bb6-1dfd-11eb-904d-12aeb0a6c83d</guid>
      <link>https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/</link>
      <description>
        <![CDATA[<p>If you want to make the world a better place, would it be better to help your niece with her SATs, or try to join the State Department to lower the risk that the US and China go to war?</p><p> People involved in 80,000 Hours or the effective altruism community would be comfortable recommending the latter. This week's guest — Russ Roberts, host of the long-running podcast EconTalk, and author of a forthcoming book on decision-making under uncertainty and the limited ability of data to help — worries that might be a mistake.</p><p> <a href="https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/?utm_campaign=podcast__russ-roberts&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> I've been a big fan of Russ' show EconTalk for 12 years — in fact I have a list of <a href="https://docs.google.com/document/d/1TejVbFNWkAaqRYQ4UaZfwNDKaGGgbUNeGYKJLa7dQ5I/edit#"><em>my top 100 recommended episodes</em></a> — so I invited him to talk about his concerns with how the effective altruism community tries to improve the world.</p><p> These include:</p><p> • Being too focused on the measurable<br> • Being too confident we've figured out 'the best thing'<br> • Being too credulous about the results of social science or medical experiments<br> • Undermining people's altruism by encouraging them to focus on strangers, who it's naturally harder to care for<br> • Thinking it's possible to predictably help strangers, who you don't understand well enough to know what will truly help<br> • Adding levels of wellbeing across people when this is inappropriate<br> • Encouraging people to pursue careers they won't enjoy</p><p> These worries are partly informed by Russ' 'classical liberal' worldview, which involves a preference for free market solutions to problems, and nervousness about the big plans that sometimes come out of consequentialist thinking.</p><p> While we do disagree on a range of things — such as whether it's possible to add up wellbeing across different people, and whether it's more effective to help strangers than people you know — I make the case that some of these worries are founded on common misunderstandings about effective altruism, or at least misunderstandings of what we believe here at 80,000 Hours.</p><p> We primarily care about making the world a better place over thousands or even millions of years — and we wouldn’t dream of claiming that we could accurately measure the effects of our actions on that timescale.</p><p> I'm more skeptical of medicine and empirical social science than most people, though not quite as skeptical as Russ (check out this <a href="https://80000hours.org/psychology-replication-quiz/"><em>quiz</em></a> I made where you can guess which academic findings will replicate, and which won't).</p><p> And while I do think that people should occasionally take jobs they dislike in order to have a social impact, those situations seem pretty few and far between.</p><p> But Russ and I disagree about how much we really disagree. In addition to all the above we also discuss:</p><p> • How to decide whether to have kids<br> • Was the case for deworming children oversold?<br> • Whether it would be better for countries around the world to be better coordinated</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:48)</li><li>RCTs and donations (00:05:15)</li><li>The 80,000 Hours project (00:12:35)</li><li>Expanding the moral circle (00:28:37)</li><li>Global coordination (00:39:48)</li><li>How to act if you're pessimistic about improving the long-term future (00:55:49)</li><li>Communicating uncertainty (01:03:31)</li><li>How much to trust empirical research (01:09:19)</li><li>How to decide whether to have kids (01:24:13)</li><li>Utilitarianism (01:34:01)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If you want to make the world a better place, would it be better to help your niece with her SATs, or try to join the State Department to lower the risk that the US and China go to war?</p><p> People involved in 80,000 Hours or the effective altruism community would be comfortable recommending the latter. This week's guest — Russ Roberts, host of the long-running podcast EconTalk, and author of a forthcoming book on decision-making under uncertainty and the limited ability of data to help — worries that might be a mistake.</p><p> <a href="https://80000hours.org/podcast/episodes/russ-roberts-effective-altruism-empirical-research-utilitarianism/?utm_campaign=podcast__russ-roberts&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> I've been a big fan of Russ' show EconTalk for 12 years — in fact I have a list of <a href="https://docs.google.com/document/d/1TejVbFNWkAaqRYQ4UaZfwNDKaGGgbUNeGYKJLa7dQ5I/edit#"><em>my top 100 recommended episodes</em></a> — so I invited him to talk about his concerns with how the effective altruism community tries to improve the world.</p><p> These include:</p><p> • Being too focused on the measurable<br> • Being too confident we've figured out 'the best thing'<br> • Being too credulous about the results of social science or medical experiments<br> • Undermining people's altruism by encouraging them to focus on strangers, who it's naturally harder to care for<br> • Thinking it's possible to predictably help strangers, who you don't understand well enough to know what will truly help<br> • Adding levels of wellbeing across people when this is inappropriate<br> • Encouraging people to pursue careers they won't enjoy</p><p> These worries are partly informed by Russ' 'classical liberal' worldview, which involves a preference for free market solutions to problems, and nervousness about the big plans that sometimes come out of consequentialist thinking.</p><p> While we do disagree on a range of things — such as whether it's possible to add up wellbeing across different people, and whether it's more effective to help strangers than people you know — I make the case that some of these worries are founded on common misunderstandings about effective altruism, or at least misunderstandings of what we believe here at 80,000 Hours.</p><p> We primarily care about making the world a better place over thousands or even millions of years — and we wouldn’t dream of claiming that we could accurately measure the effects of our actions on that timescale.</p><p> I'm more skeptical of medicine and empirical social science than most people, though not quite as skeptical as Russ (check out this <a href="https://80000hours.org/psychology-replication-quiz/"><em>quiz</em></a> I made where you can guess which academic findings will replicate, and which won't).</p><p> And while I do think that people should occasionally take jobs they dislike in order to have a social impact, those situations seem pretty few and far between.</p><p> But Russ and I disagree about how much we really disagree. In addition to all the above we also discuss:</p><p> • How to decide whether to have kids<br> • Was the case for deworming children oversold?<br> • Whether it would be better for countries around the world to be better coordinated</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:48)</li><li>RCTs and donations (00:05:15)</li><li>The 80,000 Hours project (00:12:35)</li><li>Expanding the moral circle (00:28:37)</li><li>Global coordination (00:39:48)</li><li>How to act if you're pessimistic about improving the long-term future (00:55:49)</li><li>Communicating uncertainty (01:03:31)</li><li>How much to trust empirical research (01:09:19)</li><li>How to decide whether to have kids (01:24:13)</li><li>Utilitarianism (01:34:01)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 03 Nov 2020 19:43:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/574e553c/d3b3fb89.mp3" length="52967873" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/FjeJmGqtpspApMojn-atP1-8p6-Os5MPecVjbmwoWl8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjYv/MTY4MzU0NDY1Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6576</itunes:duration>
      <itunes:summary>If you want to make the world a better place, would it be better to help your niece with her SATs, or try to join the State Department to lower the risk that the US and China go to war?  

People involved in 80,000 Hours or the effective altruism community would be comfortable recommending the latter. This week's guest — Russ Roberts, host of the long-running podcast EconTalk, and author of a forthcoming book on decision-making under uncertainty and the limited ability of data to help — worries that might be a mistake. 

Links to learn more, summary and full transcript.  

I've been a big fan of Russ' show EconTalk for 12 years — in fact I have a list of my top 100 recommended episodes — so I invited him to talk about his concerns with how the effective altruism community tries to improve the world.  

These include: 

• Being too focused on the measurable 
• Being too confident we've figured out 'the best thing' 
• Being too credulous about the results of social science or medical experiments 
• Undermining people's altruism by encouraging them to focus on strangers, who it's naturally harder to care for 
• Thinking it's possible to predictably help strangers, who you don't understand well enough to know what will truly help 
• Adding levels of wellbeing across people when this is inappropriate 
• Encouraging people to pursue careers they won't enjoy 
 
These worries are partly informed by Russ' 'classical liberal' worldview, which involves a preference for free market solutions to problems, and nervousness about the big plans that sometimes come out of consequentialist thinking. 

While we do disagree on a range of things — such as whether it's possible to add up wellbeing across different people, and whether it's more effective to help strangers than people you know — I make the case that some of these worries are founded on common misunderstandings about effective altruism, or at least misunderstandings of what we believe here at 80,000 Hours.  

We primarily care about making the world a better place over thousands or even millions of years — and we wouldn’t dream of claiming that we could accurately measure the effects of our actions on that timescale.  

I'm more skeptical of medicine and empirical social science than most people, though not quite as skeptical as Russ (check out this quiz I made where you can guess which academic findings will replicate, and which won't). 

And while I do think that people should occasionally take jobs they dislike in order to have a social impact, those situations seem pretty few and far between. 

But Russ and I disagree about how much we really disagree. In addition to all the above we also discuss: 

• How to decide whether to have kids 
• Was the case for deworming children oversold? 
• Whether it would be better for countries around the world to be better coordinated 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>If you want to make the world a better place, would it be better to help your niece with her SATs, or try to join the State Department to lower the risk that the US and China go to war?  

People involved in 80,000 Hours or the effective altruism commun</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/574e553c/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/574e553c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>How much does a vote matter? (Article)</title>
      <itunes:title>How much does a vote matter? (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">584f5726-1966-11eb-b844-125284b19883</guid>
      <link>https://share.transistor.fm/s/e9cb49be</link>
      <description>
        <![CDATA[<p>Today’s release is the latest in our series of audio versions of our articles.</p><p>In this one — <a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>How much does a vote matter?</b></a> — I investigate the two key things that determine the impact of your vote:</p><p> 

• The chances of your vote changing an election’s outcome<br> 
• How much better some candidates are for the world as a whole, compared to others</p><p> 

I then discuss what I think are the best arguments against voting in important elections:</p><p> 

• If an election is competitive, that means other people disagree about which option is better, and you’re at some risk of voting for the worse candidate by mistake.<br> 
• While voting itself doesn’t take long, knowing enough to accurately pick which candidate is better for the world actually does take substantial effort — effort that could be better allocated elsewhere.</p><p> 

Finally, I look into the impact of donating to campaigns or working to ‘get out the vote’, which can be effective ways to generate additional votes for your preferred candidate.</p><p> 

If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here.</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>

<em>Producer: Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s release is the latest in our series of audio versions of our articles.</p><p>In this one — <a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>How much does a vote matter?</b></a> — I investigate the two key things that determine the impact of your vote:</p><p> 

• The chances of your vote changing an election’s outcome<br> 
• How much better some candidates are for the world as a whole, compared to others</p><p> 

I then discuss what I think are the best arguments against voting in important elections:</p><p> 

• If an election is competitive, that means other people disagree about which option is better, and you’re at some risk of voting for the worse candidate by mistake.<br> 
• While voting itself doesn’t take long, knowing enough to accurately pick which candidate is better for the world actually does take substantial effort — effort that could be better allocated elsewhere.</p><p> 

Finally, I look into the impact of donating to campaigns or working to ‘get out the vote’, which can be effective ways to generate additional votes for your preferred candidate.</p><p> 

If you want to check out the links, footnotes and figures in today’s article, you can find those <a href="https://80000hours.org/articles/how-much-does-a-vote-matter/?utm_campaign=podcast__value-vote&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here.</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>

<em>Producer: Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 29 Oct 2020 13:47:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e9cb49be/686f06be.mp3" length="15149473" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/o5H8vivbJHgQakEa_-eDv1MZP-TdwDMtgJbroKdmFx0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjUv/MTY4MzU0NDY1MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>1874</itunes:duration>
      <itunes:summary>Today’s release is the latest in our series of audio versions of our articles.In this one — How much does a vote matter? — I investigate the two key things that determine the impact of your vote: 

• The chances of your vote changing an election’s outcome 
• How much better some candidates are for the world as a whole, compared to others 

I then discuss what I think are the best arguments against voting in important elections: 

• If an election is competitive, that means other people disagree about which option is better, and you’re at some risk of voting for the worse candidate by mistake. 
• While voting itself doesn’t take long, knowing enough to accurately pick which candidate is better for the world actually does take substantial effort — effort that could be better allocated elsewhere. 

Finally, I look into the impact of donating to campaigns or working to ‘get out the vote’, which can be effective ways to generate additional votes for your preferred candidate. 

If you want to check out the links, footnotes and figures in today’s article, you can find those here. 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.

Producer: Keiran Harris.</itunes:summary>
      <itunes:subtitle>Today’s release is the latest in our series of audio versions of our articles.In this one — How much does a vote matter? — I investigate the two key things that determine the impact of your vote: 

• The chances of your vote changing an election’s outcome</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/e9cb49be/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#86 – Hilary Greaves on Pascal's mugging, strong longtermism, and whether existing can be good for us</title>
      <itunes:title>#86 – Hilary Greaves on Pascal's mugging, strong longtermism, and whether existing can be good for us</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">015e6c7a-13d7-11eb-b95a-127785cddfcb</guid>
      <link>https://80000hours.org/podcast/episodes/hilary-greaves-comparing-existence-and-non-existence</link>
      <description>
        <![CDATA[<p>Had World War 1 never happened, you might never have existed.</p><p> It’s very unlikely that the exact chain of events that led to your conception would have happened otherwise — so perhaps <em>you</em> wouldn't have been born.</p><p> Would that mean that it's better for <em>you</em> that World War 1 happened (regardless of whether it was better for the world overall)?</p><p> On the one hand, if you're living a pretty good life, you might think the answer is yes – you get to live rather than not.</p><p> On the other hand, it sounds strange to say that it's better for you to be alive, because if you'd never existed there'd be no <em>you</em> to be worse off. But if you wouldn't be worse off if you hadn't existed, can you be better off because you do?</p><p> In this episode, philosophy professor Hilary Greaves – Director of Oxford University’s Global Priorities Institute – helps untangle this puzzle for us and walks me and Rob through the space of possible answers. She argues that philosophers have been too quick to conclude what she calls <em>existence non-comparativism</em> – i.e, that it can't be better for someone to exist vs. not.</p><p> <a href="https://80000hours.org/podcast/episodes/hilary-greaves-comparing-existence-and-non-existence/?utm_campaign=podcast__hilary-greaves-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Where we come down on this issue matters. If people are not made better off by existing and having good lives, you might conclude that bringing more people into existence isn't better for them, and thus, perhaps, that it's not <em>better</em> at all.</p><p> This would imply that bringing about a world in which more people live happy lives might not actually be a good thing (if the people wouldn't otherwise have existed) — which would affect how we try to make the world a better place.</p><p> Those wanting to have children in order to give them the pleasure of a good life would in some sense be mistaken. And if humanity stopped bothering to have kids and just gradually died out we would have no particular reason to be concerned.</p><p> Furthermore it might mean we should deprioritise issues that primarily affect future generations, like climate change or the risk of humanity accidentally wiping itself out.</p><p> This is our second episode with Professor Greaves. The <a href="https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>first one</strong></a> was a big hit, so we thought we'd come back and dive into even more complex ethical issues.</p><p> We discuss:</p><p> • The case for different types of ‘strong longtermism’ — the idea that we ought morally to try to make the very long run future go as well as possible<br> • What it means for us to be 'clueless' about the consequences of our actions<br> • Moral uncertainty -- what we should do when we don't know which moral theory is correct<br> • Whether we should take a bet on a <em>really</em> small probability of a <em>really</em> great outcome<br> • The field of global priorities research at the Global Priorities Institute and beyond</p><p>Chapters:</p><ul><li>The interview begins (00:02:53)</li><li>The Case for Strong Longtermism (00:05:49)</li><li>Compatible moral views (00:20:03)</li><li>Defining cluelessness (00:39:26)</li><li>Why cluelessness isn’t an objection to longtermism (00:51:05)</li><li>Theories of what to do under moral uncertainty (01:07:42)</li><li>Pascal’s mugging (01:16:37)</li><li>Comparing Existence and Non-Existence (01:30:58)</li><li>Philosophers who reject existence comparativism (01:48:56)</li><li>Lives framework (02:01:52)</li><li>Global priorities research (02:09:25)</li></ul><p><br> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</strong></p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Had World War 1 never happened, you might never have existed.</p><p> It’s very unlikely that the exact chain of events that led to your conception would have happened otherwise — so perhaps <em>you</em> wouldn't have been born.</p><p> Would that mean that it's better for <em>you</em> that World War 1 happened (regardless of whether it was better for the world overall)?</p><p> On the one hand, if you're living a pretty good life, you might think the answer is yes – you get to live rather than not.</p><p> On the other hand, it sounds strange to say that it's better for you to be alive, because if you'd never existed there'd be no <em>you</em> to be worse off. But if you wouldn't be worse off if you hadn't existed, can you be better off because you do?</p><p> In this episode, philosophy professor Hilary Greaves – Director of Oxford University’s Global Priorities Institute – helps untangle this puzzle for us and walks me and Rob through the space of possible answers. She argues that philosophers have been too quick to conclude what she calls <em>existence non-comparativism</em> – i.e, that it can't be better for someone to exist vs. not.</p><p> <a href="https://80000hours.org/podcast/episodes/hilary-greaves-comparing-existence-and-non-existence/?utm_campaign=podcast__hilary-greaves-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Where we come down on this issue matters. If people are not made better off by existing and having good lives, you might conclude that bringing more people into existence isn't better for them, and thus, perhaps, that it's not <em>better</em> at all.</p><p> This would imply that bringing about a world in which more people live happy lives might not actually be a good thing (if the people wouldn't otherwise have existed) — which would affect how we try to make the world a better place.</p><p> Those wanting to have children in order to give them the pleasure of a good life would in some sense be mistaken. And if humanity stopped bothering to have kids and just gradually died out we would have no particular reason to be concerned.</p><p> Furthermore it might mean we should deprioritise issues that primarily affect future generations, like climate change or the risk of humanity accidentally wiping itself out.</p><p> This is our second episode with Professor Greaves. The <a href="https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>first one</strong></a> was a big hit, so we thought we'd come back and dive into even more complex ethical issues.</p><p> We discuss:</p><p> • The case for different types of ‘strong longtermism’ — the idea that we ought morally to try to make the very long run future go as well as possible<br> • What it means for us to be 'clueless' about the consequences of our actions<br> • Moral uncertainty -- what we should do when we don't know which moral theory is correct<br> • Whether we should take a bet on a <em>really</em> small probability of a <em>really</em> great outcome<br> • The field of global priorities research at the Global Priorities Institute and beyond</p><p>Chapters:</p><ul><li>The interview begins (00:02:53)</li><li>The Case for Strong Longtermism (00:05:49)</li><li>Compatible moral views (00:20:03)</li><li>Defining cluelessness (00:39:26)</li><li>Why cluelessness isn’t an objection to longtermism (00:51:05)</li><li>Theories of what to do under moral uncertainty (01:07:42)</li><li>Pascal’s mugging (01:16:37)</li><li>Comparing Existence and Non-Existence (01:30:58)</li><li>Philosophers who reject existence comparativism (01:48:56)</li><li>Lives framework (02:01:52)</li><li>Global priorities research (02:09:25)</li></ul><p><br> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</strong></p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 21 Oct 2020 21:30:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cee7388c/86d5f177.mp3" length="69772875" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/btVqf3jS0h-TPGyQGyiZ2YhGizusYW75pc766kgNzWg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjQv/MTY4MzU0NDY1MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8694</itunes:duration>
      <itunes:summary>Had World War 1 never happened, you might never have existed. 

It’s very unlikely that the exact chain of events that led to your conception would have happened otherwise — so perhaps you wouldn't have been born. 

Would that mean that it's better for you that World War 1 happened (regardless of whether it was better for the world overall)? 

On the one hand, if you're living a pretty good life, you might think the answer is yes – you get to live rather than not. 

On the other hand, it sounds strange to say that it's better for you to be alive, because if you'd never existed there'd be no you to be worse off. But if you wouldn't be worse off if you hadn't existed, can you be better off because you do? 

In this episode, philosophy professor Hilary Greaves – Director of Oxford University’s Global Priorities Institute – helps untangle this puzzle for us and walks me and Rob through the space of possible answers. She argues that philosophers have been too quick to conclude what she calls existence non-comparativism – i.e, that it can't be better for someone to exist vs. not. 

Links to learn more, summary and full transcript. 

Where we come down on this issue matters. If people are not made better off by existing and having good lives, you might conclude that bringing more people into existence isn't better for them, and thus, perhaps, that it's not better at all.  

This would imply that bringing about a world in which more people live happy lives might not actually be a good thing (if the people wouldn't otherwise have existed) — which would affect how we try to make the world a better place.  

Those wanting to have children in order to give them the pleasure of a good life would in some sense be mistaken. And if humanity stopped bothering to have kids and just gradually died out we would have no particular reason to be concerned. 

Furthermore it might mean we should deprioritise issues that primarily affect future generations, like climate change or the risk of humanity accidentally wiping itself out. 

This is our second episode with Professor Greaves. The first one was a big hit, so we thought we'd come back and dive into even more complex ethical issues.  

We discuss: 

• The case for different types of ‘strong longtermism’ — the idea that we ought morally to try to make the very long run future go as well as possible  
• What it means for us to be 'clueless' about the consequences of our actions  
• Moral uncertainty -- what we should do when we don't know which moral theory is correct  
• Whether we should take a bet on a really small probability of a really great outcome  
• The field of global priorities research at the Global Priorities Institute and beyond 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Had World War 1 never happened, you might never have existed. 

It’s very unlikely that the exact chain of events that led to your conception would have happened otherwise — so perhaps you wouldn't have been born. 

Would that mean that it's better fo</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/cee7388c/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/cee7388c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Benjamin Todd on the core of effective altruism and how to argue for it (80k team chat #3)</title>
      <itunes:title>Benjamin Todd on the core of effective altruism and how to argue for it (80k team chat #3)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">e85892a2-fd1d-11ea-b69a-12230287f561</guid>
      <link>https://share.transistor.fm/s/07303ca3</link>
      <description>
        <![CDATA[Today’s episode is the latest conversation between Arden Koehler, and our CEO, Ben Todd.<p> 

Ben’s been thinking a lot about effective altruism recently, including what it really is, how it's framed, and how people misunderstand it.</p><p>  

We recently released an article on <b><a href="https://80000hours.org/2020/08/misconceptions-effective-altruism/">misconceptions about effective altruism</a></b> – based on Will MacAskill’s recent paper <em><b><a href="https://80k.link/DEA">The Definition of Effective Altruism</a></b></em> – and this episode can act as a companion piece.</p><p> 

<a href="https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/?utm_campaign=podcast__ben-todd&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Arden and Ben cover a bunch of topics related to effective altruism:</p><p> 

• How it isn’t just about donating money to fight poverty<br> 
• Whether it includes a moral obligation to give<br> 
• The rigorous argument for its importance<br> 
• Objections to that argument<br> 
• How to talk about effective altruism for people who aren't already familiar with it</p><p> 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature.</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today’s episode is the latest conversation between Arden Koehler, and our CEO, Ben Todd.<p> 

Ben’s been thinking a lot about effective altruism recently, including what it really is, how it's framed, and how people misunderstand it.</p><p>  

We recently released an article on <b><a href="https://80000hours.org/2020/08/misconceptions-effective-altruism/">misconceptions about effective altruism</a></b> – based on Will MacAskill’s recent paper <em><b><a href="https://80k.link/DEA">The Definition of Effective Altruism</a></b></em> – and this episode can act as a companion piece.</p><p> 

<a href="https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/?utm_campaign=podcast__ben-todd&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Arden and Ben cover a bunch of topics related to effective altruism:</p><p> 

• How it isn’t just about donating money to fight poverty<br> 
• Whether it includes a moral obligation to give<br> 
• The rigorous argument for its importance<br> 
• Objections to that argument<br> 
• How to talk about effective altruism for people who aren't already familiar with it</p><p> 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature.</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 22 Sep 2020 22:47:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/07303ca3/b7a2cc20.mp3" length="40569061" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/QvmcIu86YvxgN9tFNYbl0WADVQ7_48otCHvXm_VSLBg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjMv/MTY4MzU0NDY0OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5047</itunes:duration>
      <itunes:summary>Today’s episode is the latest conversation between Arden Koehler, and our CEO, Ben Todd. 

Ben’s been thinking a lot about effective altruism recently, including what it really is, how it's framed, and how people misunderstand it.  

We recently released an article on misconceptions about effective altruism – based on Will MacAskill’s recent paper The Definition of Effective Altruism – and this episode can act as a companion piece. 

Links to learn more, summary and full transcript.

Arden and Ben cover a bunch of topics related to effective altruism: 

• How it isn’t just about donating money to fight poverty 
• Whether it includes a moral obligation to give 
• The rigorous argument for its importance 
• Objections to that argument 
• How to talk about effective altruism for people who aren't already familiar with it 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature. 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Today’s episode is the latest conversation between Arden Koehler, and our CEO, Ben Todd. 

Ben’s been thinking a lot about effective altruism recently, including what it really is, how it's framed, and how people misunderstand it.  

We recently released </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/07303ca3/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Ideas for high impact careers beyond our priority paths (Article)</title>
      <itunes:title>Ideas for high impact careers beyond our priority paths (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f6f5e4e2-eed4-11ea-b33d-0ef24749efad</guid>
      <link>https://share.transistor.fm/s/b39cc07e</link>
      <description>
        <![CDATA[<p>Today’s release is the latest in our series of audio versions of our articles.</p><p> 

In this one, we go through some more career options beyond our priority paths that seem promising to us for positively influencing the long-term future.</p><p> 

Some of these are likely to be written up as priority paths in the future, or wrapped into existing ones, but we haven’t written full profiles for them yet—for example policy careers outside AI and biosecurity policy that seem promising from a longtermist perspective.</p><p> 

Others, like information security, we think might be as promising for many people as our priority paths, but because we haven’t investigated them much we’re still unsure.</p><p> 

Still others seem like they’ll typically be less impactful than our priority paths for people who can succeed equally in either, but still seem high-impact to us and like they could be top options for a substantial number of people, depending on personal fit—for example research management.</p><p> 

Finally some—like becoming a public intellectual—clearly have the potential for a lot of impact, but we can’t recommend them widely because they don’t have the capacity to absorb a large number of people, are particularly risky, or both.</p><p> 

If you want to check out the links in today’s article, you can find those <a href="https://80000hours.org/2020/08/ideas-for-high-impact-careers-beyond-our-priority-paths/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here.</b></a></p><p> 

Our <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-paths&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>annual user survey</b></a> is also now open for submissions.</p><p> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.</p><p> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.</p><p> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.</p><p> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.</p><p> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.</p><p> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.</p><p> 

So please do take a moment to fill out the user survey before it closes on Sunday (13th of September).</p><p> 

You can find it at <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-paths&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/survey</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s release is the latest in our series of audio versions of our articles.</p><p> 

In this one, we go through some more career options beyond our priority paths that seem promising to us for positively influencing the long-term future.</p><p> 

Some of these are likely to be written up as priority paths in the future, or wrapped into existing ones, but we haven’t written full profiles for them yet—for example policy careers outside AI and biosecurity policy that seem promising from a longtermist perspective.</p><p> 

Others, like information security, we think might be as promising for many people as our priority paths, but because we haven’t investigated them much we’re still unsure.</p><p> 

Still others seem like they’ll typically be less impactful than our priority paths for people who can succeed equally in either, but still seem high-impact to us and like they could be top options for a substantial number of people, depending on personal fit—for example research management.</p><p> 

Finally some—like becoming a public intellectual—clearly have the potential for a lot of impact, but we can’t recommend them widely because they don’t have the capacity to absorb a large number of people, are particularly risky, or both.</p><p> 

If you want to check out the links in today’s article, you can find those <a href="https://80000hours.org/2020/08/ideas-for-high-impact-careers-beyond-our-priority-paths/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here.</b></a></p><p> 

Our <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-paths&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>annual user survey</b></a> is also now open for submissions.</p><p> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.</p><p> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.</p><p> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.</p><p> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.</p><p> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.</p><p> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.</p><p> 

So please do take a moment to fill out the user survey before it closes on Sunday (13th of September).</p><p> 

You can find it at <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-paths&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/survey</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 07 Sep 2020 12:03:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b39cc07e/94a4a419.mp3" length="14052783" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/kDGnnLGjqScIl0r8jjEb6Qf2P1FEGNPti0MI1gN22CU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjIv/MTY4MzU0NDY0OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>1674</itunes:duration>
      <itunes:summary>Today’s release is the latest in our series of audio versions of our articles. 

In this one, we go through some more career options beyond our priority paths that seem promising to us for positively influencing the long-term future. 

Some of these are likely to be written up as priority paths in the future, or wrapped into existing ones, but we haven’t written full profiles for them yet—for example policy careers outside AI and biosecurity policy that seem promising from a longtermist perspective. 

Others, like information security, we think might be as promising for many people as our priority paths, but because we haven’t investigated them much we’re still unsure. 

Still others seem like they’ll typically be less impactful than our priority paths for people who can succeed equally in either, but still seem high-impact to us and like they could be top options for a substantial number of people, depending on personal fit—for example research management. 

Finally some—like becoming a public intellectual—clearly have the potential for a lot of impact, but we can’t recommend them widely because they don’t have the capacity to absorb a large number of people, are particularly risky, or both. 

If you want to check out the links in today’s article, you can find those here. 

Our annual user survey is also now open for submissions. 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you. 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand. 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too. 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative. 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation. 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year. 

So please do take a moment to fill out the user survey before it closes on Sunday (13th of September). 

You can find it at 80000hours.org/survey 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Today’s release is the latest in our series of audio versions of our articles. 

In this one, we go through some more career options beyond our priority paths that seem promising to us for positively influencing the long-term future. 

Some of these are l</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/b39cc07e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Benjamin Todd on varieties of longtermism and things 80,000 Hours might be getting wrong (80k team chat #2)</title>
      <itunes:title>Benjamin Todd on varieties of longtermism and things 80,000 Hours might be getting wrong (80k team chat #2)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d36b2f70-ec66-11ea-a788-0e9351e7db55</guid>
      <link>https://share.transistor.fm/s/643f6c7a</link>
      <description>
        <![CDATA[Today’s bonus episode is a conversation between Arden Koehler, and our CEO, Ben Todd.<p> 

Ben’s been doing a bunch of research recently, and we thought it’d be interesting to hear about how he’s currently thinking about a couple of different topics – including different types of longtermism, and things 80,000 Hours might be getting wrong.</p><p>  

<a href="https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/?utm_campaign=podcast__guest-name&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

This is very off-the-cut compared to our regular episodes, and just 54 minutes long.</p><p> 

In the first half, Arden and Ben talk about varieties of longtermism:</p><p> 

• Patient longtermism<br> 
• Broad urgent longtermism<br> 
• Targeted urgent longtermism focused on existential risks<br> 
• Targeted urgent longtermism focused on other trajectory changes<br> 
• And their distinctive implications for people trying to do good with their careers.</p><p>  

In the second half, they move on to:</p><p> 

• How to trade-off transferable versus specialist career capital<br> 
• How much weight to put on personal fit<br> 
• Whether we might be highlighting the wrong problems and career paths.</p><p> 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature.</p><p> 

Our <a href="http://80000hours.org/survey/?utm_campaign=podcast__bt-1&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>annual user survey</b></a> is also now open for submissions.</p><p> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.</p><p> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.</p><p> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.</p><p> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.</p><p> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.</p><p> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.</p><p> 

So please do take a moment to fill out the user survey.</p><p> 

You can find it at <a href="http://80000hours.org/survey/?utm_campaign=podcast__bt-1&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/survey</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today’s bonus episode is a conversation between Arden Koehler, and our CEO, Ben Todd.<p> 

Ben’s been doing a bunch of research recently, and we thought it’d be interesting to hear about how he’s currently thinking about a couple of different topics – including different types of longtermism, and things 80,000 Hours might be getting wrong.</p><p>  

<a href="https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/?utm_campaign=podcast__guest-name&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

This is very off-the-cut compared to our regular episodes, and just 54 minutes long.</p><p> 

In the first half, Arden and Ben talk about varieties of longtermism:</p><p> 

• Patient longtermism<br> 
• Broad urgent longtermism<br> 
• Targeted urgent longtermism focused on existential risks<br> 
• Targeted urgent longtermism focused on other trajectory changes<br> 
• And their distinctive implications for people trying to do good with their careers.</p><p>  

In the second half, they move on to:</p><p> 

• How to trade-off transferable versus specialist career capital<br> 
• How much weight to put on personal fit<br> 
• Whether we might be highlighting the wrong problems and career paths.</p><p> 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature.</p><p> 

Our <a href="http://80000hours.org/survey/?utm_campaign=podcast__bt-1&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>annual user survey</b></a> is also now open for submissions.</p><p> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.</p><p> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.</p><p> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.</p><p> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.</p><p> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.</p><p> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.</p><p> 

So please do take a moment to fill out the user survey.</p><p> 

You can find it at <a href="http://80000hours.org/survey/?utm_campaign=podcast__bt-1&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/survey</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 01 Sep 2020 15:56:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/643f6c7a/15219449.mp3" length="27912122" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/uJPLNNsrLQSVzclbmVzltmZTp63dXGtcoY9fZD9AA04/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjEv/MTY4MzU0NDY0Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3471</itunes:duration>
      <itunes:summary>Today’s bonus episode is a conversation between Arden Koehler, and our CEO, Ben Todd. 

Ben’s been doing a bunch of research recently, and we thought it’d be interesting to hear about how he’s currently thinking about a couple of different topics – including different types of longtermism, and things 80,000 Hours might be getting wrong.  

Links to learn more, summary and full transcript.

This is very off-the-cut compared to our regular episodes, and just 54 minutes long. 

In the first half, Arden and Ben talk about varieties of longtermism: 

• Patient longtermism 
• Broad urgent longtermism 
• Targeted urgent longtermism focused on existential risks 
• Targeted urgent longtermism focused on other trajectory changes 
• And their distinctive implications for people trying to do good with their careers.  

In the second half, they move on to: 

• How to trade-off transferable versus specialist career capital 
• How much weight to put on personal fit 
• Whether we might be highlighting the wrong problems and career paths. 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature. 

Our annual user survey is also now open for submissions. 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you. 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand. 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too. 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative. 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation. 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year. 

So please do take a moment to fill out the user survey. 

You can find it at 80000hours.org/survey 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Today’s bonus episode is a conversation between Arden Koehler, and our CEO, Ben Todd. 

Ben’s been doing a bunch of research recently, and we thought it’d be interesting to hear about how he’s currently thinking about a couple of different topics – includ</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/643f6c7a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Global issues beyond 80,000 Hours’ current priorities (Article)</title>
      <itunes:title>Global issues beyond 80,000 Hours’ current priorities (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1ad3f726-e941-11ea-85ac-12037350e273</guid>
      <link>https://share.transistor.fm/s/68bcd2d9</link>
      <description>
        <![CDATA[Today’s release is the latest in our series of audio versions of our articles.<p> 

In this one, we go through 30 global issues beyond the ones we usually prioritize most highly in our work, and that you might consider focusing your career on tackling.</p><p> 

Although we spend the majority of our time at 80,000 Hours on our highest priority problem areas, and we recommend working on them to many of our readers, these are just the most promising issues among those we’ve spent time investigating. There are many other global issues that we haven’t properly investigated, and which might be very promising for more people to work on.</p><p> 

In fact, we think working on some of the issues in this article could be as high-impact for some people as working on our priority problem areas — though we haven’t looked into them enough to be confident.</p><p> 

If you want to check out the links in today’s article, you can find those <a href="https://80000hours.org/2020/08/other-problems/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here.</b></a></p><p> 

Our <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>annual user survey</b></a> is also now open for submissions.</p><p> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.</p><p> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.</p><p> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.</p><p> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.</p><p> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.</p><p> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.</p><p> 

So please do take a moment to fill out the user survey.</p><p> 

You can find it at <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/survey</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today’s release is the latest in our series of audio versions of our articles.<p> 

In this one, we go through 30 global issues beyond the ones we usually prioritize most highly in our work, and that you might consider focusing your career on tackling.</p><p> 

Although we spend the majority of our time at 80,000 Hours on our highest priority problem areas, and we recommend working on them to many of our readers, these are just the most promising issues among those we’ve spent time investigating. There are many other global issues that we haven’t properly investigated, and which might be very promising for more people to work on.</p><p> 

In fact, we think working on some of the issues in this article could be as high-impact for some people as working on our priority problem areas — though we haven’t looked into them enough to be confident.</p><p> 

If you want to check out the links in today’s article, you can find those <a href="https://80000hours.org/2020/08/other-problems/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here.</b></a></p><p> 

Our <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>annual user survey</b></a> is also now open for submissions.</p><p> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.</p><p> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.</p><p> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.</p><p> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.</p><p> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.</p><p> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.</p><p> 

So please do take a moment to fill out the user survey.</p><p> 

You can find it at <a href="http://80000hours.org/survey/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/survey</b></a></p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 28 Aug 2020 16:24:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/68bcd2d9/4d3f8fa8.mp3" length="16214155" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HWqqngCGrgzxeBpStZUV0EKSnrBXoRPPh8ImRzpbm4Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MjAv/MTY4MzU0NDY0Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>1974</itunes:duration>
      <itunes:summary>Today’s release is the latest in our series of audio versions of our articles. 

In this one, we go through 30 global issues beyond the ones we usually prioritize most highly in our work, and that you might consider focusing your career on tackling. 

Although we spend the majority of our time at 80,000 Hours on our highest priority problem areas, and we recommend working on them to many of our readers, these are just the most promising issues among those we’ve spent time investigating. There are many other global issues that we haven’t properly investigated, and which might be very promising for more people to work on. 

In fact, we think working on some of the issues in this article could be as high-impact for some people as working on our priority problem areas — though we haven’t looked into them enough to be confident. 

If you want to check out the links in today’s article, you can find those here. 

Our annual user survey is also now open for submissions. 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you. 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand. 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too. 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative. 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation. 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year. 

So please do take a moment to fill out the user survey. 

You can find it at 80000hours.org/survey 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Today’s release is the latest in our series of audio versions of our articles. 

In this one, we go through 30 global issues beyond the ones we usually prioritize most highly in our work, and that you might consider focusing your career on tackling. 

Alt</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/68bcd2d9/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#85 - Mark Lynas on climate change, societal collapse &amp; nuclear energy</title>
      <itunes:title>#85 - Mark Lynas on climate change, societal collapse &amp; nuclear energy</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">dfec50fc-e30c-11ea-a7df-0e0ca07a19b3</guid>
      <link>https://share.transistor.fm/s/4c4fbd31</link>
      <description>
        <![CDATA[A golf-ball sized lump of uranium can deliver more than enough power to cover all of your lifetime energy use. To get the same energy from coal, you’d need 3,200 tonnes of black rock — a mass equivalent to 800 adult elephants, which would produce more than 11,000 tonnes of CO2. That’s about 11,000 tonnes more than the uranium.<p>  

Many people aren’t comfortable with the danger posed by nuclear power. But given the climatic stakes, it’s worth asking: Just how much more dangerous is it compared to fossil fuels?</p><p> 

According to today’s guest, Mark Lynas — author of <em><b><a href="https://80k.link/OFW">Six Degrees: Our Future on a Hotter Planet</a></b></em> (winner of the prestigious Royal Society Prizes for Science Books) and <em><b><a href="https://80k.link/N20">Nuclear 2.0</a></b></em> — it’s actually much, much <i>safer</i>.</p><p> 

<a href="https://80k.link/MLPod"><b>Links to learn more, summary and full transcript.</b></a></p><p>  

Climatologists James Hansen and Pushker Kharecha calculated that the use of nuclear power between 1971 and 2009 avoided the premature deaths of 1.84 million people by avoiding air pollution from burning coal.</p><p>  

What about radiation or nuclear disasters? According to Our World In Data, in generating a given amount of electricity, nuclear, wind, and solar all cause about the same number of deaths — and it's a tiny number.</p><p> 

So what’s going on? Why isn’t everyone demanding a massive scale-up of nuclear energy to save lives and stop climate change? Mark and many other activists believe that unchecked climate change will result in the collapse of human civilization, so the stakes could not be higher.</p><p> 


Mark says that many environmentalists — including him — simply grew up with anti-nuclear attitudes all around them (possibly stemming from a conflation of nuclear weapons and nuclear energy) and haven't thought to question them.</p><p>  

But he thinks that once you believe in the climate emergency, you have to rethink your opposition to nuclear energy.</p><p>  

At 80,000 Hours we haven’t analysed the merits and flaws of the case for nuclear energy — especially compared to wind and solar paired with gas, hydro, or battery power to handle intermittency — but Mark is convinced.</p><p>  

He says it comes down to physics: Nuclear power is just so much denser.</p><p> 

We need to find an energy source that provides carbon-free power to ~10 billion people, and we need to do it while humanity is doubling or tripling (or more) its energy demand.</p><p>  

How do you do that without destroying the world's ecology? Mark thinks that nuclear is the only way.</p><p>  

<a href="https://80k.link/MLPod"><b>Read a more in-depth version of the case for nuclear energy in the full blog post.</b></a></p><p> 

For Mark, the only argument against nuclear power is a political one -- that people won't want or accept it.</p><p> 

He says that he knows people in all kinds of mainstream environmental groups — such as Greenpeace — who agree that nuclear must be a vital part of any plan to solve climate change. But, because they think they'll be ostracized if they speak up, they keep their mouths shut.</p><p> 

Mark thinks this willingness to indulge beliefs that contradict scientific evidence stands in the way of actually fully addressing climate change, and so he’s helping to build a movement of folks who are out and proud about their support for nuclear energy.</p><p> 

This is only one topic of many in today’s interview. Arden, Rob, and Mark also discuss:</p><p> 

• At what degrees of warming does societal collapse become likely<br> 
• Whether climate change could lead to human extinction<br> 
• What environmentalists are getting wrong about climate change<br>  
• And much more.</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[A golf-ball sized lump of uranium can deliver more than enough power to cover all of your lifetime energy use. To get the same energy from coal, you’d need 3,200 tonnes of black rock — a mass equivalent to 800 adult elephants, which would produce more than 11,000 tonnes of CO2. That’s about 11,000 tonnes more than the uranium.<p>  

Many people aren’t comfortable with the danger posed by nuclear power. But given the climatic stakes, it’s worth asking: Just how much more dangerous is it compared to fossil fuels?</p><p> 

According to today’s guest, Mark Lynas — author of <em><b><a href="https://80k.link/OFW">Six Degrees: Our Future on a Hotter Planet</a></b></em> (winner of the prestigious Royal Society Prizes for Science Books) and <em><b><a href="https://80k.link/N20">Nuclear 2.0</a></b></em> — it’s actually much, much <i>safer</i>.</p><p> 

<a href="https://80k.link/MLPod"><b>Links to learn more, summary and full transcript.</b></a></p><p>  

Climatologists James Hansen and Pushker Kharecha calculated that the use of nuclear power between 1971 and 2009 avoided the premature deaths of 1.84 million people by avoiding air pollution from burning coal.</p><p>  

What about radiation or nuclear disasters? According to Our World In Data, in generating a given amount of electricity, nuclear, wind, and solar all cause about the same number of deaths — and it's a tiny number.</p><p> 

So what’s going on? Why isn’t everyone demanding a massive scale-up of nuclear energy to save lives and stop climate change? Mark and many other activists believe that unchecked climate change will result in the collapse of human civilization, so the stakes could not be higher.</p><p> 


Mark says that many environmentalists — including him — simply grew up with anti-nuclear attitudes all around them (possibly stemming from a conflation of nuclear weapons and nuclear energy) and haven't thought to question them.</p><p>  

But he thinks that once you believe in the climate emergency, you have to rethink your opposition to nuclear energy.</p><p>  

At 80,000 Hours we haven’t analysed the merits and flaws of the case for nuclear energy — especially compared to wind and solar paired with gas, hydro, or battery power to handle intermittency — but Mark is convinced.</p><p>  

He says it comes down to physics: Nuclear power is just so much denser.</p><p> 

We need to find an energy source that provides carbon-free power to ~10 billion people, and we need to do it while humanity is doubling or tripling (or more) its energy demand.</p><p>  

How do you do that without destroying the world's ecology? Mark thinks that nuclear is the only way.</p><p>  

<a href="https://80k.link/MLPod"><b>Read a more in-depth version of the case for nuclear energy in the full blog post.</b></a></p><p> 

For Mark, the only argument against nuclear power is a political one -- that people won't want or accept it.</p><p> 

He says that he knows people in all kinds of mainstream environmental groups — such as Greenpeace — who agree that nuclear must be a vital part of any plan to solve climate change. But, because they think they'll be ostracized if they speak up, they keep their mouths shut.</p><p> 

Mark thinks this willingness to indulge beliefs that contradict scientific evidence stands in the way of actually fully addressing climate change, and so he’s helping to build a movement of folks who are out and proud about their support for nuclear energy.</p><p> 

This is only one topic of many in today’s interview. Arden, Rob, and Mark also discuss:</p><p> 

• At what degrees of warming does societal collapse become likely<br> 
• Whether climate change could lead to human extinction<br> 
• What environmentalists are getting wrong about climate change<br>  
• And much more.</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 20 Aug 2020 19:50:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4c4fbd31/78e1f9d5.mp3" length="62082785" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/JOusoY2srrM8o1mjDb84UBJA8amgzWkBCpT9AfL-t80/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTkv/MTY4MzU0NDY0NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7706</itunes:duration>
      <itunes:summary>A golf-ball sized lump of uranium can deliver more than enough power to cover all of your lifetime energy use. To get the same energy from coal, you’d need 3,200 tonnes of black rock — a mass equivalent to 800 adult elephants, which would produce more than 11,000 tonnes of CO2. That’s about 11,000 tonnes more than the uranium.  

Many people aren’t comfortable with the danger posed by nuclear power. But given the climatic stakes, it’s worth asking: Just how much more dangerous is it compared to fossil fuels? 

According to today’s guest, Mark Lynas — author of Six Degrees: Our Future on a Hotter Planet (winner of the prestigious Royal Society Prizes for Science Books) and Nuclear 2.0 — it’s actually much, much safer. 

Links to learn more, summary and full transcript.  

Climatologists James Hansen and Pushker Kharecha calculated that the use of nuclear power between 1971 and 2009 avoided the premature deaths of 1.84 million people by avoiding air pollution from burning coal.  

What about radiation or nuclear disasters? According to Our World In Data, in generating a given amount of electricity, nuclear, wind, and solar all cause about the same number of deaths — and it's a tiny number. 

So what’s going on? Why isn’t everyone demanding a massive scale-up of nuclear energy to save lives and stop climate change? Mark and many other activists believe that unchecked climate change will result in the collapse of human civilization, so the stakes could not be higher. 


Mark says that many environmentalists — including him — simply grew up with anti-nuclear attitudes all around them (possibly stemming from a conflation of nuclear weapons and nuclear energy) and haven't thought to question them.  

But he thinks that once you believe in the climate emergency, you have to rethink your opposition to nuclear energy.  

At 80,000 Hours we haven’t analysed the merits and flaws of the case for nuclear energy — especially compared to wind and solar paired with gas, hydro, or battery power to handle intermittency — but Mark is convinced.  

He says it comes down to physics: Nuclear power is just so much denser. 

We need to find an energy source that provides carbon-free power to ~10 billion people, and we need to do it while humanity is doubling or tripling (or more) its energy demand.  

How do you do that without destroying the world's ecology? Mark thinks that nuclear is the only way.  

Read a more in-depth version of the case for nuclear energy in the full blog post. 

For Mark, the only argument against nuclear power is a political one -- that people won't want or accept it. 

He says that he knows people in all kinds of mainstream environmental groups — such as Greenpeace — who agree that nuclear must be a vital part of any plan to solve climate change. But, because they think they'll be ostracized if they speak up, they keep their mouths shut. 

Mark thinks this willingness to indulge beliefs that contradict scientific evidence stands in the way of actually fully addressing climate change, and so he’s helping to build a movement of folks who are out and proud about their support for nuclear energy. 

This is only one topic of many in today’s interview. Arden, Rob, and Mark also discuss: 

• At what degrees of warming does societal collapse become likely 
• Whether climate change could lead to human extinction 
• What environmentalists are getting wrong about climate change  
• And much more. 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>A golf-ball sized lump of uranium can deliver more than enough power to cover all of your lifetime energy use. To get the same energy from coal, you’d need 3,200 tonnes of black rock — a mass equivalent to 800 adult elephants, which would produce more tha</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/4c4fbd31/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#84 – Shruti Rajagopalan on what India did to stop COVID-19 and how well it worked</title>
      <itunes:title>#84 – Shruti Rajagopalan on what India did to stop COVID-19 and how well it worked</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3b705026-dda5-11ea-afb8-0e4d332dadc1</guid>
      <link>https://80000hours.org/podcast/episodes/shruti-rajagopalan-covid19-and-india/</link>
      <description>
        <![CDATA[<p>When COVID-19 struck the US, everyone was told that hand sanitizer needed to be saved for healthcare professionals, so they should just wash their hands instead. But in India, many homes lack reliable piped water, so they had to do the opposite: distribute hand sanitizer as widely as possible.</p><p> American advocates for banning single-use plastic straws might be outraged at the widespread adoption of single-use hand sanitizer sachets in India. But the US and India are very different places, and it might be the only way out when you're facing a pandemic without running water.</p><p> According to today’s guest, Shruti Rajagopalan, Senior Research Fellow at the <em>Mercatus Center</em> at George Mason University, that's typical and context is key to policy-making. This prompted Shruti to propose a set of <a href="https://80k.link/srpaper">policy responses</a> designed for India specifically back in April.</p><p> Unfortunately she thinks it's surprisingly hard to know what one should and shouldn't imitate from overseas.</p><p> <a href="https://80k.link/sr"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> For instance, some places in India installed shared handwashing stations in bus stops and train stations, which is something no developed country would advise. But in India, you can't necessarily wash your hands at home — so shared faucets might be the lesser of two evils. (Though note scientists have downgraded the importance of hand hygiene lately.)</p><p> Stay-at-home orders offer a more serious example. Developing countries find themselves in a serious bind that rich countries do not.</p><p> With nearly no slack in healthcare capacity, India lacks equipment to treat even a small number of COVID-19 patients. That suggests strict controls on movement and economic activity might be necessary to control the pandemic.</p><p> But many people in India and elsewhere can't afford to shelter in place for weeks, let alone months. And governments in poorer countries may not be able to afford to send everyone money — even where they have the infrastructure to do so fast enough.</p><p> India ultimately did impose strict lockdowns, lasting almost 70 days, but the human toll has been larger than in rich countries, with vast numbers of migrant workers stranded far from home with limited if any income support.</p><p> There were no trains or buses, and the government made no provision to deal with the situation. Unable to afford rent where they were, many people had to walk hundreds of kilometers to reach home, carrying children and belongings with them. </p><p> But in some other ways the context of developing countries is more promising. In the US many people melted down when asked to wear facemasks. But in South Asia, people just wore them.</p><p> Shruti isn’t sure whether that's because of existing challenges with high pollution, past experiences with pandemics, or because intergenerational living makes the wellbeing of others more salient, but the end result is that masks weren’t politicised in the way they were in the US.</p><p> In addition, despite the suffering caused by India's policy response to COVID-19, public support for the measures and the government remains high — and India's population is much younger and so less affected by the virus.</p><p> In this episode, Howie and Shruti explore the unique policy challenges facing India in its battle with COVID-19, what they've tried to do, and how it has gone.</p><p> They also cover:</p><p> • What an economist can bring to the table during a pandemic<br> • The mystery of India’s surprisingly low mortality rate<br> • Policies that should be implemented today<br> • What makes a good constitution</p><p> <strong>Chapters:<br></strong> • Rob’s intro (00:00:00)<br>• The interview begins (00:02:27)<br>• What an economist can bring to the table for COVID-19 (00:07:54)<br>• What India has done about the coronavirus (00:12:24)<br>• Why it took so long for India to start seeing a lot of cases (00:25:08)<br>• How India is doing at the moment with COVID-19 (00:27:55)<br>• Is the mortality rate surprisingly low in India? (00:40:32)<br>• Why Southeast Asians countries have done so well so far (00:55:43)<br>• Different attitudes to masks globally (00:59:25)<br>• Differences in policy approaches for developing countries (01:07:27)<br>• India’s strict lockdown (01:25:56)<br>• Lockdown for the average rural Indian (01:39:11)<br>• Public reaction to the lockdown in India (01:44:39)<br>• Policies that should be implemented today (01:50:29)<br>• India’s overall reaction to COVID-19 (01:57:23)<br>• Constitutional economics (02:03:28)<br>• What makes a good constitution (02:11:47)<br>• Emergent Ventures (02:27:34)<br>• Careers (02:47:57)<br>• Rob’s outro (02:57:51) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>When COVID-19 struck the US, everyone was told that hand sanitizer needed to be saved for healthcare professionals, so they should just wash their hands instead. But in India, many homes lack reliable piped water, so they had to do the opposite: distribute hand sanitizer as widely as possible.</p><p> American advocates for banning single-use plastic straws might be outraged at the widespread adoption of single-use hand sanitizer sachets in India. But the US and India are very different places, and it might be the only way out when you're facing a pandemic without running water.</p><p> According to today’s guest, Shruti Rajagopalan, Senior Research Fellow at the <em>Mercatus Center</em> at George Mason University, that's typical and context is key to policy-making. This prompted Shruti to propose a set of <a href="https://80k.link/srpaper">policy responses</a> designed for India specifically back in April.</p><p> Unfortunately she thinks it's surprisingly hard to know what one should and shouldn't imitate from overseas.</p><p> <a href="https://80k.link/sr"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> For instance, some places in India installed shared handwashing stations in bus stops and train stations, which is something no developed country would advise. But in India, you can't necessarily wash your hands at home — so shared faucets might be the lesser of two evils. (Though note scientists have downgraded the importance of hand hygiene lately.)</p><p> Stay-at-home orders offer a more serious example. Developing countries find themselves in a serious bind that rich countries do not.</p><p> With nearly no slack in healthcare capacity, India lacks equipment to treat even a small number of COVID-19 patients. That suggests strict controls on movement and economic activity might be necessary to control the pandemic.</p><p> But many people in India and elsewhere can't afford to shelter in place for weeks, let alone months. And governments in poorer countries may not be able to afford to send everyone money — even where they have the infrastructure to do so fast enough.</p><p> India ultimately did impose strict lockdowns, lasting almost 70 days, but the human toll has been larger than in rich countries, with vast numbers of migrant workers stranded far from home with limited if any income support.</p><p> There were no trains or buses, and the government made no provision to deal with the situation. Unable to afford rent where they were, many people had to walk hundreds of kilometers to reach home, carrying children and belongings with them. </p><p> But in some other ways the context of developing countries is more promising. In the US many people melted down when asked to wear facemasks. But in South Asia, people just wore them.</p><p> Shruti isn’t sure whether that's because of existing challenges with high pollution, past experiences with pandemics, or because intergenerational living makes the wellbeing of others more salient, but the end result is that masks weren’t politicised in the way they were in the US.</p><p> In addition, despite the suffering caused by India's policy response to COVID-19, public support for the measures and the government remains high — and India's population is much younger and so less affected by the virus.</p><p> In this episode, Howie and Shruti explore the unique policy challenges facing India in its battle with COVID-19, what they've tried to do, and how it has gone.</p><p> They also cover:</p><p> • What an economist can bring to the table during a pandemic<br> • The mystery of India’s surprisingly low mortality rate<br> • Policies that should be implemented today<br> • What makes a good constitution</p><p> <strong>Chapters:<br></strong> • Rob’s intro (00:00:00)<br>• The interview begins (00:02:27)<br>• What an economist can bring to the table for COVID-19 (00:07:54)<br>• What India has done about the coronavirus (00:12:24)<br>• Why it took so long for India to start seeing a lot of cases (00:25:08)<br>• How India is doing at the moment with COVID-19 (00:27:55)<br>• Is the mortality rate surprisingly low in India? (00:40:32)<br>• Why Southeast Asians countries have done so well so far (00:55:43)<br>• Different attitudes to masks globally (00:59:25)<br>• Differences in policy approaches for developing countries (01:07:27)<br>• India’s strict lockdown (01:25:56)<br>• Lockdown for the average rural Indian (01:39:11)<br>• Public reaction to the lockdown in India (01:44:39)<br>• Policies that should be implemented today (01:50:29)<br>• India’s overall reaction to COVID-19 (01:57:23)<br>• Constitutional economics (02:03:28)<br>• What makes a good constitution (02:11:47)<br>• Emergent Ventures (02:27:34)<br>• Careers (02:47:57)<br>• Rob’s outro (02:57:51) </p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 13 Aug 2020 21:22:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3067dd21/dc8a61ab.mp3" length="86166883" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/rF17xlh5SX1o1nzP4VyGS1bRgwWq60bCZpoD0pWI-Ww/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTgv/MTY4MzU0NDY0NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10694</itunes:duration>
      <itunes:summary>When COVID-19 struck the US, everyone was told that hand sanitizer needed to be saved for healthcare professionals, so they should just wash their hands instead. But in India, many homes lack reliable piped water, so they had to do the opposite: distribute hand sanitizer as widely as possible.

American advocates for banning single-use plastic straws might be outraged at the widespread adoption of single-use hand sanitizer sachets in India. But the US and India are very different places, and it might be the only way out when you're facing a pandemic without running water.

According to today’s guest, Shruti Rajagopalan, Senior Research Fellow at the Mercatus Center at George Mason University, that's typical and context is key to policy-making. This prompted Shruti to propose a set of policy responses designed for India specifically back in April.

Unfortunately she thinks it's surprisingly hard to know what one should and shouldn't imitate from overseas.

Links to learn more, summary and full transcript.

For instance, some places in India installed shared handwashing stations in bus stops and train stations, which is something no developed country would advise. But in India, you can't necessarily wash your hands at home — so shared faucets might be the lesser of two evils. (Though note scientists have downgraded the importance of hand hygiene lately.)

Stay-at-home orders offer a more serious example. Developing countries find themselves in a serious bind that rich countries do not.
 
With nearly no slack in healthcare capacity, India lacks equipment to treat even a small number of COVID-19 patients. That suggests strict controls on movement and economic activity might be necessary to control the pandemic.
 
But many people in India and elsewhere can't afford to shelter in place for weeks, let alone months. And governments in poorer countries may not be able to afford to send everyone money — even where they have the infrastructure to do so fast enough.
 
India ultimately did impose strict lockdowns, lasting almost 70 days, but the human toll has been larger than in rich countries, with vast numbers of migrant workers stranded far from home with limited if any income support.

There were no trains or buses, and the government made no provision to deal with the situation. Unable to afford rent where they were, many people had to walk hundreds of kilometers to reach home, carrying children and belongings with them. 
 
But in some other ways the context of developing countries is more promising. In the US many people melted down when asked to wear facemasks. But in South Asia, people just wore them.
 
Shruti isn’t sure whether that's because of existing challenges with high pollution, past experiences with pandemics, or because intergenerational living makes the wellbeing of others more salient, but the end result is that masks weren’t politicised in the way they were in the US.
 
In addition, despite the suffering caused by India's policy response to COVID-19, public support for the measures and the government remains high — and India's population is much younger and so less affected by the virus.
 
In this episode, Howie and Shruti explore the unique policy challenges facing India in its battle with COVID-19, what they've tried to do, and how it has gone.

They also cover:

• What an economist can bring to the table during a pandemic
• The mystery of India’s surprisingly low mortality rate
• Policies that should be implemented today
• What makes a good constitution

Get this episode by subscribing: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>When COVID-19 struck the US, everyone was told that hand sanitizer needed to be saved for healthcare professionals, so they should just wash their hands instead. But in India, many homes lack reliable piped water, so they had to do the opposite: distribut</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/3067dd21/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/3067dd21/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#83 - Jennifer Doleac on preventing crime without police and prisons</title>
      <itunes:title>#83 - Jennifer Doleac on preventing crime without police and prisons</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">732ff4bc-d364-11ea-9f3a-12e29931da63</guid>
      <link>https://share.transistor.fm/s/a0781817</link>
      <description>
        <![CDATA[<p>The killing of George Floyd has prompted a great deal of debate over whether the US should reduce the size of its police departments. The research literature suggests that the presence of police officers does reduce crime, though they're expensive and as is increasingly recognised, impose substantial harms on the populations they are meant to be protecting, especially communities of colour.</p><p> So maybe we ought to shift our focus to effective but unconventional approaches to crime prevention, approaches that don't require police or prisons and the human toll they bring with them.</p><p> Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;M University, and Director of the <em>Justice Tech Lab</em> — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three alternative ways to effectively prevent crime: better street lighting, cognitive behavioral therapy, and lead reduction.</p><p> One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double.</p><p> <a href="https://80k.link/jdpod"><strong>Links to sources for the claims in these show notes, other resources to learn more, and a full transcript.</strong></a></p><p> The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught.</p><p> You might think: "Well, people will just commit crime in the morning instead". But it looks like criminals aren’t early risers, and that doesn’t happen.</p><p> On her unusually rigorous podcast <a href="https://80k.link/pcac">Probable Causation</a>, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost. </p><p> The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone.</p><p> The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting.</p><p> There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%.</p><p> Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives.</p><p> Finally, Jennifer thinks that lead reduction might be the best buy of all in crime prevention…</p><p> <strong>Blog post truncated due to length limits. </strong><a href="https://80k.link/jdpod">Finish reading the full post here.</a></p><p> In today’s conversation, Rob and Jennifer also cover, among many other things:</p><p> • Misconduct, hiring practices and accountability among US police<br> • Procedural justice training<br> • Overrated policy ideas<br> • Policies to try to reduce racial discrimination<br> • The effects of DNA databases<br> • Diversity in economics<br> • The quality of social science research</p><p> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The killing of George Floyd has prompted a great deal of debate over whether the US should reduce the size of its police departments. The research literature suggests that the presence of police officers does reduce crime, though they're expensive and as is increasingly recognised, impose substantial harms on the populations they are meant to be protecting, especially communities of colour.</p><p> So maybe we ought to shift our focus to effective but unconventional approaches to crime prevention, approaches that don't require police or prisons and the human toll they bring with them.</p><p> Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;M University, and Director of the <em>Justice Tech Lab</em> — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three alternative ways to effectively prevent crime: better street lighting, cognitive behavioral therapy, and lead reduction.</p><p> One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double.</p><p> <a href="https://80k.link/jdpod"><strong>Links to sources for the claims in these show notes, other resources to learn more, and a full transcript.</strong></a></p><p> The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught.</p><p> You might think: "Well, people will just commit crime in the morning instead". But it looks like criminals aren’t early risers, and that doesn’t happen.</p><p> On her unusually rigorous podcast <a href="https://80k.link/pcac">Probable Causation</a>, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost. </p><p> The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone.</p><p> The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting.</p><p> There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%.</p><p> Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives.</p><p> Finally, Jennifer thinks that lead reduction might be the best buy of all in crime prevention…</p><p> <strong>Blog post truncated due to length limits. </strong><a href="https://80k.link/jdpod">Finish reading the full post here.</a></p><p> In today’s conversation, Rob and Jennifer also cover, among many other things:</p><p> • Misconduct, hiring practices and accountability among US police<br> • Procedural justice training<br> • Overrated policy ideas<br> • Policies to try to reduce racial discrimination<br> • The effects of DNA databases<br> • Diversity in economics<br> • The quality of social science research</p><p> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</strong></p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 31 Jul 2020 20:16:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a0781817/420ec467.mp3" length="69180940" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/gRcFpAKTrV0o-Ld6WpbdEEK-mb6LmTnKD_3nj9zfPsM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTcv/MTY4MzU0NDY0My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8583</itunes:duration>
      <itunes:summary>The killing of George Floyd has prompted a great deal of debate over whether the US should reduce the size of its police departments. The research literature suggests that the presence of police officers does reduce crime, though they're expensive and as is increasingly recognised, impose substantial harms on the populations they are meant to be protecting, especially communities of colour. 

So maybe we ought to shift our focus to effective but unconventional approaches to crime prevention, approaches that don't require police or prisons and the human toll they bring with them. 

Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;amp;M University, and Director of the Justice Tech Lab — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three alternative ways to effectively prevent crime: better street lighting, cognitive behavioral therapy, and lead reduction. 

One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double. 

Links to sources for the claims in these show notes, other resources to learn more, and a full transcript. 

The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught. 

You might think: "Well, people will just commit crime in the morning instead". But it looks like criminals aren’t early risers, and that doesn’t happen. 

On her unusually rigorous podcast Probable Causation, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost.  

The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone. 

The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting. 

There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%. 

Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives. 

Finally, Jennifer thinks that lead reduction might be the best buy of all in crime prevention… 

Blog post truncated due to length limits. Finish reading the full post here. 

In today’s conversation, Rob and Jennifer also cover, among many other things: 

• Misconduct, hiring practices and accountability among US police
• Procedural justice training
• Overrated policy ideas
• Policies to try to reduce racial discrimination
• The effects of DNA databases
• Diversity in economics
• The quality of social science research 

Get this episode by subscribing: type 80,000 Hours into your podcasting app.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>The killing of George Floyd has prompted a great deal of debate over whether the US should reduce the size of its police departments. The research literature suggests that the presence of police officers does reduce crime, though they're expensive and as </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#82 – James Forman Jr on reducing the cruelty of the US criminal legal system</title>
      <itunes:title>#82 – James Forman Jr on reducing the cruelty of the US criminal legal system</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b7f03eb0-d042-11ea-8e60-0ef97b41ad5b</guid>
      <link>https://80000hours.org/podcast/episodes/james-forman-jr-cruelty-in-the-us-criminal-legal-system/</link>
      <description>
        <![CDATA[<p>No democracy has ever incarcerated as many people as the United States. To get its incarceration rate down to the global average, the US would have to release 3 in 4 people in its prisons today. </p><p> The effects on Black Americans have been especially severe — Black people make up 12% of the US population but 33% of its prison population. In the early 2000's when incarceration reached its peak, the US government estimated that 32% of Black boys would go to prison at some point in their lives, 5.5 times the figure for whites. </p><p> Contrary to popular understanding, nonviolent drug offenders make up <a href="https://www.prisonpolicy.org/reports/pie2020.html"><strong>less than a fifth</strong></a> of the incarcerated population. The only way to get its incarceration rate near the global average will be to shorten prison sentences for so-called 'violent criminals' — a politically toxic idea. But could we change that?</p><p> According to today’s guest, Professor James Forman Jr — a former public defender in Washington DC, Pulitzer Prize-winning author of <a href="https://80k.link/JFJ"><em>Locking Up Our Own: Crime and Punishment in Black America</em></a>, and now a professor at Yale Law School — there are two things we have to do to make that happen.</p><p> <a href="https://80k.link/JFJpod"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> First, he thinks we should lose the term 'violent offender', and maybe even 'violent crime'. When you say 'violent crime', most people immediately think of murder and rape — but they're only a small fraction of the crimes that the law deems as violent.</p><p> In reality, the crime that puts the most people in prison in the US is robbery. And the law says that robbery is a violent crime whether a weapon is involved or not. By moving away from the catch-all category of 'violent criminals' we can judge the risk posed by individual people more sensibly.</p><p> Second, he thinks we should embrace the restorative justice movement. Instead of asking "What was the law? Who broke it? What should the punishment be", restorative justice asks "Who was harmed? Who harmed them? And what can we as a society, including the person who committed the harm, do to try to remedy that harm?"</p><p> Instead of being narrowly focused on how many years people should spend in prison as retribution, it starts a different conversation.</p><p> You might think this apparently softer approach would be unsatisfying to victims of crime. But James has discovered that a lot of victims of crime find that the current system doesn't help them in any meaningful way. What they primarily want to know is: why did this happen to me?</p><p> The best way to find that out is to actually talk to the person who harmed them, and in doing so gain a better understanding of the underlying factors behind the crime. The restorative justice approach facilitates these conversations in a way the current system doesn't allow, and can include restitution, apologies, and face-to-face reconciliation.</p><p> That’s just one topic of many covered in today’s episode, with much of the conversation focusing on Professor Forman’s 2018 book <em>Locking Up Our Own</em> — an examination of the historical roots of contemporary criminal justice practices in the US, and his experience setting up a charter school for at-risk youth in DC.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:02)</li><li>How did we get here? (00:04:07)</li><li>The role racism plays in policing today (00:14:47)</li><li>Black American views on policing and criminal justice (00:22:37)</li><li>Has the core argument of the book been controversial? (00:31:51)</li><li>The role that class divisions played in forming the current legal system (00:37:33)</li><li>What are the biggest problems today? (00:40:56)</li><li>What changes in policy would make the biggest difference? (00:52:41)</li><li>Shorter sentences for violent crimes (00:58:26)</li><li>Important recent successes (01:08:21)</li><li>What can people actually do to help? (01:14:38)</li></ul><p><br> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</strong></p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>No democracy has ever incarcerated as many people as the United States. To get its incarceration rate down to the global average, the US would have to release 3 in 4 people in its prisons today. </p><p> The effects on Black Americans have been especially severe — Black people make up 12% of the US population but 33% of its prison population. In the early 2000's when incarceration reached its peak, the US government estimated that 32% of Black boys would go to prison at some point in their lives, 5.5 times the figure for whites. </p><p> Contrary to popular understanding, nonviolent drug offenders make up <a href="https://www.prisonpolicy.org/reports/pie2020.html"><strong>less than a fifth</strong></a> of the incarcerated population. The only way to get its incarceration rate near the global average will be to shorten prison sentences for so-called 'violent criminals' — a politically toxic idea. But could we change that?</p><p> According to today’s guest, Professor James Forman Jr — a former public defender in Washington DC, Pulitzer Prize-winning author of <a href="https://80k.link/JFJ"><em>Locking Up Our Own: Crime and Punishment in Black America</em></a>, and now a professor at Yale Law School — there are two things we have to do to make that happen.</p><p> <a href="https://80k.link/JFJpod"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> First, he thinks we should lose the term 'violent offender', and maybe even 'violent crime'. When you say 'violent crime', most people immediately think of murder and rape — but they're only a small fraction of the crimes that the law deems as violent.</p><p> In reality, the crime that puts the most people in prison in the US is robbery. And the law says that robbery is a violent crime whether a weapon is involved or not. By moving away from the catch-all category of 'violent criminals' we can judge the risk posed by individual people more sensibly.</p><p> Second, he thinks we should embrace the restorative justice movement. Instead of asking "What was the law? Who broke it? What should the punishment be", restorative justice asks "Who was harmed? Who harmed them? And what can we as a society, including the person who committed the harm, do to try to remedy that harm?"</p><p> Instead of being narrowly focused on how many years people should spend in prison as retribution, it starts a different conversation.</p><p> You might think this apparently softer approach would be unsatisfying to victims of crime. But James has discovered that a lot of victims of crime find that the current system doesn't help them in any meaningful way. What they primarily want to know is: why did this happen to me?</p><p> The best way to find that out is to actually talk to the person who harmed them, and in doing so gain a better understanding of the underlying factors behind the crime. The restorative justice approach facilitates these conversations in a way the current system doesn't allow, and can include restitution, apologies, and face-to-face reconciliation.</p><p> That’s just one topic of many covered in today’s episode, with much of the conversation focusing on Professor Forman’s 2018 book <em>Locking Up Our Own</em> — an examination of the historical roots of contemporary criminal justice practices in the US, and his experience setting up a charter school for at-risk youth in DC.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:02)</li><li>How did we get here? (00:04:07)</li><li>The role racism plays in policing today (00:14:47)</li><li>Black American views on policing and criminal justice (00:22:37)</li><li>Has the core argument of the book been controversial? (00:31:51)</li><li>The role that class divisions played in forming the current legal system (00:37:33)</li><li>What are the biggest problems today? (00:40:56)</li><li>What changes in policy would make the biggest difference? (00:52:41)</li><li>Shorter sentences for violent crimes (00:58:26)</li><li>Important recent successes (01:08:21)</li><li>What can people actually do to help? (01:14:38)</li></ul><p><br> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</strong></p><p> <em>Producer: Keiran Harris.<br> Audio mastering: Ben Cordell.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 27 Jul 2020 22:07:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b2698626/f69ecc8b.mp3" length="43344995" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lWT-KArl_XIq3zQYwp6cuv5Llm_NI6tU67St910GDYM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTYv/MTY4MzU0NDY0Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5288</itunes:duration>
      <itunes:summary>No democracy has ever incarcerated as many people as the United States. To get its incarceration rate down to the global average, the US would have to release 3 in 4 people in its prisons today.  

The effects on Black Americans have been especially severe — Black people make up 12% of the US population but 33% of its prison population. In the early 2000's when incarceration reached its peak, the US government estimated that 32% of Black boys would go to prison at some point in their lives, 5.5 times the figure for whites. 

Contrary to popular understanding, nonviolent drug offenders make up less than a fifth of the incarcerated population. The only way to get its incarceration rate near the global average will be to shorten prison sentences for so-called 'violent criminals' — a politically toxic idea. But could we change that? 

According to today’s guest, Professor James Forman Jr — a former public defender in Washington DC, Pulitzer Prize-winning author of Locking Up Our Own: Crime and Punishment in Black America, and now a professor at Yale Law School — there are two things we have to do to make that happen. 

Links to learn more, summary and full transcript. 

First, he thinks we should lose the term 'violent offender', and maybe even 'violent crime'. When you say 'violent crime', most people immediately think of murder and rape — but they're only a small fraction of the crimes that the law deems as violent. 

In reality, the crime that puts the most people in prison in the US is robbery. And the law says that robbery is a violent crime whether a weapon is involved or not. By moving away from the catch-all category of 'violent criminals' we can judge the risk posed by individual people more sensibly.  

Second, he thinks we should embrace the restorative justice movement. Instead of asking "What was the law? Who broke it? What should the punishment be", restorative justice asks "Who was harmed? Who harmed them? And what can we as a society, including the person who committed the harm, do to try to remedy that harm?" 

Instead of being narrowly focused on how many years people should spend in prison as retribution, it starts a different conversation.  

You might think this apparently softer approach would be unsatisfying to victims of crime.  But James has discovered that a lot of victims of crime find that the current system doesn't help them in any meaningful way. What they primarily want to know is: why did this happen to me?  

The best way to find that out is to actually talk to the person who harmed them, and in doing so gain a better understanding of the underlying factors behind the crime. The restorative justice approach facilitates these conversations in a way the current system doesn't allow, and can include restitution, apologies, and face-to-face reconciliation. 

That’s just one topic of many covered in today’s episode, with much of the conversation focusing on Professor Forman’s 2018 book Locking Up Our Own — an examination of the historical roots of contemporary criminal justice practices in the US, and his experience setting up a charter school for at-risk youth in DC. 

Rob and James also discuss: 

• How racism shaped the US criminal legal system 
• How Black America viewed policing through the 20th century 
• How class divisions fostered a 'tough on crime' approach 
• How you can have a positive impact as a public prosecutor 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>No democracy has ever incarcerated as many people as the United States. To get its incarceration rate down to the global average, the US would have to release 3 in 4 people in its prisons today.  

The effects on Black Americans have been especially sev</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/b2698626/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#81 - Ben Garfinkel on scrutinising classic AI risk arguments</title>
      <itunes:title>#81 - Ben Garfinkel on scrutinising classic AI risk arguments</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c2242b1a-c1f3-11ea-9bab-0e13d48c95ef</guid>
      <link>https://share.transistor.fm/s/059e0ce2</link>
      <description>
        <![CDATA[<p>80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments.</p><p> 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment.</p><p> 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances.</p><p> 

<a href="https://80k.link/BostromTT">Nick Bostrom</a> wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents.</p><p> 

<a href="https://80k.link/BGpod"><b>Links to learn more, summary and full transcript.</b></a></p><p>

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world.</p><p> 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and <a href="https://80k.link/pcm">toy thought experiments</a>. And he doesn’t think it’s clear we should take these as a strong source of evidence.</p><p> 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.</p><p>  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences?</p><p> 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.</p><p>  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.</p><p>  

This is the second episode hosted by our Strategy Advisor Howie Lempel, and he and Ben cover, among many other things:</p><p> 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse<br> 
• The possibility of permanently locking in a positive or negative future<br> 
• Contenders for types of advanced systems<br> 
• What role AI should play in the effective altruism portfolio</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments.</p><p> 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment.</p><p> 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances.</p><p> 

<a href="https://80k.link/BostromTT">Nick Bostrom</a> wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents.</p><p> 

<a href="https://80k.link/BGpod"><b>Links to learn more, summary and full transcript.</b></a></p><p>

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world.</p><p> 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and <a href="https://80k.link/pcm">toy thought experiments</a>. And he doesn’t think it’s clear we should take these as a strong source of evidence.</p><p> 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.</p><p>  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences?</p><p> 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.</p><p>  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.</p><p>  

This is the second episode hosted by our Strategy Advisor Howie Lempel, and he and Ben cover, among many other things:</p><p> 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse<br> 
• The possibility of permanently locking in a positive or negative future<br> 
• Contenders for types of advanced systems<br> 
• What role AI should play in the effective altruism portfolio</p><p> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b></p><p>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 09 Jul 2020 17:42:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/059e0ce2/6a3734b1.mp3" length="76420178" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/YPozb4o6xuZTz5pFVLvxHIGiQObyF3o-xo9Fay2FNmY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTUv/MTY4MzU0NDY0MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9508</itunes:duration>
      <itunes:summary>80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments. 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment. 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances. 

Nick Bostrom wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents. 

Links to learn more, summary and full transcript.

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world. 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and toy thought experiments. And he doesn’t think it’s clear we should take these as a strong source of evidence. 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences? 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.  

This is the second episode hosted by our Strategy Advisor Howie Lempel, and he and Ben cover, among many other things: 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse 
• The possibility of permanently locking in a positive or negative future 
• Contenders for types of advanced systems 
• What role AI should play in the effective altruism portfolio 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/059e0ce2/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Advice on how to read our advice (Article)</title>
      <itunes:title>Advice on how to read our advice (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">b77b1f98-ba37-11ea-a113-0e5125d2c025</guid>
      <link>https://share.transistor.fm/s/a09d3f47</link>
      <description>
        <![CDATA[<p><i>This is the fourth release in our new series of audio articles. If you want to read the original article or check out the links within it, you can find them <a href="https://80000hours.org/articles/advice-on-how-to-read-our-advice/?utm_campaign=podcast__advice-on-our-advice-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>. </i></p><p>

"We’ve found that readers sometimes interpret or apply our advice in ways we didn’t anticipate and wouldn’t exactly recommend. That’s hard to avoid when you’re writing for a range of people with different personalities and initial views. </p><p>

To help get on the same page, here’s some advice about our advice, for those about to launch into reading our site. </p><p>

We want our writing to inform people’s views, but only in proportion to the likelihood that we’re actually right. So we need to make sure you have a balanced perspective on how compelling the evidence is for the different claims we make on the site, and how much weight to put on our advice in your situation. </p><p>

This piece includes a list of points to bear in mind when reading our site, and some thoughts on how to avoid the communication problems we face..." </p><p>

As the title suggests, this was written with our web site content in mind, but plenty of it applies to the careers sections of the podcast too — as well as our bonus episodes with members of the 80,000 Hours team, such as Arden and Rob’s episode on demandingness, work-life balance and injustice, which aired on February 25th of this year. </p><p>

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><i>This is the fourth release in our new series of audio articles. If you want to read the original article or check out the links within it, you can find them <a href="https://80000hours.org/articles/advice-on-how-to-read-our-advice/?utm_campaign=podcast__advice-on-our-advice-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>. </i></p><p>

"We’ve found that readers sometimes interpret or apply our advice in ways we didn’t anticipate and wouldn’t exactly recommend. That’s hard to avoid when you’re writing for a range of people with different personalities and initial views. </p><p>

To help get on the same page, here’s some advice about our advice, for those about to launch into reading our site. </p><p>

We want our writing to inform people’s views, but only in proportion to the likelihood that we’re actually right. So we need to make sure you have a balanced perspective on how compelling the evidence is for the different claims we make on the site, and how much weight to put on our advice in your situation. </p><p>

This piece includes a list of points to bear in mind when reading our site, and some thoughts on how to avoid the communication problems we face..." </p><p>

As the title suggests, this was written with our web site content in mind, but plenty of it applies to the careers sections of the podcast too — as well as our bonus episodes with members of the 80,000 Hours team, such as Arden and Rob’s episode on demandingness, work-life balance and injustice, which aired on February 25th of this year. </p><p>

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org.</p>]]>
      </content:encoded>
      <pubDate>Mon, 29 Jun 2020 19:38:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a09d3f47/92c9db53.mp3" length="7560989" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/splkCU7gR7o3xIxv9j-k5gLvB_mZhqTFh8XUspu6iwg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTQv/MTY4MzU0NDY0MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>923</itunes:duration>
      <itunes:summary>This is the fourth release in our new series of audio articles. If you want to read the original article or check out the links within it, you can find them here. 

"We’ve found that readers sometimes interpret or apply our advice in ways we didn’t anticipate and wouldn’t exactly recommend. That’s hard to avoid when you’re writing for a range of people with different personalities and initial views. 

To help get on the same page, here’s some advice about our advice, for those about to launch into reading our site. 

We want our writing to inform people’s views, but only in proportion to the likelihood that we’re actually right. So we need to make sure you have a balanced perspective on how compelling the evidence is for the different claims we make on the site, and how much weight to put on our advice in your situation. 

This piece includes a list of points to bear in mind when reading our site, and some thoughts on how to avoid the communication problems we face..." 

As the title suggests, this was written with our web site content in mind, but plenty of it applies to the careers sections of the podcast too — as well as our bonus episodes with members of the 80,000 Hours team, such as Arden and Rob’s episode on demandingness, work-life balance and injustice, which aired on February 25th of this year. 

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org.</itunes:summary>
      <itunes:subtitle>This is the fourth release in our new series of audio articles. If you want to read the original article or check out the links within it, you can find them here. 

"We’ve found that readers sometimes interpret or apply our advice in ways we didn’t antici</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/a09d3f47/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#80 – Stuart Russell on why our approach to AI is broken and how to fix it</title>
      <itunes:title>#80 – Stuart Russell on why our approach to AI is broken and how to fix it</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">14261466-b4d7-11ea-a57b-0e9ad5d55cf9</guid>
      <link>https://80000hours.org/podcast/episodes/stuart-russell-human-compatible-ai/</link>
      <description>
        <![CDATA[<p>Stuart Russell, Professor at UC Berkeley and co-author of the <a href="http://aima.cs.berkeley.edu/"><strong>most popular AI textbook</strong></a>, thinks the way we approach machine learning today is fundamentally flawed.</p><p> In his new book, <a href="https://80k.link/hcbook"><em>Human Compatible</em></a>, he outlines the 'standard model' of AI development, in which intelligence is measured as the ability to achieve some definite, completely-known objective that we've stated explicitly. This is so obvious it almost doesn't even seem like a design choice, but it is.</p><p> Unfortunately there's a big problem with this approach: it's incredibly hard to say exactly what you want. AI today lacks common sense, and simply does whatever we've asked it to. That's true even if the goal isn't what we really want, or the methods it's choosing are ones we would never accept.</p><p> We already see <a href="https://80k.link/aiwe"><strong>AIs misbehaving</strong></a> for this reason. Stuart points to the example of YouTube's recommender algorithm, which reportedly nudged users towards extreme political views because that made it easier to keep them on the site. This isn't something we wanted, but it helped achieve the algorithm's objective: maximise viewing time.</p><p> Like King Midas, who asked to be able to turn everything into gold but ended up unable to eat, we get too much of what we've asked for.</p><p> <a href="https://80k.link/srpod"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This 'alignment' problem will get more and more severe as machine learning is embedded in more and more places: recommending us news, operating power grids, deciding prison sentences, doing surgery, and fighting wars. If we're ever to hand over much of the economy to thinking machines, we can't count on ourselves correctly saying exactly what we want the AI to do every time.</p><p> Stuart isn't just dissatisfied with the current model though, he has a specific solution. According to him we need to redesign AI around 3 principles:</p><p> 1. The AI system's objective is to achieve what humans want.<br> 2. But the system isn't sure what we want.<br> 3. And it figures out what we want by observing our behaviour.<br> Stuart thinks this design architecture, if implemented, would be a big step forward towards reliably beneficial AI. </p><p> For instance, a machine built on these principles would be happy to be turned off if that's what its owner thought was best, while one built on the standard model should resist being turned off because being deactivated prevents it from achieving its goal. As Stuart says, "you can't fetch the coffee if you're dead."</p><p> These principles lend themselves towards machines that are modest and cautious, and check in when they aren't confident they're truly achieving what we want.</p><p> We've made progress toward putting these principles into practice, but the remaining engineering problems are substantial. Among other things, the resulting AIs need to be able to interpret what people really mean to say based on the context of a situation. And they need to guess when we've rejected an option because we've considered it and decided it's a bad idea, and when we simply haven't thought about it at all.</p><p> Stuart thinks all of these problems are surmountable, if we put in the work. The harder problems may end up being social and political.</p><p> When each of us can have an AI of our own — one smarter than any person — how do we resolve conflicts between people and their AI agents? And if AIs end up doing most work that people do today, how can humans avoid becoming enfeebled, like lazy children tended to by machines, but not intellectually developed enough to know what they really want?</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:19:06)</li><li>Human Compatible: Artificial Intelligence and the Problem of Control (00:21:27)</li><li>Principles for Beneficial Machines (00:29:25)</li><li>AI moral rights (00:33:05)</li><li>Humble machines (00:39:35)</li><li>Learning to predict human preferences (00:45:55)</li><li>Animals and AI (00:49:33)</li><li>Enfeeblement problem (00:58:21)</li><li>Counterarguments (01:07:09)</li><li>Orthogonality thesis (01:24:25)</li><li>Intelligence explosion (01:29:15)</li><li>Policy ideas (01:38:39)</li><li>What most needs to be done (01:50:14)</li></ul><p><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Stuart Russell, Professor at UC Berkeley and co-author of the <a href="http://aima.cs.berkeley.edu/"><strong>most popular AI textbook</strong></a>, thinks the way we approach machine learning today is fundamentally flawed.</p><p> In his new book, <a href="https://80k.link/hcbook"><em>Human Compatible</em></a>, he outlines the 'standard model' of AI development, in which intelligence is measured as the ability to achieve some definite, completely-known objective that we've stated explicitly. This is so obvious it almost doesn't even seem like a design choice, but it is.</p><p> Unfortunately there's a big problem with this approach: it's incredibly hard to say exactly what you want. AI today lacks common sense, and simply does whatever we've asked it to. That's true even if the goal isn't what we really want, or the methods it's choosing are ones we would never accept.</p><p> We already see <a href="https://80k.link/aiwe"><strong>AIs misbehaving</strong></a> for this reason. Stuart points to the example of YouTube's recommender algorithm, which reportedly nudged users towards extreme political views because that made it easier to keep them on the site. This isn't something we wanted, but it helped achieve the algorithm's objective: maximise viewing time.</p><p> Like King Midas, who asked to be able to turn everything into gold but ended up unable to eat, we get too much of what we've asked for.</p><p> <a href="https://80k.link/srpod"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This 'alignment' problem will get more and more severe as machine learning is embedded in more and more places: recommending us news, operating power grids, deciding prison sentences, doing surgery, and fighting wars. If we're ever to hand over much of the economy to thinking machines, we can't count on ourselves correctly saying exactly what we want the AI to do every time.</p><p> Stuart isn't just dissatisfied with the current model though, he has a specific solution. According to him we need to redesign AI around 3 principles:</p><p> 1. The AI system's objective is to achieve what humans want.<br> 2. But the system isn't sure what we want.<br> 3. And it figures out what we want by observing our behaviour.<br> Stuart thinks this design architecture, if implemented, would be a big step forward towards reliably beneficial AI. </p><p> For instance, a machine built on these principles would be happy to be turned off if that's what its owner thought was best, while one built on the standard model should resist being turned off because being deactivated prevents it from achieving its goal. As Stuart says, "you can't fetch the coffee if you're dead."</p><p> These principles lend themselves towards machines that are modest and cautious, and check in when they aren't confident they're truly achieving what we want.</p><p> We've made progress toward putting these principles into practice, but the remaining engineering problems are substantial. Among other things, the resulting AIs need to be able to interpret what people really mean to say based on the context of a situation. And they need to guess when we've rejected an option because we've considered it and decided it's a bad idea, and when we simply haven't thought about it at all.</p><p> Stuart thinks all of these problems are surmountable, if we put in the work. The harder problems may end up being social and political.</p><p> When each of us can have an AI of our own — one smarter than any person — how do we resolve conflicts between people and their AI agents? And if AIs end up doing most work that people do today, how can humans avoid becoming enfeebled, like lazy children tended to by machines, but not intellectually developed enough to know what they really want?</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:19:06)</li><li>Human Compatible: Artificial Intelligence and the Problem of Control (00:21:27)</li><li>Principles for Beneficial Machines (00:29:25)</li><li>AI moral rights (00:33:05)</li><li>Humble machines (00:39:35)</li><li>Learning to predict human preferences (00:45:55)</li><li>Animals and AI (00:49:33)</li><li>Enfeeblement problem (00:58:21)</li><li>Counterarguments (01:07:09)</li><li>Orthogonality thesis (01:24:25)</li><li>Intelligence explosion (01:29:15)</li><li>Policy ideas (01:38:39)</li><li>What most needs to be done (01:50:14)</li></ul><p><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 22 Jun 2020 23:17:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/19c5bb44/6dc902af.mp3" length="64348592" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/UfXK9ni2jofgMRVFR8TkNFqhu2OlfTujgnc4nkAbZ-Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTMv/MTY4MzU0NDYzOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7997</itunes:duration>
      <itunes:summary>Stuart Russell, Professor at UC Berkeley and co-author of the most popular AI textbook, thinks the way we approach machine learning today is fundamentally flawed.

In his new book, Human Compatible, he outlines the 'standard model' of AI development, in which intelligence is measured as the ability to achieve some definite, completely-known objective that we've stated explicitly. This is so obvious it almost doesn't even seem like a design choice, but it is.

Unfortunately there's a big problem with this approach: it's incredibly hard to say exactly what you want. AI today lacks common sense, and simply does whatever we've asked it to. That's true even if the goal isn't what we really want, or the methods it's choosing are ones we would never accept.

We already see AIs misbehaving for this reason. Stuart points to the example of YouTube's recommender algorithm, which reportedly nudged users towards extreme political views because that made it easier to keep them on the site. This isn't something we wanted, but it helped achieve the algorithm's objective: maximise viewing time.

Like King Midas, who asked to be able to turn everything into gold but ended up unable to eat, we get too much of what we've asked for.

Links to learn more, summary and full transcript.

This 'alignment' problem will get more and more severe as machine learning is embedded in more and more places: recommending us news, operating power grids, deciding prison sentences, doing surgery, and fighting wars. If we're ever to hand over much of the economy to thinking machines, we can't count on ourselves correctly saying exactly what we want the AI to do every time.

Stuart isn't just dissatisfied with the current model though, he has a specific solution. According to him we need to redesign AI around 3 principles:

1. The AI system's objective is to achieve what humans want.
2. But the system isn't sure what we want.
3. And it figures out what we want by observing our behaviour.

Stuart thinks this design architecture, if implemented, would be a big step forward towards reliably beneficial AI. 

For instance, a machine built on these principles would be happy to be turned off if that's what its owner thought was best, while one built on the standard model should resist being turned off because being deactivated prevents it from achieving its goal. As Stuart says, "you can't fetch the coffee if you're dead."

These principles lend themselves towards machines that are modest and cautious, and check in when they aren't confident they're truly achieving what we want.

We've made progress toward putting these principles into practice, but the remaining engineering problems are substantial. Among other things, the resulting AIs need to be able to interpret what people really mean to say based on the context of a situation. And they need to guess when we've rejected an option because we've considered it and decided it's a bad idea, and when we simply haven't thought about it at all.

Stuart thinks all of these problems are surmountable, if we put in the work. The harder problems may end up being social and political.

When each of us can have an AI of our own — one smarter than any person — how do we resolve conflicts between people and their AI agents?

And if AIs end up doing most work that people do today, how can humans avoid becoming enfeebled, like lazy children tended to by machines, but not intellectually developed enough to know what they really want?

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Stuart Russell, Professor at UC Berkeley and co-author of the most popular AI textbook, thinks the way we approach machine learning today is fundamentally flawed.

In his new book, Human Compatible, he outlines the 'standard model' of AI development, in</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/19c5bb44/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>What anonymous contributors think about important life and career questions (Article)</title>
      <itunes:title>What anonymous contributors think about important life and career questions (Article)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">da846b14-a742-11ea-8629-1292b68da39f</guid>
      <link>https://share.transistor.fm/s/64437173</link>
      <description>
        <![CDATA[Today we’re launching the final entry of our ‘anonymous answers' series on the website.<p> 

It features answers to 23 different questions including <i>“How have you seen talented people fail in their work?”</i> and <i>“What’s one way to be successful you don’t think people talk about enough?”</i>, from anonymous people whose work we admire.</p><p> 

We thought a lot of the responses were really interesting; some were provocative, others just surprising. And as intended, they span a very wide range of opinions.</p><p> 

So we decided to share some highlights here with you podcast subscribers. This is only a sample though, including a few answers from just 10 of those 23 questions.</p><p> 

You can find the rest of the answers at <a href="https://80000hours.org/articles/anonymous-answers/?utm_campaign=podcast__anonymous-answers-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/anonymous</b></a> or follow a link here to an individual entry:</p><p> 

1. What's good career <a href="https://80000hours.org/2019/10/anonymous-advice-careers/"><b>advice you wouldn’t want to have your name on?</b></a><br>
2. How have you seen <a href="https://80000hours.org/2019/10/anonymous-advice-fail-at-work/"><b>talented people fail in their work?</b></a><br>
3. What’s the thing <a href="https://80000hours.org/2019/11/anonymous-answers-most-overrated/"><b>people most overrate in their career?</b></a><br>
4. If you were at the start of your career again, <a href="https://80000hours.org/2019/11/anonymous-answers-personal-reflections/"><b>what would you do differently this time?</b></a> <br>
5. If you're a talented young person <a href="https://80000hours.org/2019/11/anonymous-answers-risk-aversion/"><b>how risk averse should you be?</b></a><br>
6. Among people trying to improve the world, <a href="https://80000hours.org/2019/12/anonymous-answers-bad-habits/"><b>what are the bad habits you see most often?</b></a><br>
7. What mistakes do people most often make <a href="https://80000hours.org/2019/12/anon-answers-what-to-work-on/"><b>when deciding what work to do?</b></a><br>
8. What's one way to be successful you don't think people <a href="https://80000hours.org/2020/01/anon-answers-one-way-successful/"><b>talk about enough?</b></a><br>
9. How honest &amp; candid should high-profile people <a href="https://80000hours.org/2020/02/anon-answers-honesty/"><b>really be?</b></a><br>
10. What’s some underrated <a href="https://80000hours.org/2020/02/anonymous-answers-general-life-advice/"><b>general life advice?</b></a><br>
11. Should the effective altruism community grow <a href="https://80000hours.org/2020/02/anonymous-answers-effective-altruism-community-and-growth/"><b>faster or slower? And should it be broader, or narrower?</b></a><br>
12. What are the <a href="https://80000hours.org/2020/02/anonymous-answers-flaws-80000hours/"><b>biggest flaws of 80,000 Hours?</b></a><br>
13. What are the <a href="https://80000hours.org/2020/02/anonymous-answers-flaws-effective-altruism-community"><b>biggest flaws of the effective altruism community?</b></a><br>
14. How should the effective altruism community <a href="https://80000hours.org/2020/04/anonymous-answers-diversity/"><b>think about diversity?</b></a><br>
15. Are there any myths that you feel obligated to support publicly? <a href="https://80000hours.org/2020/06/anonymous-answers-myths-and-other-questions/"><b>And five other questions.</b></a></p><p>

Finally, if you’d like us to produce more or less content like this, please let us know your opinion podcast@80000hours.org.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today we’re launching the final entry of our ‘anonymous answers' series on the website.<p> 

It features answers to 23 different questions including <i>“How have you seen talented people fail in their work?”</i> and <i>“What’s one way to be successful you don’t think people talk about enough?”</i>, from anonymous people whose work we admire.</p><p> 

We thought a lot of the responses were really interesting; some were provocative, others just surprising. And as intended, they span a very wide range of opinions.</p><p> 

So we decided to share some highlights here with you podcast subscribers. This is only a sample though, including a few answers from just 10 of those 23 questions.</p><p> 

You can find the rest of the answers at <a href="https://80000hours.org/articles/anonymous-answers/?utm_campaign=podcast__anonymous-answers-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>80000hours.org/anonymous</b></a> or follow a link here to an individual entry:</p><p> 

1. What's good career <a href="https://80000hours.org/2019/10/anonymous-advice-careers/"><b>advice you wouldn’t want to have your name on?</b></a><br>
2. How have you seen <a href="https://80000hours.org/2019/10/anonymous-advice-fail-at-work/"><b>talented people fail in their work?</b></a><br>
3. What’s the thing <a href="https://80000hours.org/2019/11/anonymous-answers-most-overrated/"><b>people most overrate in their career?</b></a><br>
4. If you were at the start of your career again, <a href="https://80000hours.org/2019/11/anonymous-answers-personal-reflections/"><b>what would you do differently this time?</b></a> <br>
5. If you're a talented young person <a href="https://80000hours.org/2019/11/anonymous-answers-risk-aversion/"><b>how risk averse should you be?</b></a><br>
6. Among people trying to improve the world, <a href="https://80000hours.org/2019/12/anonymous-answers-bad-habits/"><b>what are the bad habits you see most often?</b></a><br>
7. What mistakes do people most often make <a href="https://80000hours.org/2019/12/anon-answers-what-to-work-on/"><b>when deciding what work to do?</b></a><br>
8. What's one way to be successful you don't think people <a href="https://80000hours.org/2020/01/anon-answers-one-way-successful/"><b>talk about enough?</b></a><br>
9. How honest &amp; candid should high-profile people <a href="https://80000hours.org/2020/02/anon-answers-honesty/"><b>really be?</b></a><br>
10. What’s some underrated <a href="https://80000hours.org/2020/02/anonymous-answers-general-life-advice/"><b>general life advice?</b></a><br>
11. Should the effective altruism community grow <a href="https://80000hours.org/2020/02/anonymous-answers-effective-altruism-community-and-growth/"><b>faster or slower? And should it be broader, or narrower?</b></a><br>
12. What are the <a href="https://80000hours.org/2020/02/anonymous-answers-flaws-80000hours/"><b>biggest flaws of 80,000 Hours?</b></a><br>
13. What are the <a href="https://80000hours.org/2020/02/anonymous-answers-flaws-effective-altruism-community"><b>biggest flaws of the effective altruism community?</b></a><br>
14. How should the effective altruism community <a href="https://80000hours.org/2020/04/anonymous-answers-diversity/"><b>think about diversity?</b></a><br>
15. Are there any myths that you feel obligated to support publicly? <a href="https://80000hours.org/2020/06/anonymous-answers-myths-and-other-questions/"><b>And five other questions.</b></a></p><p>

Finally, if you’d like us to produce more or less content like this, please let us know your opinion podcast@80000hours.org.</p>]]>
      </content:encoded>
      <pubDate>Fri, 05 Jun 2020 17:40:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/64437173/a753b3e0.mp3" length="35906516" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/JXVLUzYK-KdiQRG_SKqqz1qpj2o-aUC9hILhDGBCN74/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTIv/MTY4MzU0NDYzOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2230</itunes:duration>
      <itunes:summary>Today we’re launching the final entry of our ‘anonymous answers' series on the website. 

It features answers to 23 different questions including “How have you seen talented people fail in their work?” and “What’s one way to be successful you don’t think people talk about enough?”, from anonymous people whose work we admire. 

We thought a lot of the responses were really interesting; some were provocative, others just surprising. And as intended, they span a very wide range of opinions. 

So we decided to share some highlights here with you podcast subscribers. This is only a sample though, including a few answers from just 10 of those 23 questions. 

You can find the rest of the answers at 80000hours.org/anonymous or follow a link here to an individual entry: 

1. What's good career advice you wouldn’t want to have your name on?
2. How have you seen talented people fail in their work?
3. What’s the thing people most overrate in their career?
4. If you were at the start of your career again, what would you do differently this time? 
5. If you're a talented young person how risk averse should you be?
6. Among people trying to improve the world, what are the bad habits you see most often?
7. What mistakes do people most often make when deciding what work to do?
8. What's one way to be successful you don't think people talk about enough?
9. How honest &amp;amp; candid should high-profile people really be?
10. What’s some underrated general life advice?
11. Should the effective altruism community grow faster or slower? And should it be broader, or narrower?
12. What are the biggest flaws of 80,000 Hours?
13. What are the biggest flaws of the effective altruism community?
14. How should the effective altruism community think about diversity?
15. Are there any myths that you feel obligated to support publicly? And five other questions.

Finally, if you’d like us to produce more or less content like this, please let us know your opinion podcast@80000hours.org.</itunes:summary>
      <itunes:subtitle>Today we’re launching the final entry of our ‘anonymous answers' series on the website. 

It features answers to 23 different questions including “How have you seen talented people fail in their work?” and “What’s one way to be successful you don’t think </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/64437173/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#79 – A.J. Jacobs on radical honesty, following the whole Bible, and reframing global problems as puzzles</title>
      <itunes:title>#79 – A.J. Jacobs on radical honesty, following the whole Bible, and reframing global problems as puzzles</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">eed1ea6a-a44a-11ea-892e-127f4bdf8959</guid>
      <link>https://80000hours.org/podcast/episodes/aj-jacobs-on-writing-reframing-problems-as-puzzles/</link>
      <description>
        <![CDATA[<p>Today’s guest, <em>New York Times</em> bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what? She's not so bad."</p><p> Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the <a href="https://80k.link/AJ1"><em>biggest family tree in history</em></a>.</p><p> He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His next book will ask: if we reframe global problems as puzzles, would the world be a better place?</p><p> <a href="https://80000hours.org/podcast/episodes/aj-jacobs-on-writing-reframing-problems-as-puzzles/?utm_campaign=podcast__aj-jacobs&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (<a href="https://80k.link/AJ2"><em>Radical Honesty.</em></a>)</p><p> We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (<a href="https://80k.link/AJ3"><em>Drop Dead Healthy.</em></a>)</p><p> Another reason to listen is for the facts:</p><ul><li>The Bayer aspirin company invented heroin as a cough suppressant</li><li>Coriander is just the British way of saying cilantro</li><li>Dogs have a third eyelid to protect the eyeball from irritants</li><li>A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples) (<a href="https://80k.link/AJ4"><em>The Know-It-All.</em></a>)</li></ul><p>One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the Bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (<a href="https://80k.link/AJ5"><em>The Year of Living Biblically.</em></a>)</p><p> I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and <a href="https://80k.link/CU3">Queen Jadwiga</a> for making bagels popular in the 1300s, which kept me going during the recording. (<a href="https://80k.link/AJ6"><em>Thanks a Thousand.</em></a>)</p><p> We also discuss:<br> • Blackmailing yourself<br> • The most extreme ideas A.J.’s ever considered<br> • Doing good as a writer<br> • And much more.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:51)</li><li>Puzzles (00:05:41)</li><li>Radical honesty (00:12:18)</li><li>The Year of Living Biblically (00:24:17)</li><li>Thanks A Thousand (00:38:04)</li><li>Drop Dead Healthy (00:49:22)</li><li>Blackmailing yourself (00:57:46)</li><li>The Know-It-All (01:03:00)</li><li>Effective altruism (01:31:38)</li><li>Longtermism (01:55:35)</li><li>It’s All Relative (02:01:00)</li><li>Journalism (02:10:06)</li><li>Writing careers (02:17:15)</li><li>Rob’s outro (02:34:37)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Zakee Ulhaq</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Today’s guest, <em>New York Times</em> bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what? She's not so bad."</p><p> Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the <a href="https://80k.link/AJ1"><em>biggest family tree in history</em></a>.</p><p> He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His next book will ask: if we reframe global problems as puzzles, would the world be a better place?</p><p> <a href="https://80000hours.org/podcast/episodes/aj-jacobs-on-writing-reframing-problems-as-puzzles/?utm_campaign=podcast__aj-jacobs&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (<a href="https://80k.link/AJ2"><em>Radical Honesty.</em></a>)</p><p> We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (<a href="https://80k.link/AJ3"><em>Drop Dead Healthy.</em></a>)</p><p> Another reason to listen is for the facts:</p><ul><li>The Bayer aspirin company invented heroin as a cough suppressant</li><li>Coriander is just the British way of saying cilantro</li><li>Dogs have a third eyelid to protect the eyeball from irritants</li><li>A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples) (<a href="https://80k.link/AJ4"><em>The Know-It-All.</em></a>)</li></ul><p>One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the Bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (<a href="https://80k.link/AJ5"><em>The Year of Living Biblically.</em></a>)</p><p> I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and <a href="https://80k.link/CU3">Queen Jadwiga</a> for making bagels popular in the 1300s, which kept me going during the recording. (<a href="https://80k.link/AJ6"><em>Thanks a Thousand.</em></a>)</p><p> We also discuss:<br> • Blackmailing yourself<br> • The most extreme ideas A.J.’s ever considered<br> • Doing good as a writer<br> • And much more.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:51)</li><li>Puzzles (00:05:41)</li><li>Radical honesty (00:12:18)</li><li>The Year of Living Biblically (00:24:17)</li><li>Thanks A Thousand (00:38:04)</li><li>Drop Dead Healthy (00:49:22)</li><li>Blackmailing yourself (00:57:46)</li><li>The Know-It-All (01:03:00)</li><li>Effective altruism (01:31:38)</li><li>Longtermism (01:55:35)</li><li>It’s All Relative (02:01:00)</li><li>Journalism (02:10:06)</li><li>Writing careers (02:17:15)</li><li>Rob’s outro (02:34:37)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Zakee Ulhaq</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 01 Jun 2020 22:08:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d410983a/25e699b7.mp3" length="152832007" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/m7qDb5sc2j6DpnWqPCV7aax2n3ftcYcjuSBSK8WfhTU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTEv/MTY4MzU0NDYzNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9527</itunes:duration>
      <itunes:summary>Today’s guest, New York Times bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what, she's not so bad". 

Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the biggest family tree in history. 

He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His next book will ask: if we reframe global problems as puzzles, would the world be a better place? 

Links to learn more, summary and full transcript.

This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (Radical Honesty.) 

We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (Drop Dead Healthy.) 

Another reason to listen is for the facts: 

• The Bayer aspirin company invented heroin as a cough suppressant
•  Coriander is just the British way of saying cilantro
•  Dogs have a third eyelid to protect the eyeball from irritants
•  and A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples). (The Know-It-All.) 

One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the Bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (The Year of Living Biblically.) 

I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and Queen Jadwiga for making bagels popular in the 1300s, which kept me going during the recording. (Thanks a Thousand.) 

We also discuss: 

• Blackmailing yourself 
• The most extreme ideas A.J.’s ever considered 
• Doing good as a writer 
• And much more. 

Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Today’s guest, New York Times bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, "You know what, she's not so bad". 

Hijacking this bias towards family and trying to broaden it t</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/d410983a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/d410983a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#78 – Danny Hernandez on forecasting and the drivers of AI progress</title>
      <itunes:title>#78 – Danny Hernandez on forecasting and the drivers of AI progress</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a0d93b94-9b74-11ea-b611-1204eba4b6f3</guid>
      <link>https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/</link>
      <description>
        <![CDATA[<p>Companies use about 300,000 times more computation training the best AI systems today than they did in 2012 and algorithmic innovations have also made them 25 times more efficient at the same tasks.</p><p>These are the headline results of two recent papers — <a href="https://openai.com/blog/ai-and-compute/">AI and Compute</a> and <a href="https://openai.com/blog/ai-and-efficiency/">AI and Efficiency</a> — from the <em>Foresight Team</em> at OpenAI. In today's episode I spoke with one of the authors, Danny Hernandez, who joined OpenAI after helping develop better forecasting methods at <a href="https://www.twitch.tv/">Twitch</a> and <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>. </p><p> Danny and I talk about how to understand his team's results and what they mean (and don't mean) for how we should think about progress in AI going forward.</p><p> <a href="https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/?utm_campaign=podcast__danny-hernandez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Debates around the future of AI can sometimes be pretty abstract and theoretical. Danny hopes that providing rigorous measurements of some of the inputs to AI progress so far can help us better understand what causes that progress, as well as ground debates about the future of AI in a better shared understanding of the field.</p><p> If this research sounds appealing, you might be interested in applying to join OpenAI's Foresight team — <a href="https://jobs.lever.co/openai/2b4e17f4-d3be-4ac9-be21-664c211c413a">they're currently hiring research engineers</a>.</p><p> In the interview, Danny and I (Arden Koehler) also discuss a range of other topics, including:<br> • The question of which experts to believe<br> • Danny's journey to working at OpenAI<br> • The usefulness of "decision boundaries"<br> • The importance of Moore's law for people who care about the long-term future<br> • What OpenAI's Foresight Team's findings might imply for policy<br> • The question whether progress in the performance of AI systems is linear<br> • The safety teams at OpenAI and who they're looking to hire<br> • One idea for finding someone to guide your learning<br> • The importance of hardware expertise for making a positive impact</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:29)</li><li>Forecasting (00:07:11)</li><li>Improving the public conversation around AI (00:14:41)</li><li>Danny’s path to OpenAI (00:24:08)</li><li>Calibration training (00:27:18)</li><li>AI and Compute (00:45:22)</li><li>AI and Efficiency (01:09:22)</li><li>Safety teams at OpenAI (01:39:03)</li><li>Careers (01:49:46)</li><li>AI hardware as a possible path to impact (01:55:57)</li><li>Triggers for people’s major decisions (02:08:44)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Zakee Ulhaq</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Companies use about 300,000 times more computation training the best AI systems today than they did in 2012 and algorithmic innovations have also made them 25 times more efficient at the same tasks.</p><p>These are the headline results of two recent papers — <a href="https://openai.com/blog/ai-and-compute/">AI and Compute</a> and <a href="https://openai.com/blog/ai-and-efficiency/">AI and Efficiency</a> — from the <em>Foresight Team</em> at OpenAI. In today's episode I spoke with one of the authors, Danny Hernandez, who joined OpenAI after helping develop better forecasting methods at <a href="https://www.twitch.tv/">Twitch</a> and <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>. </p><p> Danny and I talk about how to understand his team's results and what they mean (and don't mean) for how we should think about progress in AI going forward.</p><p> <a href="https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/?utm_campaign=podcast__danny-hernandez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Debates around the future of AI can sometimes be pretty abstract and theoretical. Danny hopes that providing rigorous measurements of some of the inputs to AI progress so far can help us better understand what causes that progress, as well as ground debates about the future of AI in a better shared understanding of the field.</p><p> If this research sounds appealing, you might be interested in applying to join OpenAI's Foresight team — <a href="https://jobs.lever.co/openai/2b4e17f4-d3be-4ac9-be21-664c211c413a">they're currently hiring research engineers</a>.</p><p> In the interview, Danny and I (Arden Koehler) also discuss a range of other topics, including:<br> • The question of which experts to believe<br> • Danny's journey to working at OpenAI<br> • The usefulness of "decision boundaries"<br> • The importance of Moore's law for people who care about the long-term future<br> • What OpenAI's Foresight Team's findings might imply for policy<br> • The question whether progress in the performance of AI systems is linear<br> • The safety teams at OpenAI and who they're looking to hire<br> • One idea for finding someone to guide your learning<br> • The importance of hardware expertise for making a positive impact</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:29)</li><li>Forecasting (00:07:11)</li><li>Improving the public conversation around AI (00:14:41)</li><li>Danny’s path to OpenAI (00:24:08)</li><li>Calibration training (00:27:18)</li><li>AI and Compute (00:45:22)</li><li>AI and Efficiency (01:09:22)</li><li>Safety teams at OpenAI (01:39:03)</li><li>Careers (01:49:46)</li><li>AI hardware as a possible path to impact (01:55:57)</li><li>Triggers for people’s major decisions (02:08:44)</li></ul><p><em>Producer: Keiran Harris<br>Audio mastering: Ben Cordell<br>Transcriptions: Zakee Ulhaq</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 22 May 2020 16:17:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b7805c03/698cd720.mp3" length="126474676" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/o-SImPjKdVCNDohC6mNoAsPSBFVel8VQNYksNNGiIL4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MTAv/MTY4MzU0NDYzNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7897</itunes:duration>
      <itunes:summary>Companies use about 300,000 times more computation training the best AI systems today than they did in 2012 and algorithmic innovations have also made them 25 times more efficient at the same tasks.  

These are the headline results of two recent papers — AI and Compute and AI and Efficiency — from the Foresight Team at OpenAI. In today's episode I spoke with one of the authors, Danny Hernandez, who joined OpenAI after helping develop better forecasting methods at Twitch and Open Philanthropy.    

Danny and I talk about how to understand his team's results and what they mean (and don't mean) for how we should think about progress in AI going forward.  

Links to learn more, summary and full transcript. 

Debates around the future of AI can sometimes be pretty abstract and theoretical. Danny hopes that providing rigorous measurements of some of the inputs to AI progress so far can help us better understand what causes that progress, as well as ground debates about the future of AI in a better shared understanding of the field.   

If this research sounds appealing, you might be interested in applying to join OpenAI's Foresight team — they're currently hiring research engineers.   

In the interview, Danny and I (Arden Koehler) also discuss a range of other topics, including:  

• The question of which experts to believe 
• Danny's journey to working at OpenAI 
• The usefulness of "decision boundaries" 
• The importance of Moore's law for people who care about the long-term future 
• What OpenAI's Foresight Team's findings might imply for policy 
• The question whether progress in the performance of AI systems is linear 
• The safety teams at OpenAI and who they're looking to hire 
• One idea for finding someone to guide your learning 
• The importance of hardware expertise for making a positive impact  

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript. 


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Companies use about 300,000 times more computation training the best AI systems today than they did in 2012 and algorithmic innovations have also made them 25 times more efficient at the same tasks.  

These are the headline results of two recent papers</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/b7805c03/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/b7805c03/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#77 – Marc Lipsitch on whether we're winning or losing against COVID-19</title>
      <itunes:title>#77 – Marc Lipsitch on whether we're winning or losing against COVID-19</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">65e57a50-995a-11ea-b994-122380125305</guid>
      <link>https://80000hours.org/podcast/episodes/marc-lipsitch-winning-or-losing-against-covid19-and-epidemiology/</link>
      <description>
        <![CDATA[<p>In March Professor Marc Lipsitch — Director of Harvard's <em>Center for Communicable Disease Dynamics</em> — abruptly found himself a global celebrity, his social media following growing 40-fold and journalists knocking down his door, as everyone turned to him for information they could trust.</p><p> Here he lays out where the fight against COVID-19 stands today, why he's open to deliberately giving people COVID-19 to speed up vaccine development, and how we could do better next time.</p><p> As Marc tells us, island nations like Taiwan and New Zealand are successfully suppressing SARS-COV-2. But everyone else is struggling.</p><p> <a href="https://80000hours.org/podcast/episodes/marc-lipsitch-winning-or-losing-against-covid19-and-epidemiology/?utm_campaign=podcast__marc-lipsitch&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Even Singapore, with plenty of warning and one of the best test and trace systems in the world, lost control of the virus in mid-April after successfully holding back the tide for 2 months.</p><p> This doesn't bode well for how the US or Europe will cope as they ease their lockdowns. It also suggests it would have been exceedingly hard for China to stop the virus before it spread overseas.</p><p> But sadly, there's no easy way out.</p><p> The original estimates of COVID-19's infection fatality rate, of 0.5-1%, have turned out to be basically right. And the latest serology surveys indicate only 5-10% of people in countries like the US, UK and Spain have been infected so far, leaving us far short of herd immunity. To get there, even these worst affected countries would need to endure something like ten times the number of deaths they have so far.</p><p> Marc has one good piece of news: research suggests that most of those who get infected do indeed develop immunity, for a while at least.</p><p> To escape the COVID-19 trap sooner rather than later, Marc recommends we go hard on all the familiar options — vaccines, antivirals, and mass testing — but also open our minds to creative options we've so far left on the shelf.</p><p> Despite the importance of his work, even now the training and grant programs that produced the community of experts Marc is a part of, are shrinking. We look at a new article he's written about how to instead build and improve the field of epidemiology, so humanity can respond faster and smarter next time we face a disease that could kill millions and cost tens of trillions of dollars.</p><p> We also cover:</p><p> • How listeners might contribute as future contagious disease experts, or donors to current projects<br> • How we can learn from cross-country comparisons<br> • Modelling that has gone wrong in an instructive way<br> • What governments should stop doing<br> • How people can figure out who to trust, and who has been most on the mark this time<br> • Why Marc supports infecting people with COVID-19 to speed up the development of a vaccines<br> • How we can ensure there's population-level surveillance early during the next pandemic<br> • Whether people from other fields trying to help with COVID-19 has done more good than harm<br> • Whether it's experts in diseases, or experts in forecasting, who produce better disease forecasts</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:45)</li><li>Things Rob wishes he knew about COVID-19 (00:05:23)</li><li>Cross-country comparisons (00:10:53)</li><li>Any government activities we should stop? (00:21:24)</li><li>Lessons from COVID-19 (00:33:31)</li><li>Global catastrophic biological risks (00:37:58)</li><li>Human challenge trials (00:43:12)</li><li>Disease surveillance (00:50:07)</li><li>Who should we trust? (00:58:12)</li><li>Epidemiology as a field (01:13:05)</li><li>Careers (01:31:28)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In March Professor Marc Lipsitch — Director of Harvard's <em>Center for Communicable Disease Dynamics</em> — abruptly found himself a global celebrity, his social media following growing 40-fold and journalists knocking down his door, as everyone turned to him for information they could trust.</p><p> Here he lays out where the fight against COVID-19 stands today, why he's open to deliberately giving people COVID-19 to speed up vaccine development, and how we could do better next time.</p><p> As Marc tells us, island nations like Taiwan and New Zealand are successfully suppressing SARS-COV-2. But everyone else is struggling.</p><p> <a href="https://80000hours.org/podcast/episodes/marc-lipsitch-winning-or-losing-against-covid19-and-epidemiology/?utm_campaign=podcast__marc-lipsitch&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Even Singapore, with plenty of warning and one of the best test and trace systems in the world, lost control of the virus in mid-April after successfully holding back the tide for 2 months.</p><p> This doesn't bode well for how the US or Europe will cope as they ease their lockdowns. It also suggests it would have been exceedingly hard for China to stop the virus before it spread overseas.</p><p> But sadly, there's no easy way out.</p><p> The original estimates of COVID-19's infection fatality rate, of 0.5-1%, have turned out to be basically right. And the latest serology surveys indicate only 5-10% of people in countries like the US, UK and Spain have been infected so far, leaving us far short of herd immunity. To get there, even these worst affected countries would need to endure something like ten times the number of deaths they have so far.</p><p> Marc has one good piece of news: research suggests that most of those who get infected do indeed develop immunity, for a while at least.</p><p> To escape the COVID-19 trap sooner rather than later, Marc recommends we go hard on all the familiar options — vaccines, antivirals, and mass testing — but also open our minds to creative options we've so far left on the shelf.</p><p> Despite the importance of his work, even now the training and grant programs that produced the community of experts Marc is a part of, are shrinking. We look at a new article he's written about how to instead build and improve the field of epidemiology, so humanity can respond faster and smarter next time we face a disease that could kill millions and cost tens of trillions of dollars.</p><p> We also cover:</p><p> • How listeners might contribute as future contagious disease experts, or donors to current projects<br> • How we can learn from cross-country comparisons<br> • Modelling that has gone wrong in an instructive way<br> • What governments should stop doing<br> • How people can figure out who to trust, and who has been most on the mark this time<br> • Why Marc supports infecting people with COVID-19 to speed up the development of a vaccines<br> • How we can ensure there's population-level surveillance early during the next pandemic<br> • Whether people from other fields trying to help with COVID-19 has done more good than harm<br> • Whether it's experts in diseases, or experts in forecasting, who produce better disease forecasts</p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:45)</li><li>Things Rob wishes he knew about COVID-19 (00:05:23)</li><li>Cross-country comparisons (00:10:53)</li><li>Any government activities we should stop? (00:21:24)</li><li>Lessons from COVID-19 (00:33:31)</li><li>Global catastrophic biological risks (00:37:58)</li><li>Human challenge trials (00:43:12)</li><li>Disease surveillance (00:50:07)</li><li>Who should we trust? (00:58:12)</li><li>Epidemiology as a field (01:13:05)</li><li>Careers (01:31:28)</li></ul><p><br><em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell.<br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 18 May 2020 23:32:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/bed23271/219b8254.mp3" length="94000112" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/5NDmQgsS2NggcCMB-DDLxu7WPzFuJWnLMYICfS0BOao/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDkv/MTY4MzU0NDYzNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5825</itunes:duration>
      <itunes:summary>In March Professor Marc Lipsitch — Director of Harvard's Center for Communicable Disease Dynamics — abruptly found himself a global celebrity, his social media following growing 40-fold and journalists knocking down his door, as everyone turned to him for information they could trust.

Here he lays out where the fight against COVID-19 stands today, why he's open to deliberately giving people COVID-19 to speed up vaccine development, and how we could do better next time.

As Marc tells us, island nations like Taiwan and New Zealand are successfully suppressing SARS-COV-2. But everyone else is struggling.

Links to learn more, summary and full transcript.

Even Singapore, with plenty of warning and one of the best test and trace systems in the world, lost control of the virus in mid-April after successfully holding back the tide for 2 months.

This doesn't bode well for how the US or Europe will cope as they ease their lockdowns. It also suggests it would have been exceedingly hard for China to stop the virus before it spread overseas.

But sadly, there's no easy way out.

The original estimates of COVID-19's infection fatality rate, of 0.5-1%, have turned out to be basically right. And the latest serology surveys indicate only 5-10% of people in countries like the US, UK and Spain have been infected so far, leaving us far short of herd immunity. To get there, even these worst affected countries would need to endure something like ten times the number of deaths they have so far.

Marc has one good piece of news: research suggests that most of those who get infected do indeed develop immunity, for a while at least.

To escape the COVID-19 trap sooner rather than later, Marc recommends we go hard on all the familiar options — vaccines, antivirals, and mass testing — but also open our minds to creative options we've so far left on the shelf.

Despite the importance of his work, even now the training and grant programs that produced the community of experts Marc is a part of, are shrinking. We look at a new article he's written about how to instead build and improve the field of epidemiology, so humanity can respond faster and smarter next time we face a disease that could kill millions and cost tens of trillions of dollars.

We also cover:

• How listeners might contribute as future contagious disease experts, or donors to current projects
• How we can learn from cross-country comparisons
• Modelling that has gone wrong in an instructive way
• What governments should stop doing
• How people can figure out who to trust, and who has been most on the mark this time
• Why Marc supports infecting people with COVID-19 to speed up the development of a vaccines
• How we can ensure there's population-level surveillance early during the next pandemic
• Whether people from other fields trying to help with COVID-19 has done more good than harm
• Whether it's experts in diseases, or experts in forecasting, who produce better disease forecasts

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>In March Professor Marc Lipsitch — Director of Harvard's Center for Communicable Disease Dynamics — abruptly found himself a global celebrity, his social media following growing 40-fold and journalists knocking down his door, as everyone turned to him for</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/bed23271/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>Article: Ways people trying to do good accidentally make things worse, and how to avoid them</title>
      <itunes:title>Article: Ways people trying to do good accidentally make things worse, and how to avoid them</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">5390c816-9467-11ea-abd5-1292e2b23893</guid>
      <link>https://share.transistor.fm/s/f2e2d51f</link>
      <description>
        <![CDATA[Today’s release is the second experiment in making audio versions of our articles. <p>

The first was a narration of Greg Lewis’ terrific <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__accidental-harm-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>problem profile</b></a> on ‘Reducing global catastrophic biological risks’, which you can find on the podcast feed just before episode #74 - that is, our interview with Greg about the piece. </p><p>

If you want to check out the links in today’s article, you can find those <a href="https://80000hours.org/articles/accidental-harm/?utm_campaign=podcast__accidental-harm-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>. </p><p>

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org. </p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today’s release is the second experiment in making audio versions of our articles. <p>

The first was a narration of Greg Lewis’ terrific <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__accidental-harm-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>problem profile</b></a> on ‘Reducing global catastrophic biological risks’, which you can find on the podcast feed just before episode #74 - that is, our interview with Greg about the piece. </p><p>

If you want to check out the links in today’s article, you can find those <a href="https://80000hours.org/articles/accidental-harm/?utm_campaign=podcast__accidental-harm-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>. </p><p>

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org. </p>]]>
      </content:encoded>
      <pubDate>Tue, 12 May 2020 19:45:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f2e2d51f/1506f814.mp3" length="26499906" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/CBBQJMfZyAd2bwdUAn7u7qhjQ-3XhlsWs4UO1pPCuWM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDgv/MTY4MzU0NDYzMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>1606</itunes:duration>
      <itunes:summary>Today’s release is the second experiment in making audio versions of our articles. 

The first was a narration of Greg Lewis’ terrific problem profile on ‘Reducing global catastrophic biological risks’, which you can find on the podcast feed just before episode #74 - that is, our interview with Greg about the piece. 

If you want to check out the links in today’s article, you can find those here. 

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org. </itunes:summary>
      <itunes:subtitle>Today’s release is the second experiment in making audio versions of our articles. 

The first was a narration of Greg Lewis’ terrific problem profile on ‘Reducing global catastrophic biological risks’, which you can find on the podcast feed just before e</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/f2e2d51f/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#76 – Tara Kirk Sell on misinformation, who's done well and badly, &amp; what to reopen first</title>
      <itunes:title>#76 – Tara Kirk Sell on misinformation, who's done well and badly, &amp; what to reopen first</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ed2571d0-9182-11ea-a3eb-0e2c7113b43d</guid>
      <link>https://80000hours.org/podcast/episodes/tara-kirk-sell-covid-19-misinformation-performance-reopen/</link>
      <description>
        <![CDATA[<p>Amid a rising COVID-19 death toll, and looming economic disaster, we’ve been looking for good news — and one thing we're especially thankful for is the <em>Johns Hopkins Center for Health Security</em> (CHS). </p><p> CHS focuses on protecting us from major biological, chemical or nuclear disasters, through research that informs governments around the world. While this pandemic surprised many, just last October the Center ran a simulation of a 'new coronavirus' scenario to identify weaknesses in our ability to quickly respond. Their expertise has given them a key role in figuring out how to fight COVID-19. </p><p> Today’s guest, Dr Tara Kirk Sell, did her PhD in policy and communication during disease outbreaks, and has worked at CHS for 11 years on a range of important projects. </p><p> • <a href="https://80k.link/tks"><strong>Links to learn more, summary and full transcript.</strong></a> </p><p> Last year she was a leader on <a href="https://80k.link/84X"><strong>Collective Intelligence for Disease Prediction</strong></a>, designed to sound the alarm about upcoming pandemics before others are paying attention. </p><p> Incredibly, the project almost closed in December, with COVID-19 just starting to spread around the world — but received new funding that allowed the project to respond quickly to the emerging disease. </p><p> She also contributed to a recent <a href="https://80k.link/BQN"><strong>report</strong></a> attempting to explain the risks of specific types of activities resuming when COVID-19 lockdowns end. </p><p> We can't achieve zero risk — so differentiating activities on a spectrum is crucial. Choosing wisely can help us lead more normal lives without reviving the pandemic. </p><p> Dance clubs will have to stay closed, but hairdressers can adapt to minimise transmission, and Tara, who happens to be an Olympic silver-medalist in swimming, suggests outdoor non-contact sports could resume soon without much risk. </p><p> Her latest project deals with the <a href="https://80k.link/RJH"><strong>challenge of misinformation during disease outbreaks</strong></a>. </p><p> Analysing the Ebola communication crisis of 2014, they found that even trained coders with public health expertise sometimes needed help to distinguish between true and misleading tweets — showing the danger of a continued lack of definitive information surrounding a virus and how it’s transmitted. </p><p> The challenge for governments is not simple. If they acknowledge how much they don't know, people may look elsewhere for guidance. But if they pretend to know things they don't, the result can be a huge loss of trust. </p><p> Despite their intense focus on COVID-19, researchers at CHS know that this is no one-off event. Many aspects of our collective response this time around have been alarmingly poor, and it won’t be long before Tara and her colleagues need to turn their mind to next time. </p><p> <em>You can now donate to CHS through </em><a href="https://80k.link/FMG"><strong><em>Effective Altruism Funds</em></strong></a><em>. Donations made through EA Funds are tax-deductible in the US, the UK, and the Netherlands.</em> </p><p> Tara and Rob also discuss: </p><p> • Who has overperformed and underperformed expectations during COVID-19? <br> • Whe are people right to mistrust authorities? <br> • The media’s responsibility to be right <br> • What policy changes should be prioritised for next time <br> • Should we prepare for future pandemic while the COVID-19 is still going? <br> • The importance of keeping non-COVID health problems in mind <br> • The psychological difference between staying home voluntarily and being forced to <br> • Mistakes that we in the general public might be making <br> • Emerging technologies with the potential to reduce global catastrophic biological risks </p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:43)</li><li>Misinformation (00:05:07)</li><li>Who has done well during COVID-19? (00:22:19)</li><li>Guidance for governors on reopening (00:34:05)</li><li>Collective Intelligence for Disease Prediction project (00:45:35)</li><li>What else is CHS trying to do to address the pandemic? (00:59:51)</li><li>Deaths are not the only health impact of importance (01:05:33)</li><li>Policy change for future pandemics (01:10:57)</li><li>Emerging technologies with potential to reduce global catastrophic biological risks (01:22:37)</li><li>Careers (01:38:52)</li><li>Good news about COVID-19 (01:44:23)</li></ul><p><em>Producer: Keiran Harris. <br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Amid a rising COVID-19 death toll, and looming economic disaster, we’ve been looking for good news — and one thing we're especially thankful for is the <em>Johns Hopkins Center for Health Security</em> (CHS). </p><p> CHS focuses on protecting us from major biological, chemical or nuclear disasters, through research that informs governments around the world. While this pandemic surprised many, just last October the Center ran a simulation of a 'new coronavirus' scenario to identify weaknesses in our ability to quickly respond. Their expertise has given them a key role in figuring out how to fight COVID-19. </p><p> Today’s guest, Dr Tara Kirk Sell, did her PhD in policy and communication during disease outbreaks, and has worked at CHS for 11 years on a range of important projects. </p><p> • <a href="https://80k.link/tks"><strong>Links to learn more, summary and full transcript.</strong></a> </p><p> Last year she was a leader on <a href="https://80k.link/84X"><strong>Collective Intelligence for Disease Prediction</strong></a>, designed to sound the alarm about upcoming pandemics before others are paying attention. </p><p> Incredibly, the project almost closed in December, with COVID-19 just starting to spread around the world — but received new funding that allowed the project to respond quickly to the emerging disease. </p><p> She also contributed to a recent <a href="https://80k.link/BQN"><strong>report</strong></a> attempting to explain the risks of specific types of activities resuming when COVID-19 lockdowns end. </p><p> We can't achieve zero risk — so differentiating activities on a spectrum is crucial. Choosing wisely can help us lead more normal lives without reviving the pandemic. </p><p> Dance clubs will have to stay closed, but hairdressers can adapt to minimise transmission, and Tara, who happens to be an Olympic silver-medalist in swimming, suggests outdoor non-contact sports could resume soon without much risk. </p><p> Her latest project deals with the <a href="https://80k.link/RJH"><strong>challenge of misinformation during disease outbreaks</strong></a>. </p><p> Analysing the Ebola communication crisis of 2014, they found that even trained coders with public health expertise sometimes needed help to distinguish between true and misleading tweets — showing the danger of a continued lack of definitive information surrounding a virus and how it’s transmitted. </p><p> The challenge for governments is not simple. If they acknowledge how much they don't know, people may look elsewhere for guidance. But if they pretend to know things they don't, the result can be a huge loss of trust. </p><p> Despite their intense focus on COVID-19, researchers at CHS know that this is no one-off event. Many aspects of our collective response this time around have been alarmingly poor, and it won’t be long before Tara and her colleagues need to turn their mind to next time. </p><p> <em>You can now donate to CHS through </em><a href="https://80k.link/FMG"><strong><em>Effective Altruism Funds</em></strong></a><em>. Donations made through EA Funds are tax-deductible in the US, the UK, and the Netherlands.</em> </p><p> Tara and Rob also discuss: </p><p> • Who has overperformed and underperformed expectations during COVID-19? <br> • Whe are people right to mistrust authorities? <br> • The media’s responsibility to be right <br> • What policy changes should be prioritised for next time <br> • Should we prepare for future pandemic while the COVID-19 is still going? <br> • The importance of keeping non-COVID health problems in mind <br> • The psychological difference between staying home voluntarily and being forced to <br> • Mistakes that we in the general public might be making <br> • Emerging technologies with the potential to reduce global catastrophic biological risks </p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:43)</li><li>Misinformation (00:05:07)</li><li>Who has done well during COVID-19? (00:22:19)</li><li>Guidance for governors on reopening (00:34:05)</li><li>Collective Intelligence for Disease Prediction project (00:45:35)</li><li>What else is CHS trying to do to address the pandemic? (00:59:51)</li><li>Deaths are not the only health impact of importance (01:05:33)</li><li>Policy change for future pandemics (01:10:57)</li><li>Emerging technologies with potential to reduce global catastrophic biological risks (01:22:37)</li><li>Careers (01:38:52)</li><li>Good news about COVID-19 (01:44:23)</li></ul><p><em>Producer: Keiran Harris. <br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 08 May 2020 23:43:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/48db98a3/bc8ec384.mp3" length="108918599" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/o9RCZB3yJJixAcBMdDnfyTIx3qj_0SJsuyDecptIFgk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDcv/MTY4MzU0NDYzMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6780</itunes:duration>
      <itunes:summary>Amid a rising COVID-19 death toll, and looming economic disaster, we’ve been looking for good news — and one thing we're especially thankful for is the Johns Hopkins Center for Health Security (CHS). 

CHS focuses on protecting us from major biological, chemical or nuclear disasters, through research that informs governments around the world. While this pandemic surprised many, just last October the Center ran a simulation of a 'new coronavirus' scenario to identify weaknesses in our ability to quickly respond. Their expertise has given them a key role in figuring out how to fight COVID-19. 

Today’s guest, Dr Tara Kirk Sell, did her PhD in policy and communication during disease outbreaks, and has worked at CHS for 11 years on a range of important projects. 

• Links to learn more, summary and full transcript. 

Last year she was a leader on Collective Intelligence for Disease Prediction, designed to sound the alarm about upcoming pandemics before others are paying attention. 

Incredibly, the project almost closed in December, with COVID-19 just starting to spread around the world — but received new funding that allowed the project to respond quickly to the emerging disease. 

She also contributed to a recent report attempting to explain the risks of specific types of activities resuming when COVID-19 lockdowns end. 

We can't achieve zero risk — so differentiating activities on a spectrum is crucial. Choosing wisely can help us lead more normal lives without reviving the pandemic. 

Dance clubs will have to stay closed, but hairdressers can adapt to minimise transmission, and Tara, who happens to be an Olympic silver-medalist in swimming, suggests outdoor non-contact sports could resume soon without much risk. 

Her latest project deals with the challenge of misinformation during disease outbreaks. 

Analysing the Ebola communication crisis of 2014, they found that even trained coders with public health expertise sometimes needed help to distinguish between true and misleading tweets — showing the danger of a continued lack of definitive information surrounding a virus and how it’s transmitted. 

The challenge for governments is not simple. If they acknowledge how much they don't know, people may look elsewhere for guidance. But if they pretend to know things they don't, the result can be a huge loss of trust. 

Despite their intense focus on COVID-19, researchers at CHS know that this is no one-off event. Many aspects of our collective response this time around have been alarmingly poor, and it won’t be long before Tara and her colleagues need to turn their mind to next time. 

You can now donate to CHS through Effective Altruism Funds. Donations made through EA Funds are tax-deductible in the US, the UK, and the Netherlands.  

Tara and Rob also discuss: 

• Who has overperformed and underperformed expectations during COVID-19? 
• Whe are people right to mistrust authorities? 
• The media’s responsibility to be right 
• What policy changes should be prioritised for next time 
• Should we prepare for future pandemic while the COVID-19 is still going? 
• The importance of keeping non-COVID health problems in mind 
• The psychological difference between staying home voluntarily and being forced to 
• Mistakes that we in the general public might be making 
• Emerging technologies with the potential to reduce global catastrophic biological risks 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript. 

Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Amid a rising COVID-19 death toll, and looming economic disaster, we’ve been looking for good news — and one thing we're especially thankful for is the Johns Hopkins Center for Health Security (CHS). 

CHS focuses on protecting us from major biological,</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/48db98a3/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#75 – Michelle Hutchinson on what people most often ask 80,000 Hours</title>
      <itunes:title>#75 – Michelle Hutchinson on what people most often ask 80,000 Hours</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">9e607c36-8956-11ea-a149-0e6a0a387ea5</guid>
      <link>https://80000hours.org/podcast/episodes/michelle-hutchinson-giving-career-advice/</link>
      <description>
        <![CDATA[<p>Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. </p><p> I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's <em>Global Priorities Institute</em>, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. </p><p>So we thought it would be useful to discuss some on the show for everyone to hear. </p><p> • <a href="https://80k.link/mh2"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://80k.link/mh-jb"><strong>See over 500 vacancies on our job board.</strong></a><br> • <a href="https://80k.link/mh-a"><strong>Apply for one-on-one career advising.</strong></a></p><p> Among other common topics, we cover: </p><p> • Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. <br> • That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. <br> • Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. <br> • Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. <br> • The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. </p><p> I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. </p><p> If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: </p><p> 1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. <br> 2. Who don’t yet have close connections with people working at effective altruist organisations. <br> 3. Who aren’t strongly locationally constrained. </p><p> If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. </p><p> Also in this episode: </p><p> • I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. <br> • Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. <br> • I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. </p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:50)</li><li>The process of advising (00:09:34)</li><li>We’re not just excited about our priority paths (00:14:37)</li><li>Common things Michelle says during advising (00:18:13)</li><li>Interpersonal comparisons (00:31:18)</li><li>Thinking about current impact (00:40:31)</li><li>Applying to different kinds of orgs (00:42:29)</li><li>Difference in impact between jobs / causes (00:49:04)</li><li>Common mistakes (00:55:40)</li><li>Career change stories (01:11:44)</li><li>When is advising really useful for people? (01:24:28)</li><li>Managing risk in careers (01:55:29)</li></ul><p><br><em>Producer: Keiran Harris. <br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em> </p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. </p><p> I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's <em>Global Priorities Institute</em>, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. </p><p>So we thought it would be useful to discuss some on the show for everyone to hear. </p><p> • <a href="https://80k.link/mh2"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://80k.link/mh-jb"><strong>See over 500 vacancies on our job board.</strong></a><br> • <a href="https://80k.link/mh-a"><strong>Apply for one-on-one career advising.</strong></a></p><p> Among other common topics, we cover: </p><p> • Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. <br> • That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. <br> • Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. <br> • Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. <br> • The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. </p><p> I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. </p><p> If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: </p><p> 1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. <br> 2. Who don’t yet have close connections with people working at effective altruist organisations. <br> 3. Who aren’t strongly locationally constrained. </p><p> If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. </p><p> Also in this episode: </p><p> • I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. <br> • Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. <br> • I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. </p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:50)</li><li>The process of advising (00:09:34)</li><li>We’re not just excited about our priority paths (00:14:37)</li><li>Common things Michelle says during advising (00:18:13)</li><li>Interpersonal comparisons (00:31:18)</li><li>Thinking about current impact (00:40:31)</li><li>Applying to different kinds of orgs (00:42:29)</li><li>Difference in impact between jobs / causes (00:49:04)</li><li>Common mistakes (00:55:40)</li><li>Career change stories (01:11:44)</li><li>When is advising really useful for people? (01:24:28)</li><li>Managing risk in careers (01:55:29)</li></ul><p><br><em>Producer: Keiran Harris. <br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em> </p>]]>
      </content:encoded>
      <pubDate>Tue, 28 Apr 2020 14:45:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b5fed75b/afec3ea9.mp3" length="128618163" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/eXWpuph5MhaNxHcKmfTQtMsoDy3A1RG80tEPcRYAQkY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDYv/MTY4MzU0NDYzMS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7986</itunes:duration>
      <itunes:summary>Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. 

I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's Global Priorities Institute, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. So we thought it would be useful to discuss some on the show for everyone to hear. 

• Links to learn more, summary and full transcript.
• See over 500 vacancies on our job board.
• Apply for one-on-one career advising.

Among other common topics, we cover: 

• Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. 
• That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. 
• Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. 
• Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. 
• The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. 

I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. 

If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: 

1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. 
2. Who don’t yet have close connections with people working at effective altruist organisations. 
3. Who aren’t strongly locationally constrained. 

If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. 

Also in this episode: 

• I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. 
• Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. 
• I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq. </itunes:summary>
      <itunes:subtitle>Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/b5fed75b/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/b5fed75b/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#74 – Dr Greg Lewis on COVID-19 &amp; catastrophic biological risks</title>
      <itunes:title>#74 – Dr Greg Lewis on COVID-19 &amp; catastrophic biological risks</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f7c12b0c-80c5-11ea-b6ce-0e5740bfbb89</guid>
      <link>https://80000hours.org/podcast/episodes/greg-lewis-covid-19-global-catastrophic-biological-risks</link>
      <description>
        <![CDATA[<p>Our lives currently revolve around the global emergency of COVID-19; you’re probably reading this while confined to your house, as the death toll from the worst pandemic since 1918 continues to rise. </p><p> The question of how to tackle COVID-19 has been foremost in the minds of many, <a href="https://80k.link/gl-c19"><strong>including here at 80,000 Hours</strong></a>. </p><p> Today's guest, Dr Gregory Lewis, acting head of the Biosecurity Research Group at Oxford University's Future of Humanity Institute, puts the crisis in context, explaining how COVID-19 compares to other diseases, pandemics of the past, and possible worse crises in the future. </p><p> COVID-19 is a vivid reminder that we are unprepared to contain or respond to new pathogens. </p><p> How would we cope with a virus that was even more contagious and even more deadly? Greg's work focuses on these risks -- of outbreaks that threaten our entire future through an unrecoverable collapse of civilisation, or even the extinction of humanity. </p><p> <a href="https://80k.link/gregl"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> If such a catastrophe were to occur, Greg believes it’s more likely to be caused by accidental or deliberate misuse of biotechnology than by a pathogen developed by nature. </p><p> There are a few direct causes for concern: humans now have the ability to produce some of the most dangerous diseases in history in the lab; technological progress may enable the creation of pathogens which are nastier than anything we see in nature; and most biotechnology has yet to even be conceived, so we can’t assume all the dangers will be familiar. </p><p> This is grim stuff, but it needn’t be paralysing. In the years following COVID-19, humanity may be inspired to better prepare for the existential risks of the next century: improving our science, updating our policy options, and enhancing our social cohesion. </p><p> COVID-19 is a tragedy of stunning proportions, and its immediate threat is undoubtedly worthy of significant resources. </p><p> But we will get through it; if a future biological catastrophe poses an <a href="https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911"><em>existential</em> risk</a>, we may not get a second chance. It is therefore vital to learn every lesson we can from this pandemic, and provide our descendants with the security we wish for ourselves. </p><p> Today’s episode is the hosting debut of our Strategy Advisor, Howie Lempel. </p><p> 80,000 Hours has focused on COVID-19 for the last few weeks and published <a href="https://80k.link/gl-c19"><strong>over ten</strong></a> pieces about it, and a substantial benefit of this interview was to help inform our own views. As such, at times this episode may feel like eavesdropping on a private conversation, and it is likely to be of most interest to people primarily focused on making the <a href="https://80k.link/gl-fg"><strong>long-term future</strong></a> go as well as possible. </p><p> In this episode, Howie and Greg cover: </p><p> • Reflections on the first few months of the pandemic <br> • Common confusions around COVID-19 <br> • How COVID-19 compares to other diseases <br> • What types of interventions have been available to policymakers <br> • Arguments for and against working on <a href="https://80k.link/gcbrs"><strong>global catastrophic biological risks</strong></a> (GCBRs) <br> • How to know if you’re a good fit to work on GCBRs <br> • The response of the <a href="https://80k.link/gl-ea"><strong>effective altruism community</strong></a>, as well as 80,000 Hours in particular, to COVID-19 <br> • And much more.<br> <br>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:15)</li><li>What is COVID-19? (00:16:05)</li><li>If you end up infected, how severe is it likely to be? (00:19:21)</li><li>How does COVID-19 compare to other diseases? (00:25:42)</li><li>Common confusions around COVID-19 (00:32:02)</li><li>What types of interventions were available to policymakers? (00:46:20)</li><li>Nonpharmaceutical Interventions (01:04:18)</li><li>What can you do personally? (01:18:25)</li><li>Reflections on the first few months of the pandemic (01:23:46)</li><li>Global catastrophic biological risks (GCBRs) (01:26:17)</li><li>Counterarguments to working on GCBRs (01:45:56)</li><li>How do GCBRs compare to other problems? (01:49:05)</li><li>Careers (01:59:50)</li><li>The response of the effective altruism community to COVID-19 (02:11:42)</li><li>The response of 80,000 Hours to COVID-19 (02:28:12)</li></ul><p><br><strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript.</strong> </p><p> <em>Producer: Keiran Harris.</em> <br> <em>Audio mastering: Ben Cordell.</em><br> <em>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Our lives currently revolve around the global emergency of COVID-19; you’re probably reading this while confined to your house, as the death toll from the worst pandemic since 1918 continues to rise. </p><p> The question of how to tackle COVID-19 has been foremost in the minds of many, <a href="https://80k.link/gl-c19"><strong>including here at 80,000 Hours</strong></a>. </p><p> Today's guest, Dr Gregory Lewis, acting head of the Biosecurity Research Group at Oxford University's Future of Humanity Institute, puts the crisis in context, explaining how COVID-19 compares to other diseases, pandemics of the past, and possible worse crises in the future. </p><p> COVID-19 is a vivid reminder that we are unprepared to contain or respond to new pathogens. </p><p> How would we cope with a virus that was even more contagious and even more deadly? Greg's work focuses on these risks -- of outbreaks that threaten our entire future through an unrecoverable collapse of civilisation, or even the extinction of humanity. </p><p> <a href="https://80k.link/gregl"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> If such a catastrophe were to occur, Greg believes it’s more likely to be caused by accidental or deliberate misuse of biotechnology than by a pathogen developed by nature. </p><p> There are a few direct causes for concern: humans now have the ability to produce some of the most dangerous diseases in history in the lab; technological progress may enable the creation of pathogens which are nastier than anything we see in nature; and most biotechnology has yet to even be conceived, so we can’t assume all the dangers will be familiar. </p><p> This is grim stuff, but it needn’t be paralysing. In the years following COVID-19, humanity may be inspired to better prepare for the existential risks of the next century: improving our science, updating our policy options, and enhancing our social cohesion. </p><p> COVID-19 is a tragedy of stunning proportions, and its immediate threat is undoubtedly worthy of significant resources. </p><p> But we will get through it; if a future biological catastrophe poses an <a href="https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911"><em>existential</em> risk</a>, we may not get a second chance. It is therefore vital to learn every lesson we can from this pandemic, and provide our descendants with the security we wish for ourselves. </p><p> Today’s episode is the hosting debut of our Strategy Advisor, Howie Lempel. </p><p> 80,000 Hours has focused on COVID-19 for the last few weeks and published <a href="https://80k.link/gl-c19"><strong>over ten</strong></a> pieces about it, and a substantial benefit of this interview was to help inform our own views. As such, at times this episode may feel like eavesdropping on a private conversation, and it is likely to be of most interest to people primarily focused on making the <a href="https://80k.link/gl-fg"><strong>long-term future</strong></a> go as well as possible. </p><p> In this episode, Howie and Greg cover: </p><p> • Reflections on the first few months of the pandemic <br> • Common confusions around COVID-19 <br> • How COVID-19 compares to other diseases <br> • What types of interventions have been available to policymakers <br> • Arguments for and against working on <a href="https://80k.link/gcbrs"><strong>global catastrophic biological risks</strong></a> (GCBRs) <br> • How to know if you’re a good fit to work on GCBRs <br> • The response of the <a href="https://80k.link/gl-ea"><strong>effective altruism community</strong></a>, as well as 80,000 Hours in particular, to COVID-19 <br> • And much more.<br> <br>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:15)</li><li>What is COVID-19? (00:16:05)</li><li>If you end up infected, how severe is it likely to be? (00:19:21)</li><li>How does COVID-19 compare to other diseases? (00:25:42)</li><li>Common confusions around COVID-19 (00:32:02)</li><li>What types of interventions were available to policymakers? (00:46:20)</li><li>Nonpharmaceutical Interventions (01:04:18)</li><li>What can you do personally? (01:18:25)</li><li>Reflections on the first few months of the pandemic (01:23:46)</li><li>Global catastrophic biological risks (GCBRs) (01:26:17)</li><li>Counterarguments to working on GCBRs (01:45:56)</li><li>How do GCBRs compare to other problems? (01:49:05)</li><li>Careers (01:59:50)</li><li>The response of the effective altruism community to COVID-19 (02:11:42)</li><li>The response of 80,000 Hours to COVID-19 (02:28:12)</li></ul><p><br><strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript.</strong> </p><p> <em>Producer: Keiran Harris.</em> <br> <em>Audio mastering: Ben Cordell.</em><br> <em>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 17 Apr 2020 17:20:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/0a513dce/9d75a4c7.mp3" length="151421188" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/xG6cqHx3l_o-nHiF2Xh8G2sES5ANe7I4cgqhhJM0yrA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDUv/MTY4MzU0NDYzMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9437</itunes:duration>
      <itunes:summary>Our lives currently revolve around the global emergency of COVID-19; you’re probably reading this while confined to your house, as the death toll from the worst pandemic since 1918 continues to rise. 

The question of how to tackle COVID-19 has been foremost in the minds of many, including here at 80,000 Hours. 

Today's guest, Dr Gregory Lewis, acting head of the Biosecurity Research Group at Oxford University's Future of Humanity Institute, puts the crisis in context, explaining how COVID-19 compares to other diseases, pandemics of the past, and possible worse crises in the future. 

COVID-19 is a vivid reminder that we are unprepared to contain or respond to new pathogens. 

How would we cope with a virus that was even more contagious and even more deadly? Greg's work focuses on these risks -- of outbreaks that threaten our entire future through an unrecoverable collapse of civilisation, or even the extinction of humanity. 

Links to learn more, summary and full transcript.

If such a catastrophe were to occur, Greg believes it’s more likely to be caused by accidental or deliberate misuse of biotechnology than by a pathogen developed by nature. 

There are a few direct causes for concern: humans now have the ability to produce some of the most dangerous diseases in history in the lab; technological progress may enable the creation of pathogens which are nastier than anything we see in nature; and most biotechnology has yet to even be conceived, so we can’t assume all the dangers will be familiar. 

This is grim stuff, but it needn’t be paralysing. In the years following COVID-19, humanity may be inspired to better prepare for the existential risks of the next century: improving our science, updating our policy options, and enhancing our social cohesion. 

COVID-19 is a tragedy of stunning proportions, and its immediate threat is undoubtedly worthy of significant resources. 

But we will get through it; if a future biological catastrophe poses an existential risk, we may not get a second chance. It is therefore vital to learn every lesson we can from this pandemic, and provide our descendants with the security we wish for ourselves. 

Today’s episode is the hosting debut of our Strategy Advisor, Howie Lempel. 

80,000 Hours has focused on COVID-19 for the last few weeks and published over ten pieces about it, and a substantial benefit of this interview was to help inform our own views. As such, at times this episode may feel like eavesdropping on a private conversation, and it is likely to be of most interest to people primarily focused on making the long-term future go as well as possible. 

In this episode, Howie and Greg cover: 

• Reflections on the first few months of the pandemic 
• Common confusions around COVID-19 
• How COVID-19 compares to other diseases 
• What types of interventions have been available to policymakers 
• Arguments for and against working on global catastrophic biological risks (GCBRs) 
• How to know if you’re a good fit to work on GCBRs 
• The response of the effective altruism community, as well as 80,000 Hours in particular, to COVID-19 
• And much more. 

Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>Our lives currently revolve around the global emergency of COVID-19; you’re probably reading this while confined to your house, as the death toll from the worst pandemic since 1918 continues to rise. 

The question of how to tackle COVID-19 has been for</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/0a513dce/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/0a513dce/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Article: Reducing global catastrophic biological risks</title>
      <itunes:title>Article: Reducing global catastrophic biological risks</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3b9ad4b2-7f69-11ea-bdcc-0e4b74461eeb</guid>
      <link>https://share.transistor.fm/s/e04b21db</link>
      <description>
        <![CDATA[<p>In a few days we'll be putting out a conversation with Dr Greg Lewis, who studies how to prevent global catastrophic biological risks at Oxford's Future of Humanity Institute. </p><p>

Greg also wrote <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>a new problem profile</b></a> on that topic for our website, and reading that is a good lead-in to our interview with him. So in a bit of an experiment we decided to make this audio version of that article, narrated by the producer of the 80,000 Hours Podcast, Keiran Harris. </p><p>

We’re thinking about having audio versions of other important articles we write, so it’d be great if you could let us know if you’d like more of these. You can email us your view at podcast@80000hours.org. </p><p>

If you want to check out all of Greg’s graphs and footnotes that we didn’t include, and get links to learn more about GCBRs - you can find those <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>. </p><p>

And if you want to read more about COVID-19, the 80,000 Hours team has produced a fantastic package of 10 pieces about how to stop the pandemic. You can find those <a href="https://80000hours.org/80000hours.org/covid-19/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>.  </p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In a few days we'll be putting out a conversation with Dr Greg Lewis, who studies how to prevent global catastrophic biological risks at Oxford's Future of Humanity Institute. </p><p>

Greg also wrote <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>a new problem profile</b></a> on that topic for our website, and reading that is a good lead-in to our interview with him. So in a bit of an experiment we decided to make this audio version of that article, narrated by the producer of the 80,000 Hours Podcast, Keiran Harris. </p><p>

We’re thinking about having audio versions of other important articles we write, so it’d be great if you could let us know if you’d like more of these. You can email us your view at podcast@80000hours.org. </p><p>

If you want to check out all of Greg’s graphs and footnotes that we didn’t include, and get links to learn more about GCBRs - you can find those <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>. </p><p>

And if you want to read more about COVID-19, the 80,000 Hours team has produced a fantastic package of 10 pieces about how to stop the pandemic. You can find those <a href="https://80000hours.org/80000hours.org/covid-19/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>here</b></a>.  </p>]]>
      </content:encoded>
      <pubDate>Wed, 15 Apr 2020 22:45:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e04b21db/125b868e.mp3" length="62903490" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/cfCdqFid0zYx6FESEkOA3aioU8jnHhekkhaLx5LQ5Nk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDQv/MTY4MzU0NDYyOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3855</itunes:duration>
      <itunes:summary>In a few days we'll be putting out a conversation with Dr Greg Lewis, who studies how to prevent global catastrophic biological risks at Oxford's Future of Humanity Institute. 

Greg also wrote a new problem profile on that topic for our website, and reading that is a good lead-in to our interview with him. So in a bit of an experiment we decided to make this audio version of that article, narrated by the producer of the 80,000 Hours Podcast, Keiran Harris. 

We’re thinking about having audio versions of other important articles we write, so it’d be great if you could let us know if you’d like more of these. You can email us your view at podcast@80000hours.org. 

If you want to check out all of Greg’s graphs and footnotes that we didn’t include, and get links to learn more about GCBRs - you can find those here. 

And if you want to read more about COVID-19, the 80,000 Hours team has produced a fantastic package of 10 pieces about how to stop the pandemic. You can find those here.  </itunes:summary>
      <itunes:subtitle>In a few days we'll be putting out a conversation with Dr Greg Lewis, who studies how to prevent global catastrophic biological risks at Oxford's Future of Humanity Institute. 

Greg also wrote a new problem profile on that topic for our website, and read</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/e04b21db/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Emergency episode: Rob &amp; Howie on the menace of COVID-19, and what both governments &amp; individuals might do to help</title>
      <itunes:title>Emergency episode: Rob &amp; Howie on the menace of COVID-19, and what both governments &amp; individuals might do to help</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2178bf0e-6a38-11ea-9e38-0e0b70a6519d</guid>
      <link>https://share.transistor.fm/s/575aea7e</link>
      <description>
        <![CDATA[<p>From home isolation Rob and Howie just recorded an episode on: </p><p>1. How many could die in the crisis, and the risk to your health personally. <br>2. What individuals might be able to do help tackle the coronavirus crisis. <br>3. What we suspect governments should do in response to the coronavirus crisis. <br>4. The importance of personally not spreading the virus, the properties of the SARS-CoV-2 virus, and how you can personally avoid it. <br>5. The many places society screwed up, how we can avoid this happening again, and why be optimistic. </p><p>

We have rushed this episode out to share information as quickly as possible in a fast-moving situation. If you would prefer to read you can find the <b><a href="https://80000hours.org/podcast/episodes/rob-howie-coronavirus-crisis/#transcript">transcript here</a></b>.</p><p>

We list a wide range of valuable resources and links in the <b><a href="https://80000hours.org/podcast/episodes/rob-howie-coronavirus-crisis/">blog post attached to the show</a></b> (over 60, including links to projects you can join). </p><p>

See our 'problem profile' on <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/"><b>global catastrophic biological risks</b></a> for information on these grave threats and how you can contribute to preventing them. </p><p>

We have also just added a <a href="https://80000hours.org/covid-19/"><b>COVID-19 landing page</b></a> on our site. </p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b> </p><p>

<em>Producer: Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>From home isolation Rob and Howie just recorded an episode on: </p><p>1. How many could die in the crisis, and the risk to your health personally. <br>2. What individuals might be able to do help tackle the coronavirus crisis. <br>3. What we suspect governments should do in response to the coronavirus crisis. <br>4. The importance of personally not spreading the virus, the properties of the SARS-CoV-2 virus, and how you can personally avoid it. <br>5. The many places society screwed up, how we can avoid this happening again, and why be optimistic. </p><p>

We have rushed this episode out to share information as quickly as possible in a fast-moving situation. If you would prefer to read you can find the <b><a href="https://80000hours.org/podcast/episodes/rob-howie-coronavirus-crisis/#transcript">transcript here</a></b>.</p><p>

We list a wide range of valuable resources and links in the <b><a href="https://80000hours.org/podcast/episodes/rob-howie-coronavirus-crisis/">blog post attached to the show</a></b> (over 60, including links to projects you can join). </p><p>

See our 'problem profile' on <a href="https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/"><b>global catastrophic biological risks</b></a> for information on these grave threats and how you can contribute to preventing them. </p><p>

We have also just added a <a href="https://80000hours.org/covid-19/"><b>COVID-19 landing page</b></a> on our site. </p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b> </p><p>

<em>Producer: Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 19 Mar 2020 23:43:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/575aea7e/e6ea8055.mp3" length="108215932" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/868w0AYhsTTY_hTxD-xVaLJI16DUI6KLMd8Ta_Uv06A/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDMv/MTY4MzU0NDYyNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6732</itunes:duration>
      <itunes:summary>From home isolation Rob and Howie just recorded an episode on: 1. How many could die in the crisis, and the risk to your health personally. 2. What individuals might be able to do help tackle the coronavirus crisis. 3. What we suspect governments should do in response to the coronavirus crisis. 4. The importance of personally not spreading the virus, the properties of the SARS-CoV-2 virus, and how you can personally avoid it. 5. The many places society screwed up, how we can avoid this happening again, and why be optimistic. 

We have rushed this episode out to share information as quickly as possible in a fast-moving situation. If you would prefer to read you can find the transcript here.

We list a wide range of valuable resources and links in the blog post attached to the show (over 60, including links to projects you can join). 

See our 'problem profile' on global catastrophic biological risks for information on these grave threats and how you can contribute to preventing them. 

We have also just added a COVID-19 landing page on our site. 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. 

Producer: Keiran Harris.</itunes:summary>
      <itunes:subtitle>From home isolation Rob and Howie just recorded an episode on: 1. How many could die in the crisis, and the risk to your health personally. 2. What individuals might be able to do help tackle the coronavirus crisis. 3. What we suspect governments should d</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/575aea7e/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#73 – Phil Trammell on patient philanthropy and waiting to do good</title>
      <itunes:title>#73 – Phil Trammell on patient philanthropy and waiting to do good</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6b68d184-6854-11ea-82a3-0e93e4083793</guid>
      <link>https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy</link>
      <description>
        <![CDATA[<p>To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. </p><p>If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. </p><p>This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to <a href="https://80k.link/pt-pdf"><strong>investigate the case for and against</strong></a> so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. </p><p> He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. </p><p> <em>ADDED: Does the COVID-19 emergency mean we should actually use resources right now? See Phil's </em><a href="https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy/#covid"><strong><em>first thoughts on this question here.</em></strong></a> </p><p> • <a href="https://80k.link/ptpp"><strong>Links to learn more, summary and full transcript.</strong></a> </p><p> What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? </p><p> And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. </p><p> Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? </p><p> Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? </p><p> Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. </p><p> Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. </p><p> Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my 80,000 Hours colleague Howie Lempel, we try to answer that, and also discuss: </p><p> • Real attempts at patient philanthropy in history and how they worked out <br> • Should we have a mixed strategy, where some altruists are patient and others impatient? <br> • Which causes most need money now, and which later? <br> • What is the research frontier here? <br> • What does this all mean for what listeners should do differently? </p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:23)</li><li>Consequences for getting this question wrong (00:06:03)</li><li>What have people had to say about this question in the past? (00:07:22)</li><li>The case for saving (00:11:51)</li><li>Hundred year leases (00:29:28)</li><li>Should we be concerned about one group taking control of the world? (00:34:51)</li><li>Finding better interventions in the future (00:37:20)</li><li>The hinge of history (00:43:46)</li><li>Does uncertainty lead us to wanting to wait? (01:01:52)</li><li>Counterarguments (01:11:36)</li><li>What about groups who have a particular sense of urgency? (01:40:46)</li><li>How much should we actually save? (02:01:35)</li><li>Implications for career choices (02:19:49) </li></ul><p><br> <em>Producer: Keiran Harris.</em> <br> <em>Audio mastering: Ben Cordell.</em> <br> <em>Transcriptions: Zakee Ulhaq.</em> </p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. </p><p>If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. </p><p>This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to <a href="https://80k.link/pt-pdf"><strong>investigate the case for and against</strong></a> so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. </p><p> He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. </p><p> <em>ADDED: Does the COVID-19 emergency mean we should actually use resources right now? See Phil's </em><a href="https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy/#covid"><strong><em>first thoughts on this question here.</em></strong></a> </p><p> • <a href="https://80k.link/ptpp"><strong>Links to learn more, summary and full transcript.</strong></a> </p><p> What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? </p><p> And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. </p><p> Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? </p><p> Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? </p><p> Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. </p><p> Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. </p><p> Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my 80,000 Hours colleague Howie Lempel, we try to answer that, and also discuss: </p><p> • Real attempts at patient philanthropy in history and how they worked out <br> • Should we have a mixed strategy, where some altruists are patient and others impatient? <br> • Which causes most need money now, and which later? <br> • What is the research frontier here? <br> • What does this all mean for what listeners should do differently? </p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:02:23)</li><li>Consequences for getting this question wrong (00:06:03)</li><li>What have people had to say about this question in the past? (00:07:22)</li><li>The case for saving (00:11:51)</li><li>Hundred year leases (00:29:28)</li><li>Should we be concerned about one group taking control of the world? (00:34:51)</li><li>Finding better interventions in the future (00:37:20)</li><li>The hinge of history (00:43:46)</li><li>Does uncertainty lead us to wanting to wait? (01:01:52)</li><li>Counterarguments (01:11:36)</li><li>What about groups who have a particular sense of urgency? (01:40:46)</li><li>How much should we actually save? (02:01:35)</li><li>Implications for career choices (02:19:49) </li></ul><p><br> <em>Producer: Keiran Harris.</em> <br> <em>Audio mastering: Ben Cordell.</em> <br> <em>Transcriptions: Zakee Ulhaq.</em> </p>]]>
      </content:encoded>
      <pubDate>Tue, 17 Mar 2020 15:08:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/64657aa7/e01636b1.mp3" length="149671584" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/VKV9VZfFu23hq03-u-R4Kiv3Oxq7IYC6sKoAc_iqS1o/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDIv/MTY4MzU0NDYyNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9322</itunes:duration>
      <itunes:summary>To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. 

If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. 

This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to investigate the case for and against so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. 

He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. 

ADDED: Does the COVID-19 emergency mean we should actually use resources right now? See Phil's first thoughts on this question here. 

• Links to learn more, summary and full transcript. 

What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? 

And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. 

Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? 

Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? 

Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. 

Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. 

Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my 80,000 Hours colleague Howie Lempel, we try to answer that, and also discuss: 

• Real attempts at patient philanthropy in history and how they worked out 
• Should we have a mixed strategy, where some altruists are patient and others impatient? 
• Which causes most need money now, and which later? 
• What is the research frontier here? 
• What does this all mean for what listeners should do differently? 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the transcript linked above. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq. </itunes:summary>
      <itunes:subtitle>To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. 

If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 y</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/64657aa7/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/64657aa7/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#72 - Toby Ord on the precipice and humanity's potential futures</title>
      <itunes:title>#72 - Toby Ord on the precipice and humanity's potential futures</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4359f866-60a8-11ea-87ff-0e1b40f62329</guid>
      <link>https://share.transistor.fm/s/61299128</link>
      <description>
        <![CDATA[This week Oxford academic and 80,000 Hours trustee Dr Toby Ord released his new book <a href="https://theprecipice.com/"><i>The Precipice: Existential Risk and the Future of Humanity</i></a>. It's about how our long-term future could be better than almost anyone believes, but also how humanity's recklessness is putting that future at grave risk — in Toby's reckoning, a 1 in 6 chance of being extinguished this century. <p>

I loved the book and learned a great deal from it (<a href="https://80k.link/buy-tp"><b>buy it here</b></a>, US and audiobook release March 24). While preparing for this interview I copied out 87 facts that were surprising, shocking or important. Here's a sample of 16: </p><p>

1. The probability of a supervolcano causing a civilisation-threatening catastrophe in the next century is estimated to be 100x that of asteroids and comets combined. </p><p>
2. The Biological Weapons Convention — a global agreement to protect humanity — has just four employees, and a smaller budget than an average McDonald’s. </p><p>
3. In 2008 a 'gamma ray burst' reached Earth from another galaxy, 10 billion light years away. It was still bright enough to be visible to the naked eye. We aren't sure what generates gamma ray bursts but one cause may be two neutron stars colliding. </p><p>
4. Before detonating the first nuclear weapon, scientists in the Manhattan Project feared that the high temperatures in the core, unprecedented for Earth, might be able to ignite the hydrogen in water. This would set off a self-sustaining reaction that would burn off the Earth’s oceans, killing all life above ground. They thought this was unlikely, but many atomic scientists feared their calculations could be missing something. As far as we know, the US President was never informed of this possibility, but similar risks were one reason Hitler stopped… </p><p>

<i>N.B. I've had to cut off this list as we only get 4,000 characters in these show notes, so:</i> </p><p>

<a href="https://80k.link/ord-2"><b>Click here to read the whole list, see a full transcript, and find related links.</b></a> </p><p>

And if you like the list, you can <a href="https://80000hours.org/the-precipice/"><b>get a free copy of the introduction and first chapter</b></a> by joining our mailing list. </p><p>

While I've been studying these topics for years and known Toby for the last eight, a remarkable amount of what's in <i>The Precipice</i> was new to me. </p><p>

Of course the book isn't a series of isolated amusing facts, but rather a systematic review of the many ways humanity's future could go better or worse, how we might know about them, and what might be done to improve the odds. </p><p>

And that's how we approach this conversation, first talking about each of the main threats, then how we can learn about things that have never happened before, then finishing with what a great future for humanity might look like and how it might be achieved. </p><p>

Toby is a famously good explainer of complex issues — a bit of a modern Carl Sagan character — so as expected this was a great interview, and one which Arden Koehler and I barely even had to work for. </p><p>

Some topics Arden and I ask about include: </p><p>

• What Toby changed his mind about while writing the book <br>
• Are people exaggerating when they say that climate change could actually end civilization? <br>
• What can we learn from historical pandemics? <br>
• Toby’s estimate of unaligned AI causing human extinction in the next century <br>
• Is this century the most important time in human history, or is that a narcissistic delusion? <br>
• Competing vision for humanity's ideal future <br>
• And more. </p><p>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript.</b> </p><p>


<i>Producer: Keiran Harris.</i> <br>
<i>Audio mastering: Ben Cordell.</i><br>
<i>Transcriptions: Zakee Ulhaq.</i></p>]]>
      </description>
      <content:encoded>
        <![CDATA[This week Oxford academic and 80,000 Hours trustee Dr Toby Ord released his new book <a href="https://theprecipice.com/"><i>The Precipice: Existential Risk and the Future of Humanity</i></a>. It's about how our long-term future could be better than almost anyone believes, but also how humanity's recklessness is putting that future at grave risk — in Toby's reckoning, a 1 in 6 chance of being extinguished this century. <p>

I loved the book and learned a great deal from it (<a href="https://80k.link/buy-tp"><b>buy it here</b></a>, US and audiobook release March 24). While preparing for this interview I copied out 87 facts that were surprising, shocking or important. Here's a sample of 16: </p><p>

1. The probability of a supervolcano causing a civilisation-threatening catastrophe in the next century is estimated to be 100x that of asteroids and comets combined. </p><p>
2. The Biological Weapons Convention — a global agreement to protect humanity — has just four employees, and a smaller budget than an average McDonald’s. </p><p>
3. In 2008 a 'gamma ray burst' reached Earth from another galaxy, 10 billion light years away. It was still bright enough to be visible to the naked eye. We aren't sure what generates gamma ray bursts but one cause may be two neutron stars colliding. </p><p>
4. Before detonating the first nuclear weapon, scientists in the Manhattan Project feared that the high temperatures in the core, unprecedented for Earth, might be able to ignite the hydrogen in water. This would set off a self-sustaining reaction that would burn off the Earth’s oceans, killing all life above ground. They thought this was unlikely, but many atomic scientists feared their calculations could be missing something. As far as we know, the US President was never informed of this possibility, but similar risks were one reason Hitler stopped… </p><p>

<i>N.B. I've had to cut off this list as we only get 4,000 characters in these show notes, so:</i> </p><p>

<a href="https://80k.link/ord-2"><b>Click here to read the whole list, see a full transcript, and find related links.</b></a> </p><p>

And if you like the list, you can <a href="https://80000hours.org/the-precipice/"><b>get a free copy of the introduction and first chapter</b></a> by joining our mailing list. </p><p>

While I've been studying these topics for years and known Toby for the last eight, a remarkable amount of what's in <i>The Precipice</i> was new to me. </p><p>

Of course the book isn't a series of isolated amusing facts, but rather a systematic review of the many ways humanity's future could go better or worse, how we might know about them, and what might be done to improve the odds. </p><p>

And that's how we approach this conversation, first talking about each of the main threats, then how we can learn about things that have never happened before, then finishing with what a great future for humanity might look like and how it might be achieved. </p><p>

Toby is a famously good explainer of complex issues — a bit of a modern Carl Sagan character — so as expected this was a great interview, and one which Arden Koehler and I barely even had to work for. </p><p>

Some topics Arden and I ask about include: </p><p>

• What Toby changed his mind about while writing the book <br>
• Are people exaggerating when they say that climate change could actually end civilization? <br>
• What can we learn from historical pandemics? <br>
• Toby’s estimate of unaligned AI causing human extinction in the next century <br>
• Is this century the most important time in human history, or is that a narcissistic delusion? <br>
• Competing vision for humanity's ideal future <br>
• And more. </p><p>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript.</b> </p><p>


<i>Producer: Keiran Harris.</i> <br>
<i>Audio mastering: Ben Cordell.</i><br>
<i>Transcriptions: Zakee Ulhaq.</i></p>]]>
      </content:encoded>
      <pubDate>Sat, 07 Mar 2020 19:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/61299128/b9349b48.mp3" length="187259959" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/5g5XBOU1od7OStrw7tGXPmqZcYpYTnL3wdY3mtj7B8o/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDEv/MTY4MzU0NDYyNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11657</itunes:duration>
      <itunes:summary>This week Oxford academic and 80,000 Hours trustee Dr Toby Ord released his new book The Precipice: Existential Risk and the Future of Humanity. It's about how our long-term future could be better than almost anyone believes, but also how humanity's recklessness is putting that future at grave risk — in Toby's reckoning, a 1 in 6 chance of being extinguished this century. 

I loved the book and learned a great deal from it (buy it here, US and audiobook release March 24). While preparing for this interview I copied out 87 facts that were surprising, shocking or important. Here's a sample of 16: 

1. The probability of a supervolcano causing a civilisation-threatening catastrophe in the next century is estimated to be 100x that of asteroids and comets combined. 
2. The Biological Weapons Convention — a global agreement to protect humanity — has just four employees, and a smaller budget than an average McDonald’s. 
3. In 2008 a 'gamma ray burst' reached Earth from another galaxy, 10 billion light years away. It was still bright enough to be visible to the naked eye. We aren't sure what generates gamma ray bursts but one cause may be two neutron stars colliding. 
4. Before detonating the first nuclear weapon, scientists in the Manhattan Project feared that the high temperatures in the core, unprecedented for Earth, might be able to ignite the hydrogen in water. This would set off a self-sustaining reaction that would burn off the Earth’s oceans, killing all life above ground. They thought this was unlikely, but many atomic scientists feared their calculations could be missing something. As far as we know, the US President was never informed of this possibility, but similar risks were one reason Hitler stopped… 

N.B. I've had to cut off this list as we only get 4,000 characters in these show notes, so: 

Click here to read the whole list, see a full transcript, and find related links. 

And if you like the list, you can get a free copy of the introduction and first chapter by joining our mailing list. 

While I've been studying these topics for years and known Toby for the last eight, a remarkable amount of what's in The Precipice was new to me. 

Of course the book isn't a series of isolated amusing facts, but rather a systematic review of the many ways humanity's future could go better or worse, how we might know about them, and what might be done to improve the odds. 

And that's how we approach this conversation, first talking about each of the main threats, then how we can learn about things that have never happened before, then finishing with what a great future for humanity might look like and how it might be achieved. 

Toby is a famously good explainer of complex issues — a bit of a modern Carl Sagan character — so as expected this was a great interview, and one which Arden Koehler and I barely even had to work for. 

Some topics Arden and I ask about include: 

• What Toby changed his mind about while writing the book 
• Are people exaggerating when they say that climate change could actually end civilization? 
• What can we learn from historical pandemics? 
• Toby’s estimate of unaligned AI causing human extinction in the next century 
• Is this century the most important time in human history, or is that a narcissistic delusion? 
• Competing vision for humanity's ideal future 
• And more. 

Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>This week Oxford academic and 80,000 Hours trustee Dr Toby Ord released his new book The Precipice: Existential Risk and the Future of Humanity. It's about how our long-term future could be better than almost anyone believes, but also how humanity's reckl</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/61299128/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#71 - Benjamin Todd on the key ideas of 80,000 Hours</title>
      <itunes:title>#71 - Benjamin Todd on the key ideas of 80,000 Hours</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">4224d342-5cea-11ea-9bae-0ebdefbc3bc9</guid>
      <link>https://share.transistor.fm/s/bfe83923</link>
      <description>
        <![CDATA[The 80,000 Hours Podcast is about “the world’s most pressing problems and how you can use your career to solve them”, and in this episode we tackle that question in the most direct way possible. <p>

Last year we published a summary of all our <a href="https://80k.link/ki-bt"><b>key ideas</b></a>, which links to many of our other articles, and which we are aiming to keep updated as our opinions shift. </p><p>

All of us added something to it, but the single biggest contributor was our CEO and today's guest, Ben Todd, who founded 80,000 Hours along with Will MacAskill back in 2012. </p><p>

This key ideas page is the most read on the site. By itself it can teach you a large fraction of the most important things we've discovered since we started investigating high impact careers. </p><p>

<b>• <a href="https://80000hours.org/podcast/episodes/ben-todd-key-ideas-of-80000hours/?utm_campaign=podcast__ben-todd&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></b> </p><p>

But it's perhaps more accurate to think of it as a mini-book, as it weighs in at over 20,000 words. </p><p>

Fortunately it's designed to be highly modular and it's easy to work through it over multiple sessions, scanning over the articles it links to on each topic. </p><p>

Perhaps though, you'd prefer to absorb our most essential ideas in conversation form, in which case this episode is for you. </p><p>

If you want to have a big impact with your career, and you say you're only going to read one article from us, we recommend you read our <a href="https://80k.link/ki-bt"><b>key ideas</b></a> page. </p><p>

And likewise, if you're only going to listen to one of our podcast episodes, it should be this one. We have fun and set a strong pace, running through: </p><p>

• Common misunderstandings of our advice <br>
• A high level overview of what 80,000 Hours generally recommends <br>
• Our key moral positions <br>
• What are the most pressing problems to work on and why? <br>
• Which careers effectively contribute to solving those problems? <br>
• Central aspects of career strategy like how to weigh up career capital, personal fit, and exploration <br>
• As well as plenty more. </p><p>

One benefit of this podcast over the article is that we can more easily communicate uncertainty, and dive into the things we're least sure about, or didn’t yet cover within the article. </p><p>

Note though that our what’s in the article is more precisely stated, our advice is going to keep shifting, and we're aiming to keep the key ideas page current as our thinking evolves over time. This episode was recorded in November 2019, so if you notice a conflict between the page and this episode in the future, go with the page! </p><p>

<b>Get the episode by subscribing: type 80,000 Hours into your podcasting app. </b></p><p>


<em>Producer: Keiran Harris. </em><br>
<em>Audio mastering: Ben Cordell. </em><br>
<em>Transcriptions: Zakee Ulhaq. </em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[The 80,000 Hours Podcast is about “the world’s most pressing problems and how you can use your career to solve them”, and in this episode we tackle that question in the most direct way possible. <p>

Last year we published a summary of all our <a href="https://80k.link/ki-bt"><b>key ideas</b></a>, which links to many of our other articles, and which we are aiming to keep updated as our opinions shift. </p><p>

All of us added something to it, but the single biggest contributor was our CEO and today's guest, Ben Todd, who founded 80,000 Hours along with Will MacAskill back in 2012. </p><p>

This key ideas page is the most read on the site. By itself it can teach you a large fraction of the most important things we've discovered since we started investigating high impact careers. </p><p>

<b>• <a href="https://80000hours.org/podcast/episodes/ben-todd-key-ideas-of-80000hours/?utm_campaign=podcast__ben-todd&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></b> </p><p>

But it's perhaps more accurate to think of it as a mini-book, as it weighs in at over 20,000 words. </p><p>

Fortunately it's designed to be highly modular and it's easy to work through it over multiple sessions, scanning over the articles it links to on each topic. </p><p>

Perhaps though, you'd prefer to absorb our most essential ideas in conversation form, in which case this episode is for you. </p><p>

If you want to have a big impact with your career, and you say you're only going to read one article from us, we recommend you read our <a href="https://80k.link/ki-bt"><b>key ideas</b></a> page. </p><p>

And likewise, if you're only going to listen to one of our podcast episodes, it should be this one. We have fun and set a strong pace, running through: </p><p>

• Common misunderstandings of our advice <br>
• A high level overview of what 80,000 Hours generally recommends <br>
• Our key moral positions <br>
• What are the most pressing problems to work on and why? <br>
• Which careers effectively contribute to solving those problems? <br>
• Central aspects of career strategy like how to weigh up career capital, personal fit, and exploration <br>
• As well as plenty more. </p><p>

One benefit of this podcast over the article is that we can more easily communicate uncertainty, and dive into the things we're least sure about, or didn’t yet cover within the article. </p><p>

Note though that our what’s in the article is more precisely stated, our advice is going to keep shifting, and we're aiming to keep the key ideas page current as our thinking evolves over time. This episode was recorded in November 2019, so if you notice a conflict between the page and this episode in the future, go with the page! </p><p>

<b>Get the episode by subscribing: type 80,000 Hours into your podcasting app. </b></p><p>


<em>Producer: Keiran Harris. </em><br>
<em>Audio mastering: Ben Cordell. </em><br>
<em>Transcriptions: Zakee Ulhaq. </em></p>]]>
      </content:encoded>
      <pubDate>Mon, 02 Mar 2020 23:50:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/bfe83923/9a2d56f4.mp3" length="170950554" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/wxX1mJ5LCi6MxSu1cC4X46q-YjPeGJ6BuNq7GwxArVE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ4MDAv/MTY4MzU0NDYyNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10649</itunes:duration>
      <itunes:summary>The 80,000 Hours Podcast is about “the world’s most pressing problems and how you can use your career to solve them”, and in this episode we tackle that question in the most direct way possible. 

Last year we published a summary of all our key ideas, which links to many of our other articles, and which we are aiming to keep updated as our opinions shift. 

All of us added something to it, but the single biggest contributor was our CEO and today's guest, Ben Todd, who founded 80,000 Hours along with Will MacAskill back in 2012. 

This key ideas page is the most read on the site. By itself it can teach you a large fraction of the most important things we've discovered since we started investigating high impact careers. 

• Links to learn more, summary and full transcript. 

But it's perhaps more accurate to think of it as a mini-book, as it weighs in at over 20,000 words. 

Fortunately it's designed to be highly modular and it's easy to work through it over multiple sessions, scanning over the articles it links to on each topic. 

Perhaps though, you'd prefer to absorb our most essential ideas in conversation form, in which case this episode is for you. 

If you want to have a big impact with your career, and you say you're only going to read one article from us, we recommend you read our key ideas page. 

And likewise, if you're only going to listen to one of our podcast episodes, it should be this one. We have fun and set a strong pace, running through: 

• Common misunderstandings of our advice 
• A high level overview of what 80,000 Hours generally recommends 
• Our key moral positions 
• What are the most pressing problems to work on and why? 
• Which careers effectively contribute to solving those problems? 
• Central aspects of career strategy like how to weigh up career capital, personal fit, and exploration 
• As well as plenty more. 

One benefit of this podcast over the article is that we can more easily communicate uncertainty, and dive into the things we're least sure about, or didn’t yet cover within the article. 

Note though that our what’s in the article is more precisely stated, our advice is going to keep shifting, and we're aiming to keep the key ideas page current as our thinking evolves over time. This episode was recorded in November 2019, so if you notice a conflict between the page and this episode in the future, go with the page! 

Get the episode by subscribing: type 80,000 Hours into your podcasting app. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq. </itunes:summary>
      <itunes:subtitle>The 80,000 Hours Podcast is about “the world’s most pressing problems and how you can use your career to solve them”, and in this episode we tackle that question in the most direct way possible. 

Last year we published a summary of all our key ideas, whi</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/bfe83923/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Arden &amp; Rob on demandingness, work-life balance &amp; injustice (80k team chat #1)</title>
      <itunes:title>Arden &amp; Rob on demandingness, work-life balance &amp; injustice (80k team chat #1)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f975e7bc-5767-11ea-baf0-0ebe1d36953f</guid>
      <link>https://share.transistor.fm/s/a8e7faa3</link>
      <description>
        <![CDATA[Today's bonus episode of the podcast is a quick conversation between me and my fellow 80,000 Hours researcher Arden Koehler about a few topics, including the demandingness of morality, work-life balance, and emotional reactions to injustice. <p>

Arden is about to graduate with a philosophy PhD from New York University, so naturally we dive right into some challenging implications of utilitarian philosophy and how it might be applied to real life. Issues we talk about include: </p><p>

• If you’re not going to be completely moral, should you try being a bit more ethical, or give up? <br>
• Should you feel angry if you see an injustice, and if so, why? <br>
• How much should we ask people to live frugally? </p><p>

So far the feedback on the post-episode chats that we've done have been positive, so we thought we'd go ahead and try out this freestanding one. But fair warning: it's among the more difficult episodes to follow, and probably not the best one to listen to first, as you'll benefit from having more context!</p><p>

If you'd like to listen to more of Arden you can find her in episode 67, <b><a href="https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/">David Chalmers on the nature and ethics of consciousness</a>,</b> or episode 66, <a href="https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/"><b>Peter Singer on being provocative, EA, and how his moral views have changed</b></a>. </p><p>

Here's more information on some of the issues we touch on:</p><p>

• <a href="https://en.wikipedia.org/wiki/Consequentialism"><b>Consequentialism</b></a> on Wikipedia<br>
• <a href="https://plato.stanford.edu/entries/dispositions/"><b>Appropriate dispositions</b></a> on the Stanford Encyclopaedia of Philosophy<br>
• <a href="https://en.wikipedia.org/wiki/Demandingness_objection"><b>Demandingness objection</b></a> on Wikipedia<br>
• And a paper on <a href="https://www.jstor.org/stable/20117752?seq=1"><b>epistemic normativity</b></a>. </p><p>

——— </p><p>

I mention the call for papers of the Academic Workshop on Global Priorities in the introduction — you can <a href="https://www.eagxaustralia.com/workshop-on-global-priorities/"><b>learn more here</b></a>. </p><p>

And finally, Toby Ord — one of our founding Trustees and a Senior Research Fellow in Philosophy at Oxford University — has his new book <a href="https://theprecipice.com/"><b>The Precipice: Existential Risk and the Future of Humanity</b></a> coming out next week. I've read it and very much enjoyed it. Find out where you can pre-order it <a href="https://theprecipice.com/purchase"><b>here</b></a>. We'll have an interview with him coming up soon.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today's bonus episode of the podcast is a quick conversation between me and my fellow 80,000 Hours researcher Arden Koehler about a few topics, including the demandingness of morality, work-life balance, and emotional reactions to injustice. <p>

Arden is about to graduate with a philosophy PhD from New York University, so naturally we dive right into some challenging implications of utilitarian philosophy and how it might be applied to real life. Issues we talk about include: </p><p>

• If you’re not going to be completely moral, should you try being a bit more ethical, or give up? <br>
• Should you feel angry if you see an injustice, and if so, why? <br>
• How much should we ask people to live frugally? </p><p>

So far the feedback on the post-episode chats that we've done have been positive, so we thought we'd go ahead and try out this freestanding one. But fair warning: it's among the more difficult episodes to follow, and probably not the best one to listen to first, as you'll benefit from having more context!</p><p>

If you'd like to listen to more of Arden you can find her in episode 67, <b><a href="https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/">David Chalmers on the nature and ethics of consciousness</a>,</b> or episode 66, <a href="https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/"><b>Peter Singer on being provocative, EA, and how his moral views have changed</b></a>. </p><p>

Here's more information on some of the issues we touch on:</p><p>

• <a href="https://en.wikipedia.org/wiki/Consequentialism"><b>Consequentialism</b></a> on Wikipedia<br>
• <a href="https://plato.stanford.edu/entries/dispositions/"><b>Appropriate dispositions</b></a> on the Stanford Encyclopaedia of Philosophy<br>
• <a href="https://en.wikipedia.org/wiki/Demandingness_objection"><b>Demandingness objection</b></a> on Wikipedia<br>
• And a paper on <a href="https://www.jstor.org/stable/20117752?seq=1"><b>epistemic normativity</b></a>. </p><p>

——— </p><p>

I mention the call for papers of the Academic Workshop on Global Priorities in the introduction — you can <a href="https://www.eagxaustralia.com/workshop-on-global-priorities/"><b>learn more here</b></a>. </p><p>

And finally, Toby Ord — one of our founding Trustees and a Senior Research Fellow in Philosophy at Oxford University — has his new book <a href="https://theprecipice.com/"><b>The Precipice: Existential Risk and the Future of Humanity</b></a> coming out next week. I've read it and very much enjoyed it. Find out where you can pre-order it <a href="https://theprecipice.com/purchase"><b>here</b></a>. We'll have an interview with him coming up soon.</p>]]>
      </content:encoded>
      <pubDate>Tue, 25 Feb 2020 22:17:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a8e7faa3/b878862f.mp3" length="43543689" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/bI8CiOKsVJnNGgyMwCYw615yefH_MtbKQWDI8BbhB2I/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTkv/MTY4MzU0NDYyMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2652</itunes:duration>
      <itunes:summary>Today's bonus episode of the podcast is a quick conversation between me and my fellow 80,000 Hours researcher Arden Koehler about a few topics, including the demandingness of morality, work-life balance, and emotional reactions to injustice. 

Arden is about to graduate with a philosophy PhD from New York University, so naturally we dive right into some challenging implications of utilitarian philosophy and how it might be applied to real life. Issues we talk about include: 

• If you’re not going to be completely moral, should you try being a bit more ethical, or give up? 
• Should you feel angry if you see an injustice, and if so, why? 
• How much should we ask people to live frugally? 

So far the feedback on the post-episode chats that we've done have been positive, so we thought we'd go ahead and try out this freestanding one. But fair warning: it's among the more difficult episodes to follow, and probably not the best one to listen to first, as you'll benefit from having more context!

If you'd like to listen to more of Arden you can find her in episode 67, David Chalmers on the nature and ethics of consciousness, or episode 66, Peter Singer on being provocative, EA, and how his moral views have changed. 

Here's more information on some of the issues we touch on:

• Consequentialism on Wikipedia
• Appropriate dispositions on the Stanford Encyclopaedia of Philosophy
• Demandingness objection on Wikipedia
• And a paper on epistemic normativity. 

——— 

I mention the call for papers of the Academic Workshop on Global Priorities in the introduction — you can learn more here. 

And finally, Toby Ord — one of our founding Trustees and a Senior Research Fellow in Philosophy at Oxford University — has his new book The Precipice: Existential Risk and the Future of Humanity coming out next week. I've read it and very much enjoyed it. Find out where you can pre-order it here. We'll have an interview with him coming up soon.</itunes:summary>
      <itunes:subtitle>Today's bonus episode of the podcast is a quick conversation between me and my fellow 80,000 Hours researcher Arden Koehler about a few topics, including the demandingness of morality, work-life balance, and emotional reactions to injustice. 

Arden is ab</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#70 - Dr Cassidy Nelson on the 12 best ways to stop the next pandemic (and limit nCoV)</title>
      <itunes:title>#70 - Dr Cassidy Nelson on the 12 best ways to stop the next pandemic (and limit nCoV)</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2dacdfc0-4eaa-11ea-b06b-0e3f4a697fb5</guid>
      <link>https://80000hours.org/podcast/episodes/cassidy-nelson-12-ways-to-stop-pandemics/</link>
      <description>
        <![CDATA[<p>nCoV is alarming governments and citizens around the world. It has killed more than 1,000 people, brought the Chinese economy to a standstill, and continues to show up in more and more places. But bad though it is, it's much closer to a warning shot than a worst case scenario. The next emerging infectious disease could easily be more contagious, more fatal, or both.</p><p> Despite improvements in the last few decades, humanity is still not nearly prepared enough to contain new diseases. We identify them too slowly. We can't do enough to reduce their spread. And we lack vaccines or drugs treatments for at least a year, if they ever arrive at all.</p><p> • <a href="https://80000hours.org/podcast/episodes/cassidy-nelson-12-ways-to-stop-pandemics/?utm_campaign=podcast__cassidy-nelson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This is a precarious situation, especially with advances in biotechnology increasing our ability to modify viruses and bacteria as we like.</p><p> In today's episode, Cassidy Nelson, a medical doctor and research scholar at Oxford University's <em>Future of Humanity Institute</em>, explains 12 things her research group think urgently need to happen if we're to keep the risk at acceptable levels. The ideas are:</p><p> <strong>Science</strong></p><p> 1. Roll out genetic sequencing tests that lets you test someone for all known and unknown pathogens in one go.<br> 2. Fund research into faster ‘platform’ methods for going from pathogen to vaccine, perhaps using innovation prizes.<br> 3. Fund R&amp;D into broad-spectrum drugs, especially antivirals, similar to how we have generic antibiotics against multiple types of bacteria. </p><p> <strong>Response</strong> </p><p> 4. Develop a national plan for responding to a severe pandemic, regardless of the cause. Have a backup plan for when things are so bad the normal processes have stopped working entirely.<br> 5. Rigorously evaluate in what situations travel bans are warranted. (They're more often counterproductive.)<br> 6. Coax countries into more rapidly sharing their medical data, so that during an outbreak the disease can be understood and countermeasures deployed as quickly as possible.<br> 7. Set up genetic surveillance in hospitals, public transport and elsewhere, to detect new pathogens before an outbreak — or even before patients develop symptoms.<br> 8. Run regular tabletop exercises within governments to simulate how a pandemic response would play out. </p><p> <strong>Oversight</strong> </p><p> 9. Mandate disclosure of accidents in the biosafety labs which handle the most dangerous pathogens.<br> 10. Figure out how to govern DNA synthesis businesses, to make it harder to mail order the DNA of a dangerous pathogen.<br> 11. Require full cost-benefit analysis of 'dual-use' research projects that can generate global risks.<br> <br> 12. And finally, to maintain momentum, it's necessary to clearly assign responsibility for the above to particular individuals and organisations.</p><p> These advances can be pursued by politicians and public servants, as well as academics, entrepreneurs and doctors, opening the door for many listeners to pitch in to help solve this incredibly pressing problem.</p><p> In the episode Rob and Cassidy also talk about:</p><p> • How Cassidy went from clinical medicine to a PhD studying novel pathogens with pandemic potential.<br> • The pros, and significant cons, of travel restrictions.<br> • Whether the same policies work for natural and anthropogenic pandemics.<br> • Ways listeners can pursue a career in biosecurity.<br> • Where we stand with nCoV as of today.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:27)</li><li>Where we stand with nCov today (00:07:24)</li><li>Policy idea 1: A drastic change to diagnostic testing (00:34:58)</li><li>Policy idea 2: Vaccine platforms (00:47:08)</li><li>Policy idea 3: Broad-spectrum therapeutics (00:54:48)</li><li>Policy idea 4: Develop a national plan for responding to a severe pandemic, regardless of the cause (01:02:15)</li><li>Policy idea 5: A different approach to travel bans (01:15:59)</li><li>Policy idea 6: Data sharing (01:16:48)</li><li>Policy idea 7: Prevention (01:24:45)</li><li>Policy idea 8: transparency around lab accidents (01:33:58)</li><li>Policy idea 9: DNA synthesis screening (01:39:22)</li><li>Policy idea 10: Dual Use Research oversight (01:48:47)</li><li>Policy idea 11: Pandemic tabletop exercises (02:00:00)</li><li>Policy idea 12: Coordination (02:12:20)</li></ul><p><br> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</strong></p><p> <em>Producer: Keiran Harris.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>nCoV is alarming governments and citizens around the world. It has killed more than 1,000 people, brought the Chinese economy to a standstill, and continues to show up in more and more places. But bad though it is, it's much closer to a warning shot than a worst case scenario. The next emerging infectious disease could easily be more contagious, more fatal, or both.</p><p> Despite improvements in the last few decades, humanity is still not nearly prepared enough to contain new diseases. We identify them too slowly. We can't do enough to reduce their spread. And we lack vaccines or drugs treatments for at least a year, if they ever arrive at all.</p><p> • <a href="https://80000hours.org/podcast/episodes/cassidy-nelson-12-ways-to-stop-pandemics/?utm_campaign=podcast__cassidy-nelson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This is a precarious situation, especially with advances in biotechnology increasing our ability to modify viruses and bacteria as we like.</p><p> In today's episode, Cassidy Nelson, a medical doctor and research scholar at Oxford University's <em>Future of Humanity Institute</em>, explains 12 things her research group think urgently need to happen if we're to keep the risk at acceptable levels. The ideas are:</p><p> <strong>Science</strong></p><p> 1. Roll out genetic sequencing tests that lets you test someone for all known and unknown pathogens in one go.<br> 2. Fund research into faster ‘platform’ methods for going from pathogen to vaccine, perhaps using innovation prizes.<br> 3. Fund R&amp;D into broad-spectrum drugs, especially antivirals, similar to how we have generic antibiotics against multiple types of bacteria. </p><p> <strong>Response</strong> </p><p> 4. Develop a national plan for responding to a severe pandemic, regardless of the cause. Have a backup plan for when things are so bad the normal processes have stopped working entirely.<br> 5. Rigorously evaluate in what situations travel bans are warranted. (They're more often counterproductive.)<br> 6. Coax countries into more rapidly sharing their medical data, so that during an outbreak the disease can be understood and countermeasures deployed as quickly as possible.<br> 7. Set up genetic surveillance in hospitals, public transport and elsewhere, to detect new pathogens before an outbreak — or even before patients develop symptoms.<br> 8. Run regular tabletop exercises within governments to simulate how a pandemic response would play out. </p><p> <strong>Oversight</strong> </p><p> 9. Mandate disclosure of accidents in the biosafety labs which handle the most dangerous pathogens.<br> 10. Figure out how to govern DNA synthesis businesses, to make it harder to mail order the DNA of a dangerous pathogen.<br> 11. Require full cost-benefit analysis of 'dual-use' research projects that can generate global risks.<br> <br> 12. And finally, to maintain momentum, it's necessary to clearly assign responsibility for the above to particular individuals and organisations.</p><p> These advances can be pursued by politicians and public servants, as well as academics, entrepreneurs and doctors, opening the door for many listeners to pitch in to help solve this incredibly pressing problem.</p><p> In the episode Rob and Cassidy also talk about:</p><p> • How Cassidy went from clinical medicine to a PhD studying novel pathogens with pandemic potential.<br> • The pros, and significant cons, of travel restrictions.<br> • Whether the same policies work for natural and anthropogenic pandemics.<br> • Ways listeners can pursue a career in biosecurity.<br> • Where we stand with nCoV as of today.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:03:27)</li><li>Where we stand with nCov today (00:07:24)</li><li>Policy idea 1: A drastic change to diagnostic testing (00:34:58)</li><li>Policy idea 2: Vaccine platforms (00:47:08)</li><li>Policy idea 3: Broad-spectrum therapeutics (00:54:48)</li><li>Policy idea 4: Develop a national plan for responding to a severe pandemic, regardless of the cause (01:02:15)</li><li>Policy idea 5: A different approach to travel bans (01:15:59)</li><li>Policy idea 6: Data sharing (01:16:48)</li><li>Policy idea 7: Prevention (01:24:45)</li><li>Policy idea 8: transparency around lab accidents (01:33:58)</li><li>Policy idea 9: DNA synthesis screening (01:39:22)</li><li>Policy idea 10: Dual Use Research oversight (01:48:47)</li><li>Policy idea 11: Pandemic tabletop exercises (02:00:00)</li><li>Policy idea 12: Coordination (02:12:20)</li></ul><p><br> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</strong></p><p> <em>Producer: Keiran Harris.<br> Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 13 Feb 2020 23:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/04a6ca32/aa1a8d8b.mp3" length="141548616" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Ss22b4WqRIPDI7Vlzv6vdfSpOo3MLUtZGxBaokiIRys/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTgv/MTY4MzU0NDYyMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8793</itunes:duration>
      <itunes:summary>nCoV is alarming governments and citizens around the world. It has killed more than 1,000 people, brought the Chinese economy to a standstill, and continues to show up in more and more places. But bad though it is, it's much closer to a warning shot than a worst case scenario. The next emerging infectious disease could easily be more contagious, more fatal, or both.

Despite improvements in the last few decades, humanity is still not nearly prepared enough to contain new diseases. We identify them too slowly. We can't do enough to reduce their spread. And we lack vaccines or drugs treatments for at least a year, if they ever arrive at all.

• Links to learn more, summary and full transcript.

This is a precarious situation, especially with advances in biotechnology increasing our ability to modify viruses and bacteria as we like.

In today's episode, Cassidy Nelson, a medical doctor and research scholar at Oxford University's Future of Humanity Institute, explains 12 things her research group think urgently need to happen if we're to keep the risk at acceptable levels. The ideas are:

Science
1. Roll out genetic sequencing tests that lets you test someone for all known and unknown pathogens in one go.
2. Fund research into faster ‘platform’ methods for going from pathogen to vaccine, perhaps using innovation prizes.
3. Fund R&amp;amp;D into broad-spectrum drugs, especially antivirals, similar to how we have generic antibiotics against multiple types of bacteria.

Response

4. Develop a national plan for responding to a severe pandemic, regardless of the cause. Have a backup plan for when things are so bad the normal processes have stopped working entirely.
5. Rigorously evaluate in what situations travel bans are warranted. (They're more often counterproductive.)
6. Coax countries into more rapidly sharing their medical data, so that during an outbreak the disease can be understood and countermeasures deployed as quickly as possible.
7. Set up genetic surveillance in hospitals, public transport and elsewhere, to detect new pathogens before an outbreak — or even before patients develop symptoms.
8. Run regular tabletop exercises within governments to simulate how a pandemic response would play out.

Oversight

9. Mandate disclosure of accidents in the biosafety labs which handle the most dangerous pathogens.
10. Figure out how to govern DNA synthesis businesses, to make it harder to mail order the DNA of a dangerous pathogen.
11. Require full cost-benefit analysis of 'dual-use' research projects that can generate global risks.
 
12. And finally, to maintain momentum, it's necessary to clearly assign responsibility for the above to particular individuals and organisations.

These advances can be pursued by politicians and public servants, as well as academics, entrepreneurs and doctors, opening the door for many listeners to pitch in to help solve this incredibly pressing problem.

In the episode Rob and Cassidy also talk about:

• How Cassidy went from clinical medicine to a PhD studying novel pathogens with pandemic potential.
• The pros, and significant cons, of travel restrictions.
• Whether the same policies work for natural and anthropogenic pandemics.
• Ways listeners can pursue a career in biosecurity.
• Where we stand with nCoV as of today.

Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.


Producer: Keiran Harris.
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>nCoV is alarming governments and citizens around the world. It has killed more than 1,000 people, brought the Chinese economy to a standstill, and continues to show up in more and more places. But bad though it is, it's much closer to a warning shot than </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/04a6ca32/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/04a6ca32/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#69 – Jeffrey Ding on China, its AI dream, and what we get wrong about both</title>
      <itunes:title>#69 – Jeffrey Ding on China, its AI dream, and what we get wrong about both</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">70fcfa7a-490b-11ea-bb0a-0ef900b9bcb9</guid>
      <link>https://80000hours.org/podcast/episodes/jeffrey-ding-china-ai-dream/</link>
      <description>
        <![CDATA[<p>The State Council of China's 2017 AI plan was the starting point of China’s AI planning; China’s approach to AI is defined by its top-down and monolithic nature; China is winning the AI arms race; and there is little to no discussion of issues of AI ethics and safety in China. How many of these ideas have you heard? </p><p>In his paper <a href="https://80k.link/dcaid"><strong>Deciphering China's AI Dream</strong></a>, today's guest, PhD student Jeff Ding, outlines why he believes none of these claims are true. </p><p> • <a href="https://80k.link/jding">Links to learn more, summary and full transcript.</a></p><p> • <a href="https://80k.link/best-charity">What’s the best charity to donate to?</a></p><p> He first places China’s new AI strategy in the <strong>context</strong> of its past science and technology plans, as well as other countries’ AI plans. What is China actually doing in the space of AI development? </p><p> Jeff emphasises that China's AI strategy did not appear out of nowhere with the 2017 state council AI development plan, which attracted a lot of overseas attention. Rather that was just another step forward in a long trajectory of increasing focus on science and technology. It's connected with a plan to develop an 'Internet of Things', and linked to a history of strategic planning for technology in areas like aerospace and biotechnology. </p><p> And it was not just the central government that was moving in this space; companies were already pushing forward in AI development, and local level governments already had their own AI plans. You could argue that the central government was following their lead in AI more than the reverse. </p><p> What are the different levers that China is pulling to try to spur AI development? </p><p> Here, Jeff wanted to challenge the myth that China's AI development plan is based on a monolithic central plan requiring people to develop AI. In fact, bureaucratic agencies, companies, academic labs, and local governments each set up their own strategies, which sometimes conflict with the central government. </p><p> Are China's AI <strong>capabilities</strong> especially impressive? In the paper Jeff develops a new index to measure and compare the US and China's progress in AI. </p><p> Jeff’s AI Potential Index — which incorporates trends and capabilities in data, hardware, research and talent, and the commercial AI ecosystem — indicates China’s AI capabilities are about half those of America. His measure, though imperfect, dispels the notion that China's AI capabilities have surpassed the US or make it the world's leading AI power. </p><p> Following that 2017 plan, a lot of Western observers thought that to have a good national AI strategy we'd need to figure out how to play catch-up with China. Yet Chinese strategic thinkers and writers at the time actually thought that they were behind — because the Obama administration had issued a series of three white papers in 2016. </p><p> Finally, Jeff turns to the potential <strong>consequences</strong> of China’s AI dream for issues of national security, economic development, AI safety and social governance. </p><p> He claims that, despite the widespread belief to the contrary, substantive discussions about AI safety and ethics are indeed emerging in China. For instance, a new book from Tencent’s Research Institute is proactive in calling for stronger awareness of AI safety issues. </p><p> In today’s episode, Rob and Jeff go through this widely-discussed report, and also cover: </p><p> • The best analogies for thinking about the growing influence of AI <br> • How do prominent Chinese figures think about AI? <br> • Coordination with China<br> • China’s social credit system <br> • Suggestions for people who want to become professional China specialists <br> • And more.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:02)</li><li>Deciphering China’s AI Dream (00:04:17)</li><li>Analogies for thinking about AI (00:12:30)</li><li>How do prominent Chinese figures think about AI? (00:16:15)</li><li>Cultural cliches in the West and China (00:18:59)</li><li>Coordination with China on AI (00:24:03)</li><li>Private companies vs. government research (00:28:55)</li><li>Compute (00:31:58)</li><li>China’s social credit system (00:41:26)</li><li>Relationship between China and other countries beyond AI (00:43:51)</li><li>Careers advice (00:54:40)</li><li>Jeffrey’s talk at EAG (01:16:01)</li><li>Rob’s outro (01:37:12) </li></ul><p><br> <em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The State Council of China's 2017 AI plan was the starting point of China’s AI planning; China’s approach to AI is defined by its top-down and monolithic nature; China is winning the AI arms race; and there is little to no discussion of issues of AI ethics and safety in China. How many of these ideas have you heard? </p><p>In his paper <a href="https://80k.link/dcaid"><strong>Deciphering China's AI Dream</strong></a>, today's guest, PhD student Jeff Ding, outlines why he believes none of these claims are true. </p><p> • <a href="https://80k.link/jding">Links to learn more, summary and full transcript.</a></p><p> • <a href="https://80k.link/best-charity">What’s the best charity to donate to?</a></p><p> He first places China’s new AI strategy in the <strong>context</strong> of its past science and technology plans, as well as other countries’ AI plans. What is China actually doing in the space of AI development? </p><p> Jeff emphasises that China's AI strategy did not appear out of nowhere with the 2017 state council AI development plan, which attracted a lot of overseas attention. Rather that was just another step forward in a long trajectory of increasing focus on science and technology. It's connected with a plan to develop an 'Internet of Things', and linked to a history of strategic planning for technology in areas like aerospace and biotechnology. </p><p> And it was not just the central government that was moving in this space; companies were already pushing forward in AI development, and local level governments already had their own AI plans. You could argue that the central government was following their lead in AI more than the reverse. </p><p> What are the different levers that China is pulling to try to spur AI development? </p><p> Here, Jeff wanted to challenge the myth that China's AI development plan is based on a monolithic central plan requiring people to develop AI. In fact, bureaucratic agencies, companies, academic labs, and local governments each set up their own strategies, which sometimes conflict with the central government. </p><p> Are China's AI <strong>capabilities</strong> especially impressive? In the paper Jeff develops a new index to measure and compare the US and China's progress in AI. </p><p> Jeff’s AI Potential Index — which incorporates trends and capabilities in data, hardware, research and talent, and the commercial AI ecosystem — indicates China’s AI capabilities are about half those of America. His measure, though imperfect, dispels the notion that China's AI capabilities have surpassed the US or make it the world's leading AI power. </p><p> Following that 2017 plan, a lot of Western observers thought that to have a good national AI strategy we'd need to figure out how to play catch-up with China. Yet Chinese strategic thinkers and writers at the time actually thought that they were behind — because the Obama administration had issued a series of three white papers in 2016. </p><p> Finally, Jeff turns to the potential <strong>consequences</strong> of China’s AI dream for issues of national security, economic development, AI safety and social governance. </p><p> He claims that, despite the widespread belief to the contrary, substantive discussions about AI safety and ethics are indeed emerging in China. For instance, a new book from Tencent’s Research Institute is proactive in calling for stronger awareness of AI safety issues. </p><p> In today’s episode, Rob and Jeff go through this widely-discussed report, and also cover: </p><p> • The best analogies for thinking about the growing influence of AI <br> • How do prominent Chinese figures think about AI? <br> • Coordination with China<br> • China’s social credit system <br> • Suggestions for people who want to become professional China specialists <br> • And more.</p><p> Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:02)</li><li>Deciphering China’s AI Dream (00:04:17)</li><li>Analogies for thinking about AI (00:12:30)</li><li>How do prominent Chinese figures think about AI? (00:16:15)</li><li>Cultural cliches in the West and China (00:18:59)</li><li>Coordination with China on AI (00:24:03)</li><li>Private companies vs. government research (00:28:55)</li><li>Compute (00:31:58)</li><li>China’s social credit system (00:41:26)</li><li>Relationship between China and other countries beyond AI (00:43:51)</li><li>Careers advice (00:54:40)</li><li>Jeffrey’s talk at EAG (01:16:01)</li><li>Rob’s outro (01:37:12) </li></ul><p><br> <em>Producer: Keiran Harris.<br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 06 Feb 2020 23:07:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c067d733/c63d5aed.mp3" length="93639635" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/eBBzoEYZ1sjRuwrpm8vAB6bunsxLhT7B3qZ5uvN0KFY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9mMDAx/M2Y4YjBmOGQxYzYz/M2Y0ZjBkMTZkOTYy/N2U1ZC53ZWJw.jpg"/>
      <itunes:duration>5834</itunes:duration>
      <itunes:summary>The State Council of China's 2017 AI plan was the starting point of China’s AI planning; China’s approach to AI is defined by its top-down and monolithic nature; China is winning the AI arms race; and there is little to no discussion of issues of AI ethics and safety in China. How many of these ideas have you heard? 

In his paper Deciphering China's AI Dream, today's guest, PhD student Jeff Ding, outlines why he believes none of these claims are true.  

• Links to learn more, summary and full transcript. 

• What’s the best charity to donate to?

He first places China’s new AI strategy in the context of its past science and technology plans, as well as other countries’ AI plans. What is China actually doing in the space of AI development? 

Jeff emphasises that China's AI strategy did not appear out of nowhere with the 2017 state council AI development plan, which attracted a lot of overseas attention. Rather that was just another step forward in a long trajectory of increasing focus on science and technology. It's connected with a plan to develop an 'Internet of Things', and linked to a history of strategic planning for technology in areas like aerospace and biotechnology. 

And it was not just the central government that was moving in this space; companies were already pushing forward in AI development, and local level governments already had their own AI plans. You could argue that the central government was following their lead in AI more than the reverse. 

What are the different levers that China is pulling to try to spur AI development? 

Here, Jeff wanted to challenge the myth that China's AI development plan is based on a monolithic central plan requiring people to develop AI. In fact, bureaucratic agencies, companies, academic labs, and local governments each set up their own strategies, which sometimes conflict with the central government. 

Are China's AI capabilities especially impressive? In the paper Jeff develops a new index to measure and compare the US and China's progress in AI. 

Jeff’s AI Potential Index — which incorporates trends and capabilities in data, hardware, research and talent, and the commercial AI ecosystem — indicates China’s AI capabilities are about half those of America. His measure, though imperfect, dispels the notion that China's AI capabilities have surpassed the US or make it the world's leading AI power.  

Following that 2017 plan, a lot of Western observers thought that to have a good national AI strategy we'd need to figure out how to play catch-up with China. Yet Chinese strategic thinkers and writers at the time actually thought that they were behind — because the Obama administration had issued a series of three white papers in 2016. 

Finally, Jeff turns to the potential consequences of China’s AI dream for issues of national security, economic development, AI safety and social governance. 

He claims that, despite the widespread belief to the contrary, substantive discussions about AI safety and ethics are indeed emerging in China. For instance, a new book from Tencent’s Research Institute is proactive in calling for stronger awareness of AI safety issues. 

In today’s episode, Rob and Jeff go through this widely-discussed report, and also cover: 

• The best analogies for thinking about the growing influence of AI 
• How do prominent Chinese figures think about AI? 
• Coordination with China
• China’s social credit system 
• Suggestions for people who want to become professional China specialists 
• And more.

Get this episode by subscribing: type 80,000 Hours into your podcasting app. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>The State Council of China's 2017 AI plan was the starting point of China’s AI planning; China’s approach to AI is defined by its top-down and monolithic nature; China is winning the AI arms race; and there is little to no discussion of issues of AI ethic</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/c067d733/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/c067d733/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>Rob &amp; Howie on what we do and don't know about 2019-nCoV</title>
      <itunes:title>Rob &amp; Howie on what we do and don't know about 2019-nCoV</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">2552e4f4-46a4-11ea-a277-0ed81a817f7d</guid>
      <link>https://share.transistor.fm/s/b9c7f274</link>
      <description>
        <![CDATA[<p>Two 80,000 Hours researchers, Robert Wiblin and Howie Lempel, record an experimental bonus episode about the new 2019-nCoV virus.</p><p>See this <a href="%20https://80000hours.org/2020/02/experimental-episode-about-2019-ncov-coronavirus/">list of resources</a>, including many discussed in the episode, to learn more.</p><p>In the 1h15m conversation we cover:</p><p>• What is it? <br>• How many people have it? <br>• How contagious is it? <br>• What fraction of people who contract it die?<br>• How likely is it to spread out of control?<br>• What's the range of plausible fatalities worldwide?<br>• How does it compare to other epidemics?<br>• What don't we know and why? <br>• What actions should listeners take, if any?<br>• How should the complexities of the above be communicated by public health professionals?</p><p>Here's a link to the <a href="https://foreignpolicy.com/2020/01/25/wuhan-coronavirus-safety-china/"><b>hygiene advice from Laurie Garrett</b></a> mentioned in the episode.</p><p>Recorded 2 Feb 2020.</p><p><i>The 80,000 Hours Podcast is produced by Keiran Harris.</i><br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Two 80,000 Hours researchers, Robert Wiblin and Howie Lempel, record an experimental bonus episode about the new 2019-nCoV virus.</p><p>See this <a href="%20https://80000hours.org/2020/02/experimental-episode-about-2019-ncov-coronavirus/">list of resources</a>, including many discussed in the episode, to learn more.</p><p>In the 1h15m conversation we cover:</p><p>• What is it? <br>• How many people have it? <br>• How contagious is it? <br>• What fraction of people who contract it die?<br>• How likely is it to spread out of control?<br>• What's the range of plausible fatalities worldwide?<br>• How does it compare to other epidemics?<br>• What don't we know and why? <br>• What actions should listeners take, if any?<br>• How should the complexities of the above be communicated by public health professionals?</p><p>Here's a link to the <a href="https://foreignpolicy.com/2020/01/25/wuhan-coronavirus-safety-china/"><b>hygiene advice from Laurie Garrett</b></a> mentioned in the episode.</p><p>Recorded 2 Feb 2020.</p><p><i>The 80,000 Hours Podcast is produced by Keiran Harris.</i><br></p>]]>
      </content:encoded>
      <pubDate>Mon, 03 Feb 2020 17:42:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b9c7f274/6c2f42c9.mp3" length="75995235" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/g9g8cjMq2WbH8lOK5VSH-KLE0Db1WMj7VWLSe6kvf8E/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTYv/MTY4MzU0NDYyMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4724</itunes:duration>
      <itunes:summary>Two 80,000 Hours researchers, Robert Wiblin and Howie Lempel, record an experimental bonus episode about the new 2019-nCoV virus.See this list of resources, including many discussed in the episode, to learn more.In the 1h15m conversation we cover:• What is it? • How many people have it? • How contagious is it? • What fraction of people who contract it die?• How likely is it to spread out of control?• What's the range of plausible fatalities worldwide?• How does it compare to other epidemics?• What don't we know and why? • What actions should listeners take, if any?• How should the complexities of the above be communicated by public health professionals?Here's a link to the hygiene advice from Laurie Garrett mentioned in the episode.Recorded 2 Feb 2020.The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Two 80,000 Hours researchers, Robert Wiblin and Howie Lempel, record an experimental bonus episode about the new 2019-nCoV virus.See this list of resources, including many discussed in the episode, to learn more.In the 1h15m conversation we cover:• What i</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#68 - Will MacAskill on the paralysis argument, whether we're at the hinge of history, &amp; his new priorities</title>
      <itunes:title>#68 - Will MacAskill on the paralysis argument, whether we're at the hinge of history, &amp; his new priorities</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">3a4dd1e2-3e3a-11ea-a6ce-0e215caa46a1</guid>
      <link>https://80000hours.org/podcast/episodes/will-macaskill-paralysis-and-hinge-of-history/</link>
      <description>
        <![CDATA[<p>You’re given a box with a set of dice in it. If you roll an even number, a person's life is saved. If you roll an odd number, someone else will die. Each time you shake the box you get $10. Should you do it? </p><p> A committed consequentialist might say, <em>"Sure! Free money!"</em> But most will think it obvious that you should say no. You've only gotten a tiny benefit, in exchange for moral responsibility over whether other people live or die. </p><p> And yet, according to today’s return guest, philosophy Prof Will MacAskill, in a real sense we’re shaking this box every time we leave the house, and those who think shaking the box is wrong should probably also be shutting themselves indoors and minimising their interactions with others. </p><p> • <a href="https://80k.link/will2"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://80k.link/gpio"><strong>Job opportunities at the Global Priorities Institute.</strong></a></p><p> To see this, imagine you’re deciding whether to redeem a coupon for a free movie. If you go, you’ll need to drive to the cinema. By affecting traffic throughout the city, you’ll have slightly impacted the schedules of thousands or tens of thousands of people. The average life is about 30,000 days, and over the course of a life the average person will have about two children. So — if you’ve impacted at least 7,500 days — then, statistically speaking, you've probably influenced the exact timing of a conception event. With 200 million sperm in the running each time, changing the moment of copulation, even by a fraction of a second, will almost certainly mean you've changed the identity of a future person. </p><p> That different child will now impact all sorts of things as they go about their life, including future conception events. And then those new people will impact further future conceptions events, and so on. After 100 or maybe 200 years, basically everybody alive will be a different person because you went to the movies. </p><p> As a result, you’ll have changed when many people die. Take car crashes as one example: about 1.3% of people die in car crashes. Over that century, as the identities of everyone change as a result of your action, many of the 'new' people will cause car crashes that wouldn't have occurred in their absence, including crashes that prematurely kill people alive today. </p><p> Of course, in expectation, exactly the same number of people will have been saved from car crashes, and will die later than they would have otherwise. </p><p> So, if you go for this drive, you’ll save hundreds of people from premature death, and cause the early death of an equal number of others. But you’ll get to see a free movie, worth $10. Should you do it? </p><p> This setup forms the basis of ‘the paralysis argument’, explored in one of Will’s <a href="https://80k.link/paral"><strong>recent papers</strong></a>. </p><p> Because most 'non-consequentialists' endorse an act/omission distinction… <em>post truncated due to character limit, </em><a href="https://80k.link/will2"><strong><em>finish reading the full explanation here.</em></strong></a> </p><p> So what's the best way to fix this strange conclusion? We discuss a few options, but the most promising might bring people a lot closer to full consequentialism than is immediately apparent. In this episode Will and I also cover: </p><p> • Are, or are we not, living in the most influential time in history? <br> • The culture of the effective altruism community <br> • Will's new lower estimate of the risk of human extinction <br> • Why Will is now less focused on AI <br> • The differences between Americans and Brits <br> • Why feeling guilty about characteristics you were born with is crazy <br> • And plenty more. </p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:03)</li><li>The paralysis argument (00:15:42)</li><li>The case for strong longtermism (00:55:21)</li><li>Longtermism for risk-averse altruists (00:58:01)</li><li>Are we living in the most influential time in history? (01:14:37)</li><li>The risk of human extinction in the next hundred years (02:15:20)</li><li>Implications for the effective altruism community (02:50:03)</li><li>Culture of the effective altruism community (03:06:28)</li></ul><p><em>Producer: Keiran Harris. <br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>You’re given a box with a set of dice in it. If you roll an even number, a person's life is saved. If you roll an odd number, someone else will die. Each time you shake the box you get $10. Should you do it? </p><p> A committed consequentialist might say, <em>"Sure! Free money!"</em> But most will think it obvious that you should say no. You've only gotten a tiny benefit, in exchange for moral responsibility over whether other people live or die. </p><p> And yet, according to today’s return guest, philosophy Prof Will MacAskill, in a real sense we’re shaking this box every time we leave the house, and those who think shaking the box is wrong should probably also be shutting themselves indoors and minimising their interactions with others. </p><p> • <a href="https://80k.link/will2"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://80k.link/gpio"><strong>Job opportunities at the Global Priorities Institute.</strong></a></p><p> To see this, imagine you’re deciding whether to redeem a coupon for a free movie. If you go, you’ll need to drive to the cinema. By affecting traffic throughout the city, you’ll have slightly impacted the schedules of thousands or tens of thousands of people. The average life is about 30,000 days, and over the course of a life the average person will have about two children. So — if you’ve impacted at least 7,500 days — then, statistically speaking, you've probably influenced the exact timing of a conception event. With 200 million sperm in the running each time, changing the moment of copulation, even by a fraction of a second, will almost certainly mean you've changed the identity of a future person. </p><p> That different child will now impact all sorts of things as they go about their life, including future conception events. And then those new people will impact further future conceptions events, and so on. After 100 or maybe 200 years, basically everybody alive will be a different person because you went to the movies. </p><p> As a result, you’ll have changed when many people die. Take car crashes as one example: about 1.3% of people die in car crashes. Over that century, as the identities of everyone change as a result of your action, many of the 'new' people will cause car crashes that wouldn't have occurred in their absence, including crashes that prematurely kill people alive today. </p><p> Of course, in expectation, exactly the same number of people will have been saved from car crashes, and will die later than they would have otherwise. </p><p> So, if you go for this drive, you’ll save hundreds of people from premature death, and cause the early death of an equal number of others. But you’ll get to see a free movie, worth $10. Should you do it? </p><p> This setup forms the basis of ‘the paralysis argument’, explored in one of Will’s <a href="https://80k.link/paral"><strong>recent papers</strong></a>. </p><p> Because most 'non-consequentialists' endorse an act/omission distinction… <em>post truncated due to character limit, </em><a href="https://80k.link/will2"><strong><em>finish reading the full explanation here.</em></strong></a> </p><p> So what's the best way to fix this strange conclusion? We discuss a few options, but the most promising might bring people a lot closer to full consequentialism than is immediately apparent. In this episode Will and I also cover: </p><p> • Are, or are we not, living in the most influential time in history? <br> • The culture of the effective altruism community <br> • Will's new lower estimate of the risk of human extinction <br> • Why Will is now less focused on AI <br> • The differences between Americans and Brits <br> • Why feeling guilty about characteristics you were born with is crazy <br> • And plenty more. </p><p> <strong>Chapters:</strong></p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:04:03)</li><li>The paralysis argument (00:15:42)</li><li>The case for strong longtermism (00:55:21)</li><li>Longtermism for risk-averse altruists (00:58:01)</li><li>Are we living in the most influential time in history? (01:14:37)</li><li>The risk of human extinction in the next hundred years (02:15:20)</li><li>Implications for the effective altruism community (02:50:03)</li><li>Culture of the effective altruism community (03:06:28)</li></ul><p><em>Producer: Keiran Harris. <br>Audio mastering: Ben Cordell. <br>Transcriptions: Zakee Ulhaq.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 24 Jan 2020 00:57:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/78c53b98/7202d48f.mp3" length="197959057" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/3Z9IhBDXHCvC7cT1H4JJZCm_G837UF89mx2GnZtMq-Y/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTUv/MTY4MzU0NDYxOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>12336</itunes:duration>
      <itunes:summary>You’re given a box with a set of dice in it. If you roll an even number, a person's life is saved. If you roll an odd number, someone else will die. Each time you shake the box you get $10. Should you do it? 

A committed consequentialist might say, "Sure! Free money!" But most will think it obvious that you should say no. You've only gotten a tiny benefit, in exchange for moral responsibility over whether other people live or die. 

And yet, according to today’s return guest, philosophy Prof Will MacAskill, in a real sense we’re shaking this box every time we leave the house, and those who think shaking the box is wrong should probably also be shutting themselves indoors and minimising their interactions with others. 

• Links to learn more, summary and full transcript.
• Job opportunities at the Global Priorities Institute.

To see this, imagine you’re deciding whether to redeem a coupon for a free movie. If you go, you’ll need to drive to the cinema. By affecting traffic throughout the city, you’ll have slightly impacted the schedules of thousands or tens of thousands of people. The average life is about 30,000 days, and over the course of a life the average person will have about two children. So — if you’ve impacted at least 7,500 days — then, statistically speaking, you've probably influenced the exact timing of a conception event. With 200 million sperm in the running each time, changing the moment of copulation, even by a fraction of a second, will almost certainly mean you've changed the identity of a future person. 

That different child will now impact all sorts of things as they go about their life, including future conception events. And then those new people will impact further future conceptions events, and so on. After 100 or maybe 200 years, basically everybody alive will be a different person because you went to the movies. 

As a result, you’ll have changed when many people die. Take car crashes as one example: about 1.3% of people die in car crashes. Over that century, as the identities of everyone change as a result of your action, many of the 'new' people will cause car crashes that wouldn't have occurred in their absence, including crashes that prematurely kill people alive today. 

Of course, in expectation, exactly the same number of people will have been saved from car crashes, and will die later than they would have otherwise. 

So, if you go for this drive, you’ll save hundreds of people from premature death, and cause the early death of an equal number of others. But you’ll get to see a free movie, worth $10. Should you do it? 

This setup forms the basis of ‘the paralysis argument’, explored in one of Will’s recent papers. 

Because most 'non-consequentialists' endorse an act/omission distinction… post truncated due to character limit, finish reading the full explanation here. 

So what's the best way to fix this strange conclusion? We discuss a few options, but the most promising might bring people a lot closer to full consequentialism than is immediately apparent. In this episode Will and I also cover: 

• Are, or are we not, living in the most influential time in history? 
• The culture of the effective altruism community 
• Will's new lower estimate of the risk of human extinction 
• Why Will is now less focused on AI 
• The differences between Americans and Brits 
• Why feeling guilty about characteristics you were born with is crazy 
• And plenty more. 

Get this episode by subscribing: type 80,000 Hours into your podcasting app. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq.</itunes:summary>
      <itunes:subtitle>You’re given a box with a set of dice in it. If you roll an even number, a person's life is saved. If you roll an odd number, someone else will die. Each time you shake the box you get $10. Should you do it? 

A committed consequentialist might say, "Su</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/78c53b98/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/78c53b98/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#44 Classic episode - Paul Christiano on finding real solutions to the AI alignment problem</title>
      <itunes:title>#44 Classic episode - Paul Christiano on finding real solutions to the AI alignment problem</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a1751eac-3728-11ea-87cf-0eb4505f1211</guid>
      <link>https://share.transistor.fm/s/849fe819</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in October 2018. </b><p>

</p><p>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening — Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.</p><p>

A few of the topics we cover are:</p><p>• Why Paul expects AI to transform the world gradually rather than explosively and what that would look like<br>
• Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us<br>
• Why AI systems will probably be granted legal and property rights<br>
• How an advanced AI that doesn't share human goals could still have moral value<br>
• Why machine learning might take over science research from humans before it can do most other tasks<br>
• Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.</p><p>

<b>• <a href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></b><br>
<b>• <a href="https://rohinshah.com/alignment-newsletter/">Rohin Shah's AI alignment newsletter.</a></b></p><p>

Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.</p><p>

If given plenty of time — and enough arguments, counterarguments and counter-counter-arguments between all the experts — should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?</p><p>

In other words: does 'debate', in principle, lead to truth?</p><p>

According to Paul Christiano — researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities — this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.</p><p>

It's a method OpenAI is <a href="https://blog.openai.com/debate/">actively trying to develop</a>, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight.</p><p> 

If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.</p><p>

But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.</p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in October 2018. </b><p>

</p><p>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening — Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.</p><p>

A few of the topics we cover are:</p><p>• Why Paul expects AI to transform the world gradually rather than explosively and what that would look like<br>
• Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us<br>
• Why AI systems will probably be granted legal and property rights<br>
• How an advanced AI that doesn't share human goals could still have moral value<br>
• Why machine learning might take over science research from humans before it can do most other tasks<br>
• Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.</p><p>

<b>• <a href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></b><br>
<b>• <a href="https://rohinshah.com/alignment-newsletter/">Rohin Shah's AI alignment newsletter.</a></b></p><p>

Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.</p><p>

If given plenty of time — and enough arguments, counterarguments and counter-counter-arguments between all the experts — should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?</p><p>

In other words: does 'debate', in principle, lead to truth?</p><p>

According to Paul Christiano — researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities — this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.</p><p>

It's a method OpenAI is <a href="https://blog.openai.com/debate/">actively trying to develop</a>, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight.</p><p> 

If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.</p><p>

But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.</p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 15 Jan 2020 00:44:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/849fe819/77e1ebfb.mp3" length="222916356" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/kIA_L9r5C3hOXqN34tVIEJyE9VVSRqHLSsOrOIod-to/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTQv/MTY4MzU0NDYxOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13874</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in October 2018. 

Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening — Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.

A few of the topics we cover are:• Why Paul expects AI to transform the world gradually rather than explosively and what that would look like
• Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us
• Why AI systems will probably be granted legal and property rights
• How an advanced AI that doesn't share human goals could still have moral value
• Why machine learning might take over science research from humans before it can do most other tasks
• Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.

• Links to learn more, summary and full transcript.
• Rohin Shah's AI alignment newsletter.

Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.

If given plenty of time — and enough arguments, counterarguments and counter-counter-arguments between all the experts — should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?

In other words: does 'debate', in principle, lead to truth?

According to Paul Christiano — researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities — this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.

It's a method OpenAI is actively trying to develop, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight. 

If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.

But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in October 2018. 

Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#33 Classic episode - Anders Sandberg on cryonics, solar flares, and the annual odds of nuclear war</title>
      <itunes:title>#33 Classic episode - Anders Sandberg on cryonics, solar flares, and the annual odds of nuclear war</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">523f7a66-31de-11ea-b2b9-0e12d87d2229</guid>
      <link>https://share.transistor.fm/s/34bf675c</link>
      <description>
        <![CDATA[<b>Rebroadcast: this episode was originally released in May 2018. </b><p>

Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? </p><p>

According to <a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Bryan Caplan</a> in episode #32, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees. </p><p>

Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. </p><p>

Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. </p><p>

<a href="https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript of the conversation, summary, and links to learn more.</b></a> </p><p>

The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. </p><p>

Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. </p><p>

His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. </p><p>

Last time we asked him <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including: </p><p>

• Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br>• How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened? <br>• If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians? <br>• What long-shot drugs can people take in their 70s to stave off death? <br>• Can science extend human (waking) life by cutting our need to sleep? <br>• How bad would it be if a solar flare took down the electricity grid? Could it happen? <br>
• If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it? <br>• Will lifelike robots make us more inclined to dehumanise one another? </p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app. </b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<b>Rebroadcast: this episode was originally released in May 2018. </b><p>

Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? </p><p>

According to <a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Bryan Caplan</a> in episode #32, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees. </p><p>

Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. </p><p>

Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. </p><p>

<a href="https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript of the conversation, summary, and links to learn more.</b></a> </p><p>

The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. </p><p>

Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. </p><p>

His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. </p><p>

Last time we asked him <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including: </p><p>

• Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br>• How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened? <br>• If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians? <br>• What long-shot drugs can people take in their 70s to stave off death? <br>• Can science extend human (waking) life by cutting our need to sleep? <br>• How bad would it be if a solar flare took down the electricity grid? Could it happen? <br>
• If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it? <br>• Will lifelike robots make us more inclined to dehumanise one another? </p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app. </b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 08 Jan 2020 06:27:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/34bf675c/df2c3f30.mp3" length="82440699" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/E5hNFtYQMIoX4qUzk_X3jGOli2ToyJm845ibrpdWMeA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTMv/MTY4MzU0NDYxNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5111</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in May 2018. 

Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? 

According to Bryan Caplan in episode #32, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees. 

Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. 

Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. 

Full transcript of the conversation, summary, and links to learn more. 

The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. 

Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. 

His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. 

Last time we asked him why we don’t see aliens, and how to most efficiently colonise the universe. In today’s episode we ask about Anders’ other recent papers, including: 

• Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? • How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened? • If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians? • What long-shot drugs can people take in their 70s to stave off death? • Can science extend human (waking) life by cutting our need to sleep? • How bad would it be if a solar flare took down the electricity grid? Could it happen? 
• If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it? • Will lifelike robots make us more inclined to dehumanise one another? 

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app. 

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in May 2018. 

Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? 

According to Bryan Caplan in episode #32, there’s an 80% chance that Stalin would</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/34bf675c/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#17 Classic episode - Will MacAskill on moral uncertainty, utilitarianism &amp; how to avoid being a moral monster</title>
      <itunes:title>#17 Classic episode - Will MacAskill on moral uncertainty, utilitarianism &amp; how to avoid being a moral monster</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">659038ee-2b2b-11ea-bab9-0e25fd385935</guid>
      <link>https://share.transistor.fm/s/5d651da8</link>
      <description>
        <![CDATA[<p><strong>Rebroadcast: this episode was originally released in January 2018.</strong></p><p> Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He <a href="https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">also thought</a> that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races. </p><p> Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?</p><p><strong>• </strong><a href="https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, key points &amp; links to articles discussed in the show.</a> </p><p> If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide. </p><p> Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism (EA) community. In this interview we discuss a wide range of topics: </p><p> • How would we go about a ‘long reflection’ to fix our moral errors? <br> • Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’? <br> • If we basically solve existential risks, what does humanity do next? <br> • What are some of Will’s most unusual philosophical positions? <br> • What are the best arguments for and against utilitarianism? <br> • Given disagreements among philosophers, how much should we believe the findings of philosophy as a field? <br> • What are some the biases we should be aware of within academia? <br> • What are some of the downsides of becoming a professor? <br> • What are the merits of becoming a philosopher? <br> • How does the media image of EA differ to the actual goals of the community? <br> • What kinds of things would you like to see the EA community do differently? <br> • How much should we explore potentially controversial ideas? <br> • How focused should we be on diversity? <br> • What are the best arguments against effective altruism? </p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. </strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><strong>Rebroadcast: this episode was originally released in January 2018.</strong></p><p> Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He <a href="https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">also thought</a> that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races. </p><p> Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?</p><p><strong>• </strong><a href="https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, key points &amp; links to articles discussed in the show.</a> </p><p> If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide. </p><p> Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism (EA) community. In this interview we discuss a wide range of topics: </p><p> • How would we go about a ‘long reflection’ to fix our moral errors? <br> • Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’? <br> • If we basically solve existential risks, what does humanity do next? <br> • What are some of Will’s most unusual philosophical positions? <br> • What are the best arguments for and against utilitarianism? <br> • Given disagreements among philosophers, how much should we believe the findings of philosophy as a field? <br> • What are some the biases we should be aware of within academia? <br> • What are some of the downsides of becoming a professor? <br> • What are the merits of becoming a philosopher? <br> • How does the media image of EA differ to the actual goals of the community? <br> • What kinds of things would you like to see the EA community do differently? <br> • How much should we explore potentially controversial ideas? <br> • How focused should we be on diversity? <br> • What are the best arguments against effective altruism? </p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. </strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 31 Dec 2019 16:28:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5d651da8/bb7259bb.mp3" length="108438405" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/adw10-0gzUOAsMoJfgzcLkl3YEXGBYXPiQ_JmgQN_c4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTEv/MTY4MzU0NDYxNS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6759</itunes:duration>
      <itunes:summary>Rebroadcast: this episode was originally released in January 2018.

Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He also thought that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races. 
 
Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?• Full transcript, key points &amp;amp; links to articles discussed in the show. 
 
If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide. 
 
Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism (EA) community. In this interview we discuss a wide range of topics: 
 
• How would we go about a ‘long reflection’ to fix our moral errors? 
• Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’? 
• If we basically solve existential risks, what does humanity do next? 
• What are some of Will’s most unusual philosophical positions? 
• What are the best arguments for and against utilitarianism? 
• Given disagreements among philosophers, how much should we believe the findings of philosophy as a field? 
• What are some the biases we should be aware of within academia? 
• What are some of the downsides of becoming a professor? 
• What are the merits of becoming a philosopher? 
• How does the media image of EA differ to the actual goals of the community? 
• What kinds of things would you like to see the EA community do differently? 
• How much should we explore potentially controversial ideas? 
• How focused should we be on diversity? 
• What are the best arguments against effective altruism? 

Get this episode by subscribing: type '80,000 Hours' into your podcasting app.  

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Rebroadcast: this episode was originally released in January 2018.

Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He also thought that</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#67 – David Chalmers on the nature and ethics of consciousness</title>
      <itunes:title>#67 – David Chalmers on the nature and ethics of consciousness</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">0091a36a-203b-11ea-a4f3-0e09fb6f5fad</guid>
      <link>https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/</link>
      <description>
        <![CDATA[<p>What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. </p><p>Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. </p><p>The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': </p><p>"Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?" </p><p>Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. </p><p>So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. </p><p>• <a href="https://80k.link/chalmers"><strong>Links to learn more, summary and full transcript.</strong></a> <br> • <a href="https://80k.link/aoa"><strong>Advice on how to read our advice.</strong></a> <br> • <strong>Anonymous answers on: </strong><a href="https://80k.link/habits">bad habits</a><strong>, </strong><a href="https://80k.link/risk">risk</a><strong> and </strong><a href="https://80k.link/fail">failure.</a> </p><p> Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. </p><p> Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? </p><p> Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. </p><p> Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question <em>'what is consciousness?'</em> to the centre stage of philosophy with his 1996 book <em>'The Conscious Mind'</em>, which argued against then-dominant materialist theories of consciousness. </p><p> This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. </p><p> These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? </p><p> Dave Chalmers is probably the best person on the planet to ask these questions, and Rob &amp; Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far. <br> <br>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>The interview begins (00:02:11)</li><li>Philosopher’s survey (00:06:37)</li><li>Free will (00:13:37)</li><li>Survey correlations (00:20:06)</li><li>Progress in philosophy (00:35:01)</li><li>Simulations (00:51:30)</li><li>The problem of consciousness (01:13:01)</li><li>Dualism and panpsychism (01:26:52)</li><li>Is consciousness an illusion? (01:34:52)</li><li>Idealism (01:43:13)</li><li>Integrated information theory (01:51:08)</li><li>Moral status and consciousness (02:06:10)</li><li>Higher order views of consciousness (02:11:46)</li><li>The views of philosophers on eating meat (02:20:23)</li><li>Artificial consciousness (02:34:25)</li><li>The zombie and vulcan trolley problems (02:38:43)</li><li>Illusionism and moral status (02:56:12)</li><li>Panpsychism and moral status (03:06:19)</li><li>Mind uploading (03:15:58)</li><li>Personal identity (03:22:51)</li><li>Virtual reality and the experience machine (03:28:56)</li><li>Singularity (03:42:44)</li><li>AI alignment (04:07:39)</li><li>Careers in academia (04:23:37)</li><li>Having fun disagreements (04:32:54)</li><li>Rob’s outro (04:42:14)</li></ul><p><br> <strong>Producer: Keiran Harris.</strong></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. </p><p>Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. </p><p>The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': </p><p>"Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?" </p><p>Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. </p><p>So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. </p><p>• <a href="https://80k.link/chalmers"><strong>Links to learn more, summary and full transcript.</strong></a> <br> • <a href="https://80k.link/aoa"><strong>Advice on how to read our advice.</strong></a> <br> • <strong>Anonymous answers on: </strong><a href="https://80k.link/habits">bad habits</a><strong>, </strong><a href="https://80k.link/risk">risk</a><strong> and </strong><a href="https://80k.link/fail">failure.</a> </p><p> Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. </p><p> Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? </p><p> Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. </p><p> Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question <em>'what is consciousness?'</em> to the centre stage of philosophy with his 1996 book <em>'The Conscious Mind'</em>, which argued against then-dominant materialist theories of consciousness. </p><p> This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. </p><p> These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? </p><p> Dave Chalmers is probably the best person on the planet to ask these questions, and Rob &amp; Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far. <br> <br>Chapters:</p><ul><li>Rob's intro (00:00:00)</li><li>The interview begins (00:02:11)</li><li>Philosopher’s survey (00:06:37)</li><li>Free will (00:13:37)</li><li>Survey correlations (00:20:06)</li><li>Progress in philosophy (00:35:01)</li><li>Simulations (00:51:30)</li><li>The problem of consciousness (01:13:01)</li><li>Dualism and panpsychism (01:26:52)</li><li>Is consciousness an illusion? (01:34:52)</li><li>Idealism (01:43:13)</li><li>Integrated information theory (01:51:08)</li><li>Moral status and consciousness (02:06:10)</li><li>Higher order views of consciousness (02:11:46)</li><li>The views of philosophers on eating meat (02:20:23)</li><li>Artificial consciousness (02:34:25)</li><li>The zombie and vulcan trolley problems (02:38:43)</li><li>Illusionism and moral status (02:56:12)</li><li>Panpsychism and moral status (03:06:19)</li><li>Mind uploading (03:15:58)</li><li>Personal identity (03:22:51)</li><li>Virtual reality and the experience machine (03:28:56)</li><li>Singularity (03:42:44)</li><li>AI alignment (04:07:39)</li><li>Careers in academia (04:23:37)</li><li>Having fun disagreements (04:32:54)</li><li>Rob’s outro (04:42:14)</li></ul><p><br> <strong>Producer: Keiran Harris.</strong></p>]]>
      </content:encoded>
      <pubDate>Mon, 16 Dec 2019 21:00:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6989b5d5/24337bb2.mp3" length="270992341" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/2qr85TcuhHdTbUPPKecgM8snhAUmOz0l3ztP9PkIpwE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3OTAv/MTY4MzU0NDYxNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>16910</itunes:duration>
      <itunes:summary>What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. 

Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. 

The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': 

"Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?" 

Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. 

So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. 

• Links to learn more, summary and full transcript. 
• Advice on how to read our advice. 
• Anonymous answers on: bad habits, risk and failure. 

Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. 

Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? 

Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. 

Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question 'what is consciousness?' to the centre stage of philosophy with his 1996 book 'The Conscious Mind', which argued against then-dominant materialist theories of consciousness. 

This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. 

These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? 

Dave Chalmers is probably the best person on the planet to ask these questions, and Rob &amp;amp; Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far.  

Get this episode by subscribing to our show on the world’s most pressing problems and how to solve them: search for 80,000 Hours in your podcasting app. 


Producer: Keiran Harris.</itunes:summary>
      <itunes:subtitle>What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. 

Now imagine beings that are identical to h</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/6989b5d5/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#66 – Peter Singer on being provocative, effective altruism, &amp; how his moral views have changed</title>
      <itunes:title>#66 – Peter Singer on being provocative, effective altruism, &amp; how his moral views have changed</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">d49a9506-176f-11ea-ae99-0e4687eadef1</guid>
      <link>https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/</link>
      <description>
        <![CDATA[<p>In 1989, the professor of moral philosophy Peter Singer was all over the news for his inflammatory opinions about abortion. But the controversy stemmed from <em>Practical Ethics</em> — a book he’d actually released way back in 1979. It took a German translation ten years on for protests to kick off. </p><p> According to Singer, he honestly didn’t expect this view to be as provocative as it became, and he certainly wasn’t aiming to stir up trouble and get attention. </p><p> But after the protests and the increasing coverage of his work in German media, the previously flat sales of <a href="https://80k.link/pebook"><strong>Practical Ethics</strong></a> shot up. And the negative attention he received ultimately led him to a weekly opinion column in <em>The New York Times</em>. </p><p> <strong>• Singer's book </strong><strong><em>The Life You Can Save</em></strong><strong> has just been re-released as a 10th anniversary edition, available as a free e-book and audiobook, read by a range of celebrities. </strong><a href="https://80k.link/tlycsbook">Get it here</a><strong>.</strong><br> <strong>• </strong><a href="https://80k.link/singerinterview">Links to learn more, summary and full transcript</a><strong>.</strong></p><p> Singer points out that as a result of this increased attention, many more people also read the rest of the book — which includes chapters with a real ability to do good, covering global poverty, animal ethics, and other important topics. So should people actively try to court controversy with one view, in order to gain attention for another more important one? </p><p> Perhaps sometimes, but controversy can also just have bad consequences. His critics may view him as someone who says whatever he thinks, hang the consequences, but Singer says that he gives public relations considerations plenty of thought. </p><p> One example is that Singer opposes efforts to advocate for open borders. Not because he thinks a world with freedom of movement is a bad idea per se, but rather because it may help elect leaders like Mr Trump. </p><p> Another is the focus of the effective altruism community. Singer certainly respects those who are focused on improving the long-term future of humanity, and thinks this is important work that should continue. But he’s troubled by the possibility of extinction risks becoming the public face of the movement. </p><p> He suspects there's a much narrower group of people who are likely to respond to that kind of appeal, compared to those who are drawn to work on global poverty or preventing animal suffering. And that to really transform philanthropy and culture more generally, the effective altruism community needs to focus on smaller donors with more conventional concerns. </p><p> Rob is joined in this interview by Arden Koehler, the newest addition to the 80,000 Hours team, both for the interview and a post-episode discussion. They only had an hour with Peter, but also cover: </p><p> • What does he think is the most plausible alternatives to consequentialism? <br> • Is it more humane to eat wild caught animals than farmed animals? <br> • The re-release of The Life You Can Save <br> • His most and least strategic career decisions <br> • Population ethics, and other arguments for and against prioritising the long-term future <br> • What led to his changing his mind on significant questions in moral philosophy? <br> • And more. </p><p> In the post-episode discussion, Rob and Arden continue talking about: </p><p> • The pros and cons of keeping EA as one big movement <br> • Singer’s thoughts on immigration<br> • And consequentialism with side constraints.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript. </strong></p><p> <em>Producer: Keiran Harris. <br> Audio mastering: Ben Cordell. <br> Transcriptions: Zakee Ulhaq. <br> Illustration of Singer: Matthias Seifarth.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In 1989, the professor of moral philosophy Peter Singer was all over the news for his inflammatory opinions about abortion. But the controversy stemmed from <em>Practical Ethics</em> — a book he’d actually released way back in 1979. It took a German translation ten years on for protests to kick off. </p><p> According to Singer, he honestly didn’t expect this view to be as provocative as it became, and he certainly wasn’t aiming to stir up trouble and get attention. </p><p> But after the protests and the increasing coverage of his work in German media, the previously flat sales of <a href="https://80k.link/pebook"><strong>Practical Ethics</strong></a> shot up. And the negative attention he received ultimately led him to a weekly opinion column in <em>The New York Times</em>. </p><p> <strong>• Singer's book </strong><strong><em>The Life You Can Save</em></strong><strong> has just been re-released as a 10th anniversary edition, available as a free e-book and audiobook, read by a range of celebrities. </strong><a href="https://80k.link/tlycsbook">Get it here</a><strong>.</strong><br> <strong>• </strong><a href="https://80k.link/singerinterview">Links to learn more, summary and full transcript</a><strong>.</strong></p><p> Singer points out that as a result of this increased attention, many more people also read the rest of the book — which includes chapters with a real ability to do good, covering global poverty, animal ethics, and other important topics. So should people actively try to court controversy with one view, in order to gain attention for another more important one? </p><p> Perhaps sometimes, but controversy can also just have bad consequences. His critics may view him as someone who says whatever he thinks, hang the consequences, but Singer says that he gives public relations considerations plenty of thought. </p><p> One example is that Singer opposes efforts to advocate for open borders. Not because he thinks a world with freedom of movement is a bad idea per se, but rather because it may help elect leaders like Mr Trump. </p><p> Another is the focus of the effective altruism community. Singer certainly respects those who are focused on improving the long-term future of humanity, and thinks this is important work that should continue. But he’s troubled by the possibility of extinction risks becoming the public face of the movement. </p><p> He suspects there's a much narrower group of people who are likely to respond to that kind of appeal, compared to those who are drawn to work on global poverty or preventing animal suffering. And that to really transform philanthropy and culture more generally, the effective altruism community needs to focus on smaller donors with more conventional concerns. </p><p> Rob is joined in this interview by Arden Koehler, the newest addition to the 80,000 Hours team, both for the interview and a post-episode discussion. They only had an hour with Peter, but also cover: </p><p> • What does he think is the most plausible alternatives to consequentialism? <br> • Is it more humane to eat wild caught animals than farmed animals? <br> • The re-release of The Life You Can Save <br> • His most and least strategic career decisions <br> • Population ethics, and other arguments for and against prioritising the long-term future <br> • What led to his changing his mind on significant questions in moral philosophy? <br> • And more. </p><p> In the post-episode discussion, Rob and Arden continue talking about: </p><p> • The pros and cons of keeping EA as one big movement <br> • Singer’s thoughts on immigration<br> • And consequentialism with side constraints.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript. </strong></p><p> <em>Producer: Keiran Harris. <br> Audio mastering: Ben Cordell. <br> Transcriptions: Zakee Ulhaq. <br> Illustration of Singer: Matthias Seifarth.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 05 Dec 2019 15:58:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/bb70cc0f/6f0ff4c2.mp3" length="117629985" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ezSVgFo1JSZOSoScV1ghfQyZ8Pt1BXfBwUQVx37aong/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODkv/MTY4MzU0NDYxMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7281</itunes:duration>
      <itunes:summary>In 1989, the professor of moral philosophy Peter Singer was all over the news for his inflammatory opinions about abortion. But the controversy stemmed from Practical Ethics — a book he’d actually released way back in 1979. It took a German translation ten years on for protests to kick off. 

According to Singer, he honestly didn’t expect this view to be as provocative as it became, and he certainly wasn’t aiming to stir up trouble and get attention. 

But after the protests and the increasing coverage of his work in German media, the previously flat sales of Practical Ethics shot up. And the negative attention he received ultimately led him to a weekly opinion column in The New York Times. 

• Singer's book The Life You Can Save has just been re-released as a 10th anniversary edition, available as a free e-book and audiobook, read by a range of celebrities. Get it here.
• Links to learn more, summary and full transcript.

Singer points out that as a result of this increased attention, many more people also read the rest of the book — which includes chapters with a real ability to do good, covering global poverty, animal ethics, and other important topics. So should people actively try to court controversy with one view, in order to gain attention for another more important one? 

Perhaps sometimes, but controversy can also just have bad consequences. His critics may view him as someone who says whatever he thinks, hang the consequences, but Singer says that he gives public relations considerations plenty of thought. 

One example is that Singer opposes efforts to advocate for open borders. Not because he thinks a world with freedom of movement is a bad idea per se, but rather because it may help elect leaders like Mr Trump. 

Another is the focus of the effective altruism community. Singer certainly respects those who are focused on improving the long-term future of humanity, and thinks this is important work that should continue. But he’s troubled by the possibility of extinction risks becoming the public face of the movement. 
 
He suspects there's a much narrower group of people who are likely to respond to that kind of appeal, compared to those who are drawn to work on global poverty or preventing animal suffering. And that to really transform philanthropy and culture more generally, the effective altruism community needs to focus on smaller donors with more conventional concerns. 

Rob is joined in this interview by Arden Koehler, the newest addition to the 80,000 Hours team, both for the interview and a post-episode discussion. They only had an hour with Peter, but also cover: 

• What does he think is the most plausible alternatives to consequentialism? 
• Is it more humane to eat wild caught animals than farmed animals? 
• The re-release of The Life You Can Save 
• His most and least strategic career decisions 
• Population ethics, and other arguments for and against prioritising the long-term future 
• What led to his changing his mind on significant questions in moral philosophy? 
• And more. 

In the post-episode discussion, Rob and Arden continue talking about: 

• The pros and cons of keeping EA as one big movement 
• Singer’s thoughts on immigration
• And consequentialism with side constraints.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript. 


Producer: Keiran Harris. 
Audio mastering: Ben Cordell. 
Transcriptions: Zakee Ulhaq. 
Illustration of Singer: Matthias Seifarth.</itunes:summary>
      <itunes:subtitle>In 1989, the professor of moral philosophy Peter Singer was all over the news for his inflammatory opinions about abortion. But the controversy stemmed from Practical Ethics — a book he’d actually released way back in 1979. It took a German translation te</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/bb70cc0f/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#65 – Ambassador Bonnie Jenkins on 8 years pursuing WMD arms control, &amp; diversity in diplomacy</title>
      <itunes:title>#65 – Ambassador Bonnie Jenkins on 8 years pursuing WMD arms control, &amp; diversity in diplomacy</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">c9a479ec-0b20-11ea-b9e8-0e8b4fa1fdf9</guid>
      <link>https://80000hours.org/podcast/episodes/ambassador-bonnie-jenkins-peace-arms-control/</link>
      <description>
        <![CDATA[<p><em>"…it started when the Soviet Union fell apart and there was a real desire to ensure security of nuclear materials and pathogens, and that scientists with [WMD-related] knowledge could get paid so that they wouldn't go to countries and sell that knowledge." </em></p><p> Ambassador Bonnie Jenkins has had an incredible career in diplomacy and global security. </p><p> Today she’s a nonresident senior fellow at the <em>Brookings Institution</em> and president of <em>Global Connections Empowering Global Change</em>, where she works on global health, infectious disease and defence innovation. In 2017 she founded her own nonprofit, the <em>Women of Color Advancing Peace, Security and Conflict Transformation</em> (WCAPS). </p><p> But in this interview we focus on her time as Ambassador at the <em>U.S. Department of State</em> under the Obama administration, where she worked for eight years as Coordinator for Threat Reduction Programs in the <em>Bureau of International Security and Nonproliferation</em>. </p><p> In that role, Bonnie coordinated the Department of State’s work to prevent weapons of mass destruction (WMD) terrorism with programmes funded by other U.S. departments and agencies, and as well as other countries. </p><p> • <a href="https://80000hours.org/podcast/episodes/ambassador-bonnie-jenkins-peace-arms-control/?utm_campaign=podcast__bonnie-jenkins&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://www.youtube.com/channel/UCEfASxwPxzsHlG5Rf1-4K9w/videos?view=0&amp;sort=p&amp;flow=grid"><strong>Talks from over 100 other speakers at EA Global.</strong></a><br> • <strong>Having trouble with podcast 'chapters' on this episode? Please report any problems to keiran at 80000hours dot org.</strong> </p><p> What was it like to be an ambassador focusing on an issue, rather than an ambassador of a country? Bonnie says the travel was exhausting. She could find herself in Africa one week, and Indonesia the next. She’d meet with folks going to New York for meetings at the UN one day, then hold her own meetings at the White House the next. </p><p> Each event would have a distinct purpose. For one, she’d travel to Germany as a US Representative, talking about why the two countries should extend their partnership. For another, she could visit the <em>Food and Agriculture Organization</em> to talk about why they need to think more about biosecurity issues. No day was like the previous one. </p><p> Bonnie was also a leading U.S. official in the launch and implementation of the <em>Global Health Security Agenda</em> discussed at length <a href="https://80000hours.org/podcast/episodes/tom-inglesby-health-security/">in episode 27</a>. </p><p> Before returning to government in 2009, Bonnie served as program officer for U.S. Foreign and Security Policy at the <em>Ford Foundation</em>. She also served as counsel on the <em>9/11 Commission</em>. Bonnie was the lead staff member conducting research, interviews, and preparing commission reports on counterterrorism policies in the Office of the Secretary of Defense and on U.S. military plans targeting al-Qaeda before 9/11. </p><p> And as if that all weren't curious enough four years ago Bonnie decided to go vegan. We talk about her work so far as well as: </p><p> • How listeners can start a career like hers <br> • Mistakes made by Mr Obama and Mr Trump <br>• Networking, the value of attention, and being a vegan in DC <br> • And 2020 Presidential candidates.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:54)</li><li>What is Bonnie working on at the moment? (00:02:45)</li><li>Bonnie’s time at the Department of State (00:04:08)</li><li>The history of Cooperative Threat Reduction work (00:08:48)</li><li>Biggest uncontrolled nuclear material threats today (00:11:36)</li><li>Biggest security issues in the world today (00:13:57)</li><li>The Biological Weapons Convention (00:17:52)</li><li>Projects Bonnie worked on that she’s particularly proud of (00:20:55)</li><li>The day to day life of an Ambassador on an issue (00:23:03)</li><li>Biggest misunderstandings of the field (00:25:41)</li><li>How do we get more done in this area? (00:29:48)</li><li>The Global Health Security Agenda (00:32:52)</li><li>The implications for countries who give up WMDs (00:34:55)</li><li>The fallout from a change in government (00:38:40)</li><li>Listener submitted questions (00:39:39)</li><li>How might listeners be able to contribute to solving these problems with their own careers? (00:54:55)</li><li>Is Bonnie glad she went into the military early in her career? (01:06:25)</li><li>Networking in DC (01:12:27)</li><li>What are the downsides to pursuing a career like Bonnie’s? (01:15:27)</li><li>Being a vegan in DC (01:16:47)</li><li>Women of Color Advancing Peace, Security and Conflict Transformation (01:19:15)</li><li>The value of attention in DC (01:28:25)</li><li>Any ways WCAPS could accidentally make things worse? (01:30:08)</li><li>Message for women of colour in the audience (01:33:05)</li><li>TV shows relevant to Bonnie’s work (01:35:19)</li><li>Candidates for 2020 (01:36:57)</li></ul><p><br> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><em>"…it started when the Soviet Union fell apart and there was a real desire to ensure security of nuclear materials and pathogens, and that scientists with [WMD-related] knowledge could get paid so that they wouldn't go to countries and sell that knowledge." </em></p><p> Ambassador Bonnie Jenkins has had an incredible career in diplomacy and global security. </p><p> Today she’s a nonresident senior fellow at the <em>Brookings Institution</em> and president of <em>Global Connections Empowering Global Change</em>, where she works on global health, infectious disease and defence innovation. In 2017 she founded her own nonprofit, the <em>Women of Color Advancing Peace, Security and Conflict Transformation</em> (WCAPS). </p><p> But in this interview we focus on her time as Ambassador at the <em>U.S. Department of State</em> under the Obama administration, where she worked for eight years as Coordinator for Threat Reduction Programs in the <em>Bureau of International Security and Nonproliferation</em>. </p><p> In that role, Bonnie coordinated the Department of State’s work to prevent weapons of mass destruction (WMD) terrorism with programmes funded by other U.S. departments and agencies, and as well as other countries. </p><p> • <a href="https://80000hours.org/podcast/episodes/ambassador-bonnie-jenkins-peace-arms-control/?utm_campaign=podcast__bonnie-jenkins&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://www.youtube.com/channel/UCEfASxwPxzsHlG5Rf1-4K9w/videos?view=0&amp;sort=p&amp;flow=grid"><strong>Talks from over 100 other speakers at EA Global.</strong></a><br> • <strong>Having trouble with podcast 'chapters' on this episode? Please report any problems to keiran at 80000hours dot org.</strong> </p><p> What was it like to be an ambassador focusing on an issue, rather than an ambassador of a country? Bonnie says the travel was exhausting. She could find herself in Africa one week, and Indonesia the next. She’d meet with folks going to New York for meetings at the UN one day, then hold her own meetings at the White House the next. </p><p> Each event would have a distinct purpose. For one, she’d travel to Germany as a US Representative, talking about why the two countries should extend their partnership. For another, she could visit the <em>Food and Agriculture Organization</em> to talk about why they need to think more about biosecurity issues. No day was like the previous one. </p><p> Bonnie was also a leading U.S. official in the launch and implementation of the <em>Global Health Security Agenda</em> discussed at length <a href="https://80000hours.org/podcast/episodes/tom-inglesby-health-security/">in episode 27</a>. </p><p> Before returning to government in 2009, Bonnie served as program officer for U.S. Foreign and Security Policy at the <em>Ford Foundation</em>. She also served as counsel on the <em>9/11 Commission</em>. Bonnie was the lead staff member conducting research, interviews, and preparing commission reports on counterterrorism policies in the Office of the Secretary of Defense and on U.S. military plans targeting al-Qaeda before 9/11. </p><p> And as if that all weren't curious enough four years ago Bonnie decided to go vegan. We talk about her work so far as well as: </p><p> • How listeners can start a career like hers <br> • Mistakes made by Mr Obama and Mr Trump <br>• Networking, the value of attention, and being a vegan in DC <br> • And 2020 Presidential candidates.</p><p>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>The interview begins (00:01:54)</li><li>What is Bonnie working on at the moment? (00:02:45)</li><li>Bonnie’s time at the Department of State (00:04:08)</li><li>The history of Cooperative Threat Reduction work (00:08:48)</li><li>Biggest uncontrolled nuclear material threats today (00:11:36)</li><li>Biggest security issues in the world today (00:13:57)</li><li>The Biological Weapons Convention (00:17:52)</li><li>Projects Bonnie worked on that she’s particularly proud of (00:20:55)</li><li>The day to day life of an Ambassador on an issue (00:23:03)</li><li>Biggest misunderstandings of the field (00:25:41)</li><li>How do we get more done in this area? (00:29:48)</li><li>The Global Health Security Agenda (00:32:52)</li><li>The implications for countries who give up WMDs (00:34:55)</li><li>The fallout from a change in government (00:38:40)</li><li>Listener submitted questions (00:39:39)</li><li>How might listeners be able to contribute to solving these problems with their own careers? (00:54:55)</li><li>Is Bonnie glad she went into the military early in her career? (01:06:25)</li><li>Networking in DC (01:12:27)</li><li>What are the downsides to pursuing a career like Bonnie’s? (01:15:27)</li><li>Being a vegan in DC (01:16:47)</li><li>Women of Color Advancing Peace, Security and Conflict Transformation (01:19:15)</li><li>The value of attention in DC (01:28:25)</li><li>Any ways WCAPS could accidentally make things worse? (01:30:08)</li><li>Message for women of colour in the audience (01:33:05)</li><li>TV shows relevant to Bonnie’s work (01:35:19)</li><li>Candidates for 2020 (01:36:57)</li></ul><p><br> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 19 Nov 2019 23:49:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ed27726a/7e581d18.mp3" length="97538706" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-lKOIw94PUndm2zY_Q4yExWA6P6Xu68UrMBwauoHk6I/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODgv/MTY4MzU0NDYxMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6032</itunes:duration>
      <itunes:summary>"…it started when the Soviet Union fell apart and there was a real desire to ensure security of nuclear materials and pathogens, and that scientists with [WMD-related] knowledge could get paid so that they wouldn't go to countries and sell that knowledge." 

Ambassador Bonnie Jenkins has had an incredible career in diplomacy and global security. 

Today she’s a nonresident senior fellow at the Brookings Institution and president of Global Connections Empowering Global Change, where she works on global health, infectious disease and defence innovation. In 2017 she founded her own nonprofit, the Women of Color Advancing Peace, Security and Conflict Transformation (WCAPS). 

But in this interview we focus on her time as Ambassador at the U.S. Department of State under the Obama administration, where she worked for eight years as Coordinator for Threat Reduction Programs in the Bureau of International Security and Nonproliferation. 

In that role, Bonnie coordinated the Department of State’s work to prevent weapons of mass destruction (WMD) terrorism with programmes funded by other U.S. departments and agencies, and as well as other countries. 

• Links to learn more, summary and full transcript.
• Talks from over 100 other speakers at EA Global.
• Having trouble with podcast 'chapters' on this episode? Please report any problems to keiran at 80000hours dot org. 

What was it like to be an ambassador focusing on an issue, rather than an ambassador of a country? Bonnie says the travel was exhausting. She could find herself in Africa one week, and Indonesia the next. She’d meet with folks going to New York for meetings at the UN one day, then hold her own meetings at the White House the next. 

Each event would have a distinct purpose. For one, she’d travel to Germany as a US Representative, talking about why the two countries should extend their partnership. For another, she could visit the Food and Agriculture Organization to talk about why they need to think more about biosecurity issues. No day was like the previous one. 

Bonnie was also a leading U.S. official in the launch and implementation of the Global Health Security Agenda discussed at length in episode 27. 

Before returning to government in 2009, Bonnie served as program officer for U.S. Foreign and Security Policy at the Ford Foundation. She also served as counsel on the 9/11 Commission. Bonnie was the lead staff member conducting research, interviews, and preparing commission reports on counterterrorism policies in the Office of the Secretary of Defense and on U.S. military plans targeting al-Qaeda before 9/11. 

And as if that all weren't curious enough four years ago Bonnie decided to go vegan. We talk about her work so far as well as: 

• How listeners can start a career like hers 
• Mistakes made by Mr Obama and Mr Trump • Networking, the value of attention, and being a vegan in DC 
• And 2020 Presidential candidates.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>"…it started when the Soviet Union fell apart and there was a real desire to ensure security of nuclear materials and pathogens, and that scientists with [WMD-related] knowledge could get paid so that they wouldn't go to countries and sell that knowledge.</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/ed27726a/transcript.txt" type="text/plain"/>
      <podcast:chapters url="https://share.transistor.fm/s/ed27726a/chapters.json" type="application/json+chapters"/>
    </item>
    <item>
      <title>#64 – Bruce Schneier on how insecure electronic voting could break the United States — and surveillance without tyranny</title>
      <itunes:title>#64 – Bruce Schneier on how insecure electronic voting could break the United States — and surveillance without tyranny</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6a959f5e-f74c-11e9-83ef-0eee8760da3a</guid>
      <link>https://80000hours.org/podcast/episodes/bruce-schneier-security-secrets-and-surveillance/</link>
      <description>
        <![CDATA[<p><strong>November 3 2020, 10:32PM:</strong> CNN, NBC, and FOX report that Donald Trump has narrowly won Florida, and with it, re-election.  </p><p> <strong>November 3 2020, 11:46PM:</strong> The NY Times and Wall Street Journal report that some group has successfully hacked electronic voting systems across the country, including Florida. The malware has spread to tens of thousands of machines and deletes any record of its activity, so the returning officer of Florida concedes they actually have no idea who won the state — and don't see how they can figure it out. </p><p> What on Earth happens next? </p><p> Today’s guest — world-renowned computer security expert Bruce Schneier — thinks this scenario is plausible, and the ensuing chaos would sow so much distrust that half the country would never accept the election result. </p><p> Unfortunately the US has no recovery system for a situation like this, unlike parliamentary democracies, which can just rerun the election a few weeks later.</p><ul><li><a href="https://80k.link/schneier-sn"><strong>Links to learn more, summary and full transcript.</strong></a></li><li>Motivating article: <a href="https://80k.link/iscfgcrr"><strong>Information security careers for global catastrophic risk reduction</strong></a> by Zabel and Muehlhauser</li></ul><p>The Constitution says the state legislature decides, and they can do so however they like; one tied local election in Texas was settled by playing a hand of poker. </p><p>Elections serve two purposes. The first is the obvious one: to pick a winner. The second, but equally important, is to convince the loser to go along with it — which is why hacks often focus on convincing the losing side that the election wasn't fair. </p><p>Schneier thinks there's a need to agree how this situation should be handled before something like it happens, and America falls into severe infighting as everyone tries to turn the situation to their political advantage. </p><p>And to fix our voting systems, we urgently need two things: a voter-verifiable paper ballot and risk-limiting audits. </p><p>According to Schneier, computer security experts look at current electronic voting machines and can barely believe their eyes. But voting machine designers never understand the security weakness of what they're designing, because they have a bureaucrat's rather than a hacker's mindset. </p><p>The ideal computer security expert walks into a shop and thinks, "You know, here's how I would shoplift." They automatically see where the cameras are, whether there are alarms, and where the security guards aren't watching. </p><p>In this episode we discuss this hacker mindset, and how to use a career in security to protect democracy and guard dangerous secrets from people who shouldn't get access to them.</p><p><br>We also cover: <br> • How can we have surveillance of dangerous actors, without falling back into authoritarianism? <br> • When if ever should information about weaknesses in society's security be kept secret? <br> • How secure are nuclear weapons systems around the world? <br> • How worried should we be about deep-fakes? <br> • Schneier’s critiques of blockchain technology <br> • How technologists should be vital in shaping policy <br> • What are the most consequential computer security problems today? <br> • Could a career in information security be very useful for reducing global catastrophic risks? <br> • And more.</p><p><br>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>Bruce’s Codex talk (00:02:23)</li><li>The interview begins (00:15:42)</li><li>What is Bruce working on at the moment? (00:16:35)</li><li>How technologists could be vital in shaping policy (00:18:52)</li><li>Most consequential computer security problems today (00:24:12)</li><li>How secure are nuclear weapons systems around the world? (00:34:41)</li><li>Stuxnet and NotPetya (00:42:29)</li><li>Messing with democracy (00:44:44)</li><li>How worried should we be about deepfakes? (00:50:02)</li><li>The similarities between hacking computers and potentially hacking biology in the future (00:55:08)</li><li>Bruce’s critiques of crypto (01:00:05)</li><li>What are some of the most kind of widely-held but incorrect beliefs among computer security people? (01:03:04)</li><li>The hacking mindset (01:05:35)</li><li>Voting machines (01:09:22)</li><li>How secretive should people be about potentially harmful information? (01:16:48)</li><li>Could a career in information security be very useful for reducing global catastrophic risks? (01:21:46)</li><li>How to develop the skills needed in computer security (01:33:44)</li><li>Ubiquitous surveillance (01:52:46)</li><li>Why is Bruce optimistic? (02:05:28)</li><li>Rob’s outro (02:06:43)</li></ul><p><br></p><p><em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><strong>November 3 2020, 10:32PM:</strong> CNN, NBC, and FOX report that Donald Trump has narrowly won Florida, and with it, re-election.  </p><p> <strong>November 3 2020, 11:46PM:</strong> The NY Times and Wall Street Journal report that some group has successfully hacked electronic voting systems across the country, including Florida. The malware has spread to tens of thousands of machines and deletes any record of its activity, so the returning officer of Florida concedes they actually have no idea who won the state — and don't see how they can figure it out. </p><p> What on Earth happens next? </p><p> Today’s guest — world-renowned computer security expert Bruce Schneier — thinks this scenario is plausible, and the ensuing chaos would sow so much distrust that half the country would never accept the election result. </p><p> Unfortunately the US has no recovery system for a situation like this, unlike parliamentary democracies, which can just rerun the election a few weeks later.</p><ul><li><a href="https://80k.link/schneier-sn"><strong>Links to learn more, summary and full transcript.</strong></a></li><li>Motivating article: <a href="https://80k.link/iscfgcrr"><strong>Information security careers for global catastrophic risk reduction</strong></a> by Zabel and Muehlhauser</li></ul><p>The Constitution says the state legislature decides, and they can do so however they like; one tied local election in Texas was settled by playing a hand of poker. </p><p>Elections serve two purposes. The first is the obvious one: to pick a winner. The second, but equally important, is to convince the loser to go along with it — which is why hacks often focus on convincing the losing side that the election wasn't fair. </p><p>Schneier thinks there's a need to agree how this situation should be handled before something like it happens, and America falls into severe infighting as everyone tries to turn the situation to their political advantage. </p><p>And to fix our voting systems, we urgently need two things: a voter-verifiable paper ballot and risk-limiting audits. </p><p>According to Schneier, computer security experts look at current electronic voting machines and can barely believe their eyes. But voting machine designers never understand the security weakness of what they're designing, because they have a bureaucrat's rather than a hacker's mindset. </p><p>The ideal computer security expert walks into a shop and thinks, "You know, here's how I would shoplift." They automatically see where the cameras are, whether there are alarms, and where the security guards aren't watching. </p><p>In this episode we discuss this hacker mindset, and how to use a career in security to protect democracy and guard dangerous secrets from people who shouldn't get access to them.</p><p><br>We also cover: <br> • How can we have surveillance of dangerous actors, without falling back into authoritarianism? <br> • When if ever should information about weaknesses in society's security be kept secret? <br> • How secure are nuclear weapons systems around the world? <br> • How worried should we be about deep-fakes? <br> • Schneier’s critiques of blockchain technology <br> • How technologists should be vital in shaping policy <br> • What are the most consequential computer security problems today? <br> • Could a career in information security be very useful for reducing global catastrophic risks? <br> • And more.</p><p><br>Chapters:</p><ul><li>Rob’s intro (00:00:00)</li><li>Bruce’s Codex talk (00:02:23)</li><li>The interview begins (00:15:42)</li><li>What is Bruce working on at the moment? (00:16:35)</li><li>How technologists could be vital in shaping policy (00:18:52)</li><li>Most consequential computer security problems today (00:24:12)</li><li>How secure are nuclear weapons systems around the world? (00:34:41)</li><li>Stuxnet and NotPetya (00:42:29)</li><li>Messing with democracy (00:44:44)</li><li>How worried should we be about deepfakes? (00:50:02)</li><li>The similarities between hacking computers and potentially hacking biology in the future (00:55:08)</li><li>Bruce’s critiques of crypto (01:00:05)</li><li>What are some of the most kind of widely-held but incorrect beliefs among computer security people? (01:03:04)</li><li>The hacking mindset (01:05:35)</li><li>Voting machines (01:09:22)</li><li>How secretive should people be about potentially harmful information? (01:16:48)</li><li>Could a career in information security be very useful for reducing global catastrophic risks? (01:21:46)</li><li>How to develop the skills needed in computer security (01:33:44)</li><li>Ubiquitous surveillance (01:52:46)</li><li>Why is Bruce optimistic? (02:05:28)</li><li>Rob’s outro (02:06:43)</li></ul><p><br></p><p><em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 25 Oct 2019 18:21:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/82d97035/097dce05.mp3" length="125964949" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/MRbsUQooJuxYEFrXBChNXv0fc-03pn_g3q5a4lYLsmk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODcv/MTY4MzU0NDYxMS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7864</itunes:duration>
      <itunes:summary>November 3 2020, 10:32PM: CNN, NBC, and FOX report that Donald Trump has narrowly won Florida, and with it, re-election.  

November 3 2020, 11:46PM: The NY Times and Wall Street Journal report that some group has successfully hacked electronic voting systems across the country, including Florida. The malware has spread to tens of thousands of machines and deletes any record of its activity, so the returning officer of Florida concedes they actually have no idea who won the state — and don't see how they can figure it out. 

What on Earth happens next? 

Today’s guest — world-renowned computer security expert Bruce Schneier — thinks this scenario is plausible, and the ensuing chaos would sow so much distrust that half the country would never accept the election result. 

Unfortunately the US has no recovery system for a situation like this, unlike Parliamentary democracies, which can just rerun the election a few weeks later.

•  Links to learn more, summary and full transcript.
•  Motivating article: Information security careers for global catastrophic risk reduction by Zabel and Muehlhauser

The constitution says the state legislature decides, and they can do so however they like; one tied local election in Texas was settled by playing a hand of poker. 

Elections serve two purposes. The first is the obvious one: to pick a winner. The second, but equally important, is to convince the loser to go along with it — which is why hacks often focus on convincing the losing side that the election wasn't fair. 

Schneier thinks there's a need to agree how this situation should be handled before something like it happens, and America falls into severe infighting as everyone tries to turn the situation to their political advantage. 

And to fix our voting systems, we urgently need two things: a voter-verifiable paper ballot and risk-limiting audits. 

According to Schneier, computer security experts look at current electronic voting machines and can barely believe their eyes. But voting machine designers never understand the security weakness of what they're designing, because they have a bureaucrat's rather than a hacker's mindset. 

The ideal computer security expert walks into a shop and thinks, "You know, here's how I would shoplift." They automatically see where the cameras are, whether there are alarms, and where the security guards aren't watching. 

In this episode we discuss this hacker mindset, and how to use a career in security to protect democracy and guard dangerous secrets from people who shouldn't get access to them.

We also cover: 

• How can we have surveillance of dangerous actors, without falling back into authoritarianism? 
• When if ever should information about weaknesses in society's security be kept secret? 
• How secure are nuclear weapons systems around the world? 
• How worried should we be about deep-fakes? • Schneier’s critiques of blockchain technology 
• How technologists should be vital in shaping policy 
• What are the most consequential computer security problems today? 
• Could a career in information security be very useful for reducing global catastrophic risks? 
• And more.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>November 3 2020, 10:32PM: CNN, NBC, and FOX report that Donald Trump has narrowly won Florida, and with it, re-election.  

November 3 2020, 11:46PM: The NY Times and Wall Street Journal report that some group has successfully hacked electronic voting s</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/82d97035/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>Rob Wiblin on plastic straws, nicotine, doping, &amp; whether changing the long-term is really possible</title>
      <itunes:title>Rob Wiblin on plastic straws, nicotine, doping, &amp; whether changing the long-term is really possible</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ecb35ebc-dfd8-11e9-b86b-0e3752e5fd66</guid>
      <link>https://share.transistor.fm/s/5d06e1be</link>
      <description>
        <![CDATA[Today's episode is a compilation of interviews I recently recorded for two other shows, <i>Love Your Work</i> and <i>The Neoliberal Podcast</i>. <p>

If you've listened to absolutely everything on this podcast feed, you'll have heard four interviews with me already, but fortunately I don't think these two include much repetition, and I've gotten a decent amount of positive feedback on both. </p><p>

First up, I speak with David Kadavy on his show, <a href="https://kadavy.net/blog/archive/love-your-work/"><b>Love Your Work</b></a>. </p><p>

This is a particularly personal and relaxed interview. We talk about all sorts of things, including nicotine gum, plastic straw bans, whether recycling is important, how many lives a doctor saves, why interviews should go for at least 2 hours, how athletes doping could be good for the world, and many other fun topics. </p><p>

</p><p>• Our <b>annual impact survey</b> is about to close — I'd really appreciate if you could <b><a href="https://80k.link/isrw">take 3–10 minutes to fill it out now</a>. </b><br>
• <a href="https://80000hours.org/2019/09/rob-wiblin-on-many-things/"><b>The blog post</b></a> about this episode.</p><p>

At some points we even actually discuss effective altruism and 80,000 Hours, but you can easily skip through those bits if they feel too familiar. </p><p>

The second interview is with Jeremiah Johnson on the <a href="https://neoliberalproject.org/podcast"><b>Neoliberal Podcast</b></a>. It starts 2 hours and 15 minutes into this recording. </p><p>

Neoliberalism <a href="https://medium.com/@s8mb/im-a-neoliberal-maybe-you-are-too-b809a2a588d6"><b>in the sense used by this show</b></a> is not the free market fundamentalism you might associate with the term. Rather it's a centrist or even centre-left view that supports things like social liberalism, multilateral international institutions, trade, high rates of migration, racial justice, inclusive institutions, financial redistribution, prioritising the global poor, market urbanism, and environmental sustainability. </p><p>

This is the more demanding of the two conversations, as listeners to that show have already heard of effective altruism, so we were able to get the best arguments Jeremiah could offer against focusing on improving the long term future of the world. </p><p>

Jeremiah is more of a fan of donating to evidence-backed global health charities recommended by GiveWell, and does so himself. </p><p>

I appreciate him having done his homework and forcing me to do my best to explain how well my views can stand up to counterarguments. It was a challenge for me to paint the whole picture in the half an hour we spent on longterm and I expect there's answers in there which will be fresh even for regular listeners. </p><p>

I hope you enjoy both conversations! Feel free to email me with any feedback.</p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today's episode is a compilation of interviews I recently recorded for two other shows, <i>Love Your Work</i> and <i>The Neoliberal Podcast</i>. <p>

If you've listened to absolutely everything on this podcast feed, you'll have heard four interviews with me already, but fortunately I don't think these two include much repetition, and I've gotten a decent amount of positive feedback on both. </p><p>

First up, I speak with David Kadavy on his show, <a href="https://kadavy.net/blog/archive/love-your-work/"><b>Love Your Work</b></a>. </p><p>

This is a particularly personal and relaxed interview. We talk about all sorts of things, including nicotine gum, plastic straw bans, whether recycling is important, how many lives a doctor saves, why interviews should go for at least 2 hours, how athletes doping could be good for the world, and many other fun topics. </p><p>

</p><p>• Our <b>annual impact survey</b> is about to close — I'd really appreciate if you could <b><a href="https://80k.link/isrw">take 3–10 minutes to fill it out now</a>. </b><br>
• <a href="https://80000hours.org/2019/09/rob-wiblin-on-many-things/"><b>The blog post</b></a> about this episode.</p><p>

At some points we even actually discuss effective altruism and 80,000 Hours, but you can easily skip through those bits if they feel too familiar. </p><p>

The second interview is with Jeremiah Johnson on the <a href="https://neoliberalproject.org/podcast"><b>Neoliberal Podcast</b></a>. It starts 2 hours and 15 minutes into this recording. </p><p>

Neoliberalism <a href="https://medium.com/@s8mb/im-a-neoliberal-maybe-you-are-too-b809a2a588d6"><b>in the sense used by this show</b></a> is not the free market fundamentalism you might associate with the term. Rather it's a centrist or even centre-left view that supports things like social liberalism, multilateral international institutions, trade, high rates of migration, racial justice, inclusive institutions, financial redistribution, prioritising the global poor, market urbanism, and environmental sustainability. </p><p>

This is the more demanding of the two conversations, as listeners to that show have already heard of effective altruism, so we were able to get the best arguments Jeremiah could offer against focusing on improving the long term future of the world. </p><p>

Jeremiah is more of a fan of donating to evidence-backed global health charities recommended by GiveWell, and does so himself. </p><p>

I appreciate him having done his homework and forcing me to do my best to explain how well my views can stand up to counterarguments. It was a challenge for me to paint the whole picture in the half an hour we spent on longterm and I expect there's answers in there which will be fresh even for regular listeners. </p><p>

I hope you enjoy both conversations! Feel free to email me with any feedback.</p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 25 Sep 2019 23:23:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5d06e1be/f77f073f.mp3" length="187418402" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/AG2YnIwbfU8-k6jjzNF11ND2l3Ioya8Xa4BoUxZbqSI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODYv/MTY4MzU0NDYxMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11673</itunes:duration>
      <itunes:summary>Today's episode is a compilation of interviews I recently recorded for two other shows, Love Your Work and The Neoliberal Podcast. 

If you've listened to absolutely everything on this podcast feed, you'll have heard four interviews with me already, but fortunately I don't think these two include much repetition, and I've gotten a decent amount of positive feedback on both. 

First up, I speak with David Kadavy on his show, Love Your Work. 

This is a particularly personal and relaxed interview. We talk about all sorts of things, including nicotine gum, plastic straw bans, whether recycling is important, how many lives a doctor saves, why interviews should go for at least 2 hours, how athletes doping could be good for the world, and many other fun topics. 

• Our annual impact survey is about to close — I'd really appreciate if you could take 3–10 minutes to fill it out now. 
• The blog post about this episode.

At some points we even actually discuss effective altruism and 80,000 Hours, but you can easily skip through those bits if they feel too familiar. 

The second interview is with Jeremiah Johnson on the Neoliberal Podcast. It starts 2 hours and 15 minutes into this recording. 

Neoliberalism in the sense used by this show is not the free market fundamentalism you might associate with the term. Rather it's a centrist or even centre-left view that supports things like social liberalism, multilateral international institutions, trade, high rates of migration, racial justice, inclusive institutions, financial redistribution, prioritising the global poor, market urbanism, and environmental sustainability. 

This is the more demanding of the two conversations, as listeners to that show have already heard of effective altruism, so we were able to get the best arguments Jeremiah could offer against focusing on improving the long term future of the world. 

Jeremiah is more of a fan of donating to evidence-backed global health charities recommended by GiveWell, and does so himself. 

I appreciate him having done his homework and forcing me to do my best to explain how well my views can stand up to counterarguments. It was a challenge for me to paint the whole picture in the half an hour we spent on longterm and I expect there's answers in there which will be fresh even for regular listeners. 

I hope you enjoy both conversations! Feel free to email me with any feedback.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Today's episode is a compilation of interviews I recently recorded for two other shows, Love Your Work and The Neoliberal Podcast. 

If you've listened to absolutely everything on this podcast feed, you'll have heard four interviews with me already, but f</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>Have we helped you have a bigger social impact? Our annual survey, plus other ways we can help you.</title>
      <itunes:title>Have we helped you have a bigger social impact? Our annual survey, plus other ways we can help you.</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">63d0b9d2-d8b7-11e9-a35e-0e0bd6f362e0</guid>
      <link>https://share.transistor.fm/s/b08390a0</link>
      <description>
        <![CDATA[<p><b>1. Fill out our <a href="https://80000hours.org/impact-survey/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">annual impact survey here</a>. </b></p><p><b>2. Find a great vacancy on <a href="https://80000hours.org/job-board/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our job board</a>. </b></p><p><b>3. Learn about our <a href="https://80000hours.org/key-ideas/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">key ideas, and get links to our top articles</a>. </b></p><p><b>4. Join <a href="https://80000hours.org/newsletter/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our newsletter</a> for an email about what's new, every 2 weeks or so. </b></p><p><b>5. Or follow our pages on <a href="https://facebook.com/80000hours">Facebook</a> and <a href="https://twitter.com/80000hours">Twitter</a>. </b></p><p>—— </p><p>Once a year 80,000 Hours runs a survey to find out whether we've helped our users have a larger social impact with their life and career. </p><p>We and our donors need to know whether our services, like this podcast, are helping people enough to continue them or scale them up, and it's only by hearing from you that we can make these decisions in a sensible way. </p><p>So, if 80,000 Hours' podcast, job board, articles, headhunting, advising or other projects have somehow contributed to your life or career plans, please take 3–10 minutes to let us know how. </p><p>You can also let us know where we've fallen short, which helps us fix problems with what we're doing. </p><p>We've refreshed the survey this year, hopefully making it easier to fill out than in the past. </p><p>We'll keep this appeal up for about two weeks, but if you fill it out now that means you definitely won't forget! </p><p>Thanks so much, and talk to you again in a normal episode soon. </p><p>— Rob</p><p>Tag for internal use: this RSS feed is originating in BackTracks.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><b>1. Fill out our <a href="https://80000hours.org/impact-survey/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">annual impact survey here</a>. </b></p><p><b>2. Find a great vacancy on <a href="https://80000hours.org/job-board/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our job board</a>. </b></p><p><b>3. Learn about our <a href="https://80000hours.org/key-ideas/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">key ideas, and get links to our top articles</a>. </b></p><p><b>4. Join <a href="https://80000hours.org/newsletter/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our newsletter</a> for an email about what's new, every 2 weeks or so. </b></p><p><b>5. Or follow our pages on <a href="https://facebook.com/80000hours">Facebook</a> and <a href="https://twitter.com/80000hours">Twitter</a>. </b></p><p>—— </p><p>Once a year 80,000 Hours runs a survey to find out whether we've helped our users have a larger social impact with their life and career. </p><p>We and our donors need to know whether our services, like this podcast, are helping people enough to continue them or scale them up, and it's only by hearing from you that we can make these decisions in a sensible way. </p><p>So, if 80,000 Hours' podcast, job board, articles, headhunting, advising or other projects have somehow contributed to your life or career plans, please take 3–10 minutes to let us know how. </p><p>You can also let us know where we've fallen short, which helps us fix problems with what we're doing. </p><p>We've refreshed the survey this year, hopefully making it easier to fill out than in the past. </p><p>We'll keep this appeal up for about two weeks, but if you fill it out now that means you definitely won't forget! </p><p>Thanks so much, and talk to you again in a normal episode soon. </p><p>— Rob</p><p>Tag for internal use: this RSS feed is originating in BackTracks.</p>]]>
      </content:encoded>
      <pubDate>Mon, 16 Sep 2019 19:22:58 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b08390a0/acc4e00c.mp3" length="3622615" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Yt10qCgxeR17QTLgzbDSYIffmG78EFo2HVu2_eq84mQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODUv/MTY4MzU0NDYwOS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>219</itunes:duration>
      <itunes:summary>1. Fill out our annual impact survey here. 2. Find a great vacancy on our job board. 3. Learn about our key ideas, and get links to our top articles. 4. Join our newsletter for an email about what's new, every 2 weeks or so. 5. Or follow our pages on Facebook and Twitter. —— Once a year 80,000 Hours runs a survey to find out whether we've helped our users have a larger social impact with their life and career. We and our donors need to know whether our services, like this podcast, are helping people enough to continue them or scale them up, and it's only by hearing from you that we can make these decisions in a sensible way. So, if 80,000 Hours' podcast, job board, articles, headhunting, advising or other projects have somehow contributed to your life or career plans, please take 3–10 minutes to let us know how. You can also let us know where we've fallen short, which helps us fix problems with what we're doing. We've refreshed the survey this year, hopefully making it easier to fill out than in the past. We'll keep this appeal up for about two weeks, but if you fill it out now that means you definitely won't forget! Thanks so much, and talk to you again in a normal episode soon. — RobTag for internal use: this RSS feed is originating in BackTracks.</itunes:summary>
      <itunes:subtitle>1. Fill out our annual impact survey here. 2. Find a great vacancy on our job board. 3. Learn about our key ideas, and get links to our top articles. 4. Join our newsletter for an email about what's new, every 2 weeks or so. 5. Or follow our pages on Face</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#63 – Vitalik Buterin on better ways to fund public goods, blockchain's failures, &amp; effective giving</title>
      <itunes:title>#63 – Vitalik Buterin on better ways to fund public goods, blockchain's failures, &amp; effective giving</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">ca789fe2-cdcc-11e9-a7f4-0eacf2b0baaa</guid>
      <link>https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods</link>
      <description>
        <![CDATA[<p>Historically, progress in the field of cryptography has had <a href="https://80k.link/garfinkel">major consequences</a>. It has changed the course of major wars, made it possible to do business on the internet, and enabled private communication between both law-abiding citizens and dangerous criminals. Could it have similarly significant consequences in future?</p><p> Today's guest — Vitalik Buterin — is world-famous as the lead developer of Ethereum, a successor to the cryptographic-currency Bitcoin, which added the capacity for smart contracts and decentralised organisations. Buterin first proposed Ethereum at the age of 20, and by the age of 23 its success had likely made him a billionaire.</p><p> At the same time, far from indulging hype about these so-called 'blockchain' technologies, he has been candid about the limited good accomplished by Bitcoin and other currencies developed using cryptographic tools — and the breakthroughs that will be needed before they can have a meaningful social impact. In his own words, *"blockchains as they currently exist are in many ways a joke, right?"*</p><p> But Buterin is not just a realist. He's also an idealist, who has been helping to advance big ideas for new social institutions that might help people better coordinate to pursue their shared goals.</p><p> <a href="https://80k.link/buterin"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> By combining theories in economics and mechanism design with advances in cryptography, he has been pioneering the new interdiscriplinary field of 'cryptoeconomics'. Economist Tyler Cowen has<a href="https://80k.link/tyler-buterin">observed that</a>, <em>"at 25, Vitalik appears to repeatedly rediscover important economics results from famous papers, without knowing about the papers at all."</em></p><p> Along with <a href="https://80k.link/weyl-buterin">previous guest Glen Weyl</a>, Buterin has helped develop a model for so-called <a href="https://80k.link/qf">'quadratic funding'</a>, which in principle could transform the provision of 'public goods'. That is, goods that people benefit from whether they help pay for them or not.</p><p> Examples of goods that are fully or partially 'public goods' include sound decision-making in government, international peace, scientific advances, disease control, the existence of smart journalism, preventing climate change, deflecting asteroids headed to Earth, and the elimination of suffering. Their underprovision in part reflects the difficulty of getting people to pay for anything when they can instead free-ride on the efforts of others. Anything that could reduce this failure of coordination might transform the world.</p><p> But these and other related proposals face major hurdles. They're vulnerable to collusion, might be used to fund scams, and remain untested at a small scale — not to mention that anything with a square root sign in it is going to struggle to achieve societal legitimacy. Is the prize large enough to justify efforts to overcome these challenges?</p><p> In today's extensive three-hour interview, Buterin and I cover:</p><p> • What the blockchain has accomplished so far, and what it might achieve in the next decade;<br> • Why many social problems can be viewed as a coordination failure to provide a public good;<br> • Whether any of the ideas for decentralised social systems emerging from the blockchain community could really work;<br> • His view of 'effective altruism' and 'long-termism';<br> • Why he is optimistic about 'quadratic funding', but pessimistic about replacing existing voting with <a href="https://80k.link/qv">'quadratic voting'</a>;<br> • Why humanity might have to abandon living in cities;<br> • And much more.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Historically, progress in the field of cryptography has had <a href="https://80k.link/garfinkel">major consequences</a>. It has changed the course of major wars, made it possible to do business on the internet, and enabled private communication between both law-abiding citizens and dangerous criminals. Could it have similarly significant consequences in future?</p><p> Today's guest — Vitalik Buterin — is world-famous as the lead developer of Ethereum, a successor to the cryptographic-currency Bitcoin, which added the capacity for smart contracts and decentralised organisations. Buterin first proposed Ethereum at the age of 20, and by the age of 23 its success had likely made him a billionaire.</p><p> At the same time, far from indulging hype about these so-called 'blockchain' technologies, he has been candid about the limited good accomplished by Bitcoin and other currencies developed using cryptographic tools — and the breakthroughs that will be needed before they can have a meaningful social impact. In his own words, *"blockchains as they currently exist are in many ways a joke, right?"*</p><p> But Buterin is not just a realist. He's also an idealist, who has been helping to advance big ideas for new social institutions that might help people better coordinate to pursue their shared goals.</p><p> <a href="https://80k.link/buterin"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> By combining theories in economics and mechanism design with advances in cryptography, he has been pioneering the new interdiscriplinary field of 'cryptoeconomics'. Economist Tyler Cowen has<a href="https://80k.link/tyler-buterin">observed that</a>, <em>"at 25, Vitalik appears to repeatedly rediscover important economics results from famous papers, without knowing about the papers at all."</em></p><p> Along with <a href="https://80k.link/weyl-buterin">previous guest Glen Weyl</a>, Buterin has helped develop a model for so-called <a href="https://80k.link/qf">'quadratic funding'</a>, which in principle could transform the provision of 'public goods'. That is, goods that people benefit from whether they help pay for them or not.</p><p> Examples of goods that are fully or partially 'public goods' include sound decision-making in government, international peace, scientific advances, disease control, the existence of smart journalism, preventing climate change, deflecting asteroids headed to Earth, and the elimination of suffering. Their underprovision in part reflects the difficulty of getting people to pay for anything when they can instead free-ride on the efforts of others. Anything that could reduce this failure of coordination might transform the world.</p><p> But these and other related proposals face major hurdles. They're vulnerable to collusion, might be used to fund scams, and remain untested at a small scale — not to mention that anything with a square root sign in it is going to struggle to achieve societal legitimacy. Is the prize large enough to justify efforts to overcome these challenges?</p><p> In today's extensive three-hour interview, Buterin and I cover:</p><p> • What the blockchain has accomplished so far, and what it might achieve in the next decade;<br> • Why many social problems can be viewed as a coordination failure to provide a public good;<br> • Whether any of the ideas for decentralised social systems emerging from the blockchain community could really work;<br> • His view of 'effective altruism' and 'long-termism';<br> • Why he is optimistic about 'quadratic funding', but pessimistic about replacing existing voting with <a href="https://80k.link/qv">'quadratic voting'</a>;<br> • Why humanity might have to abandon living in cities;<br> • And much more.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 03 Sep 2019 22:52:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/1beff727/06eafbd2.mp3" length="191609908" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/65287Cr2ytxpLyuhpMIWPVAVxNkdvyhn7-fHvlkkApo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODQv/MTY4MzU0NDYwOC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11904</itunes:duration>
      <itunes:summary>Historically, progress in the field of cryptography has had major consequences. It has changed the course of major wars, made it possible to do business on the internet, and enabled private communication between both law-abiding citizens and dangerous criminals. Could it have similarly significant consequences in future?

Today's guest — Vitalik Buterin — is world-famous as the lead developer of Ethereum, a successor to the cryptographic-currency Bitcoin, which added the capacity for smart contracts and decentralised organisations. Buterin first proposed Ethereum at the age of 20, and by the age of 23 its success had likely made him a billionaire.

At the same time, far from indulging hype about these so-called 'blockchain' technologies, he has been candid about the limited good accomplished by Bitcoin and other currencies developed using cryptographic tools — and the breakthroughs that will be needed before they can have a meaningful social impact. In his own words, *"blockchains as they currently exist are in many ways a joke, right?"*

But Buterin is not just a realist. He's also an idealist, who has been helping to advance big ideas for new social institutions that might help people better coordinate to pursue their shared goals.

Links to learn more, summary and full transcript.

By combining theories in economics and mechanism design with advances in cryptography, he has been pioneering the new interdiscriplinary field of 'cryptoeconomics'. Economist Tyler Cowen hasobserved that, "at 25, Vitalik appears to repeatedly rediscover important economics results from famous papers, without knowing about the papers at all." 

Along with previous guest Glen Weyl, Buterin has helped develop a model for so-called 'quadratic funding', which in principle could transform the provision of 'public goods'. That is, goods that people benefit from whether they help pay for them or not.

Examples of goods that are fully or partially 'public goods' include sound decision-making in government, international peace, scientific advances, disease control, the existence of smart journalism, preventing climate change, deflecting asteroids headed to Earth, and the elimination of suffering. Their underprovision in part reflects the difficulty of getting people to pay for anything when they can instead free-ride on the efforts of others. Anything that could reduce this failure of coordination might transform the world.

But these and other related proposals face major hurdles. They're vulnerable to collusion, might be used to fund scams, and remain untested at a small scale — not to mention that anything with a square root sign in it is going to struggle to achieve societal legitimacy. Is the prize large enough to justify efforts to overcome these challenges?

In today's extensive three-hour interview, Buterin and I cover:

• What the blockchain has accomplished so far, and what it might achieve in the next decade;
• Why many social problems can be viewed as a coordination failure to provide a public good;
• Whether any of the ideas for decentralised social systems emerging from the blockchain community could really work;
• His view of 'effective altruism' and 'long-termism';
• Why he is optimistic about 'quadratic funding', but pessimistic about replacing existing voting with 'quadratic voting';
• Why humanity might have to abandon living in cities;
• And much more.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Historically, progress in the field of cryptography has had major consequences. It has changed the course of major wars, made it possible to do business on the internet, and enabled private communication between both law-abiding citizens and dangerous cri</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/1beff727/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#62 – Paul Christiano on messaging the future, increasing compute, &amp; how CO2 impacts your brain</title>
      <itunes:title>#62 – Paul Christiano on messaging the future, increasing compute, &amp; how CO2 impacts your brain</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">98d07d42-b78e-11e9-b068-0efe278d1810</guid>
      <link>https://80000hours.org/podcast/episodes/paul-christiano-a-message-for-the-future/</link>
      <description>
        <![CDATA[<p>Imagine that – one day – humanity dies out. At some point, many millions of years later, intelligent life might well evolve again. Is there any message we could leave that would reliably help them out?</p><p> In his second appearance on the 80,000 Hours Podcast, machine learning researcher and polymath Paul Christiano suggests we try to answer this question with a related thought experiment: are there any messages we might want to send back to our ancestors in the year 1700 that would have made history likely to go in a better direction than it did? It seems there probably are.</p><p> • <a href="https://80k.link/christiano-2"><strong>Links to learn more, summary, and full transcript.</strong></a><br> • <a href="https://80k.link/christiano-1"><strong>Paul's first appearance on the show in episode 44.</strong></a><br> • <a href="https://80k.link/PC2-outtake"><strong>An out-take on decision theory.</strong></a></p><p> We could tell them hard-won lessons from history; mention some research questions we wish we'd started addressing earlier; hand over all the social science we have that fosters peace and cooperation; and at the same time steer clear of engineering hints that would speed up the development of dangerous weapons.</p><p> But, as <a href="https://sideways-view.com/2018/06/07/messages-to-the-future/">Christiano points out</a>, even if we could satisfactorily figure out what we'd like to be able to tell our ancestors, that's just the first challenge. We'd need to leave the message somewhere that they could identify and dig up. While there are some promising options, this turns out to be remarkably hard to do, as anything we put on the Earth's surface quickly gets buried far underground.</p><p> But even if we figure out a satisfactory message, <em>and</em> a ways to ensure it's found, a civilization this far in the future won't speak any language like our own. And being another species, they presumably won't share as many fundamental concepts with us as humans from 1700. If we knew a way to leave them thousands of books and pictures in a material that wouldn't break down, would they be able to decipher what we meant to tell them, or would it simply remain a mystery?</p><p> That's just one of many playful questions discussed in today's episode with Christiano — a frequent writer who's willing to brave questions that others find too strange or hard to grapple with.</p><p> We also talk about why <a href="https://sideways-view.com/2019/05/25/analyzing-divestment/">divesting</a> a little bit from harmful companies might be more useful than I'd been thinking. Or whether creatine might make us a bit smarter, and carbon dioxide filled conference rooms make us a lot stupider.</p><p> Finally, we get a big update on progress in machine learning and efforts to make sure it's reliably aligned with our goals, which is Paul's main research project. He responds to the views that DeepMind's Pushmeet Kohli espoused <a href="https://80k.link/pushmeet-kohli">in a previous episode</a>, and we discuss whether we'd be better off if AI progress turned out to be most limited by algorithmic insights, or by our ability to manufacture enough computer processors.</p><p> Some other issues that come up along the way include:</p><p> • Are there any supplements people can take that make them think better?<br> • What implications do our views on meta-ethics have for aligning AI with our goals?<br> • Is there much of a risk that the future will contain anything optimised for causing harm?<br> • An <a href="https://80k.link/christiano-outtake">out-take about the implications of decision theory</a>, which we decided was too confusing and confused to stay in the main recording.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Imagine that – one day – humanity dies out. At some point, many millions of years later, intelligent life might well evolve again. Is there any message we could leave that would reliably help them out?</p><p> In his second appearance on the 80,000 Hours Podcast, machine learning researcher and polymath Paul Christiano suggests we try to answer this question with a related thought experiment: are there any messages we might want to send back to our ancestors in the year 1700 that would have made history likely to go in a better direction than it did? It seems there probably are.</p><p> • <a href="https://80k.link/christiano-2"><strong>Links to learn more, summary, and full transcript.</strong></a><br> • <a href="https://80k.link/christiano-1"><strong>Paul's first appearance on the show in episode 44.</strong></a><br> • <a href="https://80k.link/PC2-outtake"><strong>An out-take on decision theory.</strong></a></p><p> We could tell them hard-won lessons from history; mention some research questions we wish we'd started addressing earlier; hand over all the social science we have that fosters peace and cooperation; and at the same time steer clear of engineering hints that would speed up the development of dangerous weapons.</p><p> But, as <a href="https://sideways-view.com/2018/06/07/messages-to-the-future/">Christiano points out</a>, even if we could satisfactorily figure out what we'd like to be able to tell our ancestors, that's just the first challenge. We'd need to leave the message somewhere that they could identify and dig up. While there are some promising options, this turns out to be remarkably hard to do, as anything we put on the Earth's surface quickly gets buried far underground.</p><p> But even if we figure out a satisfactory message, <em>and</em> a ways to ensure it's found, a civilization this far in the future won't speak any language like our own. And being another species, they presumably won't share as many fundamental concepts with us as humans from 1700. If we knew a way to leave them thousands of books and pictures in a material that wouldn't break down, would they be able to decipher what we meant to tell them, or would it simply remain a mystery?</p><p> That's just one of many playful questions discussed in today's episode with Christiano — a frequent writer who's willing to brave questions that others find too strange or hard to grapple with.</p><p> We also talk about why <a href="https://sideways-view.com/2019/05/25/analyzing-divestment/">divesting</a> a little bit from harmful companies might be more useful than I'd been thinking. Or whether creatine might make us a bit smarter, and carbon dioxide filled conference rooms make us a lot stupider.</p><p> Finally, we get a big update on progress in machine learning and efforts to make sure it's reliably aligned with our goals, which is Paul's main research project. He responds to the views that DeepMind's Pushmeet Kohli espoused <a href="https://80k.link/pushmeet-kohli">in a previous episode</a>, and we discuss whether we'd be better off if AI progress turned out to be most limited by algorithmic insights, or by our ability to manufacture enough computer processors.</p><p> Some other issues that come up along the way include:</p><p> • Are there any supplements people can take that make them think better?<br> • What implications do our views on meta-ethics have for aligning AI with our goals?<br> • Is there much of a risk that the future will contain anything optimised for causing harm?<br> • An <a href="https://80k.link/christiano-outtake">out-take about the implications of decision theory</a>, which we decided was too confusing and confused to stay in the main recording.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 05 Aug 2019 15:07:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/02697742/5095a407.mp3" length="127350902" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ZlGyO5pijpDKU9hdiB-L1aPWYKHMcue-OTkRHrwx1-8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODMv/MTY4MzU0NDYwNy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7907</itunes:duration>
      <itunes:summary>Imagine that – one day – humanity dies out. At some point, many millions of years later, intelligent life might well evolve again. Is there any message we could leave that would reliably help them out?

In his second appearance on the 80,000 Hours Podcast, machine learning researcher and polymath Paul Christiano suggests we try to answer this question with a related thought experiment: are there any messages we might want to send back to our ancestors in the year 1700 that would have made history likely to go in a better direction than it did? It seems there probably are.

• Links to learn more, summary, and full transcript.
• Paul's first appearance on the show in episode 44.
• An out-take on decision theory.

We could tell them hard-won lessons from history; mention some research questions we wish we'd started addressing earlier; hand over all the social science we have that fosters peace and cooperation; and at the same time steer clear of engineering hints that would speed up the development of dangerous weapons.

But, as Christiano points out, even if we could satisfactorily figure out what we'd like to be able to tell our ancestors, that's just the first challenge. We'd need to leave the message somewhere that they could identify and dig up. While there are some promising options, this turns out to be remarkably hard to do, as anything we put on the Earth's surface quickly gets buried far underground.

But even if we figure out a satisfactory message, and a ways to ensure it's found, a civilization this far in the future won't speak any language like our own. And being another species, they presumably won't share as many fundamental concepts with us as humans from 1700. If we knew a way to leave them thousands of books and pictures in a material that wouldn't break down, would they be able to decipher what we meant to tell them, or would it simply remain a mystery?

That's just one of many playful questions discussed in today's episode with Christiano — a frequent writer who's willing to brave questions that others find too strange or hard to grapple with.

We also talk about why divesting a little bit from harmful companies might be more useful than I'd been thinking. Or whether creatine might make us a bit smarter, and carbon dioxide filled conference rooms make us a lot stupider.

Finally, we get a big update on progress in machine learning and efforts to make sure it's reliably aligned with our goals, which is Paul's main research project. He responds to the views that DeepMind's Pushmeet Kohli espoused in a previous episode, and we discuss whether we'd be better off if AI progress turned out to be most limited by algorithmic insights, or by our ability to manufacture enough computer processors.

Some other issues that come up along the way include:

• Are there any supplements people can take that make them think better?
• What implications do our views on meta-ethics have for aligning AI with our goals?
• Is there much of a risk that the future will contain anything optimised for causing harm?
• An out-take about the implications of decision theory, which we decided was too confusing and confused to stay in the main recording.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Imagine that – one day – humanity dies out. At some point, many millions of years later, intelligent life might well evolve again. Is there any message we could leave that would reliably help them out?

In his second appearance on the 80,000 Hours Podca</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/02697742/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#61 - Helen Toner on emerging technology, national security, and China</title>
      <itunes:title>#61 - Helen Toner on emerging technology, national security, and China</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">5a85a7ac-a8a1-11e9-89e5-0e66e68c3a30</guid>
      <link>https://share.transistor.fm/s/e55e0ba9</link>
      <description>
        <![CDATA[<p>From 1870 to 1950, the introduction of electricity transformed life in the US and UK, as people gained access to lighting, radio and a wide range of household appliances for the first time. Electricity turned out to be a general purpose technology that could help with almost everything people did.</p><p>

Some think this is the best historical analogy we have for how machine learning could alter life in the 21st century.</p><p>

In addition to massively changing everyday life, past general purpose technologies have also changed the nature of war. For example, when electricity was introduced to the battlefield, commanders gained the ability to communicate quickly with units in the field over great distances.</p><p>

How might international security be altered if the impact of machine learning reaches a similar scope to that of electricity? Today's guest — Helen Toner — recently helped found the <em>Center for Security and Emerging Technology</em> at Georgetown University to help policymakers prepare for such disruptive technical changes that might threaten international peace.</p><p>

• <a href="https://80k.link/helen-toner"><b>Links to learn more, summary and full transcript</b></a><br>
• <a href="https://80k.link/phil-helen"><b>Philosophy is one of the hardest grad programs. Is it worth it, if you want to use ideas to change the world?</b></a> by Arden Koehler and Will MacAskill<br>
• <a href="https://80k.link/policy-helen"><b>The case for building expertise to work on US AI policy, and how to do it</b></a> by Niel Bowerman<br>
• <a href="https://80k.link/job-board-helen"><b>AI strategy and governance roles on the job board</b></a></p><p>

Their first focus is machine learning (ML), a technology which allows computers to recognise patterns, learn from them, and develop 'intuitions' that inform their judgement about future cases. This is something humans do constantly, whether we're playing tennis, reading someone's face, diagnosing a patient, or figuring out which business ideas are likely to succeed.</p><p>

Sometimes these ML algorithms can seem uncannily insightful, and they're only getting better over time. Ultimately a wide range of different ML algorithms could end up helping us with all kinds of decisions, just as electricity wakes us up, makes us coffee, and brushes our teeth -- all in the first five minutes of our day.</p><p>

Rapid advances in ML, and the many prospective military applications, have people worrying about an 'AI arms race' between the US and China. Henry Kissinger and the past CEO of Google Eric Schmidt recently wrote that AI could "destabilize everything from nuclear détente to human friendships." Some politicians talk of classifying and restricting access to ML algorithms, lest they fall into the wrong hands.</p><p>

But if electricity is the best analogy, you could reasonably ask — was there an arms race in electricity in the 19th century? Would that have made any sense? And could someone have changed the course of history by changing who first got electricity and how they used it, or is that a fantasy?</p><p>

In today's episode we discuss the research frontier in the emerging field of AI policy and governance, how to have a career shaping US government policy, and Helen's experience living and studying in China.</p><p>

We cover:</p><p>

• Why immigration is the main policy area that should be affected by AI advances today.<br>
• Why talking about an 'arms race' in AI is premature.<br>
• How Bobby Kennedy may have positively affected the Cuban Missile Crisis.<br>
• Whether it's possible to become a China expert and still get a security clearance.<br>
• Can access to ML algorithms be restricted, or is that just not practical?<br>
• Whether AI could help stabilise authoritarian regimes.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>From 1870 to 1950, the introduction of electricity transformed life in the US and UK, as people gained access to lighting, radio and a wide range of household appliances for the first time. Electricity turned out to be a general purpose technology that could help with almost everything people did.</p><p>

Some think this is the best historical analogy we have for how machine learning could alter life in the 21st century.</p><p>

In addition to massively changing everyday life, past general purpose technologies have also changed the nature of war. For example, when electricity was introduced to the battlefield, commanders gained the ability to communicate quickly with units in the field over great distances.</p><p>

How might international security be altered if the impact of machine learning reaches a similar scope to that of electricity? Today's guest — Helen Toner — recently helped found the <em>Center for Security and Emerging Technology</em> at Georgetown University to help policymakers prepare for such disruptive technical changes that might threaten international peace.</p><p>

• <a href="https://80k.link/helen-toner"><b>Links to learn more, summary and full transcript</b></a><br>
• <a href="https://80k.link/phil-helen"><b>Philosophy is one of the hardest grad programs. Is it worth it, if you want to use ideas to change the world?</b></a> by Arden Koehler and Will MacAskill<br>
• <a href="https://80k.link/policy-helen"><b>The case for building expertise to work on US AI policy, and how to do it</b></a> by Niel Bowerman<br>
• <a href="https://80k.link/job-board-helen"><b>AI strategy and governance roles on the job board</b></a></p><p>

Their first focus is machine learning (ML), a technology which allows computers to recognise patterns, learn from them, and develop 'intuitions' that inform their judgement about future cases. This is something humans do constantly, whether we're playing tennis, reading someone's face, diagnosing a patient, or figuring out which business ideas are likely to succeed.</p><p>

Sometimes these ML algorithms can seem uncannily insightful, and they're only getting better over time. Ultimately a wide range of different ML algorithms could end up helping us with all kinds of decisions, just as electricity wakes us up, makes us coffee, and brushes our teeth -- all in the first five minutes of our day.</p><p>

Rapid advances in ML, and the many prospective military applications, have people worrying about an 'AI arms race' between the US and China. Henry Kissinger and the past CEO of Google Eric Schmidt recently wrote that AI could "destabilize everything from nuclear détente to human friendships." Some politicians talk of classifying and restricting access to ML algorithms, lest they fall into the wrong hands.</p><p>

But if electricity is the best analogy, you could reasonably ask — was there an arms race in electricity in the 19th century? Would that have made any sense? And could someone have changed the course of history by changing who first got electricity and how they used it, or is that a fantasy?</p><p>

In today's episode we discuss the research frontier in the emerging field of AI policy and governance, how to have a career shaping US government policy, and Helen's experience living and studying in China.</p><p>

We cover:</p><p>

• Why immigration is the main policy area that should be affected by AI advances today.<br>
• Why talking about an 'arms race' in AI is premature.<br>
• How Bobby Kennedy may have positively affected the Cuban Missile Crisis.<br>
• Whether it's possible to become a China expert and still get a security clearance.<br>
• Can access to ML algorithms be restricted, or is that just not practical?<br>
• Whether AI could help stabilise authoritarian regimes.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 17 Jul 2019 17:08:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/e55e0ba9/f912b185.mp3" length="110951324" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/da8NowhGjPY6Ie3Vpng-phVR1PfNP4Yq4T14no1Td5M/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODIv/MTY4MzU0NDYwNi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6897</itunes:duration>
      <itunes:summary>From 1870 to 1950, the introduction of electricity transformed life in the US and UK, as people gained access to lighting, radio and a wide range of household appliances for the first time. Electricity turned out to be a general purpose technology that could help with almost everything people did.

Some think this is the best historical analogy we have for how machine learning could alter life in the 21st century.

In addition to massively changing everyday life, past general purpose technologies have also changed the nature of war. For example, when electricity was introduced to the battlefield, commanders gained the ability to communicate quickly with units in the field over great distances.

How might international security be altered if the impact of machine learning reaches a similar scope to that of electricity? Today's guest — Helen Toner — recently helped found the Center for Security and Emerging Technology at Georgetown University to help policymakers prepare for such disruptive technical changes that might threaten international peace.

• Links to learn more, summary and full transcript
• Philosophy is one of the hardest grad programs. Is it worth it, if you want to use ideas to change the world? by Arden Koehler and Will MacAskill
• The case for building expertise to work on US AI policy, and how to do it by Niel Bowerman
• AI strategy and governance roles on the job board

Their first focus is machine learning (ML), a technology which allows computers to recognise patterns, learn from them, and develop 'intuitions' that inform their judgement about future cases. This is something humans do constantly, whether we're playing tennis, reading someone's face, diagnosing a patient, or figuring out which business ideas are likely to succeed.

Sometimes these ML algorithms can seem uncannily insightful, and they're only getting better over time. Ultimately a wide range of different ML algorithms could end up helping us with all kinds of decisions, just as electricity wakes us up, makes us coffee, and brushes our teeth -- all in the first five minutes of our day.

Rapid advances in ML, and the many prospective military applications, have people worrying about an 'AI arms race' between the US and China. Henry Kissinger and the past CEO of Google Eric Schmidt recently wrote that AI could "destabilize everything from nuclear détente to human friendships." Some politicians talk of classifying and restricting access to ML algorithms, lest they fall into the wrong hands.

But if electricity is the best analogy, you could reasonably ask — was there an arms race in electricity in the 19th century? Would that have made any sense? And could someone have changed the course of history by changing who first got electricity and how they used it, or is that a fantasy?

In today's episode we discuss the research frontier in the emerging field of AI policy and governance, how to have a career shaping US government policy, and Helen's experience living and studying in China.

We cover:

• Why immigration is the main policy area that should be affected by AI advances today.
• Why talking about an 'arms race' in AI is premature.
• How Bobby Kennedy may have positively affected the Cuban Missile Crisis.
• Whether it's possible to become a China expert and still get a security clearance.
• Can access to ML algorithms be restricted, or is that just not practical?
• Whether AI could help stabilise authoritarian regimes.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>From 1870 to 1950, the introduction of electricity transformed life in the US and UK, as people gained access to lighting, radio and a wide range of household appliances for the first time. Electricity turned out to be a general purpose technology that co</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#60 - Phil Tetlock on why accurate forecasting matters for everything, and how you can do it better</title>
      <itunes:title>#60 - Phil Tetlock on why accurate forecasting matters for everything, and how you can do it better</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">f6bacf92-992b-11e9-8563-0e5c3cf3635a</guid>
      <link>https://share.transistor.fm/s/56acb002</link>
      <description>
        <![CDATA[<p>Have you ever been infuriated by a doctor's unwillingness to give you an honest, probabilistic estimate about what to expect? Or a lawyer who won't tell you the chances you'll win your case?</p><p> Their behaviour is so frustrating because accurately predicting the future is central to every action we take. If we can't assess the likelihood of different outcomes we're in a complete bind, whether the decision concerns war and peace, work and study, or Black Mirror and RuPaul's Drag Race.</p><p> Which is why the research of Professor Philip Tetlock is relevant for all of us each and every day.</p><p> He has spent 40 years as a meticulous social scientist, collecting millions of predictions from tens of thousands of people, in order to figure out how good humans really are at foreseeing the future, and what habits of thought allow us to do better.</p><p> Along with other psychologists, he identified that many ordinary people are attracted to a 'folk probability' that draws just three distinctions — 'impossible', 'possible' and 'certain' — and which leads to major systemic mistakes. But with the right mindset and training we can become capable of accurately discriminating between differences as fine as 56% as against 57% likely.</p><p> • <a href="https://80k.link/tetlock-2"><strong>Links to learn more, summary and full transcript</strong></a><br> • <a href="https://80k.link/tetlock-calibration"><strong>The calibration training app</strong></a><br> • <a href="http://80k.link/civ"><strong>Sign up for the Civ-5 counterfactual forecasting tournament</strong></a><br> • <a href="https://80k.link/ai-impacts-summary"><strong>A review of the evidence on good forecasting practices</strong></a><br> • <a href="https://eaglobal.org"><strong>Learn more about Effective Altruism Global</strong></a></p><p> In the aftermath of Iraq and WMDs the US intelligence community hired him to prevent the same ever happening again, and his guide — <em>Superforecasting: The Art and Science of Prediction</em> — became a bestseller back in 2014.</p><p> That was five years ago. In today's interview, Tetlock explains how his research agenda continues to advance, today using the game <em>Civilization 5</em> to see how well we can predict what <em>would have</em> happened in elusive counterfactual worlds we never get to see, and discovering how simple algorithms can complement or substitute for human judgement.</p><p> We discuss how his work can be applied to your personal life to answer high-stakes questions, like how likely you are to thrive in a given career path, or whether your business idea will be a billion-dollar unicorn — or fall apart catastrophically. (To help you get better at figuring those things out, our site now has a <a href="https://80k.link/tetlock-calibration">training app</a> developed by the Open Philanthropy Project and Clearer Thinking that teaches you to distinguish your '70 percents' from your '80 percents'.)</p><p> We also bring some tough methodological questions raised by the author of a recent <a href="https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/">review of the forecasting literature</a>. And we find out what jobs people can take to make improving the reasonableness of decision-making in major institutions that shape the world their profession, as it has been for Tetlock over many decades.</p><p> We view Tetlock's work as so core to living well that we've brought him back for a second and longer appearance on the show — his first was back in <a href="https://80k.link/tetlock1">episode 15</a>.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Have you ever been infuriated by a doctor's unwillingness to give you an honest, probabilistic estimate about what to expect? Or a lawyer who won't tell you the chances you'll win your case?</p><p> Their behaviour is so frustrating because accurately predicting the future is central to every action we take. If we can't assess the likelihood of different outcomes we're in a complete bind, whether the decision concerns war and peace, work and study, or Black Mirror and RuPaul's Drag Race.</p><p> Which is why the research of Professor Philip Tetlock is relevant for all of us each and every day.</p><p> He has spent 40 years as a meticulous social scientist, collecting millions of predictions from tens of thousands of people, in order to figure out how good humans really are at foreseeing the future, and what habits of thought allow us to do better.</p><p> Along with other psychologists, he identified that many ordinary people are attracted to a 'folk probability' that draws just three distinctions — 'impossible', 'possible' and 'certain' — and which leads to major systemic mistakes. But with the right mindset and training we can become capable of accurately discriminating between differences as fine as 56% as against 57% likely.</p><p> • <a href="https://80k.link/tetlock-2"><strong>Links to learn more, summary and full transcript</strong></a><br> • <a href="https://80k.link/tetlock-calibration"><strong>The calibration training app</strong></a><br> • <a href="http://80k.link/civ"><strong>Sign up for the Civ-5 counterfactual forecasting tournament</strong></a><br> • <a href="https://80k.link/ai-impacts-summary"><strong>A review of the evidence on good forecasting practices</strong></a><br> • <a href="https://eaglobal.org"><strong>Learn more about Effective Altruism Global</strong></a></p><p> In the aftermath of Iraq and WMDs the US intelligence community hired him to prevent the same ever happening again, and his guide — <em>Superforecasting: The Art and Science of Prediction</em> — became a bestseller back in 2014.</p><p> That was five years ago. In today's interview, Tetlock explains how his research agenda continues to advance, today using the game <em>Civilization 5</em> to see how well we can predict what <em>would have</em> happened in elusive counterfactual worlds we never get to see, and discovering how simple algorithms can complement or substitute for human judgement.</p><p> We discuss how his work can be applied to your personal life to answer high-stakes questions, like how likely you are to thrive in a given career path, or whether your business idea will be a billion-dollar unicorn — or fall apart catastrophically. (To help you get better at figuring those things out, our site now has a <a href="https://80k.link/tetlock-calibration">training app</a> developed by the Open Philanthropy Project and Clearer Thinking that teaches you to distinguish your '70 percents' from your '80 percents'.)</p><p> We also bring some tough methodological questions raised by the author of a recent <a href="https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/">review of the forecasting literature</a>. And we find out what jobs people can take to make improving the reasonableness of decision-making in major institutions that shape the world their profession, as it has been for Tetlock over many decades.</p><p> We view Tetlock's work as so core to living well that we've brought him back for a second and longer appearance on the show — his first was back in <a href="https://80k.link/tetlock1">episode 15</a>.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 28 Jun 2019 15:36:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/56acb002/298d0eba.mp3" length="127053096" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/i-eyBij5GJbSJkW9pSOpDPmNGB-RScF2y5tdsJ0Yg2Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODEv/MTY4MzU0NDYwNC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7899</itunes:duration>
      <itunes:summary>Have you ever been infuriated by a doctor's unwillingness to give you an honest, probabilistic estimate about what to expect? Or a lawyer who won't tell you the chances you'll win your case?

Their behaviour is so frustrating because accurately predicting the future is central to every action we take. If we can't assess the likelihood of different outcomes we're in a complete bind, whether the decision concerns war and peace, work and study, or Black Mirror and RuPaul's Drag Race.

Which is why the research of Professor Philip Tetlock is relevant for all of us each and every day.

He has spent 40 years as a meticulous social scientist, collecting millions of predictions from tens of thousands of people, in order to figure out how good humans really are at foreseeing the future, and what habits of thought allow us to do better.

Along with other psychologists, he identified that many ordinary people are attracted to a 'folk probability' that draws just three distinctions — 'impossible', 'possible' and 'certain' — and which leads to major systemic mistakes. But with the right mindset and training we can become capable of accurately discriminating between differences as fine as 56% as against 57% likely.

• Links to learn more, summary and full transcript
• The calibration training app
• Sign up for the Civ-5 counterfactual forecasting tournament
• A review of the evidence on good forecasting practices
• Learn more about Effective Altruism Global

In the aftermath of Iraq and WMDs the US intelligence community hired him to prevent the same ever happening again, and his guide — Superforecasting: The Art and Science of Prediction — became a bestseller back in 2014.

That was five years ago. In today's interview, Tetlock explains how his research agenda continues to advance, today using the game Civilization 5 to see how well we can predict what would have happened in elusive counterfactual worlds we never get to see, and discovering how simple algorithms can complement or substitute for human judgement.

We discuss how his work can be applied to your personal life to answer high-stakes questions, like how likely you are to thrive in a given career path, or whether your business idea will be a billion-dollar unicorn — or fall apart catastrophically. (To help you get better at figuring those things out, our site now has a training app developed by the Open Philanthropy Project and Clearer Thinking that teaches you to distinguish your '70 percents' from your '80 percents'.)

We also bring some tough methodological questions raised by the author of a recent review of the forecasting literature. And we find out what jobs people can take to make improving the reasonableness of decision-making in major institutions that shape the world their profession, as it has been for Tetlock over many decades.

We view Tetlock's work as so core to living well that we've brought him back for a second and longer appearance on the show — his first was back in episode 15.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Have you ever been infuriated by a doctor's unwillingness to give you an honest, probabilistic estimate about what to expect? Or a lawyer who won't tell you the chances you'll win your case?

Their behaviour is so frustrating because accurately predicti</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#59 – Cass Sunstein on how change happens, and why it's so often abrupt &amp; unpredictable</title>
      <itunes:title>#59 – Cass Sunstein on how change happens, and why it's so often abrupt &amp; unpredictable</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">00b8d1f6-8eb9-11e9-bec4-0e5c3cf3635a</guid>
      <link>https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/</link>
      <description>
        <![CDATA[<p>It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.</p><p>The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.</p><p>In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.</p><p>How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?</p><p>Sunstein — coauthor of <em>Nudge</em>, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book <em>How Change Happens</em>.</p><p> He pulls together three phenomena which social scientists have studied in recent decades: <em>preference falsification</em>, <em>variable thresholds for action</em>, and <em>group polarisation</em>. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.</p><p> • <a href="https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/?utm_campaign=podcast__cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://80000hours.org/2019/05/annual-review-dec-2018/?utm_campaign=podcast__cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>80,000 Hours Annual Review 2018.</strong></a><br> • <a href="https://80000hours.org/2019/05/annual-review-dec-2018/#how-to-donate"><strong>How to donate to 80,000 Hours.</strong></a></p><p> In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.</p><p> According to Sunstein, it's <em>"much, much easier"</em> to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.</p><p> In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:</p><p> • How much people misrepresent their views in democratic countries.<br> • Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.<br> • When is it justified to encourage your own group to polarise?<br> • Sunstein's difficult experiences as a pioneer of animal rights law.<br> • Whether activists can do better by spending half their resources on public opinion surveys.<br> • Should people be more or less outspoken about their true views?<br> • What might be the next social revolution to take off?<br> • How can we learn about social movements that failed and disappeared?<br> • How to find out what people really think.</p><p> Chapters:<br>• Rob’s intro (00:00:00)<br>• Cass's Harvard lecture on How Change Happens (00:02:59)<br>• Rob &amp; Cass's conversation about the book (00:41:43)</p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.</p><p>The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.</p><p>In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.</p><p>How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?</p><p>Sunstein — coauthor of <em>Nudge</em>, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book <em>How Change Happens</em>.</p><p> He pulls together three phenomena which social scientists have studied in recent decades: <em>preference falsification</em>, <em>variable thresholds for action</em>, and <em>group polarisation</em>. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.</p><p> • <a href="https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/?utm_campaign=podcast__cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a><br> • <a href="https://80000hours.org/2019/05/annual-review-dec-2018/?utm_campaign=podcast__cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>80,000 Hours Annual Review 2018.</strong></a><br> • <a href="https://80000hours.org/2019/05/annual-review-dec-2018/#how-to-donate"><strong>How to donate to 80,000 Hours.</strong></a></p><p> In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.</p><p> According to Sunstein, it's <em>"much, much easier"</em> to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.</p><p> In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:</p><p> • How much people misrepresent their views in democratic countries.<br> • Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.<br> • When is it justified to encourage your own group to polarise?<br> • Sunstein's difficult experiences as a pioneer of animal rights law.<br> • Whether activists can do better by spending half their resources on public opinion surveys.<br> • Should people be more or less outspoken about their true views?<br> • What might be the next social revolution to take off?<br> • How can we learn about social movements that failed and disappeared?<br> • How to find out what people really think.</p><p> Chapters:<br>• Rob’s intro (00:00:00)<br>• Cass's Harvard lecture on How Change Happens (00:02:59)<br>• Rob &amp; Cass's conversation about the book (00:41:43)</p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 17 Jun 2019 22:45:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f028fbb3/fffe68b3.mp3" length="99624777" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/yDkGvtDtnlOybMOQD76c4oKfceAR-F77Vz7qnMHjCaA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3ODAv/MTY4MzU0NDYwMy1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6204</itunes:duration>
      <itunes:summary>It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.

The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.

In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.

How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?

Sunstein — co-author of Nudge, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book How Change Happens.

He pulls together three phenomena which social scientists have studied in recent decades: preference falsification, variable thresholds for action, and group polarisation. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.

• Links to learn more, summary and full transcript.
• 80,000 Hours Annual Review 2018.
• How to donate to 80,000 Hours.

In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.

According to Sunstein, it's "much, much easier" to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.

In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:

• How much people misrepresent their views in democratic countries.
• Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.
• When is it justified to encourage your own group to polarise?
• Sunstein's difficult experiences as a pioneer of animal rights law.
• Whether activists can do better by spending half their resources on public opinion surveys.
• Should people be more or less outspoken about their true views?
• What might be the next social revolution to take off?
• How can we learn about social movements that failed and disappeared?
• How to find out what people really think.

Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the transcript on our site.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are o</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/f028fbb3/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#58 – Pushmeet Kohli of DeepMind on designing robust &amp; reliable AI systems and how to succeed in AI</title>
      <itunes:title>#58 – Pushmeet Kohli of DeepMind on designing robust &amp; reliable AI systems and how to succeed in AI</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">73a4f462-8615-11e9-9340-0ee7196fda3c</guid>
      <link>https://80000hours.org/podcast/episodes/pushmeet-kohli-deepmind-safety-research</link>
      <description>
        <![CDATA[<p>When you're building a bridge, responsibility for making sure it won't fall over isn't handed over to a few 'bridge not falling down engineers'. Making sure a bridge is safe to use and remains standing in a storm is completely central to the design, and indeed the entire project.</p><p>When it comes to artificial intelligence, commentators often distinguish between enhancing the capabilities of machine learning systems and enhancing their safety. But to Pushmeet Kohli, principal scientist and research team leader at DeepMind, research to make AI robust and reliable is no more a side-project in AI design than keeping a bridge standing is a side-project in bridge design.</p><p>Far from being an overhead on the 'real' work, it’s an essential part of making AI systems work at all. We don’t want AI systems to be out of alignment with our intentions, and that consideration must arise throughout their development.</p><p>Professor Stuart Russell — co-author of the most popular AI textbook — <a href="https://youtu.be/GYQrNfSmQ0M?t=2651">has gone as far as to suggest</a> that if this view is right, it may be time to retire the term ‘AI safety research’ altogether.</p><p> • <strong>Want to be notified about high-impact opportunities to help ensure AI remains safe and beneficial? </strong><a href="https://80000hours.org/opportunities-in-ai/?utm_campaign=podcast__pushmeet-kohli&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Tell us a bit about yourself and we’ll get in touch if an opportunity matches your background and interests.</a></p><p> • <a href="https://80k.link/pushmeet-kohli"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> • <strong>And a few </strong><a href="https://80k.link/deepmind-roles">added thoughts on non-research roles</a><strong>.</strong></p><p> With the goal of designing systems that are reliably consistent with desired specifications, DeepMind have <a href="https://deepmind.com/blog/robust-and-verified-ai/">recently published work</a> on important technical challenges for the machine learning community.</p><p> For instance, Pushmeet is looking for efficient ways to test whether a system conforms to the desired specifications, even in peculiar situations, by creating an 'adversary' that proactively seeks out the worst failures possible. If the adversary can efficiently identify the worst-case input for a given model, DeepMind can catch rare failure cases before deploying a model in the real world. In the future single mistakes by autonomous systems may have very large consequences, which will make even small failure probabilities unacceptable. </p><p> He's also looking into 'training specification-consistent models' and formal verification', while other researchers at DeepMind working on their <a href="https://80k.link/dmsr1">AI safety agenda</a> are figuring out how to <a href="https://80k.link/dmsr2">understand agent incentives</a>, <a href="https://80k.link/dmsr3">avoid side-effects</a>, and <a href="https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84">model AI rewards</a>.</p><p> In today’s interview, we focus on the convergence between broader AI research and robustness, as well as:</p><p> • DeepMind’s work on the protein folding problem<br> • Parallels between ML problems and past challenges in software development and computer security<br> • How can you analyse the thinking of a neural network?<br> • Unique challenges faced by DeepMind’s technical AGI safety team<br> • How do you communicate with a non-human intelligence?<br> • What are the biggest misunderstandings about AI safety and reliability?<br> • Are there actually a lot of disagreements within the field?<br> • The difficulty of forecasting AI development</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>When you're building a bridge, responsibility for making sure it won't fall over isn't handed over to a few 'bridge not falling down engineers'. Making sure a bridge is safe to use and remains standing in a storm is completely central to the design, and indeed the entire project.</p><p>When it comes to artificial intelligence, commentators often distinguish between enhancing the capabilities of machine learning systems and enhancing their safety. But to Pushmeet Kohli, principal scientist and research team leader at DeepMind, research to make AI robust and reliable is no more a side-project in AI design than keeping a bridge standing is a side-project in bridge design.</p><p>Far from being an overhead on the 'real' work, it’s an essential part of making AI systems work at all. We don’t want AI systems to be out of alignment with our intentions, and that consideration must arise throughout their development.</p><p>Professor Stuart Russell — co-author of the most popular AI textbook — <a href="https://youtu.be/GYQrNfSmQ0M?t=2651">has gone as far as to suggest</a> that if this view is right, it may be time to retire the term ‘AI safety research’ altogether.</p><p> • <strong>Want to be notified about high-impact opportunities to help ensure AI remains safe and beneficial? </strong><a href="https://80000hours.org/opportunities-in-ai/?utm_campaign=podcast__pushmeet-kohli&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Tell us a bit about yourself and we’ll get in touch if an opportunity matches your background and interests.</a></p><p> • <a href="https://80k.link/pushmeet-kohli"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> • <strong>And a few </strong><a href="https://80k.link/deepmind-roles">added thoughts on non-research roles</a><strong>.</strong></p><p> With the goal of designing systems that are reliably consistent with desired specifications, DeepMind have <a href="https://deepmind.com/blog/robust-and-verified-ai/">recently published work</a> on important technical challenges for the machine learning community.</p><p> For instance, Pushmeet is looking for efficient ways to test whether a system conforms to the desired specifications, even in peculiar situations, by creating an 'adversary' that proactively seeks out the worst failures possible. If the adversary can efficiently identify the worst-case input for a given model, DeepMind can catch rare failure cases before deploying a model in the real world. In the future single mistakes by autonomous systems may have very large consequences, which will make even small failure probabilities unacceptable. </p><p> He's also looking into 'training specification-consistent models' and formal verification', while other researchers at DeepMind working on their <a href="https://80k.link/dmsr1">AI safety agenda</a> are figuring out how to <a href="https://80k.link/dmsr2">understand agent incentives</a>, <a href="https://80k.link/dmsr3">avoid side-effects</a>, and <a href="https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84">model AI rewards</a>.</p><p> In today’s interview, we focus on the convergence between broader AI research and robustness, as well as:</p><p> • DeepMind’s work on the protein folding problem<br> • Parallels between ML problems and past challenges in software development and computer security<br> • How can you analyse the thinking of a neural network?<br> • Unique challenges faced by DeepMind’s technical AGI safety team<br> • How do you communicate with a non-human intelligence?<br> • What are the biggest misunderstandings about AI safety and reliability?<br> • Are there actually a lot of disagreements within the field?<br> • The difficulty of forecasting AI development</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 03 Jun 2019 17:10:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ab619e3b/6bfd20e0.mp3" length="87416101" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/EIJxxXivnlvQV3iMOg191AsxdQd_bk0QddF4FzYAUIQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Nzkv/MTY4MzU0NDYwMi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5412</itunes:duration>
      <itunes:summary>When you're building a bridge, responsibility for making sure it won't fall over isn't handed over to a few 'bridge not falling down engineers'. Making sure a bridge is safe to use and remains standing in a storm is completely central to the design, and indeed the entire project.

When it comes to artificial intelligence, commentators often distinguish between enhancing the capabilities of machine learning systems and enhancing their safety. But to Pushmeet Kohli, principal scientist and research team leader at DeepMind, research to make AI robust and reliable is no more a side-project in AI design than keeping a bridge standing is a side-project in bridge design.

Far from being an overhead on the 'real' work, it’s an essential part of making AI systems work at all. We don’t want AI systems to be out of alignment with our intentions, and that consideration must arise throughout their development.

Professor Stuart Russell — co-author of the most popular AI textbook — has gone as far as to suggest that if this view is right, it may be time to retire the term ‘AI safety research’ altogether.

• Want to be notified about high-impact opportunities to help ensure AI remains safe and beneficial? Tell us a bit about yourself and we’ll get in touch if an opportunity matches your background and interests.

• Links to learn more, summary and full transcript.

• And a few added thoughts on non-research roles.

With the goal of designing systems that are reliably consistent with desired specifications, DeepMind have recently published work on important technical challenges for the machine learning community.

For instance, Pushmeet is looking for efficient ways to test whether a system conforms to the desired specifications, even in peculiar situations, by creating an 'adversary' that proactively seeks out the worst failures possible. If the adversary can efficiently identify the worst-case input for a given model, DeepMind can catch rare failure cases before deploying a model in the real world. In the future single mistakes by autonomous systems may have very large consequences, which will make even small failure probabilities unacceptable. 

He's also looking into 'training specification-consistent models' and formal verification', while other researchers at DeepMind working on their AI safety agenda are figuring out how to understand agent incentives, avoid side-effects, and model AI rewards.

In today’s interview, we focus on the convergence between broader AI research and robustness, as well as:

• DeepMind’s work on the protein folding problem
• Parallels between ML problems and past challenges in software development and computer security
• How can you analyse the thinking of a neural network?
• Unique challenges faced by DeepMind’s technical AGI safety team
• How do you communicate with a non-human intelligence?
• What are the biggest misunderstandings about AI safety and reliability?
• Are there actually a lot of disagreements within the field?
• The difficulty of forecasting AI development

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>When you're building a bridge, responsibility for making sure it won't fall over isn't handed over to a few 'bridge not falling down engineers'. Making sure a bridge is safe to use and remains standing in a storm is completely central to the design, and i</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/ab619e3b/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>Rob Wiblin on human nature, new technology, and living a happy, healthy &amp; ethical life</title>
      <itunes:title>Rob Wiblin on human nature, new technology, and living a happy, healthy &amp; ethical life</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">a0606768-75ab-11e9-bfa0-0e96c47ddb2a</guid>
      <link>https://share.transistor.fm/s/27315641</link>
      <description>
        <![CDATA[This is a cross-post of some interviews Rob did recently on two other podcasts — <a href="https://mission.org/missiondaily/">Mission Daily</a> (from 2m) and <a href="https://goodlifepodcast.podbean.com/">The Good Life</a> (from 1h13m).<p>

Some of the content will be familiar to regular listeners — but if you’re at all interested in Rob’s personal thoughts, there should be quite a lot of new material to make listening worthwhile.</p><p>

The first interview is with Chad Grills. They focused largely on new technologies and existential risks, but also discuss topics like:</p><p>

• Why Rob is wary of fiction<br>
• Egalitarianism in the evolution of hunter gatherers<br>
• How to stop social media screwing up politics<br>
• Careers in government versus business</p><p>

The second interview is with Prof Andrew Leigh - the Shadow Assistant Treasurer in Australia. This one gets into more personal topics than we usually cover on the show, like: </p><p>

• What advice would Rob give to his teenage self?<br>
• Which person has most shaped Rob’s view of living an ethical life?<br>
• Rob’s approach to giving to the homeless<br>
• What does Rob do to maximise his own happiness?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[This is a cross-post of some interviews Rob did recently on two other podcasts — <a href="https://mission.org/missiondaily/">Mission Daily</a> (from 2m) and <a href="https://goodlifepodcast.podbean.com/">The Good Life</a> (from 1h13m).<p>

Some of the content will be familiar to regular listeners — but if you’re at all interested in Rob’s personal thoughts, there should be quite a lot of new material to make listening worthwhile.</p><p>

The first interview is with Chad Grills. They focused largely on new technologies and existential risks, but also discuss topics like:</p><p>

• Why Rob is wary of fiction<br>
• Egalitarianism in the evolution of hunter gatherers<br>
• How to stop social media screwing up politics<br>
• Careers in government versus business</p><p>

The second interview is with Prof Andrew Leigh - the Shadow Assistant Treasurer in Australia. This one gets into more personal topics than we usually cover on the show, like: </p><p>

• What advice would Rob give to his teenage self?<br>
• Which person has most shaped Rob’s view of living an ethical life?<br>
• Rob’s approach to giving to the homeless<br>
• What does Rob do to maximise his own happiness?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 13 May 2019 22:28:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/27315641/fa1ff83d.mp3" length="133590714" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/87QCRvViro90sQ2Dg2YtCC_6IwE2xc4FBTzdrKO_33s/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Nzgv/MTY4MzU0NDYwMS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8305</itunes:duration>
      <itunes:summary>This is a cross-post of some interviews Rob did recently on two other podcasts — Mission Daily (from 2m) and The Good Life (from 1h13m).

Some of the content will be familiar to regular listeners — but if you’re at all interested in Rob’s personal thoughts, there should be quite a lot of new material to make listening worthwhile.

The first interview is with Chad Grills. They focused largely on new technologies and existential risks, but also discuss topics like:

• Why Rob is wary of fiction
• Egalitarianism in the evolution of hunter gatherers
• How to stop social media screwing up politics
• Careers in government versus business

The second interview is with Prof Andrew Leigh - the Shadow Assistant Treasurer in Australia. This one gets into more personal topics than we usually cover on the show, like: 

• What advice would Rob give to his teenage self?
• Which person has most shaped Rob’s view of living an ethical life?
• Rob’s approach to giving to the homeless
• What does Rob do to maximise his own happiness?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>This is a cross-post of some interviews Rob did recently on two other podcasts — Mission Daily (from 2m) and The Good Life (from 1h13m).

Some of the content will be familiar to regular listeners — but if you’re at all interested in Rob’s personal thought</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#57 – Tom Kalil on how to do the most good in government</title>
      <itunes:title>#57 – Tom Kalil on how to do the most good in government</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">457e3002-6546-11e9-a608-0eb1e99d174e</guid>
      <link>https://80000hours.org/podcast/episodes/tom-kalil-government-careers/</link>
      <description>
        <![CDATA[<p>You’re 29 years old, and you’ve just been given a job in the White House. How do you quickly figure out how the US Executive Branch behemoth actually works, so that you can have as much impact as possible - before you quit or get kicked out?</p><p>That was the challenge put in front of Tom Kalil in 1993.</p><p>He had enough success to last a full 16 years inside the Clinton and Obama administrations, working to foster the development of the internet, then nanotechnology, and then cutting-edge brain modelling, among other things.</p><p>But not everyone figures out how to move the needle. In today's interview, Tom shares his experience with how to increase your chances of getting an influential role in government, and how to make the most of the opportunity if you get in.</p><p><a href="https://80k.link/tom-kalil"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> <a href="https://80k.link/tom-kalil-coaching"><strong>Interested in US AI policy careers? Apply for one-on-one career advice here.</strong></a></p><p><a href="https://cset.georgetown.edu/careers/"><strong>Vacancies at the Center for Security and Emerging Technology.</strong></a></p><p><a href="https://80k.link/tom-kalil-job-board"><strong>Our high-impact job board, which features other related opportunities.</strong></a></p><p> He believes that Congressional gridlock leads people to greatly underestimate how much the Executive Branch can and does do on its own every day. Decisions by individuals change how billions of dollars are spent; regulations are enforced, and then suddenly they aren't; and a single sentence in the State of the Union can get civil servants to pay attention to a topic that would otherwise go ignored.</p><p> Over years at the White House Office of Science and Technology Policy, 'Team Kalil' built up a white board of principles. For example, 'the schedule is your friend': setting a meeting date with the President can force people to finish something, where they otherwise might procrastinate.</p><p> Or 'talk to who owns the paper'. People would wonder how Tom could get so many lines into the President's speeches. The answer was "figure out who's writing the speech, find them with the document, and tell them to add the line." Obvious, but not something most were doing.</p><p> Not everything is a precise operation though. Tom also tells us the story of NetDay, a project that was put together at the last minute because the President incorrectly believed it was already organised – and decided he was going to announce it in person.</p><p> In today's episode we get down to nuts &amp; bolts, and discuss:<br> • How did Tom spin work on a primary campaign into a job in the next White House?<br> • Why does Tom think hiring is the most important work he did, and how did he decide who to bring onto the team?<br> • How do you get people to do things when you don't have formal power over them?<br> • What roles in the US government are most likely to help with the long-term future, or reducing existential risks?<br> • Is it possible, or even desirable, to get the general public interested in abstract, long-term policy ideas?<br> • What are 'policy entrepreneurs' and why do they matter?<br> • What is the role for prizes in promoting science and technology? What are other promising policy ideas?<br> • Why you can get more done by not taking credit.<br> • What can the White House do if an agency isn't doing what it wants?<br> • How can the effective altruism community improve the maturity of our policy recommendations?<br> • How much can talented individuals accomplish during a short-term stay in government?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>You’re 29 years old, and you’ve just been given a job in the White House. How do you quickly figure out how the US Executive Branch behemoth actually works, so that you can have as much impact as possible - before you quit or get kicked out?</p><p>That was the challenge put in front of Tom Kalil in 1993.</p><p>He had enough success to last a full 16 years inside the Clinton and Obama administrations, working to foster the development of the internet, then nanotechnology, and then cutting-edge brain modelling, among other things.</p><p>But not everyone figures out how to move the needle. In today's interview, Tom shares his experience with how to increase your chances of getting an influential role in government, and how to make the most of the opportunity if you get in.</p><p><a href="https://80k.link/tom-kalil"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> <a href="https://80k.link/tom-kalil-coaching"><strong>Interested in US AI policy careers? Apply for one-on-one career advice here.</strong></a></p><p><a href="https://cset.georgetown.edu/careers/"><strong>Vacancies at the Center for Security and Emerging Technology.</strong></a></p><p><a href="https://80k.link/tom-kalil-job-board"><strong>Our high-impact job board, which features other related opportunities.</strong></a></p><p> He believes that Congressional gridlock leads people to greatly underestimate how much the Executive Branch can and does do on its own every day. Decisions by individuals change how billions of dollars are spent; regulations are enforced, and then suddenly they aren't; and a single sentence in the State of the Union can get civil servants to pay attention to a topic that would otherwise go ignored.</p><p> Over years at the White House Office of Science and Technology Policy, 'Team Kalil' built up a white board of principles. For example, 'the schedule is your friend': setting a meeting date with the President can force people to finish something, where they otherwise might procrastinate.</p><p> Or 'talk to who owns the paper'. People would wonder how Tom could get so many lines into the President's speeches. The answer was "figure out who's writing the speech, find them with the document, and tell them to add the line." Obvious, but not something most were doing.</p><p> Not everything is a precise operation though. Tom also tells us the story of NetDay, a project that was put together at the last minute because the President incorrectly believed it was already organised – and decided he was going to announce it in person.</p><p> In today's episode we get down to nuts &amp; bolts, and discuss:<br> • How did Tom spin work on a primary campaign into a job in the next White House?<br> • Why does Tom think hiring is the most important work he did, and how did he decide who to bring onto the team?<br> • How do you get people to do things when you don't have formal power over them?<br> • What roles in the US government are most likely to help with the long-term future, or reducing existential risks?<br> • Is it possible, or even desirable, to get the general public interested in abstract, long-term policy ideas?<br> • What are 'policy entrepreneurs' and why do they matter?<br> • What is the role for prizes in promoting science and technology? What are other promising policy ideas?<br> • Why you can get more done by not taking credit.<br> • What can the White House do if an agency isn't doing what it wants?<br> • How can the effective altruism community improve the maturity of our policy recommendations?<br> • How much can talented individuals accomplish during a short-term stay in government?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 23 Apr 2019 12:51:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/2a8993e3/031bb287.mp3" length="163935508" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/bNZY7IlS5x8oeOQxG0x67OFHqU8m_BtwoAQrMtDFu_M/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Nzcv/MTY4MzU0NDYwMC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10216</itunes:duration>
      <itunes:summary>You’re 29 years old, and you’ve just been given a job in the White House. How do you quickly figure out how the US Executive Branch behemoth actually works, so that you can have as much impact as possible - before you quit or get kicked out?

That was the challenge put in front of Tom Kalil in 1993.

He had enough success to last a full 16 years inside the Clinton and Obama administrations, working to foster the development of the internet, then nanotechnology, and then cutting-edge brain modelling, among other things.

But not everyone figures out how to move the needle. In today's interview, Tom shares his experience with how to increase your chances of getting an influential role in government, and how to make the most of the opportunity if you get in.

Links to learn more, summary and full transcript.
Interested in US AI policy careers? Apply for one-on-one career advice here.
Vacancies at the Center for Security and Emerging Technology.
Our high-impact job board, which features other related opportunities.

He believes that Congressional gridlock leads people to greatly underestimate how much the Executive Branch can and does do on its own every day. Decisions by individuals change how billions of dollars are spent; regulations are enforced, and then suddenly they aren't; and a single sentence in the State of the Union can get civil servants to pay attention to a topic that would otherwise go ignored.

Over years at the White House Office of Science and Technology Policy, 'Team Kalil' built up a white board of principles. For example, 'the schedule is your friend': setting a meeting date with the President can force people to finish something, where they otherwise might procrastinate.

Or 'talk to who owns the paper'. People would wonder how Tom could get so many lines into the President's speeches. The answer was "figure out who's writing the speech, find them with the document, and tell them to add the line." Obvious, but not something most were doing.

Not everything is a precise operation though. Tom also tells us the story of NetDay, a project that was put together at the last minute because the President incorrectly believed it was already organised – and decided he was going to announce it in person.

In today's episode we get down to nuts &amp;amp; bolts, and discuss:

• How did Tom spin work on a primary campaign into a job in the next White House?
• Why does Tom think hiring is the most important work he did, and how did he decide who to bring onto the team?
• How do you get people to do things when you don't have formal power over them?
• What roles in the US government are most likely to help with the long-term future, or reducing existential risks?
• Is it possible, or even desirable, to get the general public interested in abstract, long-term policy ideas?
• What are 'policy entrepreneurs' and why do they matter?
• What is the role for prizes in promoting science and technology? What are other promising policy ideas?
• Why you can get more done by not taking credit.
• What can the White House do if an agency isn't doing what it wants?
• How can the effective altruism community improve the maturity of our policy recommendations?
• How much can talented individuals accomplish during a short-term stay in government?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app..

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>You’re 29 years old, and you’ve just been given a job in the White House. How do you quickly figure out how the US Executive Branch behemoth actually works, so that you can have as much impact as possible - before you quit or get kicked out?

That was t</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/2a8993e3/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#56 - Persis Eskander on wild animal welfare and what, if anything, to do about it</title>
      <itunes:title>#56 - Persis Eskander on wild animal welfare and what, if anything, to do about it</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">499e1a60-5f99-11e9-8ce5-0e280b71e04c</guid>
      <link>https://share.transistor.fm/s/c5d0d3c8</link>
      <description>
        <![CDATA[Elephants in chains at travelling circuses; pregnant pigs trapped in coffin sized crates at factory farms; deers living in the wild. We should welcome the last as a pleasant break from the horror, right?<p>

Maybe, but maybe not. While we tend to have a romanticised view of nature, life in the wild includes a range of extremely negative experiences. </p><p>

Many animals are hunted by predators, and constantly have to remain vigilant about the risk of being killed, and perhaps experiencing the horror of being eaten alive. Resource competition often leads to chronic hunger or starvation. Their diseases and injuries are never treated. In winter animals freeze to death; in droughts they die of heat or thirst. </p><p>

There are fewer than 20 people in the world dedicating their lives to researching these problems.</p><p>

But according to Persis Eskander, researcher at the Open Philanthropy Project, if we sum up the negative experiences of all wild animals, their sheer number could make the scale of the problem larger than most other near-term concerns.</p><p>

<a href="https://80000hours.org/podcast/episodes/persis-eskander-wild-animal-welfare/?utm_campaign=podcast__persis-eskander&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Persis urges us to recognise that nature isn’t inherently good or bad, but rather the result of an amoral evolutionary process. For those that can't survive the brutal indifference of their environment, life is often a series of bad experiences, followed by an even worse death.</p><p>

But should we actually intervene? How do we know what animals are sentient? How often do animals feel hunger, cold, fear, happiness, satisfaction, boredom, and intense agony? Are there long-term technologies that could eventually allow us to massively improve wild animal welfare?</p><p>

For most of these big questions, the answer is: we don’t know. And Persis thinks we're far away from knowing enough to start interfering with ecosystems. But that's all the more reason to start looking at these questions.</p><p> 

There are <em>some</em> concrete steps we could take today, like improving the way wild caught fish are slaughtered. Fish might lack the charisma of a lion or the intelligence of a pig, but if they have the capacity to suffer — and evidence suggests that they do — we should be thinking of ways to kill them painlessly rather than allowing them to suffocate to death over hours.</p><p>

In today’s interview we explore wild animal welfare as a new field of research, and discuss:</p><p>

• Do we have a moral duty towards wild animals or not?<br>
• How should we measure the number of wild animals?<br>
• What are some key activities that generate a lot of suffering or pleasure for wild animals that people might not fully appreciate?<br>
• Is there a danger in imagining how we as humans would feel if we were put into their situation?<br>
• Should we eliminate parasites and predators?<br>
• How important are insects?<br>
• How strongly should we focus on just avoiding humans going in and making things worse?<br>
• How does this compare to work on farmed animal suffering?<br>
• The most compelling arguments for humanity not dedicating resources to wild animal welfare<br>
• Is there much of a case for the idea that this work could improve the very long-term future of humanity?</p><p>

Rob is then joined by two of his colleagues — Niel Bowerman and Michelle Hutchinson — to quickly discuss:</p><p>

• The importance of figuring out your values<br>
• Chemistry, psychology, and other different paths towards working on wild animal welfare<br>
• How to break into new fields</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Elephants in chains at travelling circuses; pregnant pigs trapped in coffin sized crates at factory farms; deers living in the wild. We should welcome the last as a pleasant break from the horror, right?<p>

Maybe, but maybe not. While we tend to have a romanticised view of nature, life in the wild includes a range of extremely negative experiences. </p><p>

Many animals are hunted by predators, and constantly have to remain vigilant about the risk of being killed, and perhaps experiencing the horror of being eaten alive. Resource competition often leads to chronic hunger or starvation. Their diseases and injuries are never treated. In winter animals freeze to death; in droughts they die of heat or thirst. </p><p>

There are fewer than 20 people in the world dedicating their lives to researching these problems.</p><p>

But according to Persis Eskander, researcher at the Open Philanthropy Project, if we sum up the negative experiences of all wild animals, their sheer number could make the scale of the problem larger than most other near-term concerns.</p><p>

<a href="https://80000hours.org/podcast/episodes/persis-eskander-wild-animal-welfare/?utm_campaign=podcast__persis-eskander&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

Persis urges us to recognise that nature isn’t inherently good or bad, but rather the result of an amoral evolutionary process. For those that can't survive the brutal indifference of their environment, life is often a series of bad experiences, followed by an even worse death.</p><p>

But should we actually intervene? How do we know what animals are sentient? How often do animals feel hunger, cold, fear, happiness, satisfaction, boredom, and intense agony? Are there long-term technologies that could eventually allow us to massively improve wild animal welfare?</p><p>

For most of these big questions, the answer is: we don’t know. And Persis thinks we're far away from knowing enough to start interfering with ecosystems. But that's all the more reason to start looking at these questions.</p><p> 

There are <em>some</em> concrete steps we could take today, like improving the way wild caught fish are slaughtered. Fish might lack the charisma of a lion or the intelligence of a pig, but if they have the capacity to suffer — and evidence suggests that they do — we should be thinking of ways to kill them painlessly rather than allowing them to suffocate to death over hours.</p><p>

In today’s interview we explore wild animal welfare as a new field of research, and discuss:</p><p>

• Do we have a moral duty towards wild animals or not?<br>
• How should we measure the number of wild animals?<br>
• What are some key activities that generate a lot of suffering or pleasure for wild animals that people might not fully appreciate?<br>
• Is there a danger in imagining how we as humans would feel if we were put into their situation?<br>
• Should we eliminate parasites and predators?<br>
• How important are insects?<br>
• How strongly should we focus on just avoiding humans going in and making things worse?<br>
• How does this compare to work on farmed animal suffering?<br>
• The most compelling arguments for humanity not dedicating resources to wild animal welfare<br>
• Is there much of a case for the idea that this work could improve the very long-term future of humanity?</p><p>

Rob is then joined by two of his colleagues — Niel Bowerman and Michelle Hutchinson — to quickly discuss:</p><p>

• The importance of figuring out your values<br>
• Chemistry, psychology, and other different paths towards working on wild animal welfare<br>
• How to break into new fields</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 15 Apr 2019 17:22:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c5d0d3c8/7140f69a.mp3" length="172721434" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ZkNhu3WZs5827l5mVzvV8SnfefL59SVerAedkm_biqM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzYv/MTY4MzU0NDU5OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10678</itunes:duration>
      <itunes:summary>Elephants in chains at travelling circuses; pregnant pigs trapped in coffin sized crates at factory farms; deers living in the wild. We should welcome the last as a pleasant break from the horror, right?

Maybe, but maybe not. While we tend to have a romanticised view of nature, life in the wild includes a range of extremely negative experiences. 

Many animals are hunted by predators, and constantly have to remain vigilant about the risk of being killed, and perhaps experiencing the horror of being eaten alive. Resource competition often leads to chronic hunger or starvation. Their diseases and injuries are never treated. In winter animals freeze to death; in droughts they die of heat or thirst. 

There are fewer than 20 people in the world dedicating their lives to researching these problems.

But according to Persis Eskander, researcher at the Open Philanthropy Project, if we sum up the negative experiences of all wild animals, their sheer number could make the scale of the problem larger than most other near-term concerns.

Links to learn more, summary and full transcript.

Persis urges us to recognise that nature isn’t inherently good or bad, but rather the result of an amoral evolutionary process. For those that can't survive the brutal indifference of their environment, life is often a series of bad experiences, followed by an even worse death.

But should we actually intervene? How do we know what animals are sentient? How often do animals feel hunger, cold, fear, happiness, satisfaction, boredom, and intense agony? Are there long-term technologies that could eventually allow us to massively improve wild animal welfare?

For most of these big questions, the answer is: we don’t know. And Persis thinks we're far away from knowing enough to start interfering with ecosystems. But that's all the more reason to start looking at these questions. 

There are some concrete steps we could take today, like improving the way wild caught fish are slaughtered. Fish might lack the charisma of a lion or the intelligence of a pig, but if they have the capacity to suffer — and evidence suggests that they do — we should be thinking of ways to kill them painlessly rather than allowing them to suffocate to death over hours.

In today’s interview we explore wild animal welfare as a new field of research, and discuss:

• Do we have a moral duty towards wild animals or not?
• How should we measure the number of wild animals?
• What are some key activities that generate a lot of suffering or pleasure for wild animals that people might not fully appreciate?
• Is there a danger in imagining how we as humans would feel if we were put into their situation?
• Should we eliminate parasites and predators?
• How important are insects?
• How strongly should we focus on just avoiding humans going in and making things worse?
• How does this compare to work on farmed animal suffering?
• The most compelling arguments for humanity not dedicating resources to wild animal welfare
• Is there much of a case for the idea that this work could improve the very long-term future of humanity?

Rob is then joined by two of his colleagues — Niel Bowerman and Michelle Hutchinson — to quickly discuss:

• The importance of figuring out your values
• Chemistry, psychology, and other different paths towards working on wild animal welfare
• How to break into new fields

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Elephants in chains at travelling circuses; pregnant pigs trapped in coffin sized crates at factory farms; deers living in the wild. We should welcome the last as a pleasant break from the horror, right?

Maybe, but maybe not. While we tend to have a roma</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#55 – Lutter &amp; Winter on founding charter cities with outstanding governance to end poverty</title>
      <itunes:title>#55 – Lutter &amp; Winter on founding charter cities with outstanding governance to end poverty</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">eaa46b80-5382-11e9-82d3-0e682b96a9c0</guid>
      <link>https://80000hours.org/podcast/episodes/lutter-and-winter-chater-cities-innovative-governance/</link>
      <description>
        <![CDATA[<p>Governance matters. Policy change quickly took China from famine to fortune; Singapore from swamps to skyscrapers; and Hong Kong from fishing village to financial centre. Unfortunately, many governments are hard to reform and — to put it mildly — it's not easy to found a new country.</p><p> This has prompted poverty-fighters and political dreamers to look for creative ways to get new and better 'pseudo-countries' off the ground. The poor could then voluntary migrate to in search of security and prosperity. And innovators would be free to experiment with new political and legal systems without having to impose their ideas on existing jurisdictions. </p><p> The 'seasteading movement' imagined founding new self-governing cities on the sea, but obvious challenges have kept that one on the drawing board. Nobel Prize winner and World Bank President Paul Romer suggested 'charter cities', where a host country would volunteer for another country with better legal institutions to effectively govern some of its territory. But that idea too ran aground for political, practical and personal reasons.</p><p> Now Mark Lutter and Tamara Winter, of <em>The Center for Innovative Governance Research</em> (CIGR), are reviving the idea of 'charter cities', with some modifications. Gone is the idea of transferring sovereignty. Instead these cities would look more like the 'special economic zones' that worked miracles for Taiwan and China among others. But rather than keep the rest of the country's rules with a few pieces removed, they hope to start from scratch, opting in to the laws they want to keep, in order to leap forward to "best practices in commercial law."</p><p> <a href="https://80k.link/lutter-and-winter"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> <a href="https://80k.link/rob-on-the-good-life">Rob on <em>The Good Life: Andrew Leigh in Conversation</em> — on 'making the most of your 80,000 hours'.</a></p><p> The project has quickly gotten attention, with Mark and Tamara <a href="https://80k.link/lutter-emergent">receiving funding from Tyler Cowen's Emergent Ventures</a> (discussed in episode 45) and winning a <a href="https://pioneer.app/blog/meet-the-pioneers-take-3/">Pioneer tournament</a>.</p><p> Starting afresh with a new city makes it possible to clear away thousands of harmful rules without having to fight each of the thousands of interest groups that will viciously defend their privileges. Initially the city can fund infrastructure and public services by gradually selling off its land, which appreciates as the city flourishes. And with 40 million people relocating to cities every year, there are plenty of prospective migrants.</p><p> <em>CIGR</em> is fleshing out how these arrangements would work, advocating for them, and developing supporting services that make it easier for any jurisdiction to implement. They're currently in the process of influencing a new prospective satellite city in Zambia.</p><p> Of course, one can raise many criticisms of this idea: Is it likely to be taken up? Is CIGR really doing the right things to make it happen? Will it really reduce poverty if it is?</p><p> We discuss those questions, as well as:</p><p> • How did Mark get a new organisation off the ground, with fundraising and other staff?<br> • What made China's 'special economic zones' so successful?<br> • What are the biggest challenges in getting new cities off the ground?<br> • How did Mark find and hire Tamara? How did he know this was a good idea?<br> • Should people care about this idea if they aren't focussed on tackling poverty?<br> • Why aren't people already doing this?<br> • Why does Tamara support more people starting families?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Governance matters. Policy change quickly took China from famine to fortune; Singapore from swamps to skyscrapers; and Hong Kong from fishing village to financial centre. Unfortunately, many governments are hard to reform and — to put it mildly — it's not easy to found a new country.</p><p> This has prompted poverty-fighters and political dreamers to look for creative ways to get new and better 'pseudo-countries' off the ground. The poor could then voluntary migrate to in search of security and prosperity. And innovators would be free to experiment with new political and legal systems without having to impose their ideas on existing jurisdictions. </p><p> The 'seasteading movement' imagined founding new self-governing cities on the sea, but obvious challenges have kept that one on the drawing board. Nobel Prize winner and World Bank President Paul Romer suggested 'charter cities', where a host country would volunteer for another country with better legal institutions to effectively govern some of its territory. But that idea too ran aground for political, practical and personal reasons.</p><p> Now Mark Lutter and Tamara Winter, of <em>The Center for Innovative Governance Research</em> (CIGR), are reviving the idea of 'charter cities', with some modifications. Gone is the idea of transferring sovereignty. Instead these cities would look more like the 'special economic zones' that worked miracles for Taiwan and China among others. But rather than keep the rest of the country's rules with a few pieces removed, they hope to start from scratch, opting in to the laws they want to keep, in order to leap forward to "best practices in commercial law."</p><p> <a href="https://80k.link/lutter-and-winter"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> <a href="https://80k.link/rob-on-the-good-life">Rob on <em>The Good Life: Andrew Leigh in Conversation</em> — on 'making the most of your 80,000 hours'.</a></p><p> The project has quickly gotten attention, with Mark and Tamara <a href="https://80k.link/lutter-emergent">receiving funding from Tyler Cowen's Emergent Ventures</a> (discussed in episode 45) and winning a <a href="https://pioneer.app/blog/meet-the-pioneers-take-3/">Pioneer tournament</a>.</p><p> Starting afresh with a new city makes it possible to clear away thousands of harmful rules without having to fight each of the thousands of interest groups that will viciously defend their privileges. Initially the city can fund infrastructure and public services by gradually selling off its land, which appreciates as the city flourishes. And with 40 million people relocating to cities every year, there are plenty of prospective migrants.</p><p> <em>CIGR</em> is fleshing out how these arrangements would work, advocating for them, and developing supporting services that make it easier for any jurisdiction to implement. They're currently in the process of influencing a new prospective satellite city in Zambia.</p><p> Of course, one can raise many criticisms of this idea: Is it likely to be taken up? Is CIGR really doing the right things to make it happen? Will it really reduce poverty if it is?</p><p> We discuss those questions, as well as:</p><p> • How did Mark get a new organisation off the ground, with fundraising and other staff?<br> • What made China's 'special economic zones' so successful?<br> • What are the biggest challenges in getting new cities off the ground?<br> • How did Mark find and hire Tamara? How did he know this was a good idea?<br> • Should people care about this idea if they aren't focussed on tackling poverty?<br> • Why aren't people already doing this?<br> • Why does Tamara support more people starting families?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Sun, 31 Mar 2019 18:42:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/de27e5d8/ee1789d6.mp3" length="145674236" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/vZNuRtuhLycWW2D7xz9xD6XWHXCZvXpL6phJvkfcNUI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzUv/MTY4MzU0NDU5OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9074</itunes:duration>
      <itunes:summary>Governance matters. Policy change quickly took China from famine to fortune; Singapore from swamps to skyscrapers; and Hong Kong from fishing village to financial centre. Unfortunately, many governments are hard to reform and — to put it mildly — it's not easy to found a new country.

This has prompted poverty-fighters and political dreamers to look for creative ways to get new and better 'pseudo-countries' off the ground. The poor could then voluntary migrate to in search of security and prosperity. And innovators would be free to experiment with new political and legal systems without having to impose their ideas on existing jurisdictions. 

The 'seasteading movement' imagined founding new self-governing cities on the sea, but obvious challenges have kept that one on the drawing board. Nobel Prize winner and World Bank President Paul Romer suggested 'charter cities', where a host country would volunteer for another country with better legal institutions to effectively govern some of its territory. But that idea too ran aground for political, practical and personal reasons.

Now Mark Lutter and Tamara Winter, of The Center for Innovative Governance Research (CIGR), are reviving the idea of 'charter cities', with some modifications. Gone is the idea of transferring sovereignty. Instead these cities would look more like the 'special economic zones' that worked miracles for Taiwan and China among others. But rather than keep the rest of the country's rules with a few pieces removed, they hope to start from scratch, opting in to the laws they want to keep, in order to leap forward to "best practices in commercial law."

Links to learn more, summary and full transcript.

Rob on The Good Life: Andrew Leigh in Conversation — on 'making the most of your 80,000 hours'.

The project has quickly gotten attention, with Mark and Tamara receiving funding from Tyler Cowen's Emergent Ventures (discussed in episode 45) and winning a Pioneer tournament.

Starting afresh with a new city makes it possible to clear away thousands of harmful rules without having to fight each of the thousands of interest groups that will viciously defend their privileges. Initially the city can fund infrastructure and public services by gradually selling off its land, which appreciates as the city flourishes. And with 40 million people relocating to cities every year, there are plenty of prospective migrants.

CIGR is fleshing out how these arrangements would work, advocating for them, and developing supporting services that make it easier for any jurisdiction to implement. They're currently in the process of influencing a new prospective satellite city in Zambia.

Of course, one can raise many criticisms of this idea: Is it likely to be taken up? Is CIGR really doing the right things to make it happen? Will it really reduce poverty if it is?

We discuss those questions, as well as:

• How did Mark get a new organisation off the ground, with fundraising and other staff?
• What made China's 'special economic zones' so successful?
• What are the biggest challenges in getting new cities off the ground?
• How did Mark find and hire Tamara? How did he know this was a good idea?
• Should people care about this idea if they aren't focussed on tackling poverty?
• Why aren't people already doing this?
• Why does Tamara support more people starting families?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Governance matters. Policy change quickly took China from famine to fortune; Singapore from swamps to skyscrapers; and Hong Kong from fishing village to financial centre. Unfortunately, many governments are hard to reform and — to put it mildly — it's not</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/de27e5d8/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#54 – OpenAI on publication norms, malicious uses of AI, and general-purpose learning algorithms</title>
      <itunes:title>#54 – OpenAI on publication norms, malicious uses of AI, and general-purpose learning algorithms</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1a203c70-4a70-11e9-8b7b-0ec2657f145c</guid>
      <link>https://80000hours.org/podcast/episodes/openai-askell-brundage-clark-latest-in-ai-policy-and-strategy/</link>
      <description>
        <![CDATA[<p>OpenAI’s <a href="https://openai.com/blog/learning-dexterity/">Dactyl</a> is an AI system that can manipulate objects with a human-like robot hand. <a href="https://openai.com/blog/openai-five/">OpenAI Five</a> is an AI system that can defeat humans at the video game Dota 2. The strange thing is they were both developed using the same general-purpose reinforcement learning algorithm.</p><p> How is this possible and what does it show?</p><p> In today's interview Jack Clark, Policy Director at OpenAI, explains that from a computational perspective using a hand and playing Dota 2 are remarkably similar problems.</p><p> A robot hand needs to hold an object, move its fingers, and rotate it to the desired position. In Dota 2 you control a team of several different people, moving them around a map to attack an enemy. </p><p> Your hand has 20 or 30 different joints to move. The number of main actions in Dota 2 is 10 to 20, as you move your characters around a map.</p><p> When you’re rotating an objecting in your hand, you sense its friction, but you don’t directly perceive the entire shape of the object. In Dota 2, you're unable to see the entire map and perceive what's there by moving around – metaphorically 'touching' the space.</p><p> <strong>Read our new in-depth article on becoming an AI policy specialist: </strong><a href="https://80k.link/openai-episode-to-guide"><em>The case for building expertise to work on US AI policy, and how to do it</em></a></p><p> <a href="https://80k.link/openai-episode"><strong>Links to learn more, summary and full transcript</strong></a></p><p> This is true of many apparently distinct problems in life. Compressing different sensory inputs down to a fundamental computational problem which we know how to solve only requires the right general-purpose software.</p><p> The creation of such increasingly 'broad-spectrum' learning algorithms like has been a key story of the last few years, and this development like have unpredictable consequences, heightening the huge challenges that already exist in AI policy.</p><p> Today’s interview is a mega-AI-policy-quad episode; Jack is joined by his colleagues <a href="https://80k.link/openai-to-amanda-episode">Amanda Askell</a> and <a href="https://80k.link/openai-to-miles">Miles Brundage</a>, on the day they released their fascinating and controversial large general language model <a href="https://openai.com/blog/better-language-models/">GPT-2</a>.</p><p> We discuss:</p><p> • What are the most significant changes in the AI policy world over the last year or two?<br> • What capabilities are likely to develop over the next five, 10, 15, 20 years?<br> • How much should we focus on the next couple of years, versus the next couple of decades?<br> • How should we approach possible malicious uses of AI?<br> • What are some of the potential ways OpenAI could make things worse, and how can they be avoided?<br> • Publication norms for AI research<br> • Where do we stand in terms of arms races between countries or different AI labs?<br> • The case for creating newsletters<br> • Should the AI community have a closer relationship to the military?<br> • Working at OpenAI vs. working in the US government<br> • How valuable is Twitter in the AI policy world?</p><p> Rob is then joined by two of his colleagues – Niel Bowerman &amp; Michelle Hutchinson – to quickly discuss:</p><p> • The reaction to OpenAI's release of GPT-2<br> • Jack’s critique of our <a href="https://80k.link/openai-episode-to-guide">US AI policy</a> article<br> • How valuable are roles in government?<br> • Where do you start if you want to write content for a specific audience?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>OpenAI’s <a href="https://openai.com/blog/learning-dexterity/">Dactyl</a> is an AI system that can manipulate objects with a human-like robot hand. <a href="https://openai.com/blog/openai-five/">OpenAI Five</a> is an AI system that can defeat humans at the video game Dota 2. The strange thing is they were both developed using the same general-purpose reinforcement learning algorithm.</p><p> How is this possible and what does it show?</p><p> In today's interview Jack Clark, Policy Director at OpenAI, explains that from a computational perspective using a hand and playing Dota 2 are remarkably similar problems.</p><p> A robot hand needs to hold an object, move its fingers, and rotate it to the desired position. In Dota 2 you control a team of several different people, moving them around a map to attack an enemy. </p><p> Your hand has 20 or 30 different joints to move. The number of main actions in Dota 2 is 10 to 20, as you move your characters around a map.</p><p> When you’re rotating an objecting in your hand, you sense its friction, but you don’t directly perceive the entire shape of the object. In Dota 2, you're unable to see the entire map and perceive what's there by moving around – metaphorically 'touching' the space.</p><p> <strong>Read our new in-depth article on becoming an AI policy specialist: </strong><a href="https://80k.link/openai-episode-to-guide"><em>The case for building expertise to work on US AI policy, and how to do it</em></a></p><p> <a href="https://80k.link/openai-episode"><strong>Links to learn more, summary and full transcript</strong></a></p><p> This is true of many apparently distinct problems in life. Compressing different sensory inputs down to a fundamental computational problem which we know how to solve only requires the right general-purpose software.</p><p> The creation of such increasingly 'broad-spectrum' learning algorithms like has been a key story of the last few years, and this development like have unpredictable consequences, heightening the huge challenges that already exist in AI policy.</p><p> Today’s interview is a mega-AI-policy-quad episode; Jack is joined by his colleagues <a href="https://80k.link/openai-to-amanda-episode">Amanda Askell</a> and <a href="https://80k.link/openai-to-miles">Miles Brundage</a>, on the day they released their fascinating and controversial large general language model <a href="https://openai.com/blog/better-language-models/">GPT-2</a>.</p><p> We discuss:</p><p> • What are the most significant changes in the AI policy world over the last year or two?<br> • What capabilities are likely to develop over the next five, 10, 15, 20 years?<br> • How much should we focus on the next couple of years, versus the next couple of decades?<br> • How should we approach possible malicious uses of AI?<br> • What are some of the potential ways OpenAI could make things worse, and how can they be avoided?<br> • Publication norms for AI research<br> • Where do we stand in terms of arms races between countries or different AI labs?<br> • The case for creating newsletters<br> • Should the AI community have a closer relationship to the military?<br> • Working at OpenAI vs. working in the US government<br> • How valuable is Twitter in the AI policy world?</p><p> Rob is then joined by two of his colleagues – Niel Bowerman &amp; Michelle Hutchinson – to quickly discuss:</p><p> • The reaction to OpenAI's release of GPT-2<br> • Jack’s critique of our <a href="https://80k.link/openai-episode-to-guide">US AI policy</a> article<br> • How valuable are roles in government?<br> • Where do you start if you want to write content for a specific audience?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 19 Mar 2019 21:32:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/dfbe0ca7/975e24b2.mp3" length="167082194" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/8YC7v3Ja340uhD92GAPBxm45RAOmlBP0CBgC-sJvlxw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzQv/MTY4MzU0NDU5Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10420</itunes:duration>
      <itunes:summary>OpenAI’s Dactyl is an AI system that can manipulate objects with a human-like robot hand. OpenAI Five is an AI system that can defeat humans at the video game Dota 2. The strange thing is they were both developed using the same general-purpose reinforcement learning algorithm.

How is this possible and what does it show?

In today's interview Jack Clark, Policy Director at OpenAI, explains that from a computational perspective using a hand and playing Dota 2 are remarkably similar problems.

A robot hand needs to hold an object, move its fingers, and rotate it to the desired position. In Dota 2 you control a team of several different people, moving them around a map to attack an enemy. 

Your hand has 20 or 30 different joints to move. The number of main actions in Dota 2 is 10 to 20, as you move your characters around a map.

When you’re rotating an objecting in your hand, you sense its friction, but you don’t directly perceive the entire shape of the object. In Dota 2, you're unable to see the entire map and perceive what's there by moving around – metaphorically 'touching' the space.

Read our new in-depth article on becoming an AI policy specialist: The case for building expertise to work on US AI policy, and how to do it 

Links to learn more, summary and full transcript

This is true of many apparently distinct problems in life. Compressing different sensory inputs down to a fundamental computational problem which we know how to solve only requires the right general-purpose software.

The creation of such increasingly 'broad-spectrum' learning algorithms like has been a key story of the last few years, and this development like have unpredictable consequences, heightening the huge challenges that already exist in AI policy.

Today’s interview is a mega-AI-policy-quad episode; Jack is joined by his colleagues Amanda Askell and Miles Brundage, on the day they released their fascinating and controversial large general language model GPT-2.

We discuss:

• What are the most significant changes in the AI policy world over the last year or two?
• What capabilities are likely to develop over the next five, 10, 15, 20 years?
• How much should we focus on the next couple of years, versus the next couple of decades?
• How should we approach possible malicious uses of AI?
• What are some of the potential ways OpenAI could make things worse, and how can they be avoided?
• Publication norms for AI research
• Where do we stand in terms of arms races between countries or different AI labs?
• The case for creating newsletters
• Should the AI community have a closer relationship to the military?
• Working at OpenAI vs. working in the US government
• How valuable is Twitter in the AI policy world?

Rob is then joined by two of his colleagues – Niel Bowerman &amp;amp; Michelle Hutchinson – to quickly discuss:

• The reaction to OpenAI's release of GPT-2
• Jack’s critique of our US AI policy article
• How valuable are roles in government?
• Where do you start if you want to write content for a specific audience?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.


The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>OpenAI’s Dactyl is an AI system that can manipulate objects with a human-like robot hand. OpenAI Five is an AI system that can defeat humans at the video game Dota 2. The strange thing is they were both developed using the same general-purpose reinforceme</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:transcript url="https://share.transistor.fm/s/dfbe0ca7/transcript.txt" type="text/plain"/>
    </item>
    <item>
      <title>#53 - Kelsey Piper on the room for important advocacy within journalism</title>
      <itunes:title>#53 - Kelsey Piper on the room for important advocacy within journalism</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">1e0ca428-3a48-11e9-8633-0eccce65e6e8</guid>
      <link>https://share.transistor.fm/s/98558f81</link>
      <description>
        <![CDATA[<i>“Politics. Business. Opinion. Science. Sports. Animal welfare. Existential risk.”</i> Is this a plausible future lineup for major news outlets?<p>

Funded by the Rockefeller Foundation and given very little editorial direction, Vox's <i><a href="https://www.vox.com/future-perfect">Future Perfect</a></i> aspires to be more or less that.</p><p>

Competition in the news business creates pressure to write quick pieces on topical political issues that can drive lots of clicks with just a few hours' work.</p><p>

But according to Kelsey Piper, staff writer for this new section of Vox's website focused on effective altruist themes, <i>Future Perfect's</i> goal is to run in the opposite direction and make room for more substantive coverage that's not tied to the news cycle.</p><p>

They hope that in the long-term talented writers from other outlets across the political spectrum can also be attracted to tackle these topics.</p><p>

<a href="https://80k.link/kelsey-piper"><b>Links to learn more, summary and full transcript.</b></a></p><p>

<a href="https://80k.link/kelsey-piper-links"><b>Links to Kelsey's top articles.</b></a></p><p>

Some skeptics of the project have questioned whether this general coverage of global catastrophic risks actually helps reduce them.</p><p>

Kelsey responds: if you decide to dedicate your life to AI safety research, what’s the likely reaction from your family and friends? Do they think of you as someone about to join <i>"that weird Silicon Valley apocalypse thing"</i>? Or do they, having read about the issues widely, simply think <i>“Oh, yeah. That seems important. I'm glad you're working on it.”</i></p><p>

Kelsey believes that really matters, and is determined by broader coverage of these kinds of topics.</p><p>

If that's right, is journalism a plausible pathway for doing the most good with your career, or did Kelsey just get particularly lucky? After all, journalism is a shrinking industry without an obvious revenue model to fund many writers looking into the world's most pressing problems.</p><p>

Kelsey points out that one needn't take the risk of committing to journalism at an early age. Instead listeners can specialise in an important topic, while leaving open the option of switching into specialist journalism later on, should a great opportunity happen to present itself.</p><p>

In today’s episode we discuss that path, as well as:</p><p>

• What’s the day to day life of a Vox journalist like?<br>
• How can good journalism get funded?<br>
• Are there meaningful tradeoffs between doing what's in the interest of Vox and doing what’s good?<br>
• How concerned should we be about the risk of effective altruism being perceived as partisan?<br>
• How well can short articles effectively communicate complicated ideas?<br>
• Are there alternative business models that could fund high quality journalism on a larger scale?<br>
• How do you approach the case for taking AI seriously to a broader audience?<br>
• How valuable might it be for media outlets to do Tetlock-style forecasting?<br>
• Is it really a good idea to heavily tax billionaires?<br>
• How do you avoid the pressure to get clicks?<br>
• How possible is it to predict which articles are going to be popular?<br>
• How did Kelsey build the skills necessary to work at Vox?<br>
• General lessons for people dealing with very difficult life circumstances</p><p>

Rob is then joined by two of his colleagues – Keiran Harris &amp; Michelle Hutchinson – to quickly discuss:</p><p>

• The risk political polarisation poses to long-termist causes<br>
• How should specialists keep journalism available as a career option?<br>
• Should we create a news aggregator that aims to make someone as well informed as possible in big-picture terms?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<i>“Politics. Business. Opinion. Science. Sports. Animal welfare. Existential risk.”</i> Is this a plausible future lineup for major news outlets?<p>

Funded by the Rockefeller Foundation and given very little editorial direction, Vox's <i><a href="https://www.vox.com/future-perfect">Future Perfect</a></i> aspires to be more or less that.</p><p>

Competition in the news business creates pressure to write quick pieces on topical political issues that can drive lots of clicks with just a few hours' work.</p><p>

But according to Kelsey Piper, staff writer for this new section of Vox's website focused on effective altruist themes, <i>Future Perfect's</i> goal is to run in the opposite direction and make room for more substantive coverage that's not tied to the news cycle.</p><p>

They hope that in the long-term talented writers from other outlets across the political spectrum can also be attracted to tackle these topics.</p><p>

<a href="https://80k.link/kelsey-piper"><b>Links to learn more, summary and full transcript.</b></a></p><p>

<a href="https://80k.link/kelsey-piper-links"><b>Links to Kelsey's top articles.</b></a></p><p>

Some skeptics of the project have questioned whether this general coverage of global catastrophic risks actually helps reduce them.</p><p>

Kelsey responds: if you decide to dedicate your life to AI safety research, what’s the likely reaction from your family and friends? Do they think of you as someone about to join <i>"that weird Silicon Valley apocalypse thing"</i>? Or do they, having read about the issues widely, simply think <i>“Oh, yeah. That seems important. I'm glad you're working on it.”</i></p><p>

Kelsey believes that really matters, and is determined by broader coverage of these kinds of topics.</p><p>

If that's right, is journalism a plausible pathway for doing the most good with your career, or did Kelsey just get particularly lucky? After all, journalism is a shrinking industry without an obvious revenue model to fund many writers looking into the world's most pressing problems.</p><p>

Kelsey points out that one needn't take the risk of committing to journalism at an early age. Instead listeners can specialise in an important topic, while leaving open the option of switching into specialist journalism later on, should a great opportunity happen to present itself.</p><p>

In today’s episode we discuss that path, as well as:</p><p>

• What’s the day to day life of a Vox journalist like?<br>
• How can good journalism get funded?<br>
• Are there meaningful tradeoffs between doing what's in the interest of Vox and doing what’s good?<br>
• How concerned should we be about the risk of effective altruism being perceived as partisan?<br>
• How well can short articles effectively communicate complicated ideas?<br>
• Are there alternative business models that could fund high quality journalism on a larger scale?<br>
• How do you approach the case for taking AI seriously to a broader audience?<br>
• How valuable might it be for media outlets to do Tetlock-style forecasting?<br>
• Is it really a good idea to heavily tax billionaires?<br>
• How do you avoid the pressure to get clicks?<br>
• How possible is it to predict which articles are going to be popular?<br>
• How did Kelsey build the skills necessary to work at Vox?<br>
• General lessons for people dealing with very difficult life circumstances</p><p>

Rob is then joined by two of his colleagues – Keiran Harris &amp; Michelle Hutchinson – to quickly discuss:</p><p>

• The risk political polarisation poses to long-termist causes<br>
• How should specialists keep journalism available as a career option?<br>
• Should we create a news aggregator that aims to make someone as well informed as possible in big-picture terms?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 27 Feb 2019 04:28:23 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/98558f81/a1d7431e.mp3" length="149035048" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/PG28iBb6sPhr9oL3cDfPDnQmk7MY-OOIGVeU-_sbrcI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzMv/MTY4MzU0NDU5Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9271</itunes:duration>
      <itunes:summary>“Politics. Business. Opinion. Science. Sports. Animal welfare. Existential risk.” Is this a plausible future lineup for major news outlets?

Funded by the Rockefeller Foundation and given very little editorial direction, Vox's Future Perfect aspires to be more or less that.

Competition in the news business creates pressure to write quick pieces on topical political issues that can drive lots of clicks with just a few hours' work.

But according to Kelsey Piper, staff writer for this new section of Vox's website focused on effective altruist themes, Future Perfect's goal is to run in the opposite direction and make room for more substantive coverage that's not tied to the news cycle.

They hope that in the long-term talented writers from other outlets across the political spectrum can also be attracted to tackle these topics.

Links to learn more, summary and full transcript.

Links to Kelsey's top articles.

Some skeptics of the project have questioned whether this general coverage of global catastrophic risks actually helps reduce them.

Kelsey responds: if you decide to dedicate your life to AI safety research, what’s the likely reaction from your family and friends? Do they think of you as someone about to join "that weird Silicon Valley apocalypse thing"? Or do they, having read about the issues widely, simply think “Oh, yeah. That seems important. I'm glad you're working on it.”

Kelsey believes that really matters, and is determined by broader coverage of these kinds of topics.

If that's right, is journalism a plausible pathway for doing the most good with your career, or did Kelsey just get particularly lucky? After all, journalism is a shrinking industry without an obvious revenue model to fund many writers looking into the world's most pressing problems.

Kelsey points out that one needn't take the risk of committing to journalism at an early age. Instead listeners can specialise in an important topic, while leaving open the option of switching into specialist journalism later on, should a great opportunity happen to present itself.

In today’s episode we discuss that path, as well as:

• What’s the day to day life of a Vox journalist like?
• How can good journalism get funded?
• Are there meaningful tradeoffs between doing what's in the interest of Vox and doing what’s good?
• How concerned should we be about the risk of effective altruism being perceived as partisan?
• How well can short articles effectively communicate complicated ideas?
• Are there alternative business models that could fund high quality journalism on a larger scale?
• How do you approach the case for taking AI seriously to a broader audience?
• How valuable might it be for media outlets to do Tetlock-style forecasting?
• Is it really a good idea to heavily tax billionaires?
• How do you avoid the pressure to get clicks?
• How possible is it to predict which articles are going to be popular?
• How did Kelsey build the skills necessary to work at Vox?
• General lessons for people dealing with very difficult life circumstances

Rob is then joined by two of his colleagues – Keiran Harris &amp;amp; Michelle Hutchinson – to quickly discuss:

• The risk political polarisation poses to long-termist causes
• How should specialists keep journalism available as a career option?
• Should we create a news aggregator that aims to make someone as well informed as possible in big-picture terms?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>“Politics. Business. Opinion. Science. Sports. Animal welfare. Existential risk.” Is this a plausible future lineup for major news outlets?

Funded by the Rockefeller Foundation and given very little editorial direction, Vox's Future Perfect aspires to be</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>Julia Galef and Rob Wiblin on an updated view of the best ways to help humanity</title>
      <itunes:title>Julia Galef and Rob Wiblin on an updated view of the best ways to help humanity</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">9cf25ebe-3236-11e9-8b8a-0ee37b0b018a</guid>
      <link>https://share.transistor.fm/s/153686ee</link>
      <description>
        <![CDATA[<p>This is a cross-post of an interview Rob did with Julia Galef on her podcast <a href="http://rationallyspeakingpodcast.org/"><b>Rationally Speaking</b></a>. Rob and Julia discuss how the career advice 80,000 Hours gives has changed over the years, and the biggest misconceptions about our views.</p><p>

The topics will be familiar to the most fervent fans of this show — but we think that if you’ve listened to less than about half of the episodes we've released so far, you’ll find something new to enjoy here. Julia may be familiar to you as the guest on episode 7 of the show, way back in September 2017.</p><p>

The conversation also covers topics like:</p><p>

• How many people should try to get a job in finance and donate their income?<br>
• The case for working to reduce global catastrophic risks in targeted ways, and historical precedents for this kind of work<br>
• Why reducing risk is a better way to help the future than increasing economic growth<br>
• What percentage of the world should ideally follow 80,000 Hours advice?</p><p>

<a href="https://80k.link/julia-galef-rationally-speaking"><b>Links to learn more, summary and full transcript.</b></a></p><p>

If you’re interested in the cooling and expansion of the universe, which comes up on the show, you should definitely check out our <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__julia-galef-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>29th episode with Dr Anders Sandberg.</b></a></p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into any podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>This is a cross-post of an interview Rob did with Julia Galef on her podcast <a href="http://rationallyspeakingpodcast.org/"><b>Rationally Speaking</b></a>. Rob and Julia discuss how the career advice 80,000 Hours gives has changed over the years, and the biggest misconceptions about our views.</p><p>

The topics will be familiar to the most fervent fans of this show — but we think that if you’ve listened to less than about half of the episodes we've released so far, you’ll find something new to enjoy here. Julia may be familiar to you as the guest on episode 7 of the show, way back in September 2017.</p><p>

The conversation also covers topics like:</p><p>

• How many people should try to get a job in finance and donate their income?<br>
• The case for working to reduce global catastrophic risks in targeted ways, and historical precedents for this kind of work<br>
• Why reducing risk is a better way to help the future than increasing economic growth<br>
• What percentage of the world should ideally follow 80,000 Hours advice?</p><p>

<a href="https://80k.link/julia-galef-rationally-speaking"><b>Links to learn more, summary and full transcript.</b></a></p><p>

If you’re interested in the cooling and expansion of the universe, which comes up on the show, you should definitely check out our <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__julia-galef-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>29th episode with Dr Anders Sandberg.</b></a></p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into any podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Sun, 17 Feb 2019 01:00:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/153686ee/53400512.mp3" length="54794123" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Dw6_CZhZtXv5wsz6imlj-EONbCJX4BoqGVDq66bPMKc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzIv/MTY4MzU0NDU5NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3406</itunes:duration>
      <itunes:summary>This is a cross-post of an interview Rob did with Julia Galef on her podcast Rationally Speaking. Rob and Julia discuss how the career advice 80,000 Hours gives has changed over the years, and the biggest misconceptions about our views.

The topics will be familiar to the most fervent fans of this show — but we think that if you’ve listened to less than about half of the episodes we've released so far, you’ll find something new to enjoy here. Julia may be familiar to you as the guest on episode 7 of the show, way back in September 2017.

The conversation also covers topics like:

• How many people should try to get a job in finance and donate their income?
• The case for working to reduce global catastrophic risks in targeted ways, and historical precedents for this kind of work
• Why reducing risk is a better way to help the future than increasing economic growth
• What percentage of the world should ideally follow 80,000 Hours advice?

Links to learn more, summary and full transcript.

If you’re interested in the cooling and expansion of the universe, which comes up on the show, you should definitely check out our 29th episode with Dr Anders Sandberg.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into any podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>This is a cross-post of an interview Rob did with Julia Galef on her podcast Rationally Speaking. Rob and Julia discuss how the career advice 80,000 Hours gives has changed over the years, and the biggest misconceptions about our views.

The topics will b</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#52 - Glen Weyl on uprooting capitalism and democracy for a just society</title>
      <itunes:title>#52 - Glen Weyl on uprooting capitalism and democracy for a just society</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">6892db34-2b34-11e9-bee7-0ece7a7d2472</guid>
      <link>https://share.transistor.fm/s/007ee33d</link>
      <description>
        <![CDATA[<p>Pro-market economists love to wax rhapsodic about the capacity of markets to pull together the valuable local information spread across all of society about what people want and how to make it.</p><p> But when it comes to politics and voting - which also aim to aggregate the preferences and knowledge found in millions of individuals - the enthusiasm for finding clever institutional designs often turns to skepticism.</p><p> Today's guest, freewheeling economist Glen Weyl, won't have it, and is on a warpath to reform liberal democratic institutions in order to save them. Just last year he wrote <em>Radical Markets: Uprooting Capitalism and Democracy for a Just Society</em> with Eric Posner, but has already moved on, saying <em>"in the 6 months since the book came out I've made more intellectual progress than in the whole 10 years before that."</em></p><p> Weyl believes we desperately need more efficient, equitable and decentralised ways to organise society, that take advantage of what each person knows, and his research agenda has already been making breakthroughs.</p><p> <a href="https://80k.link/glen-weyl-episode"><strong>Links to learn more, summary and full transcript</strong></a></p><p> <a href="https://80k.link/glen-weyl-job-board"><strong>Our high impact job board</strong></a></p><p> <a href="https://80k.link/glen-weyl-newsletter"><strong>Join our newsletter</strong></a></p><p> Despite a history in the best economics departments in the world - Harvard, Princeton, Yale and the University of Chicago - he is too worried for the future to sit in his office writing papers. Instead he has left the academy to try to inspire a social movement, <a href="https://80k.link/glen-weyl-radicalxchange"><strong>RadicalxChange</strong></a>, with a vision of social reform as expansive as his own.</p><p> <a href="https://80k.link/glen-weyl-radicalxchange"><strong>You can sign up for their conference in Detroit in March here</strong></a></p><p> Economist Alex Tabarrok called his latest proposal, known as 'liberal radicalism', <em>"a quantum leap in public-goods mechanism-design"</em> - we explain how it works in the show. But the proposal, however good in theory, might struggle in the real world because it requires large subsidies, and compensates for people's selfishness so effectively that it might even be an overcorrection.</p><p> An earlier mechanism - 'quadratic voting' (QV) - would allow people to express the relative strength of their preferences in the democratic process. No longer would 51 people who support a proposal, but barely care about the issue, outvote 49 incredibly passionate opponents, predictably making society worse in the process. We explain exactly how in the episode.</p><p> Weyl points to studies showing that people are more likely to vote strongly not only about issues they *care* more about, but issues they *know* more about. He expects that allowing people to specialise and indicate when they know what they're talking about will create a democracy that does more to aggregate careful judgement, rather than just passionate ignorance.</p><p> But these and indeed all of Weyl's ideas have faced criticism. Some say the risk of unintended consequences is too great, or that they solve the wrong problem. Others see these proposals as unproven, impractical, or just another example of an intellectual engaged in grand social planning. I raise these concerns to see how he responds.</p><p> As big a topic as all of that is, this extended conversation also goes into the blockchain, problems with the effective altruism community and how auctions could replace private property. Don't miss it.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Pro-market economists love to wax rhapsodic about the capacity of markets to pull together the valuable local information spread across all of society about what people want and how to make it.</p><p> But when it comes to politics and voting - which also aim to aggregate the preferences and knowledge found in millions of individuals - the enthusiasm for finding clever institutional designs often turns to skepticism.</p><p> Today's guest, freewheeling economist Glen Weyl, won't have it, and is on a warpath to reform liberal democratic institutions in order to save them. Just last year he wrote <em>Radical Markets: Uprooting Capitalism and Democracy for a Just Society</em> with Eric Posner, but has already moved on, saying <em>"in the 6 months since the book came out I've made more intellectual progress than in the whole 10 years before that."</em></p><p> Weyl believes we desperately need more efficient, equitable and decentralised ways to organise society, that take advantage of what each person knows, and his research agenda has already been making breakthroughs.</p><p> <a href="https://80k.link/glen-weyl-episode"><strong>Links to learn more, summary and full transcript</strong></a></p><p> <a href="https://80k.link/glen-weyl-job-board"><strong>Our high impact job board</strong></a></p><p> <a href="https://80k.link/glen-weyl-newsletter"><strong>Join our newsletter</strong></a></p><p> Despite a history in the best economics departments in the world - Harvard, Princeton, Yale and the University of Chicago - he is too worried for the future to sit in his office writing papers. Instead he has left the academy to try to inspire a social movement, <a href="https://80k.link/glen-weyl-radicalxchange"><strong>RadicalxChange</strong></a>, with a vision of social reform as expansive as his own.</p><p> <a href="https://80k.link/glen-weyl-radicalxchange"><strong>You can sign up for their conference in Detroit in March here</strong></a></p><p> Economist Alex Tabarrok called his latest proposal, known as 'liberal radicalism', <em>"a quantum leap in public-goods mechanism-design"</em> - we explain how it works in the show. But the proposal, however good in theory, might struggle in the real world because it requires large subsidies, and compensates for people's selfishness so effectively that it might even be an overcorrection.</p><p> An earlier mechanism - 'quadratic voting' (QV) - would allow people to express the relative strength of their preferences in the democratic process. No longer would 51 people who support a proposal, but barely care about the issue, outvote 49 incredibly passionate opponents, predictably making society worse in the process. We explain exactly how in the episode.</p><p> Weyl points to studies showing that people are more likely to vote strongly not only about issues they *care* more about, but issues they *know* more about. He expects that allowing people to specialise and indicate when they know what they're talking about will create a democracy that does more to aggregate careful judgement, rather than just passionate ignorance.</p><p> But these and indeed all of Weyl's ideas have faced criticism. Some say the risk of unintended consequences is too great, or that they solve the wrong problem. Others see these proposals as unproven, impractical, or just another example of an intellectual engaged in grand social planning. I raise these concerns to see how he responds.</p><p> As big a topic as all of that is, this extended conversation also goes into the blockchain, problems with the effective altruism community and how auctions could replace private property. Don't miss it.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 08 Feb 2019 02:00:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/007ee33d/0ba4a971.mp3" length="158240681" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/k8A7w7wWwQhFKyhpHjPO6L-y6c_HGagylIL7MFT84Ic/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzEv/MTY4MzU0NDU5NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9867</itunes:duration>
      <itunes:summary>Pro-market economists love to wax rhapsodic about the capacity of markets to pull together the valuable local information spread across all of society about what people want and how to make it.

But when it comes to politics and voting - which also aim to aggregate the preferences and knowledge found in millions of individuals - the enthusiasm for finding clever institutional designs often turns to skepticism.

Today's guest, freewheeling economist Glen Weyl, won't have it, and is on a warpath to reform liberal democratic institutions in order to save them. Just last year he wrote Radical Markets: Uprooting Capitalism and Democracy for a Just Society with Eric Posner, but has already moved on, saying "in the 6 months since the book came out I've made more intellectual progress than in the whole 10 years before that."

Weyl believes we desperately need more efficient, equitable and decentralised ways to organise society, that take advantage of what each person knows, and his research agenda has already been making breakthroughs.

Links to learn more, summary and full transcript

Our high impact job board

Join our newsletter

Despite a history in the best economics departments in the world - Harvard, Princeton, Yale and the University of Chicago - he is too worried for the future to sit in his office writing papers. Instead he has left the academy to try to inspire a social movement, RadicalxChange, with a vision of social reform as expansive as his own. 

You can sign up for their conference in Detroit in March here

Economist Alex Tabarrok called his latest proposal, known as 'liberal radicalism', "a quantum leap in public-goods mechanism-design" - we explain how it works in the show. But the proposal, however good in theory, might struggle in the real world because it requires large subsidies, and compensates for people's selfishness so effectively that it might even be an overcorrection.

An earlier mechanism - 'quadratic voting' (QV) - would allow people to express the relative strength of their preferences in the democratic process. No longer would 51 people who support a proposal, but barely care about the issue, outvote 49 incredibly passionate opponents, predictably making society worse in the process. We explain exactly how in the episode.

Weyl points to studies showing that people are more likely to vote strongly not only about issues they *care* more about, but issues they *know* more about. He expects that allowing people to specialise and indicate when they know what they're talking about will create a democracy that does more to aggregate careful judgement, rather than just passionate ignorance.

But these and indeed all of Weyl's ideas have faced criticism. Some say the risk of unintended consequences is too great, or that they solve the wrong problem. Others see these proposals as unproven, impractical, or just another example of an intellectual engaged in grand social planning. I raise these concerns to see how he responds.

As big a topic as all of that is, this extended conversation also goes into the blockchain, problems with the effective altruism community and how auctions could replace private property. Don't miss it.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Pro-market economists love to wax rhapsodic about the capacity of markets to pull together the valuable local information spread across all of society about what people want and how to make it.

But when it comes to politics and voting - which also aim </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#51 - Martin Gurri on the revolt of the public &amp; crisis of authority in the information age</title>
      <itunes:title>#51 - Martin Gurri on the revolt of the public &amp; crisis of authority in the information age</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/566341518</guid>
      <link>https://share.transistor.fm/s/13fa5fda</link>
      <description>
        <![CDATA[Politics in rich countries seems to be going nuts. What's the explanation? Rising inequality? The decline of manufacturing jobs? Excessive immigration?<p>

Martin Gurri spent decades as a CIA analyst and in his 2014 book <a href="https://www.amazon.com/Revolt-Public-Crisis-Authority-Millennium/dp/1732265143"><em>The Revolt of The Public and Crisis of Authority in the New Millennium</em></a>, predicted political turbulence for an entirely different reason: new communication technologies were flipping the balance of power between the public and traditional authorities.</p><p>

In 1959 the President could control the narrative by leaning on his friends at four TV stations, who felt it was proper to present the nation's leader in a positive light, no matter their flaws. Today, it's impossible to prevent someone from broadcasting any grievance online, whether it's a contrarian insight or an insane conspiracy theory.</p><p>

<a href="https://80000hours.org/podcast/episodes/martin-gurri-revolt-of-the-public/?utm_campaign=podcast__martin-gurri&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

According to Gurri, trust in society's institutions - police, journalists, scientists and more - has been undermined by constant criticism from outsiders, and exposed to a cacophony of conflicting opinions on every issue, the public takes fewer truths for granted. We are now free to see our leaders as the flawed human beings they always have been, and are not amused.</p><p>

Suspicious they are being betrayed by elites, the public can also use technology to coordinate spontaneously and express its anger. Keen to 'throw the bastards out' protesters take to the streets, united by what they don't like, but without a shared agenda or the institutional infrastructure to figure out how to fix things. Some popular movements have come to view any attempt to exercise power over others as suspect. </p><p>

If Gurri is to be believed, protest movements in Egypt, Spain, Greece and Israel in 2011 followed this script, while Brexit, Trump and the French yellow vests movement subsequently vindicated his theory.</p><p>

In this model, politics won't return to its old equilibrium any time soon. The leaders of tomorrow will need a new message and style if they hope to maintain any legitimacy in this less hierarchical world. Otherwise, we're in for decades of grinding conflict between traditional centres of authority and the general public, who doubt both their loyalty and competence. </p><p>

But how much should we believe this theory? Why do Canada and Australia remain pools of calm in the storm? Aren't some malcontents quite concrete in their demands? And are protest movements actually more common (or more nihilistic) than they were decades ago?</p><p>

In today's episode we ask these questions and add an hour-long discussion with two of Rob's colleagues - Keiran Harris and Michelle Hutchinson - to further explore the ideas in the book.</p><p>

The conversation covers:</p><p>

* How do we know that the internet is driving this rather than some other phenomenon?<br>
* How do technological changes enable social and political change?<br>
* The historical role of television<br>
* Are people also more disillusioned now with sports heroes and actors?<br>
* Which countries are finding good ways to make politics work in this new era?<br>
* What are the implications for the threat of totalitarianism?<br>
* What is this is going to do to international relations? Will it make it harder for countries to cooperate and avoid conflict?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Politics in rich countries seems to be going nuts. What's the explanation? Rising inequality? The decline of manufacturing jobs? Excessive immigration?<p>

Martin Gurri spent decades as a CIA analyst and in his 2014 book <a href="https://www.amazon.com/Revolt-Public-Crisis-Authority-Millennium/dp/1732265143"><em>The Revolt of The Public and Crisis of Authority in the New Millennium</em></a>, predicted political turbulence for an entirely different reason: new communication technologies were flipping the balance of power between the public and traditional authorities.</p><p>

In 1959 the President could control the narrative by leaning on his friends at four TV stations, who felt it was proper to present the nation's leader in a positive light, no matter their flaws. Today, it's impossible to prevent someone from broadcasting any grievance online, whether it's a contrarian insight or an insane conspiracy theory.</p><p>

<a href="https://80000hours.org/podcast/episodes/martin-gurri-revolt-of-the-public/?utm_campaign=podcast__martin-gurri&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

According to Gurri, trust in society's institutions - police, journalists, scientists and more - has been undermined by constant criticism from outsiders, and exposed to a cacophony of conflicting opinions on every issue, the public takes fewer truths for granted. We are now free to see our leaders as the flawed human beings they always have been, and are not amused.</p><p>

Suspicious they are being betrayed by elites, the public can also use technology to coordinate spontaneously and express its anger. Keen to 'throw the bastards out' protesters take to the streets, united by what they don't like, but without a shared agenda or the institutional infrastructure to figure out how to fix things. Some popular movements have come to view any attempt to exercise power over others as suspect. </p><p>

If Gurri is to be believed, protest movements in Egypt, Spain, Greece and Israel in 2011 followed this script, while Brexit, Trump and the French yellow vests movement subsequently vindicated his theory.</p><p>

In this model, politics won't return to its old equilibrium any time soon. The leaders of tomorrow will need a new message and style if they hope to maintain any legitimacy in this less hierarchical world. Otherwise, we're in for decades of grinding conflict between traditional centres of authority and the general public, who doubt both their loyalty and competence. </p><p>

But how much should we believe this theory? Why do Canada and Australia remain pools of calm in the storm? Aren't some malcontents quite concrete in their demands? And are protest movements actually more common (or more nihilistic) than they were decades ago?</p><p>

In today's episode we ask these questions and add an hour-long discussion with two of Rob's colleagues - Keiran Harris and Michelle Hutchinson - to further explore the ideas in the book.</p><p>

The conversation covers:</p><p>

* How do we know that the internet is driving this rather than some other phenomenon?<br>
* How do technological changes enable social and political change?<br>
* The historical role of television<br>
* Are people also more disillusioned now with sports heroes and actors?<br>
* Which countries are finding good ways to make politics work in this new era?<br>
* What are the implications for the threat of totalitarianism?<br>
* What is this is going to do to international relations? Will it make it harder for countries to cooperate and avoid conflict?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 29 Jan 2019 00:48:19 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/13fa5fda/920703f1.mp3" length="145299736" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/KCn1-WYG-IttvIGGK8UsMKraZgIJ6oZxR4fGWynmN2s/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NzAv/MTY4MzU0NDU5My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9071</itunes:duration>
      <itunes:summary>Politics in rich countries seems to be going nuts. What's the explanation? Rising inequality? The decline of manufacturing jobs? Excessive immigration?

Martin Gurri spent decades as a CIA analyst and in his 2014 book The Revolt of The Public and Crisis of Authority in the New Millennium, predicted political turbulence for an entirely different reason: new communication technologies were flipping the balance of power between the public and traditional authorities.

In 1959 the President could control the narrative by leaning on his friends at four TV stations, who felt it was proper to present the nation's leader in a positive light, no matter their flaws. Today, it's impossible to prevent someone from broadcasting any grievance online, whether it's a contrarian insight or an insane conspiracy theory.

Links to learn more, summary and full transcript.

According to Gurri, trust in society's institutions - police, journalists, scientists and more - has been undermined by constant criticism from outsiders, and exposed to a cacophony of conflicting opinions on every issue, the public takes fewer truths for granted. We are now free to see our leaders as the flawed human beings they always have been, and are not amused.

Suspicious they are being betrayed by elites, the public can also use technology to coordinate spontaneously and express its anger. Keen to 'throw the bastards out' protesters take to the streets, united by what they don't like, but without a shared agenda or the institutional infrastructure to figure out how to fix things. Some popular movements have come to view any attempt to exercise power over others as suspect. 

If Gurri is to be believed, protest movements in Egypt, Spain, Greece and Israel in 2011 followed this script, while Brexit, Trump and the French yellow vests movement subsequently vindicated his theory.

In this model, politics won't return to its old equilibrium any time soon. The leaders of tomorrow will need a new message and style if they hope to maintain any legitimacy in this less hierarchical world. Otherwise, we're in for decades of grinding conflict between traditional centres of authority and the general public, who doubt both their loyalty and competence. 

But how much should we believe this theory? Why do Canada and Australia remain pools of calm in the storm? Aren't some malcontents quite concrete in their demands? And are protest movements actually more common (or more nihilistic) than they were decades ago?

In today's episode we ask these questions and add an hour-long discussion with two of Rob's colleagues - Keiran Harris and Michelle Hutchinson - to further explore the ideas in the book.

The conversation covers:

* How do we know that the internet is driving this rather than some other phenomenon?
* How do technological changes enable social and political change?
* The historical role of television
* Are people also more disillusioned now with sports heroes and actors?
* Which countries are finding good ways to make politics work in this new era?
* What are the implications for the threat of totalitarianism?
* What is this is going to do to international relations? Will it make it harder for countries to cooperate and avoid conflict?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Politics in rich countries seems to be going nuts. What's the explanation? Rising inequality? The decline of manufacturing jobs? Excessive immigration?

Martin Gurri spent decades as a CIA analyst and in his 2014 book The Revolt of The Public and Crisis o</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#50 - David Denkenberger on how to feed all 8b people through an asteroid/nuclear winter</title>
      <itunes:title>#50 - David Denkenberger on how to feed all 8b people through an asteroid/nuclear winter</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/550737798</guid>
      <link>https://share.transistor.fm/s/9d26750e</link>
      <description>
        <![CDATA[<p>If an asteroid impact or nuclear winter blocked the sun for years, our inability to grow food would result in billions dying of starvation, right? According to Dr David Denkenberger, co-author of <a href="https://en.wikipedia.org/wiki/Feeding_Everyone_No_Matter_What"><em>Feeding Everyone No Matter What</em></a>: no. If he's to be believed, nobody need starve at all.</p><p> Even without the sun, David sees the Earth as a bountiful food source. Mushrooms farmed on decaying wood. Bacteria fed with natural gas. Fish and mussels supported by sudden upwelling of ocean nutrients - and more.</p><p> Dr Denkenberger is an Assistant Professor at the University of Alaska Fairbanks, and he's out to spread the word that while a nuclear winter might be horrible, experts have been mistaken to assume that mass starvation is an inevitability. In fact, the only thing that would prevent us from feeding the world is insufficient preparation.</p><p> ∙ <a href="https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/?utm_campaign=podcast__david-denkenberger&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript</strong></a></p><p> Not content to just write a book pointing this out, David has gone on to found a growing non-profit - the <a href="http://allfed.info/"><em>Alliance to Feed the Earth in Disasters (ALLFED)</em></a> - to prepare the world to feed everyone come what may. He expects that today 10% of people would find enough food to survive a massive disaster. In principle, if we did everything right, nobody need go hungry. But being more realistic about how much we're likely to invest, David thinks a plan to inform people ahead of time could save 30%, and a decent research and development scheme 80%.</p><p> ∙ 80,000 Hours' updated article on <a href="https://80k.link/best-charity-david"><strong>How to find the best charity to give to</strong></a><br> ∙ A potential donor <a href="https://forum.effectivealtruism.org/posts/SYeJnv9vYzq9oQMbQ/2017-donor-lottery-report"><strong>evaluates ALLFED</strong></a></p><p> According to David's <a href="https://link.springer.com/article/10.1007/s13753-016-0097-2">published cost-benefit analyses</a>, work on this problem may be able to save lives, in expectation, for under $100 each, making it an incredible investment. </p><p> These preparations could also help make humanity more resilient to global catastrophic risks, by forestalling an ‘everyone for themselves' mentality, which then causes trade and civilization to unravel.</p><p> But some worry that David's cost-effectiveness estimates are exaggerations, so I challenge him on the practicality of his approach, and how much his non-profit's work would actually matter in a post-apocalyptic world. In our extensive conversation, we cover:</p><p> * How could the sun end up getting blocked, or agriculture otherwise be decimated?<br> * What are all the ways we could we eat nonetheless? What kind of life would this be?<br> * Can these methods be scaled up fast?<br> * What is his organisation, ALLFED, actually working on?<br> * How does he estimate the cost-effectiveness of this work, and what are the biggest weaknesses of the approach?<br> * How would more food affect the post-apocalyptic world? Won't people figure it out at that point anyway?<br> * Why not just leave guidebooks with this information in every city?<br> * Would these preparations make nuclear war more likely?<br> * What kind of people is ALLFED trying to hire?<br> * What would ALLFED do with more money?<br> * How he ended up doing this work. And his other engineering proposals for improving the world, including ideas to prevent a supervolcano explosion.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If an asteroid impact or nuclear winter blocked the sun for years, our inability to grow food would result in billions dying of starvation, right? According to Dr David Denkenberger, co-author of <a href="https://en.wikipedia.org/wiki/Feeding_Everyone_No_Matter_What"><em>Feeding Everyone No Matter What</em></a>: no. If he's to be believed, nobody need starve at all.</p><p> Even without the sun, David sees the Earth as a bountiful food source. Mushrooms farmed on decaying wood. Bacteria fed with natural gas. Fish and mussels supported by sudden upwelling of ocean nutrients - and more.</p><p> Dr Denkenberger is an Assistant Professor at the University of Alaska Fairbanks, and he's out to spread the word that while a nuclear winter might be horrible, experts have been mistaken to assume that mass starvation is an inevitability. In fact, the only thing that would prevent us from feeding the world is insufficient preparation.</p><p> ∙ <a href="https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/?utm_campaign=podcast__david-denkenberger&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript</strong></a></p><p> Not content to just write a book pointing this out, David has gone on to found a growing non-profit - the <a href="http://allfed.info/"><em>Alliance to Feed the Earth in Disasters (ALLFED)</em></a> - to prepare the world to feed everyone come what may. He expects that today 10% of people would find enough food to survive a massive disaster. In principle, if we did everything right, nobody need go hungry. But being more realistic about how much we're likely to invest, David thinks a plan to inform people ahead of time could save 30%, and a decent research and development scheme 80%.</p><p> ∙ 80,000 Hours' updated article on <a href="https://80k.link/best-charity-david"><strong>How to find the best charity to give to</strong></a><br> ∙ A potential donor <a href="https://forum.effectivealtruism.org/posts/SYeJnv9vYzq9oQMbQ/2017-donor-lottery-report"><strong>evaluates ALLFED</strong></a></p><p> According to David's <a href="https://link.springer.com/article/10.1007/s13753-016-0097-2">published cost-benefit analyses</a>, work on this problem may be able to save lives, in expectation, for under $100 each, making it an incredible investment. </p><p> These preparations could also help make humanity more resilient to global catastrophic risks, by forestalling an ‘everyone for themselves' mentality, which then causes trade and civilization to unravel.</p><p> But some worry that David's cost-effectiveness estimates are exaggerations, so I challenge him on the practicality of his approach, and how much his non-profit's work would actually matter in a post-apocalyptic world. In our extensive conversation, we cover:</p><p> * How could the sun end up getting blocked, or agriculture otherwise be decimated?<br> * What are all the ways we could we eat nonetheless? What kind of life would this be?<br> * Can these methods be scaled up fast?<br> * What is his organisation, ALLFED, actually working on?<br> * How does he estimate the cost-effectiveness of this work, and what are the biggest weaknesses of the approach?<br> * How would more food affect the post-apocalyptic world? Won't people figure it out at that point anyway?<br> * Why not just leave guidebooks with this information in every city?<br> * Would these preparations make nuclear war more likely?<br> * What kind of people is ALLFED trying to hire?<br> * What would ALLFED do with more money?<br> * How he ended up doing this work. And his other engineering proposals for improving the world, including ideas to prevent a supervolcano explosion.</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 27 Dec 2018 20:45:53 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/9d26750e/a38cf265.mp3" length="170169504" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/397q49jSLIV_PEanCwzJqP6McorlrB51UA2Ku74zMsY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Njkv/MTY4MzU0NDU5Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10635</itunes:duration>
      <itunes:summary>If an asteroid impact or nuclear winter blocked the sun for years, our inability to grow food would result in billions dying of starvation, right? According to Dr David Denkenberger, co-author of Feeding Everyone No Matter What: no. If he's to be believed, nobody need starve at all.

Even without the sun, David sees the Earth as a bountiful food source. Mushrooms farmed on decaying wood. Bacteria fed with natural gas. Fish and mussels supported by sudden upwelling of ocean nutrients - and more.

Dr Denkenberger is an Assistant Professor at the University of Alaska Fairbanks, and he's out to spread the word that while a nuclear winter might be horrible, experts have been mistaken to assume that mass starvation is an inevitability. In fact, the only thing that would prevent us from feeding the world is insufficient preparation.

∙ Links to learn more, summary and full transcript

Not content to just write a book pointing this out, David has gone on to found a growing non-profit - the Alliance to Feed the Earth in Disasters (ALLFED) - to prepare the world to feed everyone come what may. He expects that today 10% of people would find enough food to survive a massive disaster. In principle, if we did everything right, nobody need go hungry. But being more realistic about how much we're likely to invest, David thinks a plan to inform people ahead of time could save 30%, and a decent research and development scheme 80%.

∙ 80,000 Hours' updated article on How to find the best charity to give to
∙ A potential donor evaluates ALLFED

According to David's published cost-benefit analyses, work on this problem may be able to save lives, in expectation, for under $100 each, making it an incredible investment. 

These preparations could also help make humanity more resilient to global catastrophic risks, by forestalling an ‘everyone for themselves' mentality, which then causes trade and civilization to unravel.

But some worry that David's cost-effectiveness estimates are exaggerations, so I challenge him on the practicality of his approach, and how much his non-profit's work would actually matter in a post-apocalyptic world. In our extensive conversation, we cover:

* How could the sun end up getting blocked, or agriculture otherwise be decimated?
* What are all the ways we could we eat nonetheless? What kind of life would this be?
* Can these methods be scaled up fast?
* What is his organisation, ALLFED, actually working on?
* How does he estimate the cost-effectiveness of this work, and what are the biggest weaknesses of the approach?
* How would more food affect the post-apocalyptic world? Won't people figure it out at that point anyway?
* Why not just leave guidebooks with this information in every city?
* Would these preparations make nuclear war more likely?
* What kind of people is ALLFED trying to hire?
* What would ALLFED do with more money?
* How he ended up doing this work. And his other engineering proposals for improving the world, including ideas to prevent a supervolcano explosion.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>If an asteroid impact or nuclear winter blocked the sun for years, our inability to grow food would result in billions dying of starvation, right? According to Dr David Denkenberger, co-author of Feeding Everyone No Matter What: no. If he's to be believed</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#49 - Rachel Glennerster on a year's worth of education for 30c &amp; other development 'best buys'</title>
      <itunes:title>#49 - Rachel Glennerster on a year's worth of education for 30c &amp; other development 'best buys'</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/547588083</guid>
      <link>https://share.transistor.fm/s/9b6f41b3</link>
      <description>
        <![CDATA[<p>If I told you it's possible to deliver an extra year of ideal primary-level education for under $1, would you believe me? Hopefully not - the claim is absurd on its face.</p><p> But it may be true nonetheless. The very best education interventions are phenomenally cost-effective, and they're not the kinds of things you'd expect, says Dr Rachel Glennerster.</p><p> She's Chief Economist at the UK's foreign aid agency DFID, and used to run J-PAL, the world-famous anti-poverty research centre based in MIT's Economics Department, where she studied the impact of a wide range of approaches to improving education, health, and governing institutions. According to Dr Glennerster:</p><p> <em>"...when we looked at the cost effectiveness of education programs, there were a ton of zeros, and there were a ton of zeros on the things that we spend most of our money on. So more teachers, more books, more inputs, like smaller class sizes - at least in the developing world - seem to have no impact, and that's where most government money gets spent."</em></p><p> "But measurements for the top ones - the most cost effective programs - say they deliver 460 LAYS per £100 spent ($US130). LAYS are Learning-Adjusted Years of Schooling. Each one is the equivalent of the best possible year of education you can have - Singapore-level."</p><p> <a href="https://80k.link/rachel-glennerster"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> <em>"...the two programs that come out as spectacularly effective... well, the first is just rearranging kids in a class."</em></p><p> "You have to test the kids, so that you can put the kids who are performing at grade two level in the grade two class, and the kids who are performing at grade four level in the grade four class, even if they're different ages - and they learn so much better. So that's why it's so phenomenally cost effective because, it really doesn't cost anything."</p><p> "The other one is providing information. So sending information over the phone [for example about how much more people earn if they do well in school and graduate]. So these really small nudges. Now none of those nudges will individually transform any kid's life, but they are so cheap that you get these fantastic returns on investment - and we do very little of that kind of thing."</p><p> In this episode, Dr Glennerster shares her decades of accumulated wisdom on which anti-poverty programs are overrated, which are neglected opportunities, and how we can know the difference, across a range of fields including health, empowering women and macroeconomic policy.</p><p> Regular listeners will be wondering - have we forgotten all about the lessons from <a href="https://80k.link/rachel-to-eva-episode">episode 30 of the show with Dr Eva Vivalt</a>? She threw several buckets of cold water on the hope that we could accurately measure the effectiveness of social programs at all.</p><p> According to Vivalt, her dataset of hundreds of randomised controlled trials indicates that social science findings don’t generalize well at all. The results of a trial at a school in Namibia tell us remarkably little about how a similar program will perform if delivered at another school in Namibia - let alone if it's attempted in India instead.</p><p> Rachel offers a different and more optimistic interpretation of Eva's findings. To learn more and figure out who you sympathise with more, you'll just have to listen to the episode.</p><p> Regardless, Vivalt and Glennerster agree that we should continue to run these kinds of studies, and today’s episode delves into the latest ideas in global health and development.</p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If I told you it's possible to deliver an extra year of ideal primary-level education for under $1, would you believe me? Hopefully not - the claim is absurd on its face.</p><p> But it may be true nonetheless. The very best education interventions are phenomenally cost-effective, and they're not the kinds of things you'd expect, says Dr Rachel Glennerster.</p><p> She's Chief Economist at the UK's foreign aid agency DFID, and used to run J-PAL, the world-famous anti-poverty research centre based in MIT's Economics Department, where she studied the impact of a wide range of approaches to improving education, health, and governing institutions. According to Dr Glennerster:</p><p> <em>"...when we looked at the cost effectiveness of education programs, there were a ton of zeros, and there were a ton of zeros on the things that we spend most of our money on. So more teachers, more books, more inputs, like smaller class sizes - at least in the developing world - seem to have no impact, and that's where most government money gets spent."</em></p><p> "But measurements for the top ones - the most cost effective programs - say they deliver 460 LAYS per £100 spent ($US130). LAYS are Learning-Adjusted Years of Schooling. Each one is the equivalent of the best possible year of education you can have - Singapore-level."</p><p> <a href="https://80k.link/rachel-glennerster"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> <em>"...the two programs that come out as spectacularly effective... well, the first is just rearranging kids in a class."</em></p><p> "You have to test the kids, so that you can put the kids who are performing at grade two level in the grade two class, and the kids who are performing at grade four level in the grade four class, even if they're different ages - and they learn so much better. So that's why it's so phenomenally cost effective because, it really doesn't cost anything."</p><p> "The other one is providing information. So sending information over the phone [for example about how much more people earn if they do well in school and graduate]. So these really small nudges. Now none of those nudges will individually transform any kid's life, but they are so cheap that you get these fantastic returns on investment - and we do very little of that kind of thing."</p><p> In this episode, Dr Glennerster shares her decades of accumulated wisdom on which anti-poverty programs are overrated, which are neglected opportunities, and how we can know the difference, across a range of fields including health, empowering women and macroeconomic policy.</p><p> Regular listeners will be wondering - have we forgotten all about the lessons from <a href="https://80k.link/rachel-to-eva-episode">episode 30 of the show with Dr Eva Vivalt</a>? She threw several buckets of cold water on the hope that we could accurately measure the effectiveness of social programs at all.</p><p> According to Vivalt, her dataset of hundreds of randomised controlled trials indicates that social science findings don’t generalize well at all. The results of a trial at a school in Namibia tell us remarkably little about how a similar program will perform if delivered at another school in Namibia - let alone if it's attempted in India instead.</p><p> Rachel offers a different and more optimistic interpretation of Eva's findings. To learn more and figure out who you sympathise with more, you'll just have to listen to the episode.</p><p> Regardless, Vivalt and Glennerster agree that we should continue to run these kinds of studies, and today’s episode delves into the latest ideas in global health and development.</p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 20 Dec 2018 05:19:32 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/9b6f41b3/e5ffcee4.mp3" length="91969843" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/v-BL1vVDNcOqvgx8iKVH3mViuFa_J77JnmaHOsggdbg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Njgv/MTY4MzU0NDU5MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5742</itunes:duration>
      <itunes:summary>If I told you it's possible to deliver an extra year of ideal primary-level education for under $1, would you believe me? Hopefully not - the claim is absurd on its face.

But it may be true nonetheless. The very best education interventions are phenomenally cost-effective, and they're not the kinds of things you'd expect, says Dr Rachel Glennerster.

She's Chief Economist at the UK's foreign aid agency DFID, and used to run J-PAL, the world-famous anti-poverty research centre based in MIT's Economics Department, where she studied the impact of a wide range of approaches to improving education, health, and governing institutions. According to Dr Glennerster:

"...when we looked at the cost effectiveness of education programs, there were a ton of zeros, and there were a ton of zeros on the things that we spend most of our money on. So more teachers, more books, more inputs, like smaller class sizes - at least in the developing world - seem to have no impact, and that's where most government money gets spent."

"But measurements for the top ones - the most cost effective programs - say they deliver 460 LAYS per £100 spent ($US130). LAYS are Learning-Adjusted Years of Schooling. Each one is the equivalent of the best possible year of education you can have - Singapore-level."

Links to learn more, summary and full transcript.

"...the two programs that come out as spectacularly effective... well, the first is just rearranging kids in a class."

"You have to test the kids, so that you can put the kids who are performing at grade two level in the grade two class, and the kids who are performing at grade four level in the grade four class, even if they're different ages - and they learn so much better. So that's why it's so phenomenally cost effective because, it really doesn't cost anything."

"The other one is providing information. So sending information over the phone [for example about how much more people earn if they do well in school and graduate]. So these really small nudges. Now none of those nudges will individually transform any kid's life, but they are so cheap that you get these fantastic returns on investment - and we do very little of that kind of thing."

In this episode, Dr Glennerster shares her decades of accumulated wisdom on which anti-poverty programs are overrated, which are neglected opportunities, and how we can know the difference, across a range of fields including health, empowering women and macroeconomic policy.

Regular listeners will be wondering - have we forgotten all about the lessons from episode 30 of the show with Dr Eva Vivalt? She threw several buckets of cold water on the hope that we could accurately measure the effectiveness of social programs at all.

According to Vivalt, her dataset of hundreds of randomised controlled trials indicates that social science findings don’t generalize well at all. The results of a trial at a school in Namibia tell us remarkably little about how a similar program will perform if delivered at another school in Namibia - let alone if it's attempted in India instead.

Rachel offers a different and more optimistic interpretation of Eva's findings. To learn more and figure out who you sympathise with more, you'll just have to listen to the episode.

Regardless, Vivalt and Glennerster agree that we should continue to run these kinds of studies, and today’s episode delves into the latest ideas in global health and development.

Get this episode by subscribing: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>If I told you it's possible to deliver an extra year of ideal primary-level education for under $1, would you believe me? Hopefully not - the claim is absurd on its face.

But it may be true nonetheless. The very best education interventions are phenome</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#48 - Brian Christian on better living through the wisdom of computer science</title>
      <itunes:title>#48 - Brian Christian on better living through the wisdom of computer science</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/533741832</guid>
      <link>https://share.transistor.fm/s/cffd2e96</link>
      <description>
        <![CDATA[Please let us know if we've helped you: <a href="https://80k.link/survey-christian"><b>Fill out our annual impact survey</b></a><p>

Ever felt that you were so busy you spent all your time paralysed trying to figure out where to start, and couldn't get much done? Computer scientists have a term for this - <em>thrashing</em> - and it's a common reason our computers freeze up. The solution, for people as well as laptops, is to 'work dumber': pick something at random and finish it, without wasting time thinking about the bigger picture.</p><p>

Bestselling author Brian Christian studied computer science, and in the book <b>Algorithms to Live By</b> he's out to find the lessons it can offer for a better life. He investigates into when to quit your job, when to marry, the best way to sell your house, how long to spend on a difficult decision, and how much randomness to inject into your life. In each case computer science gives us a theoretically optimal solution, and in this episode we think hard about whether its models match our reality. </p><p>

<a href="https://80k.link/brian-podcast" rel="nofollow"><b>Links to learn more, summary and full transcript.</b></a></p><p>

One genre of problems Brian explores in his book are 'optimal stopping problems', the canonical example of which is ‘the secretary problem’. Imagine you're hiring a secretary, you receive *n* applicants, they show up in a random order, and you interview them one after another. You either have to hire that person on the spot and dismiss everybody else, or send them away and lose the option to hire them in future.</p><p>

It turns out most of life can be viewed this way - a series of unique opportunities you pass by that will never be available in exactly the same way again.</p><p>

So how do you attempt to hire the very best candidate in the pool? There's a risk that you stop before finding the best, and a risk that you set your standards too high and let the best candidate pass you by.</p><p>

Mathematicians of the mid-twentieth century produced an elegant optimal approach: spend exactly one over *e*, or approximately 37% of your search, just establishing a baseline without hiring anyone, no matter how promising they seem. Then immediately hire the next person who's better than anyone you've seen so far.</p><p>

It turns out that your odds of success in this scenario are also 37%. And the optimal strategy and the odds of success are identical regardless of the size of the pool. So as *n* goes to infinity you still want to follow this 37% rule, and you still have a 37% chance of success. Even if you interview a million people.</p><p>

But if you have the option to go back, say by apologising to the first applicant and begging them to come work with you, and you have a 50% chance of your apology being accepted, then the optimal explore percentage rises all the way to 61%. </p><p>

Today’s episode focuses on Brian’s book-length exploration of how insights from computer algorithms can and can't be applied to our everyday lives. We cover:</p><p>

* Computational kindness, and the best way to schedule meetings<br>
* How can we characterize a computational model of what people are actually doing, and is there a rigorous way to analyse just how good their instincts actually are?<br>
* What’s it like being a human confederate in the Turing test competition?<br>
* Is trying to detect fake social media accounts a losing battle?<br>
* The canonical explore/exploit problem in computer science:  the multi-armed bandit<br>
* What’s the optimal way to buy or sell a house?<br>
* Why is information economics so important?<br>
* What kind of decisions should people randomize more in life?<br>
* How much time should we spend on prioritisation?</p><p>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Please let us know if we've helped you: <a href="https://80k.link/survey-christian"><b>Fill out our annual impact survey</b></a><p>

Ever felt that you were so busy you spent all your time paralysed trying to figure out where to start, and couldn't get much done? Computer scientists have a term for this - <em>thrashing</em> - and it's a common reason our computers freeze up. The solution, for people as well as laptops, is to 'work dumber': pick something at random and finish it, without wasting time thinking about the bigger picture.</p><p>

Bestselling author Brian Christian studied computer science, and in the book <b>Algorithms to Live By</b> he's out to find the lessons it can offer for a better life. He investigates into when to quit your job, when to marry, the best way to sell your house, how long to spend on a difficult decision, and how much randomness to inject into your life. In each case computer science gives us a theoretically optimal solution, and in this episode we think hard about whether its models match our reality. </p><p>

<a href="https://80k.link/brian-podcast" rel="nofollow"><b>Links to learn more, summary and full transcript.</b></a></p><p>

One genre of problems Brian explores in his book are 'optimal stopping problems', the canonical example of which is ‘the secretary problem’. Imagine you're hiring a secretary, you receive *n* applicants, they show up in a random order, and you interview them one after another. You either have to hire that person on the spot and dismiss everybody else, or send them away and lose the option to hire them in future.</p><p>

It turns out most of life can be viewed this way - a series of unique opportunities you pass by that will never be available in exactly the same way again.</p><p>

So how do you attempt to hire the very best candidate in the pool? There's a risk that you stop before finding the best, and a risk that you set your standards too high and let the best candidate pass you by.</p><p>

Mathematicians of the mid-twentieth century produced an elegant optimal approach: spend exactly one over *e*, or approximately 37% of your search, just establishing a baseline without hiring anyone, no matter how promising they seem. Then immediately hire the next person who's better than anyone you've seen so far.</p><p>

It turns out that your odds of success in this scenario are also 37%. And the optimal strategy and the odds of success are identical regardless of the size of the pool. So as *n* goes to infinity you still want to follow this 37% rule, and you still have a 37% chance of success. Even if you interview a million people.</p><p>

But if you have the option to go back, say by apologising to the first applicant and begging them to come work with you, and you have a 50% chance of your apology being accepted, then the optimal explore percentage rises all the way to 61%. </p><p>

Today’s episode focuses on Brian’s book-length exploration of how insights from computer algorithms can and can't be applied to our everyday lives. We cover:</p><p>

* Computational kindness, and the best way to schedule meetings<br>
* How can we characterize a computational model of what people are actually doing, and is there a rigorous way to analyse just how good their instincts actually are?<br>
* What’s it like being a human confederate in the Turing test competition?<br>
* Is trying to detect fake social media accounts a losing battle?<br>
* The canonical explore/exploit problem in computer science:  the multi-armed bandit<br>
* What’s the optimal way to buy or sell a house?<br>
* Why is information economics so important?<br>
* What kind of decisions should people randomize more in life?<br>
* How much time should we spend on prioritisation?</p><p>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 22 Nov 2018 22:27:45 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cffd2e96/349f86c3.mp3" length="187879804" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/XcGpDw5Kj5TR6BqW3cWHuMieJU4u2OmiH9dntYbZjck/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Njcv/MTY4MzU0NDU5MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11730</itunes:duration>
      <itunes:summary>Please let us know if we've helped you: Fill out our annual impact survey

Ever felt that you were so busy you spent all your time paralysed trying to figure out where to start, and couldn't get much done? Computer scientists have a term for this - thrashing - and it's a common reason our computers freeze up. The solution, for people as well as laptops, is to 'work dumber': pick something at random and finish it, without wasting time thinking about the bigger picture.

Bestselling author Brian Christian studied computer science, and in the book Algorithms to Live By he's out to find the lessons it can offer for a better life. He investigates into when to quit your job, when to marry, the best way to sell your house, how long to spend on a difficult decision, and how much randomness to inject into your life. In each case computer science gives us a theoretically optimal solution, and in this episode we think hard about whether its models match our reality. 

Links to learn more, summary and full transcript.

One genre of problems Brian explores in his book are 'optimal stopping problems', the canonical example of which is ‘the secretary problem’. Imagine you're hiring a secretary, you receive *n* applicants, they show up in a random order, and you interview them one after another. You either have to hire that person on the spot and dismiss everybody else, or send them away and lose the option to hire them in future.

It turns out most of life can be viewed this way - a series of unique opportunities you pass by that will never be available in exactly the same way again.

So how do you attempt to hire the very best candidate in the pool? There's a risk that you stop before finding the best, and a risk that you set your standards too high and let the best candidate pass you by.

Mathematicians of the mid-twentieth century produced an elegant optimal approach: spend exactly one over *e*, or approximately 37% of your search, just establishing a baseline without hiring anyone, no matter how promising they seem. Then immediately hire the next person who's better than anyone you've seen so far.

It turns out that your odds of success in this scenario are also 37%. And the optimal strategy and the odds of success are identical regardless of the size of the pool. So as *n* goes to infinity you still want to follow this 37% rule, and you still have a 37% chance of success. Even if you interview a million people.

But if you have the option to go back, say by apologising to the first applicant and begging them to come work with you, and you have a 50% chance of your apology being accepted, then the optimal explore percentage rises all the way to 61%. 

Today’s episode focuses on Brian’s book-length exploration of how insights from computer algorithms can and can't be applied to our everyday lives. We cover:

* Computational kindness, and the best way to schedule meetings
* How can we characterize a computational model of what people are actually doing, and is there a rigorous way to analyse just how good their instincts actually are?
* What’s it like being a human confederate in the Turing test competition?
* Is trying to detect fake social media accounts a losing battle?
* The canonical explore/exploit problem in computer science:  the multi-armed bandit
* What’s the optimal way to buy or sell a house?
* Why is information economics so important?
* What kind of decisions should people randomize more in life?
* How much time should we spend on prioritisation?

Get this episode by subscribing: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Please let us know if we've helped you: Fill out our annual impact survey

Ever felt that you were so busy you spent all your time paralysed trying to figure out where to start, and couldn't get much done? Computer scientists have a term for this - thrash</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#47 - Catherine Olsson &amp; Daniel Ziegler on the fast path into high-impact ML engineering roles</title>
      <itunes:title>#47 - Catherine Olsson &amp; Daniel Ziegler on the fast path into high-impact ML engineering roles</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/523583478</guid>
      <link>https://share.transistor.fm/s/c91ef88c</link>
      <description>
        <![CDATA[After dropping out of a machine learning PhD at Stanford, Daniel Ziegler needed to decide what to do next. He’d always enjoyed building stuff and wanted to shape the development of AI, so he thought a research engineering position at an org dedicated to aligning AI with human interests could be his best option.<p>

He decided to apply to OpenAI, and spent about 6 weeks preparing for the interview before landing the job. His PhD, by contrast, might have taken 6 years. Daniel thinks this highly accelerated career path may be possible for many others.</p><p>

On today’s episode Daniel is joined by Catherine Olsson, who has also worked at OpenAI, and left her computational neuroscience PhD to become a research engineer at Google Brain. She and Daniel share this piece of advice for those curious about this career path: just dive in. If you're trying to get good at something, just start doing that thing, and figure out that way what's necessary to be able to do it well.</p><p>

Catherine has even <a href="https://80k.link/ml-engineering-career-transition-guide" rel="nofollow">created a simple step-by-step guide</a> for 80,000 Hours, to make it as easy as possible for others to copy her and Daniel's success.</p><p>

<em>Please let us know how we've helped you: fill out <a href="https://80k.link/survey-c-and-z"><b>our 2018 annual impact survey</b></a> so that 80,000 Hours can continue to operate and grow.</em></p><p>

<a href="https://80k.link/olsson-and-ziegler-ml-engineering-and-safety" rel="nofollow"><b>Blog post with links to learn more, a summary &amp; full transcript.</b></a></p><p>

Daniel thinks the key for him was nailing the job interview.</p><p>

OpenAI needed him to be able to demonstrate the ability to do the kind of stuff he'd be working on day-to-day. So his approach was to take a list of 50 key deep reinforcement learning papers, read one or two a day, and pick a handful to actually reproduce. He spent a bunch of time coding in Python and TensorFlow, sometimes 12 hours a day, trying to debug and tune things until they were actually working.</p><p>

Daniel emphasizes that the most important thing was to practice *exactly* those things that he knew he needed to be able to do. His dedicated preparation also led to an offer from the Machine Intelligence Research Institute, and so he had the opportunity to decide between two organisations focused on the global problem that most concerns him.</p><p> 

Daniel’s path might seem unusual, but both he and Catherine expect it can be replicated by others. If they're right, it could greatly increase our ability to get new people into important ML roles in which they can make a difference, as quickly as possible.</p><p>

Catherine says that her move from OpenAI to an ML research team at Google now allows her to bring a different set of skills to the table. Technical AI safety is a multifaceted area of research, and the many sub-questions in areas such as reward learning, robustness, and interpretability all need to be answered to maximize the probability that AI development goes well for humanity.</p><p> 

Today’s episode combines the expertise of two pioneers and is a key resource for anyone wanting to follow in their footsteps. We cover:</p><p>

* What are OpenAI and Google Brain doing?<br> 
* Why work on AI?<br>
* Do you learn more on the job, or while doing a PhD?<br>
* Controversial issues within ML<br>
* Is replicating papers a good way of determining suitability?<br>
* What % of software developers could make similar transitions?<br>
* How in-demand are research engineers?<br>
* The development of Dota 2 bots<br>
* Do research scientists have more influence on the vision of an org?<br>
* Has learning more made you more or less worried about the future?</p><p>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[After dropping out of a machine learning PhD at Stanford, Daniel Ziegler needed to decide what to do next. He’d always enjoyed building stuff and wanted to shape the development of AI, so he thought a research engineering position at an org dedicated to aligning AI with human interests could be his best option.<p>

He decided to apply to OpenAI, and spent about 6 weeks preparing for the interview before landing the job. His PhD, by contrast, might have taken 6 years. Daniel thinks this highly accelerated career path may be possible for many others.</p><p>

On today’s episode Daniel is joined by Catherine Olsson, who has also worked at OpenAI, and left her computational neuroscience PhD to become a research engineer at Google Brain. She and Daniel share this piece of advice for those curious about this career path: just dive in. If you're trying to get good at something, just start doing that thing, and figure out that way what's necessary to be able to do it well.</p><p>

Catherine has even <a href="https://80k.link/ml-engineering-career-transition-guide" rel="nofollow">created a simple step-by-step guide</a> for 80,000 Hours, to make it as easy as possible for others to copy her and Daniel's success.</p><p>

<em>Please let us know how we've helped you: fill out <a href="https://80k.link/survey-c-and-z"><b>our 2018 annual impact survey</b></a> so that 80,000 Hours can continue to operate and grow.</em></p><p>

<a href="https://80k.link/olsson-and-ziegler-ml-engineering-and-safety" rel="nofollow"><b>Blog post with links to learn more, a summary &amp; full transcript.</b></a></p><p>

Daniel thinks the key for him was nailing the job interview.</p><p>

OpenAI needed him to be able to demonstrate the ability to do the kind of stuff he'd be working on day-to-day. So his approach was to take a list of 50 key deep reinforcement learning papers, read one or two a day, and pick a handful to actually reproduce. He spent a bunch of time coding in Python and TensorFlow, sometimes 12 hours a day, trying to debug and tune things until they were actually working.</p><p>

Daniel emphasizes that the most important thing was to practice *exactly* those things that he knew he needed to be able to do. His dedicated preparation also led to an offer from the Machine Intelligence Research Institute, and so he had the opportunity to decide between two organisations focused on the global problem that most concerns him.</p><p> 

Daniel’s path might seem unusual, but both he and Catherine expect it can be replicated by others. If they're right, it could greatly increase our ability to get new people into important ML roles in which they can make a difference, as quickly as possible.</p><p>

Catherine says that her move from OpenAI to an ML research team at Google now allows her to bring a different set of skills to the table. Technical AI safety is a multifaceted area of research, and the many sub-questions in areas such as reward learning, robustness, and interpretability all need to be answered to maximize the probability that AI development goes well for humanity.</p><p> 

Today’s episode combines the expertise of two pioneers and is a key resource for anyone wanting to follow in their footsteps. We cover:</p><p>

* What are OpenAI and Google Brain doing?<br> 
* Why work on AI?<br>
* Do you learn more on the job, or while doing a PhD?<br>
* Controversial issues within ML<br>
* Is replicating papers a good way of determining suitability?<br>
* What % of software developers could make similar transitions?<br>
* How in-demand are research engineers?<br>
* The development of Dota 2 bots<br>
* Do research scientists have more influence on the vision of an org?<br>
* Has learning more made you more or less worried about the future?</p><p>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 02 Nov 2018 12:47:04 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c91ef88c/12d091bc.mp3" length="119978531" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/DeMgjyvElFc9DmWPPL2mV6LGmFF7RN9K2JupdW9P4fE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjYv/MTY4MzU0NDU4OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7490</itunes:duration>
      <itunes:summary>After dropping out of a machine learning PhD at Stanford, Daniel Ziegler needed to decide what to do next. He’d always enjoyed building stuff and wanted to shape the development of AI, so he thought a research engineering position at an org dedicated to aligning AI with human interests could be his best option.

He decided to apply to OpenAI, and spent about 6 weeks preparing for the interview before landing the job. His PhD, by contrast, might have taken 6 years. Daniel thinks this highly accelerated career path may be possible for many others.

On today’s episode Daniel is joined by Catherine Olsson, who has also worked at OpenAI, and left her computational neuroscience PhD to become a research engineer at Google Brain. She and Daniel share this piece of advice for those curious about this career path: just dive in. If you're trying to get good at something, just start doing that thing, and figure out that way what's necessary to be able to do it well.

Catherine has even created a simple step-by-step guide for 80,000 Hours, to make it as easy as possible for others to copy her and Daniel's success.

Please let us know how we've helped you: fill out our 2018 annual impact survey so that 80,000 Hours can continue to operate and grow.

Blog post with links to learn more, a summary &amp;amp; full transcript.

Daniel thinks the key for him was nailing the job interview.

OpenAI needed him to be able to demonstrate the ability to do the kind of stuff he'd be working on day-to-day. So his approach was to take a list of 50 key deep reinforcement learning papers, read one or two a day, and pick a handful to actually reproduce. He spent a bunch of time coding in Python and TensorFlow, sometimes 12 hours a day, trying to debug and tune things until they were actually working.

Daniel emphasizes that the most important thing was to practice *exactly* those things that he knew he needed to be able to do. His dedicated preparation also led to an offer from the Machine Intelligence Research Institute, and so he had the opportunity to decide between two organisations focused on the global problem that most concerns him. 

Daniel’s path might seem unusual, but both he and Catherine expect it can be replicated by others. If they're right, it could greatly increase our ability to get new people into important ML roles in which they can make a difference, as quickly as possible.

Catherine says that her move from OpenAI to an ML research team at Google now allows her to bring a different set of skills to the table. Technical AI safety is a multifaceted area of research, and the many sub-questions in areas such as reward learning, robustness, and interpretability all need to be answered to maximize the probability that AI development goes well for humanity. 

Today’s episode combines the expertise of two pioneers and is a key resource for anyone wanting to follow in their footsteps. We cover:

* What are OpenAI and Google Brain doing? 
* Why work on AI?
* Do you learn more on the job, or while doing a PhD?
* Controversial issues within ML
* Is replicating papers a good way of determining suitability?
* What % of software developers could make similar transitions?
* How in-demand are research engineers?
* The development of Dota 2 bots
* Do research scientists have more influence on the vision of an org?
* Has learning more made you more or less worried about the future?

Get this episode by subscribing: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>After dropping out of a machine learning PhD at Stanford, Daniel Ziegler needed to decide what to do next. He’d always enjoyed building stuff and wanted to shape the development of AI, so he thought a research engineering position at an org dedicated to a</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#46 - Hilary Greaves on moral cluelessness &amp; tackling crucial questions in academia</title>
      <itunes:title>#46 - Hilary Greaves on moral cluelessness &amp; tackling crucial questions in academia</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/518454945</guid>
      <link>https://share.transistor.fm/s/979c0e54</link>
      <description>
        <![CDATA[<p>The barista gives you your coffee and change, and you walk away from the busy line. But you suddenly realise she gave you $1 less than she should have. Do you brush your way past the people now waiting, or just accept this as a dollar you’re never getting back? According to philosophy Professor Hilary Greaves - Director of Oxford University's <a href="https://globalprioritiesinstitute.org">Global Priorities Institute</a>, which <a href="https://globalprioritiesinstitute.org/opportunities/">is hiring</a> - this simple decision will completely change the long-term future by altering the identities of almost all future generations.</p><p> How? Because by rushing back to the counter, you slightly change the timing of everything else people in line do during that day - including changing the timing of the interactions they have with everyone else. Eventually these causal links will reach someone who was going to conceive a child.</p><p> By causing a child to be conceived a few fractions of a second earlier or later, you change the sperm that fertilizes their egg, resulting in a totally different person. So asking for that $1 has now made the difference between all the things that this actual child will do in their life, and all the things that the merely possible child - who didn't exist because of what you did - would have done if you decided not to worry about it.</p><p> As that child's actions ripple out to everyone else who conceives down the generations, ultimately the entire human population will become different, all for the sake of your dollar. Will your choice cause a future Hitler to be born, or not to be born? Probably both!</p><p> <a href="https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Some find this concerning. The actual long term effects of your decisions are so unpredictable, it looks like you’re totally clueless about what's going to lead to the best outcomes. It might lead to decision paralysis - you won’t be able to take any action at all.</p><p> Prof Greaves doesn’t share this concern for most real life decisions. If there’s no reasonable way to assign probabilities to far-future outcomes, then the possibility that you might make things better in completely unpredictable ways is more or less canceled out by equally likely opposite possibility.</p><p> But, if instead we’re talking about a decision that involves highly-structured, systematic reasons for thinking there might be a general tendency of your action to make things better or worse -- for example if we increase economic growth -- Prof Greaves says that we don’t get to just ignore the unforeseeable effects. </p><p> When there are complex arguments on both sides, it's unclear what probabilities you should assign to this or that claim. Yet, given its importance, whether you should take the action in question actually does depend on figuring out these numbers. So, what do we do?</p><p> Today’s episode blends philosophy with an exploration of the mission and research agenda of the Global Priorities Institute: to develop the effective altruism movement within academia. We cover:</p><p> * How controversial is the multiverse interpretation of quantum physics?<br> * Given moral uncertainty, how should population ethics affect our real life decisions?<br> * How should we think about archetypal decision theory problems?<br> * What are the consequences of cluelessness for those who based their donation advice on GiveWell style recommendations?<br> * How could reducing extinction risk be a good cause for risk-averse people?</p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The barista gives you your coffee and change, and you walk away from the busy line. But you suddenly realise she gave you $1 less than she should have. Do you brush your way past the people now waiting, or just accept this as a dollar you’re never getting back? According to philosophy Professor Hilary Greaves - Director of Oxford University's <a href="https://globalprioritiesinstitute.org">Global Priorities Institute</a>, which <a href="https://globalprioritiesinstitute.org/opportunities/">is hiring</a> - this simple decision will completely change the long-term future by altering the identities of almost all future generations.</p><p> How? Because by rushing back to the counter, you slightly change the timing of everything else people in line do during that day - including changing the timing of the interactions they have with everyone else. Eventually these causal links will reach someone who was going to conceive a child.</p><p> By causing a child to be conceived a few fractions of a second earlier or later, you change the sperm that fertilizes their egg, resulting in a totally different person. So asking for that $1 has now made the difference between all the things that this actual child will do in their life, and all the things that the merely possible child - who didn't exist because of what you did - would have done if you decided not to worry about it.</p><p> As that child's actions ripple out to everyone else who conceives down the generations, ultimately the entire human population will become different, all for the sake of your dollar. Will your choice cause a future Hitler to be born, or not to be born? Probably both!</p><p> <a href="https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Some find this concerning. The actual long term effects of your decisions are so unpredictable, it looks like you’re totally clueless about what's going to lead to the best outcomes. It might lead to decision paralysis - you won’t be able to take any action at all.</p><p> Prof Greaves doesn’t share this concern for most real life decisions. If there’s no reasonable way to assign probabilities to far-future outcomes, then the possibility that you might make things better in completely unpredictable ways is more or less canceled out by equally likely opposite possibility.</p><p> But, if instead we’re talking about a decision that involves highly-structured, systematic reasons for thinking there might be a general tendency of your action to make things better or worse -- for example if we increase economic growth -- Prof Greaves says that we don’t get to just ignore the unforeseeable effects. </p><p> When there are complex arguments on both sides, it's unclear what probabilities you should assign to this or that claim. Yet, given its importance, whether you should take the action in question actually does depend on figuring out these numbers. So, what do we do?</p><p> Today’s episode blends philosophy with an exploration of the mission and research agenda of the Global Priorities Institute: to develop the effective altruism movement within academia. We cover:</p><p> * How controversial is the multiverse interpretation of quantum physics?<br> * Given moral uncertainty, how should population ethics affect our real life decisions?<br> * How should we think about archetypal decision theory problems?<br> * What are the consequences of cluelessness for those who based their donation advice on GiveWell style recommendations?<br> * How could reducing extinction risk be a good cause for risk-averse people?</p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 23 Oct 2018 15:54:33 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/979c0e54/2d9551d5.mp3" length="162621938" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/u7LfzRQZSWFg-xch7aXlA_Hk4z3BlKHGgmQ5D0ZHH5Q/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjUv/MTY4MzU0NDU4OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>10164</itunes:duration>
      <itunes:summary>The barista gives you your coffee and change, and you walk away from the busy line. But you suddenly realise she gave you $1 less than she should have. Do you brush your way past the people now waiting, or just accept this as a dollar you’re never getting back? According to philosophy Professor Hilary Greaves - Director of Oxford University's Global Priorities Institute, which is hiring - this simple decision will completely change the long-term future by altering the identities of almost all future generations.

How? Because by rushing back to the counter, you slightly change the timing of everything else people in line do during that day - including changing the timing of the interactions they have with everyone else. Eventually these causal links will reach someone who was going to conceive a child.

By causing a child to be conceived a few fractions of a second earlier or later, you change the sperm that fertilizes their egg, resulting in a totally different person. So asking for that $1 has now made the difference between all the things that this actual child will do in their life, and all the things that the merely possible child - who didn't exist because of what you did - would have done if you decided not to worry about it.

As that child's actions ripple out to everyone else who conceives down the generations, ultimately the entire human population will become different, all for the sake of your dollar. Will your choice cause a future Hitler to be born, or not to be born? Probably both!

Links to learn more, summary and full transcript.

Some find this concerning. The actual long term effects of your decisions are so unpredictable, it looks like you’re totally clueless about what's going to lead to the best outcomes. It might lead to decision paralysis - you won’t be able to take any action at all.

Prof Greaves doesn’t share this concern for most real life decisions. If there’s no reasonable way to assign probabilities to far-future outcomes, then the possibility that you might make things better in completely unpredictable ways is more or less canceled out by equally likely opposite possibility.

But, if instead we’re talking about a decision that involves highly-structured, systematic reasons for thinking there might be a general tendency of your action to make things better or worse -- for example if we increase economic growth -- Prof Greaves says that we don’t get to just ignore the unforeseeable effects. 

When there are complex arguments on both sides, it's unclear what probabilities you should assign to this or that claim. Yet, given its importance, whether you should take the action in question actually does depend on figuring out these numbers. So, what do we do?

Today’s episode blends philosophy with an exploration of the mission and research agenda of the Global Priorities Institute: to develop the effective altruism movement within academia. We cover:

* How controversial is the multiverse interpretation of quantum physics?
* Given moral uncertainty, how should population ethics affect our real life decisions?
* How should we think about archetypal decision theory problems?
* What are the consequences of cluelessness for those who based their donation advice on GiveWell style recommendations?
* How could reducing extinction risk be a good cause for risk-averse people?

Get this episode by subscribing: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>The barista gives you your coffee and change, and you walk away from the busy line. But you suddenly realise she gave you $1 less than she should have. Do you brush your way past the people now waiting, or just accept this as a dollar you’re never getting</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#45 - Tyler Cowen's case for maximising econ growth, stabilising civilization &amp; thinking long-term</title>
      <itunes:title>#45 - Tyler Cowen's case for maximising econ growth, stabilising civilization &amp; thinking long-term</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/515664354</guid>
      <link>https://share.transistor.fm/s/5ac0b2c2</link>
      <description>
        <![CDATA[I've probably spent more time reading Tyler Cowen - Professor of Economics at George Mason University - than any other author. Indeed it's his incredibly popular blog <a href="https://marginalrevolution.com">Marginal Revolution</a> that prompted me to study economics in the first place. Having spent thousands of hours absorbing Tyler's work, it was a pleasure to be able to question him about his latest book and personal manifesto: <a href="https://press.stripe.com/#stubborn-attachments"><em>Stubborn Attachments: A Vision for a Society of Free, Prosperous, and Responsible Individuals</em></a>.<p>

Tyler makes the case that, despite what you may have heard, we *can* make rational judgments about what is best for society as a whole. He argues:</p><p>

1. Our top moral priority should be preserving and improving humanity's long-term future<br>
2. The way to do that is to maximise the rate of sustainable economic growth<br>
3. We should respect human rights and follow general principles while doing so.</p><p>

We discuss why Tyler believes all these things, and I push back where I disagree. In particular: is higher economic growth actually an effective way to safeguard humanity's future, or should our focus really be elsewhere?</p><p>

In the process we touch on many of moral philosophy's most pressing questions: Should we discount the future? How should we aggregate welfare across people? Should we follow rules or evaluate every situation individually? How should we deal with the massive uncertainty about the effects of our actions? And should we trust common sense morality or follow structured theories?</p><p>

<a href="https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/?utm_campaign=podcast__tyler-cowen&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

After covering the book, the conversation ranges far and wide. Will we leave the galaxy, and is it a tragedy if we don't? Is a multi-polar world less stable? Will humanity ever help wild animals? Why do we both agree that Kant and Rawls are overrated?</p><p>

Today's interview is released on both the 80,000 Hours Podcast and Tyler's own show: <a href="https://medium.com/conversations-with-tyler">Conversation with Tyler</a>.</p><p>

Tyler may have had more influence on me than any other writer but this conversation is richer for our remaining disagreements. If the above isn't enough to tempt you to listen, we also look at:</p><p>

* Why couldn’t future technology make human life a hundred or a thousand times better than it is for people today?<br>
* Why focus on increasing the rate of economic growth rather than making sure that it doesn’t go to zero?<br>
* Why shouldn’t we dedicate substantial time to the successful introduction of genetic engineering?<br>
* Why should we completely abstain from alcohol and make it a social norm?<br>
* Why is Tyler so pessimistic about space? Is it likely that humans will go extinct before we manage to escape the galaxy?<br>
* Is improving coordination and international cooperation a major priority?<br>
* Why does Tyler think institutions are keeping up with technology?<br>
* Given that our actions seem to have very large and morally significant effects in the long run, are our moral obligations very onerous?<br>
* Can art be intrinsically valuable?<br>
* What does Tyler think Derek Parfit was most wrong about, and what was he was most right about that’s unappreciated today?</p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[I've probably spent more time reading Tyler Cowen - Professor of Economics at George Mason University - than any other author. Indeed it's his incredibly popular blog <a href="https://marginalrevolution.com">Marginal Revolution</a> that prompted me to study economics in the first place. Having spent thousands of hours absorbing Tyler's work, it was a pleasure to be able to question him about his latest book and personal manifesto: <a href="https://press.stripe.com/#stubborn-attachments"><em>Stubborn Attachments: A Vision for a Society of Free, Prosperous, and Responsible Individuals</em></a>.<p>

Tyler makes the case that, despite what you may have heard, we *can* make rational judgments about what is best for society as a whole. He argues:</p><p>

1. Our top moral priority should be preserving and improving humanity's long-term future<br>
2. The way to do that is to maximise the rate of sustainable economic growth<br>
3. We should respect human rights and follow general principles while doing so.</p><p>

We discuss why Tyler believes all these things, and I push back where I disagree. In particular: is higher economic growth actually an effective way to safeguard humanity's future, or should our focus really be elsewhere?</p><p>

In the process we touch on many of moral philosophy's most pressing questions: Should we discount the future? How should we aggregate welfare across people? Should we follow rules or evaluate every situation individually? How should we deal with the massive uncertainty about the effects of our actions? And should we trust common sense morality or follow structured theories?</p><p>

<a href="https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/?utm_campaign=podcast__tyler-cowen&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

After covering the book, the conversation ranges far and wide. Will we leave the galaxy, and is it a tragedy if we don't? Is a multi-polar world less stable? Will humanity ever help wild animals? Why do we both agree that Kant and Rawls are overrated?</p><p>

Today's interview is released on both the 80,000 Hours Podcast and Tyler's own show: <a href="https://medium.com/conversations-with-tyler">Conversation with Tyler</a>.</p><p>

Tyler may have had more influence on me than any other writer but this conversation is richer for our remaining disagreements. If the above isn't enough to tempt you to listen, we also look at:</p><p>

* Why couldn’t future technology make human life a hundred or a thousand times better than it is for people today?<br>
* Why focus on increasing the rate of economic growth rather than making sure that it doesn’t go to zero?<br>
* Why shouldn’t we dedicate substantial time to the successful introduction of genetic engineering?<br>
* Why should we completely abstain from alcohol and make it a social norm?<br>
* Why is Tyler so pessimistic about space? Is it likely that humans will go extinct before we manage to escape the galaxy?<br>
* Is improving coordination and international cooperation a major priority?<br>
* Why does Tyler think institutions are keeping up with technology?<br>
* Given that our actions seem to have very large and morally significant effects in the long run, are our moral obligations very onerous?<br>
* Can art be intrinsically valuable?<br>
* What does Tyler think Derek Parfit was most wrong about, and what was he was most right about that’s unappreciated today?</p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 17 Oct 2018 18:13:48 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5ac0b2c2/29a4975b.mp3" length="144830429" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/MVscWeV84uBC1KujsXGx78HfgzUHOAJHLUKINDMfc4U/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjQv/MTY4MzU0NDU4Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9041</itunes:duration>
      <itunes:summary>I've probably spent more time reading Tyler Cowen - Professor of Economics at George Mason University - than any other author. Indeed it's his incredibly popular blog Marginal Revolution that prompted me to study economics in the first place. Having spent thousands of hours absorbing Tyler's work, it was a pleasure to be able to question him about his latest book and personal manifesto: Stubborn Attachments: A Vision for a Society of Free, Prosperous, and Responsible Individuals.

Tyler makes the case that, despite what you may have heard, we *can* make rational judgments about what is best for society as a whole. He argues:

1. Our top moral priority should be preserving and improving humanity's long-term future
2. The way to do that is to maximise the rate of sustainable economic growth
3. We should respect human rights and follow general principles while doing so.

We discuss why Tyler believes all these things, and I push back where I disagree. In particular: is higher economic growth actually an effective way to safeguard humanity's future, or should our focus really be elsewhere?

In the process we touch on many of moral philosophy's most pressing questions: Should we discount the future? How should we aggregate welfare across people? Should we follow rules or evaluate every situation individually? How should we deal with the massive uncertainty about the effects of our actions? And should we trust common sense morality or follow structured theories?

Links to learn more, summary and full transcript.

After covering the book, the conversation ranges far and wide. Will we leave the galaxy, and is it a tragedy if we don't? Is a multi-polar world less stable? Will humanity ever help wild animals? Why do we both agree that Kant and Rawls are overrated?

Today's interview is released on both the 80,000 Hours Podcast and Tyler's own show: Conversation with Tyler.

Tyler may have had more influence on me than any other writer but this conversation is richer for our remaining disagreements. If the above isn't enough to tempt you to listen, we also look at:

* Why couldn’t future technology make human life a hundred or a thousand times better than it is for people today?
* Why focus on increasing the rate of economic growth rather than making sure that it doesn’t go to zero?
* Why shouldn’t we dedicate substantial time to the successful introduction of genetic engineering?
* Why should we completely abstain from alcohol and make it a social norm?
* Why is Tyler so pessimistic about space? Is it likely that humans will go extinct before we manage to escape the galaxy?
* Is improving coordination and international cooperation a major priority?
* Why does Tyler think institutions are keeping up with technology?
* Given that our actions seem to have very large and morally significant effects in the long run, are our moral obligations very onerous?
* Can art be intrinsically valuable?
* What does Tyler think Derek Parfit was most wrong about, and what was he was most right about that’s unappreciated today?

Get this episode by subscribing: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>I've probably spent more time reading Tyler Cowen - Professor of Economics at George Mason University - than any other author. Indeed it's his incredibly popular blog Marginal Revolution that prompted me to study economics in the first place. Having spent</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#44 - Paul Christiano on how we'll hand the future off to AI, &amp; solving the alignment problem</title>
      <itunes:title>#44 - Paul Christiano on how we'll hand the future off to AI, &amp; solving the alignment problem</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/507858087</guid>
      <link>https://share.transistor.fm/s/a77690b9</link>
      <description>
        <![CDATA[<p>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening - Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.</p><p> A few of the topics we cover are:</p><p> * Why Paul expects AI to transform the world gradually rather than explosively and what that would look like<br> * Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us<br> * Why AI systems will probably be granted legal and property rights<br> * How an advanced AI that doesn't share human goals could still have moral value<br> * Why machine learning might take over science research from humans before it can do most other tasks<br> * Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.</p><p> <a href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Important new article: <a href="https://80k.link/high-impact-careers-paul"><strong>These are the world’s highest impact career paths according to our research</strong></a></p><p> Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.</p><p> If given plenty of time - and enough arguments, counterarguments and counter-counter-arguments between all the experts - should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?</p><p> In other words: does 'debate', in principle, lead to truth?</p><p> According to Paul Christiano - researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities - this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.</p><p> It's a method OpenAI is <a href="https://blog.openai.com/debate/">actively trying to develop</a>, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight.</p><p> If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.</p><p> But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.</p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening - Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.</p><p> A few of the topics we cover are:</p><p> * Why Paul expects AI to transform the world gradually rather than explosively and what that would look like<br> * Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us<br> * Why AI systems will probably be granted legal and property rights<br> * How an advanced AI that doesn't share human goals could still have moral value<br> * Why machine learning might take over science research from humans before it can do most other tasks<br> * Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.</p><p> <a href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> Important new article: <a href="https://80k.link/high-impact-careers-paul"><strong>These are the world’s highest impact career paths according to our research</strong></a></p><p> Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.</p><p> If given plenty of time - and enough arguments, counterarguments and counter-counter-arguments between all the experts - should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?</p><p> In other words: does 'debate', in principle, lead to truth?</p><p> According to Paul Christiano - researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities - this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.</p><p> It's a method OpenAI is <a href="https://blog.openai.com/debate/">actively trying to develop</a>, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight.</p><p> If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.</p><p> But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.</p><p> <strong>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 02 Oct 2018 16:25:06 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a77690b9/42307291.mp3" length="222807053" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/yHOXFSxT8JGCcFpq9vMLPg-CpardAkZq1XqOrmxRrz0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjMv/MTY4MzU0NDU4Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>13911</itunes:duration>
      <itunes:summary>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening - Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.

A few of the topics we cover are:

* Why Paul expects AI to transform the world gradually rather than explosively and what that would look like
* Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us
* Why AI systems will probably be granted legal and property rights
* How an advanced AI that doesn't share human goals could still have moral value
* Why machine learning might take over science research from humans before it can do most other tasks
* Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.

Links to learn more, summary and full transcript.

Important new article: These are the world’s highest impact career paths according to our research

Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.

If given plenty of time - and enough arguments, counterarguments and counter-counter-arguments between all the experts - should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?

In other words: does 'debate', in principle, lead to truth?

According to Paul Christiano - researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities - this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.

It's a method OpenAI is actively trying to develop, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight. 

If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.

But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.

Get this episode by subscribing: type '80,000 Hours' into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening - Pau</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#43 - Daniel Ellsberg on the institutional insanity that maintains nuclear doomsday machines</title>
      <itunes:title>#43 - Daniel Ellsberg on the institutional insanity that maintains nuclear doomsday machines</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/504884409</guid>
      <link>https://share.transistor.fm/s/cb708273</link>
      <description>
        <![CDATA[In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. <p>

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his new book <a href="https://www.amazon.com/Doomsday-Machine-Confessions-Nuclear-Planner/dp/1608196704/">The Doomsday Machine: Confessions of a Nuclear War Planner</a> that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.</p><p>
<a href="https://80000hours.org/podcast/episodes/daniel-ellsberg-doomsday-machines/?utm_campaign=podcast__daniel-ellsberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. </p><p>

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.</p><p>

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.</p><p>

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.</p><p>

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.</p><p>

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.</p><p>

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.</p><p>

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.</p><p>

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.</p><p>
Strategically, the setup is stupid. Ethically, it is monstrous.</p><p>
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?</p><p>

Daniel explores these questions eloquently and urgently in his book. Today we cover:</p><p>

* Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold<br>
* How well are secrets kept in the government?<br>
* What was the risk of the first atomic bomb test?<br>
* The effect of Trump on nuclear security<br>
* Do we have a reliable estimate of the magnitude of a ‘nuclear winter’?<br>
* Why Gorbachev allowed Russia’s covert biological warfare program to continue</p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. <p>

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his new book <a href="https://www.amazon.com/Doomsday-Machine-Confessions-Nuclear-Planner/dp/1608196704/">The Doomsday Machine: Confessions of a Nuclear War Planner</a> that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.</p><p>
<a href="https://80000hours.org/podcast/episodes/daniel-ellsberg-doomsday-machines/?utm_campaign=podcast__daniel-ellsberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. </p><p>

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.</p><p>

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.</p><p>

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.</p><p>

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.</p><p>

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.</p><p>

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.</p><p>

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.</p><p>

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.</p><p>
Strategically, the setup is stupid. Ethically, it is monstrous.</p><p>
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?</p><p>

Daniel explores these questions eloquently and urgently in his book. Today we cover:</p><p>

* Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold<br>
* How well are secrets kept in the government?<br>
* What was the risk of the first atomic bomb test?<br>
* The effect of Trump on nuclear security<br>
* Do we have a reliable estimate of the magnitude of a ‘nuclear winter’?<br>
* Why Gorbachev allowed Russia’s covert biological warfare program to continue</p><p>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 25 Sep 2018 09:36:40 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cb708273/b7e7059d.mp3" length="158045466" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/dwQ-laRAGIDg3gfdO8YE31VRsFBBGh8Em4-fThm_wqs/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjIv/MTY4MzU0NDU4NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9868</itunes:duration>
      <itunes:summary>In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. 

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his new book The Doomsday Machine: Confessions of a Nuclear War Planner that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.
Links to learn more, summary and full transcript.

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. 

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.
Strategically, the setup is stupid. Ethically, it is monstrous.
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?

Daniel explores these questions eloquently and urgently in his book. Today we cover:

* Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold
* How well are secrets kept in the government?
* What was the risk of the first atomic bomb test?
* The effect of Trump on nuclear security
* Do we have a reliable estimate of the magnitude of a ‘nuclear winter’?
* Why Gorbachev allowed Russia’s covert biological warfare program to continue

Get this episode by subscribing: type 80,000 Hours into your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bo</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#42 - Amanda Askell on moral empathy, the value of information &amp; the ethics of infinity</title>
      <itunes:title>#42 - Amanda Askell on moral empathy, the value of information &amp; the ethics of infinity</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/497939877</guid>
      <link>https://share.transistor.fm/s/dcd05e58</link>
      <description>
        <![CDATA[<p>Consider two familiar moments at a family reunion.</p><p> Our host, Uncle Bill, takes pride in his barbecuing skills. But his niece Becky says that she now refuses to eat meat. A groan goes round the table; the family mostly think of this as an annoying picky preference. But if seriously considered as a moral position, as they might if instead Becky were avoiding meat on religious grounds, it would usually receive a very different reaction.</p><p> An hour later Bill expresses a strong objection to abortion. Again, a groan goes round the table; the family mostly think that he has no business in trying to foist his regressive preference on anyone. But if considered not as a matter of personal taste, but rather as a moral position - that Bill genuinely believes he’s opposing mass-murder - his comment might start a serious conversation.</p><p> Amanda Askell, who recently completed a PhD in philosophy at NYU focused on the ethics of infinity, thinks that we often betray a complete lack of moral empathy. All sides of the political spectrum struggle to get inside the mind of people we disagree with and see issues from their point of view.</p><p> <a href="https://80k.link/amanda"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This often happens because of confusion between preferences and moral positions.</p><p> Assuming good faith on the part of the person you disagree with, and actually engaging with the beliefs they claim to hold, is perhaps the best remedy for our inability to make progress on controversial issues. </p><p> One potential path for progress surrounds contraception; a lot of people who are anti-abortion are also anti-contraception. But they’ll usually think that abortion is much worse than contraception, so why can’t we compromise and agree to have much more contraception available?</p><p> <a href="http://www.rationalreflection.net/vegetarianism-abortion-and-moral-empathy/">According to Amanda</a>, a charitable explanation for this is that people who are anti-abortion and anti-contraception engage in moral reasoning and advocacy based on what, in their minds, is the best of all possible worlds: one where people neither use contraception nor get abortions.</p><p> So instead of arguing about abortion and contraception, we could discuss the underlying principle that one should advocate for the best possible world, rather than the best probable world.</p><p> Successfully break down such ethical beliefs, absent political toxicity, and it might be possible to actually converge on a key point of agreement.</p><p> Today’s episode blends such everyday topics with in-depth philosophy, including:</p><p> * What is 'moral cluelessness' and how can we work around it?<br> * Amanda's biggest criticisms of social justice activists, and of critics of social justice activists<br> * Is there an ethical difference between prison and corporal punishment?<br> * How to resolve 'infinitarian paralysis' - the inability to make decisions when infinities are involved.<br> * What’s effective altruism doing wrong?<br> * How should we think about jargon? Are a lot of people who don’t communicate clearly just scamming us?<br> * How can people be more successful within the cocoon of school and university?<br> * How did Amanda find doing a philosophy PhD, and how will she decide what to do now?</p><p> <strong>Links:</strong><br> <a href="https://80k.link/congressional-staffer-amanda">* Career review: Congressional staffer</a><br> <a href="https://80k.link/quitting">* Randomised experiment on quitting</a><br> <a href="https://80k.link/psychology-replication-quiz-amanda">* Psychology replication quiz</a><br> <a href="https://80k.link/comparative-advantage-amanda">* Should you focus on your comparative advantage.</a></p><p> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Consider two familiar moments at a family reunion.</p><p> Our host, Uncle Bill, takes pride in his barbecuing skills. But his niece Becky says that she now refuses to eat meat. A groan goes round the table; the family mostly think of this as an annoying picky preference. But if seriously considered as a moral position, as they might if instead Becky were avoiding meat on religious grounds, it would usually receive a very different reaction.</p><p> An hour later Bill expresses a strong objection to abortion. Again, a groan goes round the table; the family mostly think that he has no business in trying to foist his regressive preference on anyone. But if considered not as a matter of personal taste, but rather as a moral position - that Bill genuinely believes he’s opposing mass-murder - his comment might start a serious conversation.</p><p> Amanda Askell, who recently completed a PhD in philosophy at NYU focused on the ethics of infinity, thinks that we often betray a complete lack of moral empathy. All sides of the political spectrum struggle to get inside the mind of people we disagree with and see issues from their point of view.</p><p> <a href="https://80k.link/amanda"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> This often happens because of confusion between preferences and moral positions.</p><p> Assuming good faith on the part of the person you disagree with, and actually engaging with the beliefs they claim to hold, is perhaps the best remedy for our inability to make progress on controversial issues. </p><p> One potential path for progress surrounds contraception; a lot of people who are anti-abortion are also anti-contraception. But they’ll usually think that abortion is much worse than contraception, so why can’t we compromise and agree to have much more contraception available?</p><p> <a href="http://www.rationalreflection.net/vegetarianism-abortion-and-moral-empathy/">According to Amanda</a>, a charitable explanation for this is that people who are anti-abortion and anti-contraception engage in moral reasoning and advocacy based on what, in their minds, is the best of all possible worlds: one where people neither use contraception nor get abortions.</p><p> So instead of arguing about abortion and contraception, we could discuss the underlying principle that one should advocate for the best possible world, rather than the best probable world.</p><p> Successfully break down such ethical beliefs, absent political toxicity, and it might be possible to actually converge on a key point of agreement.</p><p> Today’s episode blends such everyday topics with in-depth philosophy, including:</p><p> * What is 'moral cluelessness' and how can we work around it?<br> * Amanda's biggest criticisms of social justice activists, and of critics of social justice activists<br> * Is there an ethical difference between prison and corporal punishment?<br> * How to resolve 'infinitarian paralysis' - the inability to make decisions when infinities are involved.<br> * What’s effective altruism doing wrong?<br> * How should we think about jargon? Are a lot of people who don’t communicate clearly just scamming us?<br> * How can people be more successful within the cocoon of school and university?<br> * How did Amanda find doing a philosophy PhD, and how will she decide what to do now?</p><p> <strong>Links:</strong><br> <a href="https://80k.link/congressional-staffer-amanda">* Career review: Congressional staffer</a><br> <a href="https://80k.link/quitting">* Randomised experiment on quitting</a><br> <a href="https://80k.link/psychology-replication-quiz-amanda">* Psychology replication quiz</a><br> <a href="https://80k.link/comparative-advantage-amanda">* Should you focus on your comparative advantage.</a></p><p> <strong>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</strong></p><p> <em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 11 Sep 2018 18:21:28 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/dcd05e58/5219b275.mp3" length="160003268" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ih7e1jZh7rw1004RMRsmF_-BD5WZfrjWSXu6oqmTw9k/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjEv/MTY4MzU0NDU4NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9989</itunes:duration>
      <itunes:summary>Consider two familiar moments at a family reunion.

Our host, Uncle Bill, takes pride in his barbecuing skills. But his niece Becky says that she now refuses to eat meat. A groan goes round the table; the family mostly think of this as an annoying picky preference. But if seriously considered as a moral position, as they might if instead Becky were avoiding meat on religious grounds, it would usually receive a very different reaction.

An hour later Bill expresses a strong objection to abortion. Again, a groan goes round the table; the family mostly think that he has no business in trying to foist his regressive preference on anyone. But if considered not as a matter of personal taste, but rather as a moral position - that Bill genuinely believes he’s opposing mass-murder - his comment might start a serious conversation.

Amanda Askell, who recently completed a PhD in philosophy at NYU focused on the ethics of infinity, thinks that we often betray a complete lack of moral empathy. All sides of the political spectrum struggle to get inside the mind of people we disagree with and see issues from their point of view.

Links to learn more, summary and full transcript.

This often happens because of confusion between preferences and moral positions.

Assuming good faith on the part of the person you disagree with, and actually engaging with the beliefs they claim to hold, is perhaps the best remedy for our inability to make progress on controversial issues. 

One potential path for progress surrounds contraception; a lot of people who are anti-abortion are also anti-contraception. But they’ll usually think that abortion is much worse than contraception, so why can’t we compromise and agree to have much more contraception available?

According to Amanda, a charitable explanation for this is that people who are anti-abortion and anti-contraception engage in moral reasoning and advocacy based on what, in their minds, is the best of all possible worlds: one where people neither use contraception nor get abortions.

So instead of arguing about abortion and contraception, we could discuss the underlying principle that one should advocate for the best possible world, rather than the best probable world.

Successfully break down such ethical beliefs, absent political toxicity, and it might be possible to actually converge on a key point of agreement.

Today’s episode blends such everyday topics with in-depth philosophy, including:

* What is 'moral cluelessness' and how can we work around it?
* Amanda's biggest criticisms of social justice activists, and of critics of social justice activists
* Is there an ethical difference between prison and corporal punishment?
* How to resolve 'infinitarian paralysis' - the inability to make decisions when infinities are involved.
* What’s effective altruism doing wrong?
* How should we think about jargon? Are a lot of people who don’t communicate clearly just scamming us?
* How can people be more successful within the cocoon of school and university?
* How did Amanda find doing a philosophy PhD, and how will she decide what to do now?

Links:
* Career review: Congressional staffer
* Randomised experiment on quitting
* Psychology replication quiz
* Should you focus on your comparative advantage.

Get this episode by subscribing: type 80,000 Hours into your podcasting app.

The 80,000 Hours podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Consider two familiar moments at a family reunion.

Our host, Uncle Bill, takes pride in his barbecuing skills. But his niece Becky says that she now refuses to eat meat. A groan goes round the table; the family mostly think of this as an annoying picky</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#41 - David Roodman on incarceration, geomagnetic storms, &amp; becoming a world-class researcher</title>
      <itunes:title>#41 - David Roodman on incarceration, geomagnetic storms, &amp; becoming a world-class researcher</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/491806653</guid>
      <link>https://share.transistor.fm/s/b4b89fc8</link>
      <description>
        <![CDATA[With 698 inmates per 100,000 citizens, the U.S. is by far the leader among large wealthy nations in incarceration. But what effect does imprisonment actually have on crime?<br><br>

According to David Roodman, Senior Advisor to the Open Philanthropy Project, the marginal effect is zero.<br><br>

* <a href="https://80k.link/impactsurveyDR"><b>80,000 HOURS IMPACT SURVEY</b></a> - Let me know how this show has helped you with your career.<br>
* <a href="https://80k.link/audiobook"><b>ROB'S AUDIOBOOK RECOMMENDATIONS</b></a><p>

This stunning rebuke to the American criminal justice system comes from the man Holden Karnofsky’s called "the gold standard for in-depth quantitative research", whose other investigations include the risk of geomagnetic storms, whether deworming improves health and test scores, and the development impacts of microfinance.<br><br>

<a href="https://80000hours.org/podcast/episodes/david-roodman-becoming-a-world-class-researcher/?utm_campaign=podcast__david-roodman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The effects of crime can be split into three categories; before, during, and after.<br><br>

Does having tougher sentences deter people from committing crime?<br><br> 

After reviewing studies on gun laws and ‘three strikes’ in California, David concluded that the effect of deterrence is zero.<br><br>

Does imprisoning more people reduce crime by incapacitating potential offenders?<br><br> 

Here he says yes, noting that crimes like motor vehicle theft have gone up in a way that seems pretty clearly connected with recent Californian criminal justice reforms (though the effect on violent crime is far lower).<br><br>

Finally, do the after-effects of prison make you more or less likely to commit future crimes?<br><br> 

This one is more complicated.<br><br>

Concerned that he was biased towards a comfortable position against incarceration, David did a cost-benefit analysis using both his favored reading of the evidence and the devil's advocate view; that there is deterrence and that the after-effects are beneficial.<br><br>

For the devil’s advocate position David used the highest assessment of  the harm caused by crime, which suggests a year of prison prevents about $92,000 in crime. But weighed against a lost year of liberty, valued at $50,000, plus the cost of operating prisons, the numbers came out exactly the same.<br><br> 

So even using the least-favorable cost-benefit valuation of the least favorable reading of the evidence -- it just breaks even.<br><br>

The argument for incarceration melts further when you consider the significant crime that occurs within prisons, de-emphasised because of a lack of data and a perceived lack of compassion for inmates.<br><br>

In today’s episode we discuss how to conduct such impactful research, and how to proceed having reached strong conclusions.<br><br>

We also cover:<br><br>

* How do you become a world class researcher? What kinds of character traits are important?<br>
* Are academics aware of following perverse incentives?<br>
* What’s involved in data replication? How often do papers replicate?<br>
* The politics of large orgs vs. small orgs<br>
* Geomagnetic storms as a potential cause area<br>
* How much does David rely on interviews with experts?<br>
* The effects of deworming on child health and test scores<br>
* Should we have more ‘data vigilantes’?<br>
* What are David’s critiques of effective altruism?<br>
* What are the pros and cons of starting your career in the think tank world?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[With 698 inmates per 100,000 citizens, the U.S. is by far the leader among large wealthy nations in incarceration. But what effect does imprisonment actually have on crime?<br><br>

According to David Roodman, Senior Advisor to the Open Philanthropy Project, the marginal effect is zero.<br><br>

* <a href="https://80k.link/impactsurveyDR"><b>80,000 HOURS IMPACT SURVEY</b></a> - Let me know how this show has helped you with your career.<br>
* <a href="https://80k.link/audiobook"><b>ROB'S AUDIOBOOK RECOMMENDATIONS</b></a><p>

This stunning rebuke to the American criminal justice system comes from the man Holden Karnofsky’s called "the gold standard for in-depth quantitative research", whose other investigations include the risk of geomagnetic storms, whether deworming improves health and test scores, and the development impacts of microfinance.<br><br>

<a href="https://80000hours.org/podcast/episodes/david-roodman-becoming-a-world-class-researcher/?utm_campaign=podcast__david-roodman&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

The effects of crime can be split into three categories; before, during, and after.<br><br>

Does having tougher sentences deter people from committing crime?<br><br> 

After reviewing studies on gun laws and ‘three strikes’ in California, David concluded that the effect of deterrence is zero.<br><br>

Does imprisoning more people reduce crime by incapacitating potential offenders?<br><br> 

Here he says yes, noting that crimes like motor vehicle theft have gone up in a way that seems pretty clearly connected with recent Californian criminal justice reforms (though the effect on violent crime is far lower).<br><br>

Finally, do the after-effects of prison make you more or less likely to commit future crimes?<br><br> 

This one is more complicated.<br><br>

Concerned that he was biased towards a comfortable position against incarceration, David did a cost-benefit analysis using both his favored reading of the evidence and the devil's advocate view; that there is deterrence and that the after-effects are beneficial.<br><br>

For the devil’s advocate position David used the highest assessment of  the harm caused by crime, which suggests a year of prison prevents about $92,000 in crime. But weighed against a lost year of liberty, valued at $50,000, plus the cost of operating prisons, the numbers came out exactly the same.<br><br> 

So even using the least-favorable cost-benefit valuation of the least favorable reading of the evidence -- it just breaks even.<br><br>

The argument for incarceration melts further when you consider the significant crime that occurs within prisons, de-emphasised because of a lack of data and a perceived lack of compassion for inmates.<br><br>

In today’s episode we discuss how to conduct such impactful research, and how to proceed having reached strong conclusions.<br><br>

We also cover:<br><br>

* How do you become a world class researcher? What kinds of character traits are important?<br>
* Are academics aware of following perverse incentives?<br>
* What’s involved in data replication? How often do papers replicate?<br>
* The politics of large orgs vs. small orgs<br>
* Geomagnetic storms as a potential cause area<br>
* How much does David rely on interviews with experts?<br>
* The effects of deworming on child health and test scores<br>
* Should we have more ‘data vigilantes’?<br>
* What are David’s critiques of effective altruism?<br>
* What are the pros and cons of starting your career in the think tank world?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 28 Aug 2018 19:18:23 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b4b89fc8/c2bcfe38.mp3" length="132639332" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/277_Oe051qSVNvvRnmxKIW4GDmUNsusYsE1EWLokZ-g/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NjAv/MTY4MzU0NDU4My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8281</itunes:duration>
      <itunes:summary>With 698 inmates per 100,000 citizens, the U.S. is by far the leader among large wealthy nations in incarceration. But what effect does imprisonment actually have on crime?

According to David Roodman, Senior Advisor to the Open Philanthropy Project, the marginal effect is zero.

* 80,000 HOURS IMPACT SURVEY - Let me know how this show has helped you with your career.
* ROB'S AUDIOBOOK RECOMMENDATIONS

This stunning rebuke to the American criminal justice system comes from the man Holden Karnofsky’s called "the gold standard for in-depth quantitative research", whose other investigations include the risk of geomagnetic storms, whether deworming improves health and test scores, and the development impacts of microfinance.

Links to learn more, summary and full transcript.

The effects of crime can be split into three categories; before, during, and after.

Does having tougher sentences deter people from committing crime? 

After reviewing studies on gun laws and ‘three strikes’ in California, David concluded that the effect of deterrence is zero.

Does imprisoning more people reduce crime by incapacitating potential offenders? 

Here he says yes, noting that crimes like motor vehicle theft have gone up in a way that seems pretty clearly connected with recent Californian criminal justice reforms (though the effect on violent crime is far lower).

Finally, do the after-effects of prison make you more or less likely to commit future crimes? 

This one is more complicated.

Concerned that he was biased towards a comfortable position against incarceration, David did a cost-benefit analysis using both his favored reading of the evidence and the devil's advocate view; that there is deterrence and that the after-effects are beneficial.

For the devil’s advocate position David used the highest assessment of  the harm caused by crime, which suggests a year of prison prevents about $92,000 in crime. But weighed against a lost year of liberty, valued at $50,000, plus the cost of operating prisons, the numbers came out exactly the same. 

So even using the least-favorable cost-benefit valuation of the least favorable reading of the evidence -- it just breaks even.

The argument for incarceration melts further when you consider the significant crime that occurs within prisons, de-emphasised because of a lack of data and a perceived lack of compassion for inmates.

In today’s episode we discuss how to conduct such impactful research, and how to proceed having reached strong conclusions.

We also cover:

* How do you become a world class researcher? What kinds of character traits are important?
* Are academics aware of following perverse incentives?
* What’s involved in data replication? How often do papers replicate?
* The politics of large orgs vs. small orgs
* Geomagnetic storms as a potential cause area
* How much does David rely on interviews with experts?
* The effects of deworming on child health and test scores
* Should we have more ‘data vigilantes’?
* What are David’s critiques of effective altruism?
* What are the pros and cons of starting your career in the think tank world?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>With 698 inmates per 100,000 citizens, the U.S. is by far the leader among large wealthy nations in incarceration. But what effect does imprisonment actually have on crime?

According to David Roodman, Senior Advisor to the Open Philanthropy Project, the </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#40 - Katja Grace on forecasting future technology &amp; how much we should trust expert predictions</title>
      <itunes:title>#40 - Katja Grace on forecasting future technology &amp; how much we should trust expert predictions</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/488487222</guid>
      <link>https://share.transistor.fm/s/3ff37082</link>
      <description>
        <![CDATA[Experts believe that artificial intelligence will be better than humans at driving trucks by 2027, working in retail by 2031, writing bestselling books by 2049, and working as surgeons by 2053. But how seriously should we take these predictions?<br><br>

Katja Grace, lead author of <a href="https://arxiv.org/pdf/1705.08807.pdf?_sp=c803ec8d-9f8f-4843-a81e-3284733403a0.1500631875031" rel="nofollow">‘When Will AI Exceed Human Performance?’</a>, thinks we should treat such guesses as only weak evidence. But she also says there might be much better ways to forecast transformative technology, and that anticipating such advances could be one of our most important projects.<br><br>

<b>Note: Katja's organisation AI Impacts <a href="https://aiimpacts.org/jobs/">is currently hiring part- and full-time researchers.</a></b><p>

There’s often pessimism around making accurate predictions in general, and some areas of artificial intelligence might be particularly difficult to forecast.<br><br> 

But there are also many things we’re able to predict confidently today -- like the climate of Oxford in five years -- that we no longer give ourselves much credit for.<br><br>

Some aspects of transformative technologies could fall into this category. And these easier predictions could give us some structure on which to base the more complicated ones.<br><br>

<a href="https://80000hours.org/podcast/episodes/katja-grace-forecasting-technology/?utm_campaign=podcast__katja-grace&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

One controversial debate surrounds the idea of an intelligence explosion; how likely is it that there will be a sudden jump in AI capability?<br><br>

And one way to tackle this is to investigate a more concrete question: what’s the base rate of any technology having a big discontinuity?<br><br>

A significant historical example was the development of nuclear weapons. Over thousands of years, the efficacy of explosives didn’t increase by much. Then within a few years, it got thousands of times better. Discovering what leads to such anomalies may allow us to better predict the possibility of a similar jump in AI capabilities.<br><br>

In today’s interview we also discuss:</p><p>

* Why is AI impacts one of the most important projects in the world?<br> 
* How do you structure important surveys? Why do you get such different answers when asking what seem to be very similar questions?<br>
* How does writing an academic paper differ from posting a summary online?<br>
* When will unguided machines be able to produce better and cheaper work than humans for every possible task?<br>
* What’s one of the most likely jobs to be automated soon?<br>
* Are people always just predicting the same timelines for new technologies?<br>
* How do AGI researchers different from other AI researchers in their predictions?<br>
* What are attitudes to safety research like within ML? Are there regional differences?<br>
* How much should we believe experts generally?<br>
* How does the human brain compare to our best supercomputers? How many human brains are worth all the hardware in the world?<br>
* How quickly has the processing capacity for machine learning problems been increasing?<br>
* What can we learn from the development of previous technologies in figuring out how fast transformative AI will arrive?<br>
* What should we expect from a post AI dominated economy?<br>
* How much influence can people ever have on things that will happen in 20 years? Are there any examples of people really trying to do this?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Experts believe that artificial intelligence will be better than humans at driving trucks by 2027, working in retail by 2031, writing bestselling books by 2049, and working as surgeons by 2053. But how seriously should we take these predictions?<br><br>

Katja Grace, lead author of <a href="https://arxiv.org/pdf/1705.08807.pdf?_sp=c803ec8d-9f8f-4843-a81e-3284733403a0.1500631875031" rel="nofollow">‘When Will AI Exceed Human Performance?’</a>, thinks we should treat such guesses as only weak evidence. But she also says there might be much better ways to forecast transformative technology, and that anticipating such advances could be one of our most important projects.<br><br>

<b>Note: Katja's organisation AI Impacts <a href="https://aiimpacts.org/jobs/">is currently hiring part- and full-time researchers.</a></b><p>

There’s often pessimism around making accurate predictions in general, and some areas of artificial intelligence might be particularly difficult to forecast.<br><br> 

But there are also many things we’re able to predict confidently today -- like the climate of Oxford in five years -- that we no longer give ourselves much credit for.<br><br>

Some aspects of transformative technologies could fall into this category. And these easier predictions could give us some structure on which to base the more complicated ones.<br><br>

<a href="https://80000hours.org/podcast/episodes/katja-grace-forecasting-technology/?utm_campaign=podcast__katja-grace&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a></p><p>

One controversial debate surrounds the idea of an intelligence explosion; how likely is it that there will be a sudden jump in AI capability?<br><br>

And one way to tackle this is to investigate a more concrete question: what’s the base rate of any technology having a big discontinuity?<br><br>

A significant historical example was the development of nuclear weapons. Over thousands of years, the efficacy of explosives didn’t increase by much. Then within a few years, it got thousands of times better. Discovering what leads to such anomalies may allow us to better predict the possibility of a similar jump in AI capabilities.<br><br>

In today’s interview we also discuss:</p><p>

* Why is AI impacts one of the most important projects in the world?<br> 
* How do you structure important surveys? Why do you get such different answers when asking what seem to be very similar questions?<br>
* How does writing an academic paper differ from posting a summary online?<br>
* When will unguided machines be able to produce better and cheaper work than humans for every possible task?<br>
* What’s one of the most likely jobs to be automated soon?<br>
* Are people always just predicting the same timelines for new technologies?<br>
* How do AGI researchers different from other AI researchers in their predictions?<br>
* What are attitudes to safety research like within ML? Are there regional differences?<br>
* How much should we believe experts generally?<br>
* How does the human brain compare to our best supercomputers? How many human brains are worth all the hardware in the world?<br>
* How quickly has the processing capacity for machine learning problems been increasing?<br>
* What can we learn from the development of previous technologies in figuring out how fast transformative AI will arrive?<br>
* What should we expect from a post AI dominated economy?<br>
* How much influence can people ever have on things that will happen in 20 years? Are there any examples of people really trying to do this?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 21 Aug 2018 18:37:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/3ff37082/0cb01a54.mp3" length="126159992" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/gw200BLUQ8hwWy_CQaXb3on2YHU3nzoSMOQpG01oNuw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTkv/MTY4MzU0NDU4Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7878</itunes:duration>
      <itunes:summary>Experts believe that artificial intelligence will be better than humans at driving trucks by 2027, working in retail by 2031, writing bestselling books by 2049, and working as surgeons by 2053. But how seriously should we take these predictions?

Katja Grace, lead author of ‘When Will AI Exceed Human Performance?’, thinks we should treat such guesses as only weak evidence. But she also says there might be much better ways to forecast transformative technology, and that anticipating such advances could be one of our most important projects.

Note: Katja's organisation AI Impacts is currently hiring part- and full-time researchers.

There’s often pessimism around making accurate predictions in general, and some areas of artificial intelligence might be particularly difficult to forecast. 

But there are also many things we’re able to predict confidently today -- like the climate of Oxford in five years -- that we no longer give ourselves much credit for.

Some aspects of transformative technologies could fall into this category. And these easier predictions could give us some structure on which to base the more complicated ones.

Links to learn more, summary and full transcript.

One controversial debate surrounds the idea of an intelligence explosion; how likely is it that there will be a sudden jump in AI capability?

And one way to tackle this is to investigate a more concrete question: what’s the base rate of any technology having a big discontinuity?

A significant historical example was the development of nuclear weapons. Over thousands of years, the efficacy of explosives didn’t increase by much. Then within a few years, it got thousands of times better. Discovering what leads to such anomalies may allow us to better predict the possibility of a similar jump in AI capabilities.

In today’s interview we also discuss:

* Why is AI impacts one of the most important projects in the world? 
* How do you structure important surveys? Why do you get such different answers when asking what seem to be very similar questions?
* How does writing an academic paper differ from posting a summary online?
* When will unguided machines be able to produce better and cheaper work than humans for every possible task?
* What’s one of the most likely jobs to be automated soon?
* Are people always just predicting the same timelines for new technologies?
* How do AGI researchers different from other AI researchers in their predictions?
* What are attitudes to safety research like within ML? Are there regional differences?
* How much should we believe experts generally?
* How does the human brain compare to our best supercomputers? How many human brains are worth all the hardware in the world?
* How quickly has the processing capacity for machine learning problems been increasing?
* What can we learn from the development of previous technologies in figuring out how fast transformative AI will arrive?
* What should we expect from a post AI dominated economy?
* How much influence can people ever have on things that will happen in 20 years? Are there any examples of people really trying to do this?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.

The 80,000 Hours podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Experts believe that artificial intelligence will be better than humans at driving trucks by 2027, working in retail by 2031, writing bestselling books by 2049, and working as surgeons by 2053. But how seriously should we take these predictions?

Katja Gr</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#39 - Spencer Greenberg on the scientific approach to solving difficult everyday questions</title>
      <itunes:title>#39 - Spencer Greenberg on the scientific approach to solving difficult everyday questions</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/482232300</guid>
      <link>https://share.transistor.fm/s/eab0441e</link>
      <description>
        <![CDATA[Will Trump be re-elected? Will North Korea give up their nuclear weapons? Will your friend turn up to dinner?<br><br>

Spencer Greenberg, founder of <a href="https://www.clearerthinking.org" rel="nofollow">ClearerThinking.org</a> has a process for working out such real life problems.<br><br>

Let’s work through one here: how likely is it that you’ll enjoy listening to this episode?<br><br>

The first step is to figure out your ‘prior probability’; what’s your estimate of how likely you are to enjoy the interview before getting any further evidence?<br><br>

Other than applying common sense, one way to figure this out is called reference class forecasting: looking at similar cases and seeing how often something is true, on average.<br><br>

Spencer is our first ever return guest. So one reference class might be, how many Spencer Greenberg episodes of the 80,000 Hours Podcast have you enjoyed so far? Being this specific limits bias in your answer, but with a sample size of at most 1 - you’d probably want to add more data points to reduce variability.<br><br>

Zooming out, how many episodes of the 80,000 Hours Podcast have you enjoyed? Let’s say you’ve listened to 10, and enjoyed 8 of them. If so 8 out of 10 might be your prior probability.<br><br>

But maybe the two you didn’t enjoy had something in common. If you’ve liked similar episodes in the past, you’d update in favour of expecting to enjoy it, and if you’ve disliked similar episodes in the past, you’d update negatively.<br><br>

You can zoom out further; what fraction of long-form interview podcasts have you ever enjoyed?<br><br>

Then you’d look to update whenever new information became available. Do the topics seem interesting? Did Spencer make a great point in the first 5 minutes? Was this description unbearably self-referential?<br><br>

Speaking of the Question of Evidence: in a world where Spencer was not worth listening to, how likely is it that we’d invite him back for a second episode?<br><br>

<a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/?utm_campaign=podcast__spencer-greenberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a><p>

We’ll run through several diverse examples, and how to actually work out the changing probabilities as you update. But that’s only a fraction of the conversation. We also discuss:<br><br>

* How could we generate 20-30 new happy thoughts a day? What would that do to our welfare?<br>
* What do people actually value? How do EAs differ from non EAs?<br>
* Why should we care about the distinction between intrinsic and instrumental values?<br>
* Would hedonic utilitarians really want to hook themselves up to happiness machines?<br>
* What types of activities are people generally under-confident about? Why?<br>
* When should you give a lot of weight to your prior belief?<br>
* When should we trust common sense?<br>
* Does power posing have any effect?<br>
* Are resumes worthless?<br>
* Did Trump explicitly collude with Russia? What are the odds of him getting re-elected?<br>
* What’s the probability that China and the US go to War in the 21st century?<br>
* How should we treat claims of expertise on diets?<br>
* Why were Spencer’s friends suspicious of Theranos for years?<br>
* How should we think about the placebo effect?<br>
* Does a shift towards rationality typically cause alienation from family and friends? How do you deal with that?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Will Trump be re-elected? Will North Korea give up their nuclear weapons? Will your friend turn up to dinner?<br><br>

Spencer Greenberg, founder of <a href="https://www.clearerthinking.org" rel="nofollow">ClearerThinking.org</a> has a process for working out such real life problems.<br><br>

Let’s work through one here: how likely is it that you’ll enjoy listening to this episode?<br><br>

The first step is to figure out your ‘prior probability’; what’s your estimate of how likely you are to enjoy the interview before getting any further evidence?<br><br>

Other than applying common sense, one way to figure this out is called reference class forecasting: looking at similar cases and seeing how often something is true, on average.<br><br>

Spencer is our first ever return guest. So one reference class might be, how many Spencer Greenberg episodes of the 80,000 Hours Podcast have you enjoyed so far? Being this specific limits bias in your answer, but with a sample size of at most 1 - you’d probably want to add more data points to reduce variability.<br><br>

Zooming out, how many episodes of the 80,000 Hours Podcast have you enjoyed? Let’s say you’ve listened to 10, and enjoyed 8 of them. If so 8 out of 10 might be your prior probability.<br><br>

But maybe the two you didn’t enjoy had something in common. If you’ve liked similar episodes in the past, you’d update in favour of expecting to enjoy it, and if you’ve disliked similar episodes in the past, you’d update negatively.<br><br>

You can zoom out further; what fraction of long-form interview podcasts have you ever enjoyed?<br><br>

Then you’d look to update whenever new information became available. Do the topics seem interesting? Did Spencer make a great point in the first 5 minutes? Was this description unbearably self-referential?<br><br>

Speaking of the Question of Evidence: in a world where Spencer was not worth listening to, how likely is it that we’d invite him back for a second episode?<br><br>

<a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/?utm_campaign=podcast__spencer-greenberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Links to learn more, summary and full transcript.</b></a><p>

We’ll run through several diverse examples, and how to actually work out the changing probabilities as you update. But that’s only a fraction of the conversation. We also discuss:<br><br>

* How could we generate 20-30 new happy thoughts a day? What would that do to our welfare?<br>
* What do people actually value? How do EAs differ from non EAs?<br>
* Why should we care about the distinction between intrinsic and instrumental values?<br>
* Would hedonic utilitarians really want to hook themselves up to happiness machines?<br>
* What types of activities are people generally under-confident about? Why?<br>
* When should you give a lot of weight to your prior belief?<br>
* When should we trust common sense?<br>
* Does power posing have any effect?<br>
* Are resumes worthless?<br>
* Did Trump explicitly collude with Russia? What are the odds of him getting re-elected?<br>
* What’s the probability that China and the US go to War in the 21st century?<br>
* How should we treat claims of expertise on diets?<br>
* Why were Spencer’s friends suspicious of Theranos for years?<br>
* How should we think about the placebo effect?<br>
* Does a shift towards rationality typically cause alienation from family and friends? How do you deal with that?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 07 Aug 2018 19:51:01 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/eab0441e/66bb926e.mp3" length="132130234" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/RXaACPZNT5Flarnv0wakL2rsmzqWRDfWPsDffoJMbNE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTgv/MTY4MzU0NDU4MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8250</itunes:duration>
      <itunes:summary>Will Trump be re-elected? Will North Korea give up their nuclear weapons? Will your friend turn up to dinner?

Spencer Greenberg, founder of ClearerThinking.org has a process for working out such real life problems.

Let’s work through one here: how likely is it that you’ll enjoy listening to this episode?

The first step is to figure out your ‘prior probability’; what’s your estimate of how likely you are to enjoy the interview before getting any further evidence?

Other than applying common sense, one way to figure this out is called reference class forecasting: looking at similar cases and seeing how often something is true, on average.

Spencer is our first ever return guest. So one reference class might be, how many Spencer Greenberg episodes of the 80,000 Hours Podcast have you enjoyed so far? Being this specific limits bias in your answer, but with a sample size of at most 1 - you’d probably want to add more data points to reduce variability.

Zooming out, how many episodes of the 80,000 Hours Podcast have you enjoyed? Let’s say you’ve listened to 10, and enjoyed 8 of them. If so 8 out of 10 might be your prior probability.

But maybe the two you didn’t enjoy had something in common. If you’ve liked similar episodes in the past, you’d update in favour of expecting to enjoy it, and if you’ve disliked similar episodes in the past, you’d update negatively.

You can zoom out further; what fraction of long-form interview podcasts have you ever enjoyed?

Then you’d look to update whenever new information became available. Do the topics seem interesting? Did Spencer make a great point in the first 5 minutes? Was this description unbearably self-referential?

Speaking of the Question of Evidence: in a world where Spencer was not worth listening to, how likely is it that we’d invite him back for a second episode?

Links to learn more, summary and full transcript.

We’ll run through several diverse examples, and how to actually work out the changing probabilities as you update. But that’s only a fraction of the conversation. We also discuss:

* How could we generate 20-30 new happy thoughts a day? What would that do to our welfare?
* What do people actually value? How do EAs differ from non EAs?
* Why should we care about the distinction between intrinsic and instrumental values?
* Would hedonic utilitarians really want to hook themselves up to happiness machines?
* What types of activities are people generally under-confident about? Why?
* When should you give a lot of weight to your prior belief?
* When should we trust common sense?
* Does power posing have any effect?
* Are resumes worthless?
* Did Trump explicitly collude with Russia? What are the odds of him getting re-elected?
* What’s the probability that China and the US go to War in the 21st century?
* How should we treat claims of expertise on diets?
* Why were Spencer’s friends suspicious of Theranos for years?
* How should we think about the placebo effect?
* Does a shift towards rationality typically cause alienation from family and friends? How do you deal with that?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.

The 80,000 Hours podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Will Trump be re-elected? Will North Korea give up their nuclear weapons? Will your friend turn up to dinner?

Spencer Greenberg, founder of ClearerThinking.org has a process for working out such real life problems.

Let’s work through one here: how likel</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#38 - Yew-Kwang Ng on anticipating effective altruism decades ago &amp; how to make a much happier world</title>
      <itunes:title>#38 - Yew-Kwang Ng on anticipating effective altruism decades ago &amp; how to make a much happier world</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/476762145</guid>
      <link>https://share.transistor.fm/s/81e98828</link>
      <description>
        <![CDATA[<p>Will people who think carefully about how to maximize welfare eventually converge on the same views?</p><p> The effective altruism community has spent a lot of time over the past 10 years debating how best to increase happiness and reduce suffering, and gradually narrowed in on the world’s poorest people, all animals capable of suffering, and future generations.</p><p> Yew-Kwang Ng, Professor of Economics at Nanyang Technological University in Singapore, was independently working on this exact question since the 70s. Many of his conclusions have ended up foreshadowing what is now conventional wisdom within effective altruism - though other views he holds remain controversial or little-known.</p><p> For instance, he thinks we ought to explore increasing pleasure via direct brain stimulation, and that genetic engineering may be an important tool for increasing happiness in the future.</p><p> His work has suggested that the welfare of most wild animals is on balance negative and he thinks that in the future this is a problem humanity might work to solve. Yet he thinks that greatly improved conditions for farm animals could eventually justify eating meat.</p><p> He has spent most of his life advocating for the view that happiness, broadly construed, is the only intrinsically valuable thing.</p><p> If it’s true that careful researchers will converge as Prof Ng believes, these ideas may prove as prescient as his other, now widely accepted, opinions.</p><p> <a href="https://80k.link/kwang-articles"><strong>Link to our summary and appreciation of Kwang’s top publications and insights throughout a lifetime of research.</strong></a></p><p> Kwang has led an exceptional life. While in high school he was drawn to physics, mathematics, and philosophy, yet he chose to study economics because of his dream: to establish communism in an independent Malaya.</p><p> But events in the Soviet Union and China, in addition to his burgeoning knowledge and academic appreciation of economics, would change his views about the practicability of communism. He would soon complete his journey from young revolutionary to academic economist, and eventually become a columnist writing in support of Deng Xiaoping’s Chinese economic reforms in the 80s.</p><p> He got his PhD at Sydney University in 1971, and has since published over 250 refereed papers - covering economics, biology, politics, mathematics, philosophy, psychology, and sociology. </p><p> He's most well-known for his work in welfare economics, and proposed ‘welfare biology’ as a new field of study. In 2007, he was made a Distinguished Fellow of the Economic Society of Australia, the highest award that the society bestows.</p><p> <a href="https://80000hours.org/podcast/episodes/yew-kwang-ng-anticipating-effective-altruism/?utm_campaign=podcast__yew-kwangng&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In this episode we discuss how he developed some of his most unusual ideas and his fascinating life story, including:</p><p> * Why Kwang believes that *’Happiness Is Absolute, Universal, Ultimate, Unidimensional, Cardinally Measurable and Interpersonally Comparable’*<br> * What are the most pressing questions in economics?<br> * Did Kwang have to worry about censorship from the Chinese government when promoting market economics, or concern for animal welfare?<br> * Welfare economics and where Kwang thinks it went wrong</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Will people who think carefully about how to maximize welfare eventually converge on the same views?</p><p> The effective altruism community has spent a lot of time over the past 10 years debating how best to increase happiness and reduce suffering, and gradually narrowed in on the world’s poorest people, all animals capable of suffering, and future generations.</p><p> Yew-Kwang Ng, Professor of Economics at Nanyang Technological University in Singapore, was independently working on this exact question since the 70s. Many of his conclusions have ended up foreshadowing what is now conventional wisdom within effective altruism - though other views he holds remain controversial or little-known.</p><p> For instance, he thinks we ought to explore increasing pleasure via direct brain stimulation, and that genetic engineering may be an important tool for increasing happiness in the future.</p><p> His work has suggested that the welfare of most wild animals is on balance negative and he thinks that in the future this is a problem humanity might work to solve. Yet he thinks that greatly improved conditions for farm animals could eventually justify eating meat.</p><p> He has spent most of his life advocating for the view that happiness, broadly construed, is the only intrinsically valuable thing.</p><p> If it’s true that careful researchers will converge as Prof Ng believes, these ideas may prove as prescient as his other, now widely accepted, opinions.</p><p> <a href="https://80k.link/kwang-articles"><strong>Link to our summary and appreciation of Kwang’s top publications and insights throughout a lifetime of research.</strong></a></p><p> Kwang has led an exceptional life. While in high school he was drawn to physics, mathematics, and philosophy, yet he chose to study economics because of his dream: to establish communism in an independent Malaya.</p><p> But events in the Soviet Union and China, in addition to his burgeoning knowledge and academic appreciation of economics, would change his views about the practicability of communism. He would soon complete his journey from young revolutionary to academic economist, and eventually become a columnist writing in support of Deng Xiaoping’s Chinese economic reforms in the 80s.</p><p> He got his PhD at Sydney University in 1971, and has since published over 250 refereed papers - covering economics, biology, politics, mathematics, philosophy, psychology, and sociology. </p><p> He's most well-known for his work in welfare economics, and proposed ‘welfare biology’ as a new field of study. In 2007, he was made a Distinguished Fellow of the Economic Society of Australia, the highest award that the society bestows.</p><p> <a href="https://80000hours.org/podcast/episodes/yew-kwang-ng-anticipating-effective-altruism/?utm_campaign=podcast__yew-kwangng&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Links to learn more, summary and full transcript.</strong></a></p><p> In this episode we discuss how he developed some of his most unusual ideas and his fascinating life story, including:</p><p> * Why Kwang believes that *’Happiness Is Absolute, Universal, Ultimate, Unidimensional, Cardinally Measurable and Interpersonally Comparable’*<br> * What are the most pressing questions in economics?<br> * Did Kwang have to worry about censorship from the Chinese government when promoting market economics, or concern for animal welfare?<br> * Welfare economics and where Kwang thinks it went wrong</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 26 Jul 2018 00:08:27 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/81e98828/ead63371.mp3" length="114845958" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/kgmOo1p41HA-SsGl34Pa4UyRJRH5TgcnqV0luICwmlQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTcv/MTY4MzU0NDU4MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7170</itunes:duration>
      <itunes:summary>Will people who think carefully about how to maximize welfare eventually converge on the same views?

The effective altruism community has spent a lot of time over the past 10 years debating how best to increase happiness and reduce suffering, and gradually narrowed in on the world’s poorest people, all animals capable of suffering, and future generations.

Yew-Kwang Ng, Professor of Economics at Nanyang Technological University in Singapore,  was independently working on this exact question since the 70s. Many of his conclusions have ended up foreshadowing what is now conventional wisdom within effective altruism - though other views he holds remain controversial or little-known.

For instance, he thinks we ought to explore increasing pleasure via direct brain stimulation, and that genetic engineering may be an important tool for increasing happiness in the future.

His work has suggested that the welfare of most wild animals is on balance negative and he thinks that in the future this is a problem humanity might work to solve. Yet he thinks that greatly improved conditions for farm animals could eventually justify eating meat.

He has spent most of his life advocating for the view that happiness, broadly construed, is the only intrinsically valuable thing.

If it’s true that careful researchers will converge as Prof Ng believes, these ideas may prove as prescient as his other, now widely accepted, opinions.

Link to our summary and appreciation of Kwang’s top publications and insights throughout a lifetime of research.

Kwang has led an exceptional life. While in high school he was drawn to physics, mathematics, and philosophy, yet he chose to study economics because of his dream: to establish communism in an independent Malaya.

But events in the Soviet Union and China, in addition to his burgeoning knowledge and academic appreciation of economics, would change his views about the practicability of communism. He would soon complete his journey from young revolutionary to academic economist, and eventually become a columnist writing in support of Deng Xiaoping’s Chinese economic reforms in the 80s.

He got his PhD at Sydney University in 1971, and has since published over 250 refereed papers - covering economics, biology, politics, mathematics, philosophy, psychology, and sociology. 

He's most well-known for his work in welfare economics, and proposed ‘welfare biology’ as a new field of study. In 2007, he was made a Distinguished Fellow of the Economic Society of Australia, the highest award that the society bestows.

Links to learn more, summary and full transcript.

In this episode we discuss how he developed some of his most unusual ideas and his fascinating life story, including:

* Why Kwang believes that *’Happiness Is Absolute, Universal, Ultimate, Unidimensional, Cardinally Measurable and Interpersonally Comparable’*
* What are the most pressing questions in economics?
* Did Kwang have to worry about censorship from the Chinese government when promoting market economics, or concern for animal welfare?
* Welfare economics and where Kwang thinks it went wrong

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Will people who think carefully about how to maximize welfare eventually converge on the same views?

The effective altruism community has spent a lot of time over the past 10 years debating how best to increase happiness and reduce suffering, and gradu</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#37 - GiveWell picks top charities by estimating the unknowable. James Snowden on how they do it.</title>
      <itunes:title>#37 - GiveWell picks top charities by estimating the unknowable. James Snowden on how they do it.</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/472516044</guid>
      <link>https://share.transistor.fm/s/8a49ba37</link>
      <description>
        <![CDATA[What’s the value of preventing the death of a 5-year-old child, compared to a 20-year-old, or an 80-year-old?<br><br>

The global health community has generally regarded the value as proportional to the number of health-adjusted life-years the person has remaining - but GiveWell, one of the world’s foremost charity evaluators, no longer uses that approach.<p>

They found that contrary to the years-remaining’ method, many of their staff actually value preventing the death of an adult more than preventing the death of a young child. However there’s plenty of disagreement: the team’s estimates of the relative value span a four-fold range.<br><br>

As James Snowden - a research consultant at GiveWell - explains in this episode, there’s no way around making these controversial judgement calls based on limited information. If you try to ignore a question like this, you just implicitly take an unreflective stand on it instead. And for each charity they look into there’s 1 or 2 dozen of these highly uncertain parameters they need to estimate.<br><br>

GiveWell has been trying to find better ways to make these decisions since its inception in 2007. Lives hang in the balance, so they want their staff to say what they really believe and bring their private knowledge to the table, rather than just defer to a imaginary consensus.<br><br>

Their strategy is <a href="https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models" rel="nofollow">a massive spreadsheet</a> that lists dozens of things they need to estimate, and asking every staff member to give a figure and justification. Then once a year, the GiveWell team get together and try to identify what they really disagree about and think through what evidence it would take to change their minds.<br><br>

<a href="https://80000hours.org/podcast/episodes/james-snowden-givewell-research/?utm_campaign=podcast__james-snowden&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" rel="nofollow">Full transcript, summary of the conversation and links to learn more.</a><br><br>

Often the people who have the greatest familiarity with a particular intervention are the ones who drive the decision, as others defer to them. But the group can also end up with very different figures, based on different prior beliefs about moral issues and how the world works. In that case then use the median of everyone’s best guess to make their key decisions.<br><br>

In making his estimate of the relative badness of dying at different ages, James specifically considered two factors: how many years of life do you lose, and how much interest do you have in those future years? Currently, James believes that the worst time for a person to die is around 8 years of age.<br><br>

We discuss his experiences with such calculations, as well as a range of other topics:<br><br>

* Why GiveWell’s recommendations have changed more than it looks.<br>
* What are the biggest research priorities for GiveWell at the moment?<br>
* How do you take into account the long-term knock-on effects from interventions?<br>
* If GiveWell's advice were going to end up being very different in a couple years' time, how might that happen?<br>
* Are there any charities that James thinks are really cost-effective which GiveWell hasn't funded yet?<br>
* How does domestic government spending in the developing world compare to effective charities?<br>
* What are the main challenges with policy related interventions?<br>
* How much time do you spend discovering new interventions?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[What’s the value of preventing the death of a 5-year-old child, compared to a 20-year-old, or an 80-year-old?<br><br>

The global health community has generally regarded the value as proportional to the number of health-adjusted life-years the person has remaining - but GiveWell, one of the world’s foremost charity evaluators, no longer uses that approach.<p>

They found that contrary to the years-remaining’ method, many of their staff actually value preventing the death of an adult more than preventing the death of a young child. However there’s plenty of disagreement: the team’s estimates of the relative value span a four-fold range.<br><br>

As James Snowden - a research consultant at GiveWell - explains in this episode, there’s no way around making these controversial judgement calls based on limited information. If you try to ignore a question like this, you just implicitly take an unreflective stand on it instead. And for each charity they look into there’s 1 or 2 dozen of these highly uncertain parameters they need to estimate.<br><br>

GiveWell has been trying to find better ways to make these decisions since its inception in 2007. Lives hang in the balance, so they want their staff to say what they really believe and bring their private knowledge to the table, rather than just defer to a imaginary consensus.<br><br>

Their strategy is <a href="https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models" rel="nofollow">a massive spreadsheet</a> that lists dozens of things they need to estimate, and asking every staff member to give a figure and justification. Then once a year, the GiveWell team get together and try to identify what they really disagree about and think through what evidence it would take to change their minds.<br><br>

<a href="https://80000hours.org/podcast/episodes/james-snowden-givewell-research/?utm_campaign=podcast__james-snowden&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" rel="nofollow">Full transcript, summary of the conversation and links to learn more.</a><br><br>

Often the people who have the greatest familiarity with a particular intervention are the ones who drive the decision, as others defer to them. But the group can also end up with very different figures, based on different prior beliefs about moral issues and how the world works. In that case then use the median of everyone’s best guess to make their key decisions.<br><br>

In making his estimate of the relative badness of dying at different ages, James specifically considered two factors: how many years of life do you lose, and how much interest do you have in those future years? Currently, James believes that the worst time for a person to die is around 8 years of age.<br><br>

We discuss his experiences with such calculations, as well as a range of other topics:<br><br>

* Why GiveWell’s recommendations have changed more than it looks.<br>
* What are the biggest research priorities for GiveWell at the moment?<br>
* How do you take into account the long-term knock-on effects from interventions?<br>
* If GiveWell's advice were going to end up being very different in a couple years' time, how might that happen?<br>
* Are there any charities that James thinks are really cost-effective which GiveWell hasn't funded yet?<br>
* How does domestic government spending in the developing world compare to effective charities?<br>
* What are the main challenges with policy related interventions?<br>
* How much time do you spend discovering new interventions?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Mon, 16 Jul 2018 20:41:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8a49ba37/cddd896d.mp3" length="100042074" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/n1Xwzvqakgx5UwXT_xoFewuqh2k_pdIpj79LTxn0AgI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTYv/MTY4MzU0NDU3OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6247</itunes:duration>
      <itunes:summary>What’s the value of preventing the death of a 5-year-old child, compared to a 20-year-old, or an 80-year-old?

The global health community has generally regarded the value as proportional to the number of health-adjusted life-years the person has remaining - but GiveWell, one of the world’s foremost charity evaluators, no longer uses that approach.

They found that contrary to the years-remaining’ method, many of their staff actually value preventing the death of an adult more than preventing the death of a young child. However there’s plenty of disagreement: the team’s estimates of the relative value span a four-fold range.

As James Snowden - a research consultant at GiveWell - explains in this episode, there’s no way around making these controversial judgement calls based on limited information. If you try to ignore a question like this, you just implicitly take an unreflective stand on it instead. And for each charity they look into there’s 1 or 2 dozen of these highly uncertain parameters they need to estimate.

GiveWell has been trying to find better ways to make these decisions since its inception in 2007. Lives hang in the balance, so they want their staff to say what they really believe and bring their private knowledge to the table, rather than just defer to a imaginary consensus.

Their strategy is a massive spreadsheet that lists dozens of things they need to estimate, and asking every staff member to give a figure and justification. Then once a year, the GiveWell team get together and try to identify what they really disagree about and think through what evidence it would take to change their minds.

Full transcript, summary of the conversation and links to learn more.

Often the people who have the greatest familiarity with a particular intervention are the ones who drive the decision, as others defer to them. But the group can also end up with very different figures, based on different prior beliefs about moral issues and how the world works. In that case then use the median of everyone’s best guess to make their key decisions.

In making his estimate of the relative badness of dying at different ages, James specifically considered two factors: how many years of life do you lose, and how much interest do you have in those future years? Currently, James believes that the worst time for a person to die is around 8 years of age.

We discuss his experiences with such calculations, as well as a range of other topics:

* Why GiveWell’s recommendations have changed more than it looks.
* What are the biggest research priorities for GiveWell at the moment?
* How do you take into account the long-term knock-on effects from interventions?
* If GiveWell's advice were going to end up being very different in a couple years' time, how might that happen?
* Are there any charities that James thinks are really cost-effective which GiveWell hasn't funded yet?
* How does domestic government spending in the developing world compare to effective charities?
* What are the main challenges with policy related interventions?
* How much time do you spend discovering new interventions?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>What’s the value of preventing the death of a 5-year-old child, compared to a 20-year-old, or an 80-year-old?

The global health community has generally regarded the value as proportional to the number of health-adjusted life-years the person has remainin</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#36 - Tanya Singh on ending the operations management bottleneck in effective altruism</title>
      <itunes:title>#36 - Tanya Singh on ending the operations management bottleneck in effective altruism</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/470335812</guid>
      <link>https://share.transistor.fm/s/609b96b0</link>
      <description>
        <![CDATA[Almost nobody is able to do groundbreaking physics research themselves, and by the time his brilliance was appreciated, Einstein was hardly limited by funding. But what if you could find a way to unlock the secrets of the universe like Einstein nonetheless?<p>

Today’s guest, Tanya Singh, sees herself as doing something like that every day. She’s Executive Assistant to one of her intellectual heroes who she believes is making a huge contribution to improving the world: Professor Bostrom at Oxford University's Future of Humanity Institute (FHI).</p><p>

She couldn’t get more work out of Bostrom with extra donations, as his salary is already easily covered. But with her superior abilities as an Executive Assistant, Tanya frees up hours of his time every week, essentially ‘buying’ more Bostrom in a way nobody else can. She also help manage FHI more generally, in so doing freeing up more than an hour of other staff time for each hour she works. This gives her the leverage to do more good than other people or other positions.</p><p>

In our <a href="https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__tanya-singh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">previous episode</a>, Tara Mac Aulay objected to viewing operations work as predominately a way of freeing up other people's time:</p><p>

“A good ops person doesn’t just allow you to scale linearly, but also can help figure out bottlenecks and solve problems such that the organization is able to do qualitatively different work, rather than just increase the total quantity”, Tara said.</p><p>

<a href="https://80000hours.org/podcast/episodes/tanya-singh-operations-bottleneck/?utm_campaign=podcast__tanya-singh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, summary and links to learn more.</a></p><p>

Tara’s right that buying time for people at the top of their field is just one path to impact, though it’s one Tanya says she finds highly motivating. Other paths include enabling complex projects that would otherwise be impossible, allowing you to hire and grow much faster, and preventing disasters that could bring down a whole organisation - all things that Tanya does at FHI as well.</p><p>

In today’s episode we discuss all of those approaches, as we dive deeper into the broad class of roles we refer to as ‘operations management’. We cover the arguments we made in <a href="https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tanya-singh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">‘Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:</p><p>

* Does one really need to hire people aligned with an org’s mission to work in ops?<br>
* The most notable operations successes in the 20th Century.<br>
* What’s it like being the only operations person in an org?<br>
* The role of a COO as compared to a CEO, and the options for career progression.<br>
* How do good operation teams allow orgs to scale quickly?<br>
* How much do operations staff get to set their org’s strategy?<br>
* Which personal weaknesses aren’t a huge problem in operations?<br>
* How do you automate processes? Why don’t most people do this?<br>
* Cultural differences between Britain and India where Tanya grew up.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Almost nobody is able to do groundbreaking physics research themselves, and by the time his brilliance was appreciated, Einstein was hardly limited by funding. But what if you could find a way to unlock the secrets of the universe like Einstein nonetheless?<p>

Today’s guest, Tanya Singh, sees herself as doing something like that every day. She’s Executive Assistant to one of her intellectual heroes who she believes is making a huge contribution to improving the world: Professor Bostrom at Oxford University's Future of Humanity Institute (FHI).</p><p>

She couldn’t get more work out of Bostrom with extra donations, as his salary is already easily covered. But with her superior abilities as an Executive Assistant, Tanya frees up hours of his time every week, essentially ‘buying’ more Bostrom in a way nobody else can. She also help manage FHI more generally, in so doing freeing up more than an hour of other staff time for each hour she works. This gives her the leverage to do more good than other people or other positions.</p><p>

In our <a href="https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__tanya-singh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">previous episode</a>, Tara Mac Aulay objected to viewing operations work as predominately a way of freeing up other people's time:</p><p>

“A good ops person doesn’t just allow you to scale linearly, but also can help figure out bottlenecks and solve problems such that the organization is able to do qualitatively different work, rather than just increase the total quantity”, Tara said.</p><p>

<a href="https://80000hours.org/podcast/episodes/tanya-singh-operations-bottleneck/?utm_campaign=podcast__tanya-singh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, summary and links to learn more.</a></p><p>

Tara’s right that buying time for people at the top of their field is just one path to impact, though it’s one Tanya says she finds highly motivating. Other paths include enabling complex projects that would otherwise be impossible, allowing you to hire and grow much faster, and preventing disasters that could bring down a whole organisation - all things that Tanya does at FHI as well.</p><p>

In today’s episode we discuss all of those approaches, as we dive deeper into the broad class of roles we refer to as ‘operations management’. We cover the arguments we made in <a href="https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tanya-singh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">‘Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:</p><p>

* Does one really need to hire people aligned with an org’s mission to work in ops?<br>
* The most notable operations successes in the 20th Century.<br>
* What’s it like being the only operations person in an org?<br>
* The role of a COO as compared to a CEO, and the options for career progression.<br>
* How do good operation teams allow orgs to scale quickly?<br>
* How much do operations staff get to set their org’s strategy?<br>
* Which personal weaknesses aren’t a huge problem in operations?<br>
* How do you automate processes? Why don’t most people do this?<br>
* Cultural differences between Britain and India where Tanya grew up.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b></p><p>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 11 Jul 2018 18:00:46 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/609b96b0/ccafc084.mp3" length="119678188" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/SWqBEj-pNoE_Ynt-WO_TZ5LGHSYLftOp0WTBs9HKVsw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTUv/MTY4MzU0NDU3Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7473</itunes:duration>
      <itunes:summary>Almost nobody is able to do groundbreaking physics research themselves, and by the time his brilliance was appreciated, Einstein was hardly limited by funding. But what if you could find a way to unlock the secrets of the universe like Einstein nonetheless?

Today’s guest, Tanya Singh, sees herself as doing something like that every day. She’s Executive Assistant to one of her intellectual heroes who she believes is making a huge contribution to improving the world: Professor Bostrom at Oxford University's Future of Humanity Institute (FHI).

She couldn’t get more work out of Bostrom with extra donations, as his salary is already easily covered. But with her superior abilities as an Executive Assistant, Tanya frees up hours of his time every week, essentially ‘buying’ more Bostrom in a way nobody else can. She also help manage FHI more generally, in so doing freeing up more than an hour of other staff time for each hour she works. This gives her the leverage to do more good than other people or other positions.

In our previous episode, Tara Mac Aulay objected to viewing operations work as predominately a way of freeing up other people's time:

“A good ops person doesn’t just allow you to scale linearly, but also can help figure out bottlenecks and solve problems such that the organization is able to do qualitatively different work, rather than just increase the total quantity”, Tara said.

Full transcript, summary and links to learn more.

Tara’s right that buying time for people at the top of their field is just one path to impact, though it’s one Tanya says she finds highly motivating. Other paths include enabling complex projects that would otherwise be impossible, allowing you to hire and grow much faster, and preventing disasters that could bring down a whole organisation - all things that Tanya does at FHI as well.

In today’s episode we discuss all of those approaches, as we dive deeper into the broad class of roles we refer to as ‘operations management’. We cover the arguments we made in ‘Why operations management is one of the biggest bottlenecks in effective altruism’, as well as:

* Does one really need to hire people aligned with an org’s mission to work in ops?
* The most notable operations successes in the 20th Century.
* What’s it like being the only operations person in an org?
* The role of a COO as compared to a CEO, and the options for career progression.
* How do good operation teams allow orgs to scale quickly?
* How much do operations staff get to set their org’s strategy?
* Which personal weaknesses aren’t a huge problem in operations?
* How do you automate processes? Why don’t most people do this?
* Cultural differences between Britain and India where Tanya grew up.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.

The 80,000 Hours podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Almost nobody is able to do groundbreaking physics research themselves, and by the time his brilliance was appreciated, Einstein was hardly limited by funding. But what if you could find a way to unlock the secrets of the universe like Einstein nonetheles</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#35 - Tara Mac Aulay on the audacity to fix the world without asking permission</title>
      <itunes:title>#35 - Tara Mac Aulay on the audacity to fix the world without asking permission</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/461614122</guid>
      <link>https://share.transistor.fm/s/4e15acfc</link>
      <description>
        <![CDATA[<em>"You don't need permission. You don't need to be allowed to do something that's not in your job description. If you think that it's gonna make your company or your organization more successful and more efficient, you can often just go and do it."</em><p>

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.</p><p>

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.</p><p>

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.</p><p>

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. </p><p>

<a href="https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__tara-mac-aulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, key quotes and links to learn more.</b></a></p><p>

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.</p><p>

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.</p><p>

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article <a href="https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tara-mac-aulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">'Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:</p><p>

* Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.<br>
* How a student can save a hospital millions with a simple spreadsheet model.<br>
* The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.<br>
* What most people misunderstand about operations, and how to tell if you have what it takes.<br>
* And finally, operations jobs people should consider applying for, such as those open now at the <a href="https://www.centreforeffectivealtruism.org/careers/">Centre for Effective Altruism</a>.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<em>"You don't need permission. You don't need to be allowed to do something that's not in your job description. If you think that it's gonna make your company or your organization more successful and more efficient, you can often just go and do it."</em><p>

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.</p><p>

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.</p><p>

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.</p><p>

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. </p><p>

<a href="https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__tara-mac-aulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, key quotes and links to learn more.</b></a></p><p>

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.</p><p>

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.</p><p>

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article <a href="https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tara-mac-aulay&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">'Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:</p><p>

* Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.<br>
* How a student can save a hospital millions with a simple spreadsheet model.<br>
* The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.<br>
* What most people misunderstand about operations, and how to tell if you have what it takes.<br>
* And finally, operations jobs people should consider applying for, such as those open now at the <a href="https://www.centreforeffectivealtruism.org/careers/">Centre for Effective Altruism</a>.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Thu, 21 Jun 2018 23:31:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4e15acfc/838f3e8f.mp3" length="79369503" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/5zRGesBlPjvD58zk0V3BOf8JC5ER3r5lJ9i1k1ipO9I/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTQv/MTY4MzU0NDU3Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4955</itunes:duration>
      <itunes:summary>"You don't need permission. You don't need to be allowed to do something that's not in your job description. If you think that it's gonna make your company or your organization more successful and more efficient, you can often just go and do it."

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. 

Full transcript, key quotes and links to learn more.

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article 'Why operations management is one of the biggest bottlenecks in effective altruism’, as well as:

* Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.
* How a student can save a hospital millions with a simple spreadsheet model.
* The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.
* What most people misunderstand about operations, and how to tell if you have what it takes.
* And finally, operations jobs people should consider applying for, such as those open now at the Centre for Effective Altruism.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>"You don't need permission. You don't need to be allowed to do something that's not in your job description. If you think that it's gonna make your company or your organization more successful and more efficient, you can often just go and do it."

How bro</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>Rob Wiblin on the art/science of a high impact career</title>
      <itunes:title>Rob Wiblin on the art/science of a high impact career</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/455404398</guid>
      <link>https://share.transistor.fm/s/80fcef5e</link>
      <description>
        <![CDATA[Today's episode is a cross-post of an interview I did with The Jolly Swagmen Podcast which came out this week. I recommend regular listeners skip to 24 minutes in to avoid hearing things they already know. Later in the episode I talk about my contrarian views, utilitarianism, how 80,000 Hours has changed and will change in the future, where I think EA is performing worst, how to use social media most effectively, and whether or not effective altruism is any sacrifice.<p>

Subscribe and get the episode by searching for '80,000 Hours' in your podcasting app.</p><p>

<a href="https://josephnoelwalker.com/51-the-art-science-of-a-high-impact-career-rob-wiblin/">Blog post of the episode to share, including a list of topics and links to learn more.</a></p><p>

"Most people want to help others with their career, but what’s the best way to do that? Become a doctor? A politician? Work at a non-profit? How can any of us figure out the best way to use our skills to improve the world?</p><p>

Rob Wiblin is the Director of Research at 80,000 Hours, an organisation founded in Oxford in 2011, which aims to answer just this question and help talented people find their highest-impact career path. He hosts a popular podcast on ‘the world’s most pressing problems and how you can use your career to solve them’.</p><p>

After seven years of research, the 80,000 Hours team recommends against becoming a teacher, or a doctor, or working at most non-profits. And they claim their research shows some common careers do 10 or 100x as much good as others.</p><p>

80,000 Hours was one of the organisations that kicked off the effective altruism movement, was a Y Combinator-backed non-profit, and has already shifted over 80 million career hours through its advice.</p><p>

Joe caught up with Rob in Berkeley, California, to discuss how 80,000 Hours assesses which of the world’s problems are most pressing, how you can build career capital and succeed in any role, and why you could easily save more lives than a doctor - if you think carefully about your impact."</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Today's episode is a cross-post of an interview I did with The Jolly Swagmen Podcast which came out this week. I recommend regular listeners skip to 24 minutes in to avoid hearing things they already know. Later in the episode I talk about my contrarian views, utilitarianism, how 80,000 Hours has changed and will change in the future, where I think EA is performing worst, how to use social media most effectively, and whether or not effective altruism is any sacrifice.<p>

Subscribe and get the episode by searching for '80,000 Hours' in your podcasting app.</p><p>

<a href="https://josephnoelwalker.com/51-the-art-science-of-a-high-impact-career-rob-wiblin/">Blog post of the episode to share, including a list of topics and links to learn more.</a></p><p>

"Most people want to help others with their career, but what’s the best way to do that? Become a doctor? A politician? Work at a non-profit? How can any of us figure out the best way to use our skills to improve the world?</p><p>

Rob Wiblin is the Director of Research at 80,000 Hours, an organisation founded in Oxford in 2011, which aims to answer just this question and help talented people find their highest-impact career path. He hosts a popular podcast on ‘the world’s most pressing problems and how you can use your career to solve them’.</p><p>

After seven years of research, the 80,000 Hours team recommends against becoming a teacher, or a doctor, or working at most non-profits. And they claim their research shows some common careers do 10 or 100x as much good as others.</p><p>

80,000 Hours was one of the organisations that kicked off the effective altruism movement, was a Y Combinator-backed non-profit, and has already shifted over 80 million career hours through its advice.</p><p>

Joe caught up with Rob in Berkeley, California, to discuss how 80,000 Hours assesses which of the world’s problems are most pressing, how you can build career capital and succeed in any role, and why you could easily save more lives than a doctor - if you think carefully about your impact."</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 08 Jun 2018 02:21:42 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/80fcef5e/559a6c8c.mp3" length="87999088" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/l5M1wU2o00uch39foY8weG-Kuv7GhfYejkC3bA2QSsw/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTMv/MTY4MzU0NDU3NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5495</itunes:duration>
      <itunes:summary>Today's episode is a cross-post of an interview I did with The Jolly Swagmen Podcast which came out this week. I recommend regular listeners skip to 24 minutes in to avoid hearing things they already know. Later in the episode I talk about my contrarian views, utilitarianism, how 80,000 Hours has changed and will change in the future, where I think EA is performing worst, how to use social media most effectively, and whether or not effective altruism is any sacrifice.

Subscribe and get the episode by searching for '80,000 Hours' in your podcasting app.

Blog post of the episode to share, including a list of topics and links to learn more.

"Most people want to help others with their career, but what’s the best way to do that? Become a doctor? A politician? Work at a non-profit? How can any of us figure out the best way to use our skills to improve the world?

Rob Wiblin is the Director of Research at 80,000 Hours, an organisation founded in Oxford in 2011, which aims to answer just this question and help talented people find their highest-impact career path. He hosts a popular podcast on ‘the world’s most pressing problems and how you can use your career to solve them’.

After seven years of research, the 80,000 Hours team recommends against becoming a teacher, or a doctor, or working at most non-profits. And they claim their research shows some common careers do 10 or 100x as much good as others.

80,000 Hours was one of the organisations that kicked off the effective altruism movement, was a Y Combinator-backed non-profit, and has already shifted over 80 million career hours through its advice.

Joe caught up with Rob in Berkeley, California, to discuss how 80,000 Hours assesses which of the world’s problems are most pressing, how you can build career capital and succeed in any role, and why you could easily save more lives than a doctor - if you think carefully about your impact."

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Today's episode is a cross-post of an interview I did with The Jolly Swagmen Podcast which came out this week. I recommend regular listeners skip to 24 minutes in to avoid hearing things they already know. Later in the episode I talk about my contrarian v</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#34 - We use the worst voting system that exists. Here's how Aaron Hamlin is going to fix it.</title>
      <itunes:title>#34 - We use the worst voting system that exists. Here's how Aaron Hamlin is going to fix it.</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/452181057</guid>
      <link>https://share.transistor.fm/s/48a740a7</link>
      <description>
        <![CDATA[In 1991 Edwin Edwards won the Louisiana gubernatorial election. In 2001, he was found guilty of racketeering and received a 10 year invitation to Federal prison. The strange thing about that election? By 1991 Edwards was already notorious for his corruption. Actually, that’s not it.<p>

The truly strange thing is that Edwards was clearly the <em>good guy</em> in the race. How is that possible?</p><p>

His opponent was former Ku Klux Klan Grand Wizard David Duke.</p><p>

How could Louisiana end up having to choose between a criminal and a Nazi sympathiser?</p><p>

It’s not like they lacked other options: the state’s moderate incumbent governor Buddy Roemer ran for re-election. Polling showed that Roemer was massively preferred to both the career criminal and the career bigot, and would easily win a head-to-head election against either.</p><p>

Unfortunately, in Louisiana every candidate from every party competes in the first round, and the top two then go on to a second - a so-called ‘jungle primary’. Vote splitting squeezed out the middle, and meant that Roemer was eliminated in the first round.</p><p>

Louisiana voters were left with only terrible options, in a run-off election mostly remembered for the proliferation of bumper stickers reading <em>“Vote for the Crook. It’s Important.”</em></p><p>

We could look at this as a cultural problem, exposing widespread enthusiasm for bribery and racism that will take generations to overcome. But according to Aaron Hamlin, Executive Director of The Center for Election Science (CES), there’s a simple way to make sure we never have to elect someone hated by more than half the electorate: change how we vote.</p><p>

He advocates an alternative voting method called <em>approval voting</em>, in which you can vote for as many candidates as you want, not just one. That means that you can always support your honest favorite candidate, even when an election seems like a choice between the lesser of two evils.</p><p>

<a href="https://80000hours.org/podcast/episodes/aaron-hamlin-voting-reform/?utm_campaign=podcast__aaron-hamlin&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, links to learn more, and summary of key points.</b></a></p><p>

<a href="https://www.eventbrite.com/o/the-center-for-election-science-14642817210"><b>If you'd like to meet Aaron he's doing events for CES in San Francisco, DC, Philadelphia, New York and Brooklyn over the next two weeks - RSVP here.</b></a></p><p>

While it might not seem sexy, this single change could transform politics. <a href="https://en.wikipedia.org/wiki/Approval_voting">Approval voting</a> is adored by voting researchers, who regard it as the best simple voting system available.</p><p>

Which do they regard as unquestionably the worst? <em>First-past-the-post</em> - precisely the disastrous system used and exported around the world by the US and UK.</p><p>

Aaron has a practical plan to spread approval voting across the US using ballot initiatives - and it just might be our best shot at making politics a bit less unreasonable.</p><p>

<a href="https://electology.org/what-we-do">The Center for Election Science</a> is a U.S. non-profit which aims to fix broken government by helping the world adopt smarter election systems. They recently received <a href="https://www.openphilanthropy.org/giving/grants/the-center-for-election-science-general-support">a $600,000 grant</a> from the Open Philanthropy Project to scale up their efforts.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[In 1991 Edwin Edwards won the Louisiana gubernatorial election. In 2001, he was found guilty of racketeering and received a 10 year invitation to Federal prison. The strange thing about that election? By 1991 Edwards was already notorious for his corruption. Actually, that’s not it.<p>

The truly strange thing is that Edwards was clearly the <em>good guy</em> in the race. How is that possible?</p><p>

His opponent was former Ku Klux Klan Grand Wizard David Duke.</p><p>

How could Louisiana end up having to choose between a criminal and a Nazi sympathiser?</p><p>

It’s not like they lacked other options: the state’s moderate incumbent governor Buddy Roemer ran for re-election. Polling showed that Roemer was massively preferred to both the career criminal and the career bigot, and would easily win a head-to-head election against either.</p><p>

Unfortunately, in Louisiana every candidate from every party competes in the first round, and the top two then go on to a second - a so-called ‘jungle primary’. Vote splitting squeezed out the middle, and meant that Roemer was eliminated in the first round.</p><p>

Louisiana voters were left with only terrible options, in a run-off election mostly remembered for the proliferation of bumper stickers reading <em>“Vote for the Crook. It’s Important.”</em></p><p>

We could look at this as a cultural problem, exposing widespread enthusiasm for bribery and racism that will take generations to overcome. But according to Aaron Hamlin, Executive Director of The Center for Election Science (CES), there’s a simple way to make sure we never have to elect someone hated by more than half the electorate: change how we vote.</p><p>

He advocates an alternative voting method called <em>approval voting</em>, in which you can vote for as many candidates as you want, not just one. That means that you can always support your honest favorite candidate, even when an election seems like a choice between the lesser of two evils.</p><p>

<a href="https://80000hours.org/podcast/episodes/aaron-hamlin-voting-reform/?utm_campaign=podcast__aaron-hamlin&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, links to learn more, and summary of key points.</b></a></p><p>

<a href="https://www.eventbrite.com/o/the-center-for-election-science-14642817210"><b>If you'd like to meet Aaron he's doing events for CES in San Francisco, DC, Philadelphia, New York and Brooklyn over the next two weeks - RSVP here.</b></a></p><p>

While it might not seem sexy, this single change could transform politics. <a href="https://en.wikipedia.org/wiki/Approval_voting">Approval voting</a> is adored by voting researchers, who regard it as the best simple voting system available.</p><p>

Which do they regard as unquestionably the worst? <em>First-past-the-post</em> - precisely the disastrous system used and exported around the world by the US and UK.</p><p>

Aaron has a practical plan to spread approval voting across the US using ballot initiatives - and it just might be our best shot at making politics a bit less unreasonable.</p><p>

<a href="https://electology.org/what-we-do">The Center for Election Science</a> is a U.S. non-profit which aims to fix broken government by helping the world adopt smarter election systems. They recently received <a href="https://www.openphilanthropy.org/giving/grants/the-center-for-election-science-general-support">a $600,000 grant</a> from the Open Philanthropy Project to scale up their efforts.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 01 Jun 2018 09:44:31 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/48a740a7/7f3be448.mp3" length="133112805" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/7hsbr6XV3ictvZEFqUpfN84csyVp0QnEE1inKnyyG-c/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTIv/MTY4MzU0NDU3NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8311</itunes:duration>
      <itunes:summary>In 1991 Edwin Edwards won the Louisiana gubernatorial election. In 2001, he was found guilty of racketeering and received a 10 year invitation to Federal prison. The strange thing about that election? By 1991 Edwards was already notorious for his corruption. Actually, that’s not it.

The truly strange thing is that Edwards was clearly the good guy in the race. How is that possible?

His opponent was former Ku Klux Klan Grand Wizard David Duke.

How could Louisiana end up having to choose between a criminal and a Nazi sympathiser?

It’s not like they lacked other options: the state’s moderate incumbent governor Buddy Roemer ran for re-election. Polling showed that Roemer was massively preferred to both the career criminal and the career bigot, and would easily win a head-to-head election against either.

Unfortunately, in Louisiana every candidate from every party competes in the first round, and the top two then go on to a second - a so-called ‘jungle primary’. Vote splitting squeezed out the middle, and meant that Roemer was eliminated in the first round.

Louisiana voters were left with only terrible options, in a run-off election mostly remembered for the proliferation of bumper stickers reading “Vote for the Crook. It’s Important.”

We could look at this as a cultural problem, exposing widespread enthusiasm for bribery and racism that will take generations to overcome. But according to Aaron Hamlin, Executive Director of The Center for Election Science (CES), there’s a simple way to make sure we never have to elect someone hated by more than half the electorate: change how we vote.

He advocates an alternative voting method called approval voting, in which you can vote for as many candidates as you want, not just one. That means that you can always support your honest favorite candidate, even when an election seems like a choice between the lesser of two evils.

Full transcript, links to learn more, and summary of key points.

If you'd like to meet Aaron he's doing events for CES in San Francisco, DC, Philadelphia, New York and Brooklyn over the next two weeks - RSVP here.

While it might not seem sexy, this single change could transform politics. Approval voting is adored by voting researchers, who regard it as the best simple voting system available.

Which do they regard as unquestionably the worst? First-past-the-post - precisely the disastrous system used and exported around the world by the US and UK.

Aaron has a practical plan to spread approval voting across the US using ballot initiatives - and it just might be our best shot at making politics a bit less unreasonable.

The Center for Election Science is a U.S. non-profit which aims to fix broken government by helping the world adopt smarter election systems. They recently received a $600,000 grant from the Open Philanthropy Project to scale up their efforts.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>In 1991 Edwin Edwards won the Louisiana gubernatorial election. In 2001, he was found guilty of racketeering and received a 10 year invitation to Federal prison. The strange thing about that election? By 1991 Edwards was already notorious for his corrupti</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#33 - Anders Sandberg on what if we ended ageing, solar flares &amp; the annual risk of nuclear war</title>
      <itunes:title>#33 - Anders Sandberg on what if we ended ageing, solar flares &amp; the annual risk of nuclear war</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/450196050</guid>
      <link>https://share.transistor.fm/s/94397217</link>
      <description>
        <![CDATA[<p>Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? </p><p> According to our last guest, <a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Bryan Caplan</a>, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees.</p><p> Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. </p><p> Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. </p><p> <a href="https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript of the conversation, summary, and links to learn more.</strong></a></p><p> The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. </p><p> Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. </p><p> His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. </p><p> ***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.*** </p><p> Last time we asked him <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including:</p><p> * Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br> * How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened?<br> * If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians?<br> * What long-shot drugs can people take in their 70s to stave off death?<br> * Can science extend human (waking) life by cutting our need to sleep?<br> * How bad would it be if a solar flare took down the electricity grid? Could it happen?<br> * If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it?<br> * Will lifelike robots make us more inclined to dehumanise one another?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? </p><p> According to our last guest, <a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Bryan Caplan</a>, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees.</p><p> Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. </p><p> Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. </p><p> <a href="https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript of the conversation, summary, and links to learn more.</strong></a></p><p> The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. </p><p> Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. </p><p> His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. </p><p> ***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.*** </p><p> Last time we asked him <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including:</p><p> * Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br> * How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened?<br> * If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians?<br> * What long-shot drugs can people take in their 70s to stave off death?<br> * Can science extend human (waking) life by cutting our need to sleep?<br> * How bad would it be if a solar flare took down the electricity grid? Could it happen?<br> * If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it?<br> * Will lifelike robots make us more inclined to dehumanise one another?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 29 May 2018 15:57:17 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/94397217/98c5ff76.mp3" length="81601991" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/EOJnZvhK30o3jzFTB5oxY3tvsK3asGXATN45K28ElIg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTEv/MTY4MzU0NDU3My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5094</itunes:duration>
      <itunes:summary>
        <![CDATA[<p>Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? </p><p> According to our last guest, <a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Bryan Caplan</a>, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees.</p><p> Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. </p><p> Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. </p><p> <a href="https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript of the conversation, summary, and links to learn more.</strong></a></p><p> The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. </p><p> Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. </p><p> His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. </p><p> ***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.*** </p><p> Last time we asked him <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including:</p><p> * Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br> * How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened?<br> * If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians?<br> * What long-shot drugs can people take in their 70s to stave off death?<br> * Can science extend human (waking) life by cutting our need to sleep?<br> * How bad would it be if a solar flare took down the electricity grid? Could it happen?<br> * If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it?<br> * Will lifelike robots make us more inclined to dehumanise one another?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </itunes:summary>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#32 - Bryan Caplan on whether his Case Against Education holds up, totalitarianism, &amp; open borders</title>
      <itunes:title>#32 - Bryan Caplan on whether his Case Against Education holds up, totalitarianism, &amp; open borders</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/447413007</guid>
      <link>https://share.transistor.fm/s/7ba7e2e9</link>
      <description>
        <![CDATA[Bryan Caplan’s claim in *The Case Against Education* is striking: education doesn’t teach people much, we use little of what we learn, and college is mostly about trying to seem smarter than other people - so the government should slash education funding.<p>

It’s a dismaying - almost profane - idea, and one people are inclined to dismiss out of hand. But having read the book, I have to admit that Bryan can point to a surprising amount of evidence in his favour.</p><p>

After all, imagine this dilemma: you can have either a Princeton education without a diploma, or a Princeton diploma without an education. Which is the bigger benefit of college - learning or convincing people you’re smart? It’s not so easy to say.</p><p>

For this interview, I searched for the best counterarguments I could find and challenged Bryan on what seem like his weakest or most controversial claims.</p><p>

Wouldn’t defunding education be especially bad for capable but low income students? If you reduced funding for education, wouldn’t that just lower prices, and not actually change the number of years people study? Is it really true that students who drop out in their final year of college earn about the same as people who never go to college at all?</p><p>

What about studies that show that extra years of education boost IQ scores? And surely the early years of primary school, when you learn reading and arithmetic, *are* useful even if college isn’t.</p><p>

I then get his advice on who should study, what they should study, and where they should study, if he’s right that college is mostly about separating yourself from the pack. </p><p>

<a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__bryan-caplan&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, links to learn more, and summary of key points.</b></a></p><p>

We then venture into some of Bryan’s other unorthodox views - like that immigration restrictions are a human rights violation, or that we should worry about the risk of global totalitarianism.</p><p>

Bryan is a Professor of Economics at George Mason University, and a blogger at *EconLog*. He is also the author of *Selfish Reasons to Have More Kids: Why Being a Great Parent is Less Work and More Fun Than You Think*, and *The Myth of the Rational Voter: Why Democracies Choose Bad Policies*.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.</b></p><p>

In this lengthy interview, Rob and Bryan cover:</p><p>

* How worried should we be about China’s new citizen ranking system as a means of authoritarian rule?<br>
* How will advances in surveillance technology impact a government’s ability to rule absolutely?<br>
* Does more global coordination make us safer, or more at risk? <br>
* Should the push for open borders be a major cause area for effective altruism? 
<br>
* Are immigration restrictions a human rights violation?<br>
* Why aren’t libertarian-minded people more focused on modern slavery?<br>
* Should altruists work on criminal justice reform or reducing land use regulations?<br>
* What’s the greatest art form: opera, or Nicki Minaj? <br>
* What are the main implications of Bryan’s thesis for society?<br>
* Is elementary school more valuable than university?<br>
* What does Bryan think are the best arguments against his view?<br>
* Do years of education affect political affiliation?<br>
* How do people really improve themselves and their circumstances?<br>
* Who should and who shouldn’t do a masters or PhD?<br>
* The value of teaching foreign languages in school<br>
* Are there some skills people can develop that have wide applicability?</p><p>

<b>Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Bryan Caplan’s claim in *The Case Against Education* is striking: education doesn’t teach people much, we use little of what we learn, and college is mostly about trying to seem smarter than other people - so the government should slash education funding.<p>

It’s a dismaying - almost profane - idea, and one people are inclined to dismiss out of hand. But having read the book, I have to admit that Bryan can point to a surprising amount of evidence in his favour.</p><p>

After all, imagine this dilemma: you can have either a Princeton education without a diploma, or a Princeton diploma without an education. Which is the bigger benefit of college - learning or convincing people you’re smart? It’s not so easy to say.</p><p>

For this interview, I searched for the best counterarguments I could find and challenged Bryan on what seem like his weakest or most controversial claims.</p><p>

Wouldn’t defunding education be especially bad for capable but low income students? If you reduced funding for education, wouldn’t that just lower prices, and not actually change the number of years people study? Is it really true that students who drop out in their final year of college earn about the same as people who never go to college at all?</p><p>

What about studies that show that extra years of education boost IQ scores? And surely the early years of primary school, when you learn reading and arithmetic, *are* useful even if college isn’t.</p><p>

I then get his advice on who should study, what they should study, and where they should study, if he’s right that college is mostly about separating yourself from the pack. </p><p>

<a href="https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__bryan-caplan&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, links to learn more, and summary of key points.</b></a></p><p>

We then venture into some of Bryan’s other unorthodox views - like that immigration restrictions are a human rights violation, or that we should worry about the risk of global totalitarianism.</p><p>

Bryan is a Professor of Economics at George Mason University, and a blogger at *EconLog*. He is also the author of *Selfish Reasons to Have More Kids: Why Being a Great Parent is Less Work and More Fun Than You Think*, and *The Myth of the Rational Voter: Why Democracies Choose Bad Policies*.</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.</b></p><p>

In this lengthy interview, Rob and Bryan cover:</p><p>

* How worried should we be about China’s new citizen ranking system as a means of authoritarian rule?<br>
* How will advances in surveillance technology impact a government’s ability to rule absolutely?<br>
* Does more global coordination make us safer, or more at risk? <br>
* Should the push for open borders be a major cause area for effective altruism? 
<br>
* Are immigration restrictions a human rights violation?<br>
* Why aren’t libertarian-minded people more focused on modern slavery?<br>
* Should altruists work on criminal justice reform or reducing land use regulations?<br>
* What’s the greatest art form: opera, or Nicki Minaj? <br>
* What are the main implications of Bryan’s thesis for society?<br>
* Is elementary school more valuable than university?<br>
* What does Bryan think are the best arguments against his view?<br>
* Do years of education affect political affiliation?<br>
* How do people really improve themselves and their circumstances?<br>
* Who should and who shouldn’t do a masters or PhD?<br>
* The value of teaching foreign languages in school<br>
* Are there some skills people can develop that have wide applicability?</p><p>

<b>Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 22 May 2018 11:45:46 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/7ba7e2e9/5f954e51.mp3" length="139560508" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/pybrSh1zT02Za2zLCasAq530xcX75ikuNlqGfNcofWk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NTAv/MTY4MzU0NDU3Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8713</itunes:duration>
      <itunes:summary>Bryan Caplan’s claim in *The Case Against Education* is striking: education doesn’t teach people much, we use little of what we learn, and college is mostly about trying to seem smarter than other people - so the government should slash education funding.

It’s a dismaying - almost profane - idea, and one people are inclined to dismiss out of hand. But having read the book, I have to admit that Bryan can point to a surprising amount of evidence in his favour.

After all, imagine this dilemma: you can have either a Princeton education without a diploma, or a Princeton diploma without an education. Which is the bigger benefit of college - learning or convincing people you’re smart? It’s not so easy to say.

For this interview, I searched for the best counterarguments I could find and challenged Bryan on what seem like his weakest or most controversial claims.

Wouldn’t defunding education be especially bad for capable but low income students? If you reduced funding for education, wouldn’t that just lower prices, and not actually change the number of years people study? Is it really true that students who drop out in their final year of college earn about the same as people who never go to college at all?

What about studies that show that extra years of education boost IQ scores? And surely the early years of primary school, when you learn reading and arithmetic, *are* useful even if college isn’t.

I then get his advice on who should study, what they should study, and where they should study, if he’s right that college is mostly about separating yourself from the pack. 

Full transcript, links to learn more, and summary of key points.

We then venture into some of Bryan’s other unorthodox views - like that immigration restrictions are a human rights violation, or that we should worry about the risk of global totalitarianism.

Bryan is a Professor of Economics at George Mason University, and a blogger at *EconLog*. He is also the author of *Selfish Reasons to Have More Kids: Why Being a Great Parent is Less Work and More Fun Than You Think*, and *The Myth of the Rational Voter: Why Democracies Choose Bad Policies*.

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.

In this lengthy interview, Rob and Bryan cover:

* How worried should we be about China’s new citizen ranking system as a means of authoritarian rule?
* How will advances in surveillance technology impact a government’s ability to rule absolutely?
* Does more global coordination make us safer, or more at risk? 
* Should the push for open borders be a major cause area for effective altruism? 

* Are immigration restrictions a human rights violation?
* Why aren’t libertarian-minded people more focused on modern slavery?
* Should altruists work on criminal justice reform or reducing land use regulations?
* What’s the greatest art form: opera, or Nicki Minaj? 
* What are the main implications of Bryan’s thesis for society?
* Is elementary school more valuable than university?
* What does Bryan think are the best arguments against his view?
* Do years of education affect political affiliation?
* How do people really improve themselves and their circumstances?
* Who should and who shouldn’t do a masters or PhD?
* The value of teaching foreign languages in school
* Are there some skills people can develop that have wide applicability?

Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Bryan Caplan’s claim in *The Case Against Education* is striking: education doesn’t teach people much, we use little of what we learn, and college is mostly about trying to seem smarter than other people - so the government should slash education funding.</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#31 - Allan Dafoe on defusing the political &amp; economic risks posed by existing AI capabilities</title>
      <itunes:title>#31 - Allan Dafoe on defusing the political &amp; economic risks posed by existing AI capabilities</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/445798662</guid>
      <link>https://share.transistor.fm/s/166665be</link>
      <description>
        <![CDATA[<p>The debate around the impacts of artificial intelligence often centres on ‘superintelligence’ - a general intellect that is much smarter than the best humans, in practically every field.</p><p> But according to Allan Dafoe - Assistant Professor of Political Science at Yale University - even if we stopped at today's AI technology and simply collected more data, built more sensors, and added more computing capacity, extreme systemic risks could emerge, including:</p><p> * Mass labor displacement, unemployment, and inequality;<br> * The rise of a more oligopolistic global market structure, potentially moving us away from our liberal economic world order;<br> * Imagery intelligence and other mechanisms for revealing most of the ballistic missile-carrying submarines that countries rely on to be able to respond to nuclear attack;<br> * Ubiquitous sensors and algorithms that can identify individuals through face recognition, leading to universal surveillance;<br> * Autonomous weapons with an independent chain of command, making it easier for authoritarian regimes to violently suppress their citizens.</p><p> Allan is Co-Director of the Governance of AI Program, at the Future of Humanity Institute within Oxford University. His goals have been to understand the causes of world peace and stability, which in the past has meant studying why war has declined, the role of reputation and honor as drivers of war, and the motivations behind provocation in crisis escalation.</p><p> <a href="https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/?utm_campaign=podcast__allan-dafoe&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript, links to learn more, and summary of key points.</strong></a></p><p> His current focus is helping humanity safely navigate the invention of advanced artificial intelligence.</p><p> I ask Allan:</p><p> * What are the distinctive characteristics of artificial intelligence from a political or international governance point of view?<br> * Is Allan’s work just a continuation of previous research on transformative technologies, like nuclear weapons?<br> * How can AI be well-governed?<br> * How should we think about the idea of arms races between companies or countries?<br> * What would you say to people skeptical about the importance of this topic?<br> * How urgently do we need to figure out solutions to these problems? When can we expect artificial intelligence to be dramatically better than today?<br> * What’s the most urgent questions to deal with in this field?<br> * What can people do if they want to get into the field?<br> * Is there anything unusual that people can look for in themselves to tell if they're a good fit to do this kind of research?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>The debate around the impacts of artificial intelligence often centres on ‘superintelligence’ - a general intellect that is much smarter than the best humans, in practically every field.</p><p> But according to Allan Dafoe - Assistant Professor of Political Science at Yale University - even if we stopped at today's AI technology and simply collected more data, built more sensors, and added more computing capacity, extreme systemic risks could emerge, including:</p><p> * Mass labor displacement, unemployment, and inequality;<br> * The rise of a more oligopolistic global market structure, potentially moving us away from our liberal economic world order;<br> * Imagery intelligence and other mechanisms for revealing most of the ballistic missile-carrying submarines that countries rely on to be able to respond to nuclear attack;<br> * Ubiquitous sensors and algorithms that can identify individuals through face recognition, leading to universal surveillance;<br> * Autonomous weapons with an independent chain of command, making it easier for authoritarian regimes to violently suppress their citizens.</p><p> Allan is Co-Director of the Governance of AI Program, at the Future of Humanity Institute within Oxford University. His goals have been to understand the causes of world peace and stability, which in the past has meant studying why war has declined, the role of reputation and honor as drivers of war, and the motivations behind provocation in crisis escalation.</p><p> <a href="https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/?utm_campaign=podcast__allan-dafoe&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript, links to learn more, and summary of key points.</strong></a></p><p> His current focus is helping humanity safely navigate the invention of advanced artificial intelligence.</p><p> I ask Allan:</p><p> * What are the distinctive characteristics of artificial intelligence from a political or international governance point of view?<br> * Is Allan’s work just a continuation of previous research on transformative technologies, like nuclear weapons?<br> * How can AI be well-governed?<br> * How should we think about the idea of arms races between companies or countries?<br> * What would you say to people skeptical about the importance of this topic?<br> * How urgently do we need to figure out solutions to these problems? When can we expect artificial intelligence to be dramatically better than today?<br> * What’s the most urgent questions to deal with in this field?<br> * What can people do if they want to get into the field?<br> * Is there anything unusual that people can look for in themselves to tell if they're a good fit to do this kind of research?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 18 May 2018 13:49:06 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/166665be/7ffc7859.mp3" length="46251742" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/diryX1VI7hkGfwO18NO8VOOKLTQbZu97R9aspIJcv-s/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDkv/MTY4MzU0NDU3MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2888</itunes:duration>
      <itunes:summary>The debate around the impacts of artificial intelligence often centres on ‘superintelligence’ - a general intellect that is much smarter than the best humans, in practically every field.

But according to Allan Dafoe - Assistant Professor of Political Science at Yale University - even if we stopped at today's AI technology and simply collected more data, built more sensors, and added more computing capacity, extreme systemic risks could emerge, including:

* Mass labor displacement, unemployment, and inequality;
* The rise of a more oligopolistic global market structure, potentially moving us away from our liberal economic world order;
* Imagery intelligence and other mechanisms for revealing most of the ballistic missile-carrying submarines that countries rely on to be able to respond to nuclear attack;
* Ubiquitous sensors and algorithms that can identify individuals through face recognition, leading to universal surveillance;
* Autonomous weapons with an independent chain of command, making it easier for authoritarian regimes to violently suppress their citizens.

Allan is Co-Director of the Governance of AI Program, at the Future of Humanity Institute within Oxford University. His goals have been to understand the causes of world peace and stability, which in the past has meant studying why war has declined, the role of reputation and honor as drivers of war, and the motivations behind provocation in crisis escalation.

Full transcript, links to learn more, and summary of key points.

His current focus is helping humanity safely navigate the invention of advanced artificial intelligence.

I ask Allan:

* What are the distinctive characteristics of artificial intelligence from a political or international governance point of view?
* Is Allan’s work just a continuation of previous research on transformative technologies, like nuclear weapons?
* How can AI be well-governed?
* How should we think about the idea of arms races between companies or countries?
* What would you say to people skeptical about the importance of this topic?
* How urgently do we need to figure out solutions to these problems? When can we expect artificial intelligence to be dramatically better than today?
* What’s the most urgent questions to deal with in this field?
* What can people do if they want to get into the field?
* Is there anything unusual that people can look for in themselves to tell if they're a good fit to do this kind of research?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>The debate around the impacts of artificial intelligence often centres on ‘superintelligence’ - a general intellect that is much smarter than the best humans, in practically every field.

But according to Allan Dafoe - Assistant Professor of Political S</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#30 - Eva Vivalt on how little social science findings generalize from one study to another</title>
      <itunes:title>#30 - Eva Vivalt on how little social science findings generalize from one study to another</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/444213924</guid>
      <link>https://share.transistor.fm/s/057fb1eb</link>
      <description>
        <![CDATA[<p>If we have a study on the impact of a social program in a particular place and time, how confident can we be that we’ll get a similar result if we study the same program again somewhere else?</p><p> Dr Eva Vivalt is a lecturer in the Research School of Economics at the Australian National University. She compiled a huge database of impact evaluations in global development - including 15,024 estimates from 635 papers across 20 types of intervention - to help answer this question.</p><p> Her finding: not confident at all.</p><p> The typical study result differs from the average effect found in similar studies so far by almost 100%. That is to say, if all existing studies of a particular education program find that it improves test scores by 10 points - the next result is as likely to be negative or greater than 20 points, as it is to be between 0-20 points.</p><p> She also observed that results from smaller studies done with an NGO - often pilot studies - were more likely to look promising. But when governments tried to implement scaled-up versions of those programs, their performance would drop considerably. </p><p> For researchers hoping to figure out what works and then take those programs global, these failures of generalizability and ‘external validity’ should be disconcerting.</p><p> Is ‘evidence-based development’ writing a cheque its methodology can’t cash? Should this make us invest less in empirical research, or more to get actually reliable results?</p><p> Or as some critics say, is interest in impact evaluation distracting us from more important issues, like national or macroeconomic reforms that can’t be easily trialled?</p><p> We discuss this as well as Eva’s other research, including Y Combinator’s basic income study where she is a principal investigator.</p><p> <a href="https://80000hours.org/podcast/episodes/eva-vivalt-social-science-generalizability/?utm_campaign=podcast__eva-vivalt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, links to related papers, and highlights from the conversation.</a></p><p> <strong>Links mentioned at the start of the show:</strong><br> * <a href="https://80000hours.org/job-board/?utm_campaign=podcast__eva-vivalt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">80,000 Hours Job Board</a><br> * <a href="https://www.surveymonkey.com/r/BX56XKV">2018 Effective Altruism Survey</a></p><p> **Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.**</p><p> Questions include:</p><p> * What is the YC basic income study looking at, and what motivates it?<br> * How do we get people to accept clean meat?<br> * How much can we generalize from impact evaluations?<br> * How much can we generalize from studies in development economics?<br> * Should we be running more or fewer studies?<br> * Do most social programs work or not?<br> * The academic incentives around data aggregation<br> * How much can impact evaluations inform policy decisions?<br> * How often do people change their minds?<br> * Do policy makers update too much or too little in the real world?<br> * How good or bad are the predictions of experts? How does that change when looking at individuals versus the average of a group?<br> * How often should we believe positive results?<br> * What’s the state of development economics?<br> * Eva’s thoughts on our article on social interventions<br> * How much can we really learn from being empirical?<br> * How much should we really value RCTs?<br> * Is an Economics PhD overrated or underrated?</p><p> <strong>Get this episode by subscribing to our podcast: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>If we have a study on the impact of a social program in a particular place and time, how confident can we be that we’ll get a similar result if we study the same program again somewhere else?</p><p> Dr Eva Vivalt is a lecturer in the Research School of Economics at the Australian National University. She compiled a huge database of impact evaluations in global development - including 15,024 estimates from 635 papers across 20 types of intervention - to help answer this question.</p><p> Her finding: not confident at all.</p><p> The typical study result differs from the average effect found in similar studies so far by almost 100%. That is to say, if all existing studies of a particular education program find that it improves test scores by 10 points - the next result is as likely to be negative or greater than 20 points, as it is to be between 0-20 points.</p><p> She also observed that results from smaller studies done with an NGO - often pilot studies - were more likely to look promising. But when governments tried to implement scaled-up versions of those programs, their performance would drop considerably. </p><p> For researchers hoping to figure out what works and then take those programs global, these failures of generalizability and ‘external validity’ should be disconcerting.</p><p> Is ‘evidence-based development’ writing a cheque its methodology can’t cash? Should this make us invest less in empirical research, or more to get actually reliable results?</p><p> Or as some critics say, is interest in impact evaluation distracting us from more important issues, like national or macroeconomic reforms that can’t be easily trialled?</p><p> We discuss this as well as Eva’s other research, including Y Combinator’s basic income study where she is a principal investigator.</p><p> <a href="https://80000hours.org/podcast/episodes/eva-vivalt-social-science-generalizability/?utm_campaign=podcast__eva-vivalt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, links to related papers, and highlights from the conversation.</a></p><p> <strong>Links mentioned at the start of the show:</strong><br> * <a href="https://80000hours.org/job-board/?utm_campaign=podcast__eva-vivalt&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">80,000 Hours Job Board</a><br> * <a href="https://www.surveymonkey.com/r/BX56XKV">2018 Effective Altruism Survey</a></p><p> **Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.**</p><p> Questions include:</p><p> * What is the YC basic income study looking at, and what motivates it?<br> * How do we get people to accept clean meat?<br> * How much can we generalize from impact evaluations?<br> * How much can we generalize from studies in development economics?<br> * Should we be running more or fewer studies?<br> * Do most social programs work or not?<br> * The academic incentives around data aggregation<br> * How much can impact evaluations inform policy decisions?<br> * How often do people change their minds?<br> * Do policy makers update too much or too little in the real world?<br> * How good or bad are the predictions of experts? How does that change when looking at individuals versus the average of a group?<br> * How often should we believe positive results?<br> * What’s the state of development economics?<br> * Eva’s thoughts on our article on social interventions<br> * How much can we really learn from being empirical?<br> * How much should we really value RCTs?<br> * Is an Economics PhD overrated or underrated?</p><p> <strong>Get this episode by subscribing to our podcast: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 15 May 2018 17:43:15 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/057fb1eb/d600b026.mp3" length="116747151" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/nJvceyAwt_4DScIWBGn41toP9asOcxAuk__It5fxE5g/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDgv/MTY4MzU0NDU3MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7289</itunes:duration>
      <itunes:summary>If we have a study on the impact of a social program in a particular place and time, how confident can we be that we’ll get a similar result if we study the same program again somewhere else?

Dr Eva Vivalt is a lecturer in the Research School of Economics at the Australian National University. She compiled a huge database of impact evaluations in global development - including 15,024 estimates from 635 papers across 20 types of intervention - to help answer this question.

Her finding: not confident at all.

The typical study result differs from the average effect found in similar studies so far by almost 100%. That is to say, if all existing studies of a particular education program find that it improves test scores by 10 points - the next result is as likely to be negative or greater than 20 points, as it is to be between 0-20 points.

She also observed that results from smaller studies done with an NGO - often pilot studies - were more likely to look promising. But when governments tried to implement scaled-up versions of those programs, their performance would drop considerably. 

For researchers hoping to figure out what works and then take those programs global, these failures of generalizability and ‘external validity’ should be disconcerting.

Is ‘evidence-based development’ writing a cheque its methodology can’t cash? Should this make us invest less in empirical research, or more to get actually reliable results?

Or as some critics say, is interest in impact evaluation distracting us from more important issues, like national or macroeconomic reforms that can’t be easily trialled?

We discuss this as well as Eva’s other research, including Y Combinator’s basic income study where she is a principal investigator.

Full transcript, links to related papers, and highlights from the conversation.

Links mentioned at the start of the show:
* 80,000 Hours Job Board
* 2018 Effective Altruism Survey

**Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.**

Questions include:

* What is the YC basic income study looking at, and what motivates it?
* How do we get people to accept clean meat?
* How much can we generalize from impact evaluations?
* How much can we generalize from studies in development economics?
* Should we be running more or fewer studies?
* Do most social programs work or not?
* The academic incentives around data aggregation
* How much can impact evaluations inform policy decisions?
* How often do people change their minds?
* Do policy makers update too much or too little in the real world?
* How good or bad are the predictions of experts? How does that change when looking at individuals versus the average of a group?
* How often should we believe positive results?
* What’s the state of development economics?
* Eva’s thoughts on our article on social interventions
* How much can we really learn from being empirical?
* How much should we really value RCTs?
* Is an Economics PhD overrated or underrated?

Get this episode by subscribing to our podcast: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>If we have a study on the impact of a social program in a particular place and time, how confident can we be that we’ll get a similar result if we study the same program again somewhere else?

Dr Eva Vivalt is a lecturer in the Research School of Econom</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#29 - Anders Sandberg on 3 new resolutions for the Fermi paradox &amp; how to colonise the universe</title>
      <itunes:title>#29 - Anders Sandberg on 3 new resolutions for the Fermi paradox &amp; how to colonise the universe</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/440509452</guid>
      <link>https://share.transistor.fm/s/7d600878</link>
      <description>
        <![CDATA[<p>Part 2 out now: <strong>#33 - Dr Anders Sandberg on what if we ended ageing, solar flares &amp; the annual risk of nuclear war</strong></p><p> The universe is so vast, yet we don’t see any alien civilizations. If they exist, where are they? Oxford University’s Anders Sandberg has an original answer: they’re ‘sleeping’, and for a very compelling reason.</p><p> Because of the thermodynamics of computation, the colder it gets, the more computations you can do. The universe is getting exponentially colder as it expands, and as the universe cools, one Joule of energy gets worth more and more. If they wait long enough this can become a 10,000,000,000,000,000,000,000,000,000,000x gain. So, if a civilization wanted to maximize its ability to perform computations – its best option might be to lie in wait for trillions of years.</p><p> Why would a civilization want to maximise the number of computations they can do? Because conscious minds are probably generated by computation, so doing twice as many computations is like living twice as long, in subjective time. Waiting will allow them to generate vastly more science, art, pleasure, or almost anything else they are likely to care about.</p><p> <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, related links, and key quotes.</a></p><p> But there’s no point waking up to find another civilization has taken over and used up the universe’s energy. So they’ll need some sort of monitoring to protect their resources from potential competitors like us.</p><p> It’s plausible that this civilization would want to keep the universe’s matter concentrated, so that each part would be in reach of the other parts, even after the universe’s expansion. But that would mean changing the trajectory of galaxies during this dormant period. That we don’t see anything like that makes it more likely that these aliens have local outposts throughout the universe, and we wouldn’t notice them until we broke their rules. But breaking their rules might be our last action as a species.</p><p> This ‘aestivation hypothesis’ is the invention of Dr Sandberg, a Senior Research Fellow at the Future of Humanity Institute at Oxford University, where he looks at low-probability, high-impact risks, predicting the capabilities of future technologies and very long-range futures for humanity.</p><p> In this incredibly fun conversation we cover this and other possible explanations to the Fermi paradox, as well as questions like:</p><p> * Should we want optimists or pessimists working on our most important problems?<br> * How should we reason about low probability, high impact risks?<br> * Would a galactic civilization want to stop the stars from burning?<br> * What would be the best strategy for exploring and colonising the universe?<br> * How can you stay coordinated when you’re spread across different galaxies?<br> * What should humanity decide to do with its future?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Part 2 out now: <strong>#33 - Dr Anders Sandberg on what if we ended ageing, solar flares &amp; the annual risk of nuclear war</strong></p><p> The universe is so vast, yet we don’t see any alien civilizations. If they exist, where are they? Oxford University’s Anders Sandberg has an original answer: they’re ‘sleeping’, and for a very compelling reason.</p><p> Because of the thermodynamics of computation, the colder it gets, the more computations you can do. The universe is getting exponentially colder as it expands, and as the universe cools, one Joule of energy gets worth more and more. If they wait long enough this can become a 10,000,000,000,000,000,000,000,000,000,000x gain. So, if a civilization wanted to maximize its ability to perform computations – its best option might be to lie in wait for trillions of years.</p><p> Why would a civilization want to maximise the number of computations they can do? Because conscious minds are probably generated by computation, so doing twice as many computations is like living twice as long, in subjective time. Waiting will allow them to generate vastly more science, art, pleasure, or almost anything else they are likely to care about.</p><p> <a href="https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, related links, and key quotes.</a></p><p> But there’s no point waking up to find another civilization has taken over and used up the universe’s energy. So they’ll need some sort of monitoring to protect their resources from potential competitors like us.</p><p> It’s plausible that this civilization would want to keep the universe’s matter concentrated, so that each part would be in reach of the other parts, even after the universe’s expansion. But that would mean changing the trajectory of galaxies during this dormant period. That we don’t see anything like that makes it more likely that these aliens have local outposts throughout the universe, and we wouldn’t notice them until we broke their rules. But breaking their rules might be our last action as a species.</p><p> This ‘aestivation hypothesis’ is the invention of Dr Sandberg, a Senior Research Fellow at the Future of Humanity Institute at Oxford University, where he looks at low-probability, high-impact risks, predicting the capabilities of future technologies and very long-range futures for humanity.</p><p> In this incredibly fun conversation we cover this and other possible explanations to the Fermi paradox, as well as questions like:</p><p> * Should we want optimists or pessimists working on our most important problems?<br> * How should we reason about low probability, high impact risks?<br> * Would a galactic civilization want to stop the stars from burning?<br> * What would be the best strategy for exploring and colonising the universe?<br> * How can you stay coordinated when you’re spread across different galaxies?<br> * What should humanity decide to do with its future?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 08 May 2018 16:17:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/7d600878/7bc6c8e9.mp3" length="78264288" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/sL8scJ_yQhFaLqGthjQvL2123SsBB-57Tgmlrg01Z-c/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDcv/MTY4MzU0NDU2OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4887</itunes:duration>
      <itunes:summary>Part 2 out now: #33 - Dr Anders Sandberg on what if we ended ageing, solar flares &amp;amp; the annual risk of nuclear war

The universe is so vast, yet we don’t see any alien civilizations. If they exist, where are they? Oxford University’s Anders Sandberg has an original answer: they’re ‘sleeping’, and for a very compelling reason.

Because of the thermodynamics of computation, the colder it gets, the more computations you can do. The universe is getting exponentially colder as it expands, and as the universe cools, one Joule of energy gets worth more and more. If they wait long enough this can become a 10,000,000,000,000,000,000,000,000,000,000x gain. So, if a civilization wanted to maximize its ability to perform computations – its best option might be to lie in wait for trillions of years.

Why would a civilization want to maximise the number of computations they can do? Because conscious minds are probably generated by computation, so doing twice as many computations is like living twice as long, in subjective time. Waiting will allow them to generate vastly more science, art, pleasure, or almost anything else they are likely to care about.

Full transcript, related links, and key quotes.

But there’s no point waking up to find another civilization has taken over and used up the universe’s energy. So they’ll need some sort of monitoring to protect their resources from potential competitors like us.

It’s plausible that this civilization would want to keep the universe’s matter concentrated, so that each part would be in reach of the other parts, even after the universe’s expansion. But that would mean changing the trajectory of galaxies during this dormant period. That we don’t see anything like that makes it more likely that these aliens have local outposts throughout the universe, and we wouldn’t notice them until we broke their rules. But breaking their rules might be our last action as a species.

This ‘aestivation hypothesis’ is the invention of Dr Sandberg, a Senior Research Fellow at the Future of Humanity Institute at Oxford University, where he looks at low-probability, high-impact risks, predicting the capabilities of future technologies and very long-range futures for humanity.

In this incredibly fun conversation we cover this and other possible explanations to the Fermi paradox, as well as questions like:

* Should we want optimists or pessimists working on our most important problems?
* How should we reason about low probability, high impact risks?
* Would a galactic civilization want to stop the stars from burning?
* What would be the best strategy for exploring and colonising the universe?
* How can you stay coordinated when you’re spread across different galaxies?
* What should humanity decide to do with its future?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>Part 2 out now: #33 - Dr Anders Sandberg on what if we ended ageing, solar flares &amp;amp; the annual risk of nuclear war

The universe is so vast, yet we don’t see any alien civilizations. If they exist, where are they? Oxford University’s Anders Sandberg</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#28 - Owen Cotton-Barratt on why scientists should need insurance, PhD strategy &amp; fast AI progresses</title>
      <itunes:title>#28 - Owen Cotton-Barratt on why scientists should need insurance, PhD strategy &amp; fast AI progresses</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/435909588</guid>
      <link>https://share.transistor.fm/s/a034c5b5</link>
      <description>
        <![CDATA[<p>A researcher is working on creating a new virus – one more dangerous than any that exist naturally. They believe they’re being as careful as possible. After all, if things go wrong, their own life and that of their colleagues will be in danger. But if an accident is capable of triggering a global pandemic – hundreds of millions of lives might be at risk. How much additional care will the researcher actually take in the face of such a staggering death toll?</p><p> In a new paper Dr Owen Cotton-Barratt, a Research Fellow at Oxford University’s Future of Humanity Institute, argues it’s impossible to expect them to make the correct adjustments. If they have an accident that kills 5 people – they’ll feel extremely bad. If they have an accident that kills 500 million people, they’ll feel even worse – but there’s no way for them to feel 100 million times worse. The brain simply doesn’t work that way.</p><p> So, rather than relying on individual judgement, we could create a system that would lead to better outcomes: research liability insurance. </p><p> <a href="https://80000hours.org/podcast/episodes/owen-cotton-barratt-regulating-risky-research/?utm_campaign=podcast__owen-cb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></p><p> Once an insurer assesses how much damage a particular project is expected to cause and with what likelihood – in order to proceed, the researcher would need to take out insurance against the predicted risk. In return, the insurer promises that they’ll pay out – potentially tens of billions of dollars – if things go really badly.</p><p> This would force researchers think very carefully about the cost and benefits of their work – and incentivize the insurer to demand safety standards on a level that individual researchers can’t be expected to impose themselves.</p><p> ***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.***</p><p> Owen is <a href="http://www.fhi.ox.ac.uk/rsp/">currently hiring</a> for a selective, two-year research scholars programme at Oxford.</p><p> In this wide-ranging conversation Owen and I also discuss:</p><p> * Are academics wrong to value personal interest in a topic over its importance?<br> * What fraction of research has very large potential negative consequences?<br> * Why do we have such different reactions to situations where the risks are known and unknown?<br> * The downsides of waiting for tenure to do the work you think is most important.<br> * What are the benefits of specifying a vague problem like ‘make AI safe’ more clearly?<br> * How should people balance the trade-offs between having a successful career and doing the most important work?<br> * Are there any blind alleys we’ve gone down when thinking about AI safety?<br> * Why did Owen give to an organisation whose research agenda he is skeptical of?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>A researcher is working on creating a new virus – one more dangerous than any that exist naturally. They believe they’re being as careful as possible. After all, if things go wrong, their own life and that of their colleagues will be in danger. But if an accident is capable of triggering a global pandemic – hundreds of millions of lives might be at risk. How much additional care will the researcher actually take in the face of such a staggering death toll?</p><p> In a new paper Dr Owen Cotton-Barratt, a Research Fellow at Oxford University’s Future of Humanity Institute, argues it’s impossible to expect them to make the correct adjustments. If they have an accident that kills 5 people – they’ll feel extremely bad. If they have an accident that kills 500 million people, they’ll feel even worse – but there’s no way for them to feel 100 million times worse. The brain simply doesn’t work that way.</p><p> So, rather than relying on individual judgement, we could create a system that would lead to better outcomes: research liability insurance. </p><p> <a href="https://80000hours.org/podcast/episodes/owen-cotton-barratt-regulating-risky-research/?utm_campaign=podcast__owen-cb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, summary and full transcript.</a></p><p> Once an insurer assesses how much damage a particular project is expected to cause and with what likelihood – in order to proceed, the researcher would need to take out insurance against the predicted risk. In return, the insurer promises that they’ll pay out – potentially tens of billions of dollars – if things go really badly.</p><p> This would force researchers think very carefully about the cost and benefits of their work – and incentivize the insurer to demand safety standards on a level that individual researchers can’t be expected to impose themselves.</p><p> ***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.***</p><p> Owen is <a href="http://www.fhi.ox.ac.uk/rsp/">currently hiring</a> for a selective, two-year research scholars programme at Oxford.</p><p> In this wide-ranging conversation Owen and I also discuss:</p><p> * Are academics wrong to value personal interest in a topic over its importance?<br> * What fraction of research has very large potential negative consequences?<br> * Why do we have such different reactions to situations where the risks are known and unknown?<br> * The downsides of waiting for tenure to do the work you think is most important.<br> * What are the benefits of specifying a vague problem like ‘make AI safe’ more clearly?<br> * How should people balance the trade-offs between having a successful career and doing the most important work?<br> * Are there any blind alleys we’ve gone down when thinking about AI safety?<br> * Why did Owen give to an organisation whose research agenda he is skeptical of?</p><p> <strong>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</strong></p><p> <em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Fri, 27 Apr 2018 22:00:28 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a034c5b5/6e561ec9.mp3" length="60655046" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/SiQ1PSkB7N7M3bzhlLZ5nmlXxvzfUJVq1VJc27dBUx8/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDYv/MTY4MzU0NDU2OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3786</itunes:duration>
      <itunes:summary>A researcher is working on creating a new virus – one more dangerous than any that exist naturally. They believe they’re being as careful as possible. After all, if things go wrong, their own life and that of their colleagues will be in danger. But if an accident is capable of triggering a global pandemic – hundreds of millions of lives might be at risk. How much additional care will the researcher actually take in the face of such a staggering death toll?

In a new paper Dr Owen Cotton-Barratt, a Research Fellow at Oxford University’s Future of Humanity Institute, argues it’s impossible to expect them to make the correct adjustments. If they have an accident that kills 5 people – they’ll feel extremely bad. If they have an accident that kills 500 million people, they’ll feel even worse – but there’s no way for them to feel 100 million times worse. The brain simply doesn’t work that way.

So, rather than relying on individual judgement, we could create a system that would lead to better outcomes: research liability insurance. 

Links to learn more, summary and full transcript.

Once an insurer assesses how much damage a particular project is expected to cause and with what likelihood – in order to proceed, the researcher would need to take out insurance against the predicted risk. In return, the insurer promises that they’ll pay out – potentially tens of billions of dollars – if things go really badly.

This would force researchers think very carefully about the cost and benefits of their work – and incentivize the insurer to demand safety standards on a level that individual researchers can’t be expected to impose themselves.

***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.***

Owen is currently hiring for a selective, two-year research scholars programme at Oxford.

In this wide-ranging conversation Owen and I also discuss:

* Are academics wrong to value personal interest in a topic over its importance?
* What fraction of research has very large potential negative consequences?
* Why do we have such different reactions to situations where the risks are known and unknown?
* The downsides of waiting for tenure to do the work you think is most important.
* What are the benefits of specifying a vague problem like ‘make AI safe’ more clearly?
* How should people balance the trade-offs between having a successful career and doing the most important work?
* Are there any blind alleys we’ve gone down when thinking about AI safety?
* Why did Owen give to an organisation whose research agenda he is skeptical of?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>A researcher is working on creating a new virus – one more dangerous than any that exist naturally. They believe they’re being as careful as possible. After all, if things go wrong, their own life and that of their colleagues will be in danger. But if an </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#27 - Dr Tom Inglesby on careers and policies that reduce global catastrophic biological risks</title>
      <itunes:title>#27 - Dr Tom Inglesby on careers and policies that reduce global catastrophic biological risks</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/430943736</guid>
      <link>https://share.transistor.fm/s/d21681b9</link>
      <description>
        <![CDATA[How about this for a movie idea: a main character has to prevent a new contagious strain of Ebola spreading around the world. She’s the best of the best. So good in fact, that her work on early detection systems contains the strain at its source. Ten minutes into the movie, we see the results of her work – nothing happens. Life goes on as usual. She continues to be amazingly competent, and nothing continues to go wrong. Fade to black. Roll credits.<p>

If your job is to prevent catastrophes, success is when nobody has to pay attention to you. But without regular disasters to remind authorities why they hired you in the first place, they can’t tell if you’re actually achieving anything. And when budgets come under pressure you may find that success condemns you to the chopping block. </p><p>

Dr Tom Inglesby, Director of the Center for Health Security at the Johns Hopkins Bloomberg School of Public Health, worries this may be about to happen to the scientists working on the ‘Global Health Security Agenda’.</p><p>

In 2014 Ebola showed the world why we have to detect and contain new diseases before they spread, and that when it comes to contagious diseases the nations of the world sink or swim together. Fifty countries decided to work together to make sure all their health systems were up to the challenge. Back then Congress provided 5 years’ funding to help some of the world’s poorest countries build the basic health security infrastructure necessary to control pathogens before they could reach the US.</p><p>

<a href="https://80000hours.org/podcast/episodes/tom-inglesby-health-security/?utm_campaign=podcast__tom-inglesby&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, job opportunities, and full transcript.</a></p><p>

But with Ebola fading from public memory and no recent tragedies to terrify us, Congress may not renew that funding and the project could fall apart. (Learn more about how you can help: http://www.nti.org/analysis/articles/protect-us-investments-global-health-security/ )</p><p>

But there are positive signs as well - the center Inglesby leads recently received a $16 million grant from the Open Philanthropy Project to further their work preventing global catastrophes. It also runs the [Emerging Leaders in Biosecurity Fellowship](http://www.centerforhealthsecurity.org/our-work/emergingbioleaders/) to train the next generation of biosecurity experts for the US government. And Inglesby regularly testifies to Congress on the threats we all face and how to address them.</p><p>

In this in-depth interview we try to provide concrete guidance for listeners who want to to pursue a career in health security. Some of the topics we cover include:</p><p>

* Should more people in medicine work on security?<br>
* What are the top jobs for people who want to improve health security and how do they work towards getting them?<br>
* What people can do to protect funding for the Global Health Security Agenda.<br>
* Should we be more concerned about natural or human caused pandemics? Which is more neglected?<br>
* Should we be allocating more attention and resources to global catastrophic risk scenarios?<br>
* Why are senior figures reluctant to prioritize one project or area at the expense of another?  <br>
* What does Tom think about the idea that in the medium term, human-caused pandemics will pose a far greater risk than natural pandemics, and so we should focus on specific counter-measures?<br>
* Are the main risks and solutions understood, and it’s just a matter of implementation? Or is the principal task to identify and understand them?<br>
* How is the current US government performing in these areas?<br>
* Which agencies are empowered to think about low probability high magnitude events?<br>
And more...</p><p>

<b>Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[How about this for a movie idea: a main character has to prevent a new contagious strain of Ebola spreading around the world. She’s the best of the best. So good in fact, that her work on early detection systems contains the strain at its source. Ten minutes into the movie, we see the results of her work – nothing happens. Life goes on as usual. She continues to be amazingly competent, and nothing continues to go wrong. Fade to black. Roll credits.<p>

If your job is to prevent catastrophes, success is when nobody has to pay attention to you. But without regular disasters to remind authorities why they hired you in the first place, they can’t tell if you’re actually achieving anything. And when budgets come under pressure you may find that success condemns you to the chopping block. </p><p>

Dr Tom Inglesby, Director of the Center for Health Security at the Johns Hopkins Bloomberg School of Public Health, worries this may be about to happen to the scientists working on the ‘Global Health Security Agenda’.</p><p>

In 2014 Ebola showed the world why we have to detect and contain new diseases before they spread, and that when it comes to contagious diseases the nations of the world sink or swim together. Fifty countries decided to work together to make sure all their health systems were up to the challenge. Back then Congress provided 5 years’ funding to help some of the world’s poorest countries build the basic health security infrastructure necessary to control pathogens before they could reach the US.</p><p>

<a href="https://80000hours.org/podcast/episodes/tom-inglesby-health-security/?utm_campaign=podcast__tom-inglesby&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Links to learn more, job opportunities, and full transcript.</a></p><p>

But with Ebola fading from public memory and no recent tragedies to terrify us, Congress may not renew that funding and the project could fall apart. (Learn more about how you can help: http://www.nti.org/analysis/articles/protect-us-investments-global-health-security/ )</p><p>

But there are positive signs as well - the center Inglesby leads recently received a $16 million grant from the Open Philanthropy Project to further their work preventing global catastrophes. It also runs the [Emerging Leaders in Biosecurity Fellowship](http://www.centerforhealthsecurity.org/our-work/emergingbioleaders/) to train the next generation of biosecurity experts for the US government. And Inglesby regularly testifies to Congress on the threats we all face and how to address them.</p><p>

In this in-depth interview we try to provide concrete guidance for listeners who want to to pursue a career in health security. Some of the topics we cover include:</p><p>

* Should more people in medicine work on security?<br>
* What are the top jobs for people who want to improve health security and how do they work towards getting them?<br>
* What people can do to protect funding for the Global Health Security Agenda.<br>
* Should we be more concerned about natural or human caused pandemics? Which is more neglected?<br>
* Should we be allocating more attention and resources to global catastrophic risk scenarios?<br>
* Why are senior figures reluctant to prioritize one project or area at the expense of another?  <br>
* What does Tom think about the idea that in the medium term, human-caused pandemics will pose a far greater risk than natural pandemics, and so we should focus on specific counter-measures?<br>
* Are the main risks and solutions understood, and it’s just a matter of implementation? Or is the principal task to identify and understand them?<br>
* How is the current US government performing in these areas?<br>
* Which agencies are empowered to think about low probability high magnitude events?<br>
And more...</p><p>

<b>Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Wed, 18 Apr 2018 01:30:03 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/d21681b9/96fe671e.mp3" length="131364895" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/0maBr63Ycl30UBlj2puU3ehbyreC1RQrePQlUOyE1Sk/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDUv/MTY4MzU0NDU2Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>8201</itunes:duration>
      <itunes:summary>How about this for a movie idea: a main character has to prevent a new contagious strain of Ebola spreading around the world. She’s the best of the best. So good in fact, that her work on early detection systems contains the strain at its source. Ten minutes into the movie, we see the results of her work – nothing happens. Life goes on as usual. She continues to be amazingly competent, and nothing continues to go wrong. Fade to black. Roll credits.

If your job is to prevent catastrophes, success is when nobody has to pay attention to you. But without regular disasters to remind authorities why they hired you in the first place, they can’t tell if you’re actually achieving anything. And when budgets come under pressure you may find that success condemns you to the chopping block. 

Dr Tom Inglesby, Director of the Center for Health Security at the Johns Hopkins Bloomberg School of Public Health, worries this may be about to happen to the scientists working on the ‘Global Health Security Agenda’.

In 2014 Ebola showed the world why we have to detect and contain new diseases before they spread, and that when it comes to contagious diseases the nations of the world sink or swim together. Fifty countries decided to work together to make sure all their health systems were up to the challenge. Back then Congress provided 5 years’ funding to help some of the world’s poorest countries build the basic health security infrastructure necessary to control pathogens before they could reach the US.

Links to learn more, job opportunities, and full transcript.

But with Ebola fading from public memory and no recent tragedies to terrify us, Congress may not renew that funding and the project could fall apart. (Learn more about how you can help: http://www.nti.org/analysis/articles/protect-us-investments-global-health-security/ )

But there are positive signs as well - the center Inglesby leads recently received a $16 million grant from the Open Philanthropy Project to further their work preventing global catastrophes. It also runs the [Emerging Leaders in Biosecurity Fellowship](http://www.centerforhealthsecurity.org/our-work/emergingbioleaders/) to train the next generation of biosecurity experts for the US government. And Inglesby regularly testifies to Congress on the threats we all face and how to address them.

In this in-depth interview we try to provide concrete guidance for listeners who want to to pursue a career in health security. Some of the topics we cover include:

* Should more people in medicine work on security?
* What are the top jobs for people who want to improve health security and how do they work towards getting them?
* What people can do to protect funding for the Global Health Security Agenda.
* Should we be more concerned about natural or human caused pandemics? Which is more neglected?
* Should we be allocating more attention and resources to global catastrophic risk scenarios?
* Why are senior figures reluctant to prioritize one project or area at the expense of another?  
* What does Tom think about the idea that in the medium term, human-caused pandemics will pose a far greater risk than natural pandemics, and so we should focus on specific counter-measures?
* Are the main risks and solutions understood, and it’s just a matter of implementation? Or is the principal task to identify and understand them?
* How is the current US government performing in these areas?
* Which agencies are empowered to think about low probability high magnitude events?
And more...

Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>How about this for a movie idea: a main character has to prevent a new contagious strain of Ebola spreading around the world. She’s the best of the best. So good in fact, that her work on early detection systems contains the strain at its source. Ten minu</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#26 - Marie Gibbons on how exactly clean meat is made &amp; what's needed to get it in every supermarket</title>
      <itunes:title>#26 - Marie Gibbons on how exactly clean meat is made &amp; what's needed to get it in every supermarket</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/427544028</guid>
      <link>https://share.transistor.fm/s/864bc232</link>
      <description>
        <![CDATA[First, decide on the type of animal. Next, pick the cell type. Then take a small, painless biopsy, and put the cells in a solution that makes them feel like they’re still in the body. Once the cells are in this comfortable state, they'll proliferate. One cell becomes two, two becomes four, four becomes eight, and so on. Continue until you have enough cells to make a burger, a nugget, a sausage, or a piece of bacon, then concentrate them until they bind into solid meat.<p>

It's all surprisingly straightforward in principle according to Marie Gibbons​, a research fellow with The Good Food Institute, who has been researching how to improve this process at Harvard Medical School. We might even see clean meat sold commercially within a year.</p><p>

The real technical challenge is developing large bioreactors and cheap solutions so that we can make huge volumes and drive down costs.</p><p>

This interview covers the science and technology involved at each stage of clean meat production, the challenges and opportunities that face cutting-edge researchers like Marie, and how you could become one of them.</p><p>

<a href="https://80000hours.org/podcast/episodes/marie-gibbons-clean-meat/?utm_campaign=podcast__marie-gibbons&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, key points, and links to learn more.</a> </p><p>

Marie’s research focuses on turkey cells. But as she explains, with clean meat the possibilities extend well beyond those of traditional meat. Chicken, cow, pig, but also panda - and even dinosaurs could be on the menus of the future.</p><p>

Today’s episode is hosted by Natalie Cargill, a barrister in London with a background in animal advocacy. Natalie and Marie also discuss:</p><p>

* Why Marie switched from being a vet to developing clean meat<br>
* For people who want to dedicate themselves to animal welfare, how does working in clean meat fare compared to other career options? How can people get jobs in the area?<br>
* How did this become an established field?<br>
* How important is the choice of animal species and cell type in this process?<br>
* What are the biggest problems with current production methods?<br>
* Is this kind of research best done in an academic setting, a commercial setting, or a balance between the two?<br>
* How easy will it be to get consumer acceptance?<br>
* How valuable would extra funding be for cellular agriculture?<br>
* Can we use genetic modification to speed up the process?<br>
* Is it reasonable to be sceptical of the possibility of clean meat becoming financially competitive with traditional meat any time in the near future?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </description>
      <content:encoded>
        <![CDATA[First, decide on the type of animal. Next, pick the cell type. Then take a small, painless biopsy, and put the cells in a solution that makes them feel like they’re still in the body. Once the cells are in this comfortable state, they'll proliferate. One cell becomes two, two becomes four, four becomes eight, and so on. Continue until you have enough cells to make a burger, a nugget, a sausage, or a piece of bacon, then concentrate them until they bind into solid meat.<p>

It's all surprisingly straightforward in principle according to Marie Gibbons​, a research fellow with The Good Food Institute, who has been researching how to improve this process at Harvard Medical School. We might even see clean meat sold commercially within a year.</p><p>

The real technical challenge is developing large bioreactors and cheap solutions so that we can make huge volumes and drive down costs.</p><p>

This interview covers the science and technology involved at each stage of clean meat production, the challenges and opportunities that face cutting-edge researchers like Marie, and how you could become one of them.</p><p>

<a href="https://80000hours.org/podcast/episodes/marie-gibbons-clean-meat/?utm_campaign=podcast__marie-gibbons&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, key points, and links to learn more.</a> </p><p>

Marie’s research focuses on turkey cells. But as she explains, with clean meat the possibilities extend well beyond those of traditional meat. Chicken, cow, pig, but also panda - and even dinosaurs could be on the menus of the future.</p><p>

Today’s episode is hosted by Natalie Cargill, a barrister in London with a background in animal advocacy. Natalie and Marie also discuss:</p><p>

* Why Marie switched from being a vet to developing clean meat<br>
* For people who want to dedicate themselves to animal welfare, how does working in clean meat fare compared to other career options? How can people get jobs in the area?<br>
* How did this become an established field?<br>
* How important is the choice of animal species and cell type in this process?<br>
* What are the biggest problems with current production methods?<br>
* Is this kind of research best done in an academic setting, a commercial setting, or a balance between the two?<br>
* How easy will it be to get consumer acceptance?<br>
* How valuable would extra funding be for cellular agriculture?<br>
* Can we use genetic modification to speed up the process?<br>
* Is it reasonable to be sceptical of the possibility of clean meat becoming financially competitive with traditional meat any time in the near future?</p><p>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b></p><p>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>]]>
      </content:encoded>
      <pubDate>Tue, 10 Apr 2018 16:52:18 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/864bc232/f1d44adb.mp3" length="100217733" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/HrWg5JGieirAiaOVUPThTV98IiAMveGZGElliDwC0AY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDQv/MTY4MzU0NDU2Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6257</itunes:duration>
      <itunes:summary>First, decide on the type of animal. Next, pick the cell type. Then take a small, painless biopsy, and put the cells in a solution that makes them feel like they’re still in the body. Once the cells are in this comfortable state, they'll proliferate. One cell becomes two, two becomes four, four becomes eight, and so on. Continue until you have enough cells to make a burger, a nugget, a sausage, or a piece of bacon, then concentrate them until they bind into solid meat.

It's all surprisingly straightforward in principle according to Marie Gibbons​, a research fellow with The Good Food Institute, who has been researching how to improve this process at Harvard Medical School. We might even see clean meat sold commercially within a year.

The real technical challenge is developing large bioreactors and cheap solutions so that we can make huge volumes and drive down costs.

This interview covers the science and technology involved at each stage of clean meat production, the challenges and opportunities that face cutting-edge researchers like Marie, and how you could become one of them.

Full transcript, key points, and links to learn more. 

Marie’s research focuses on turkey cells. But as she explains, with clean meat the possibilities extend well beyond those of traditional meat. Chicken, cow, pig, but also panda - and even dinosaurs could be on the menus of the future.

Today’s episode is hosted by Natalie Cargill, a barrister in London with a background in animal advocacy. Natalie and Marie also discuss:

* Why Marie switched from being a vet to developing clean meat
* For people who want to dedicate themselves to animal welfare, how does working in clean meat fare compared to other career options? How can people get jobs in the area?
* How did this become an established field?
* How important is the choice of animal species and cell type in this process?
* What are the biggest problems with current production methods?
* Is this kind of research best done in an academic setting, a commercial setting, or a balance between the two?
* How easy will it be to get consumer acceptance?
* How valuable would extra funding be for cellular agriculture?
* Can we use genetic modification to speed up the process?
* Is it reasonable to be sceptical of the possibility of clean meat becoming financially competitive with traditional meat any time in the near future?

Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.

The 80,000 Hours Podcast is produced by Keiran Harris.</itunes:summary>
      <itunes:subtitle>First, decide on the type of animal. Next, pick the cell type. Then take a small, painless biopsy, and put the cells in a solution that makes them feel like they’re still in the body. Once the cells are in this comfortable state, they'll proliferate. One </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#25 - Robin Hanson on why we have to lie to ourselves about why we do what we do</title>
      <itunes:title>#25 - Robin Hanson on why we have to lie to ourselves about why we do what we do</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/421240986</guid>
      <link>https://share.transistor.fm/s/be3fa650</link>
      <description>
        <![CDATA[<p>On February 2, 1685, England’s King Charles II was struck by a sudden illness. Fortunately his physicians were the best of the best. To reassure the public they kept them abreast of the King’s treatment regimen. King Charles was made to swallow a toxic metal; had blistering agents applied to his scalp; had pigeon droppings attached to his feet; was prodded with a red-hot poker; given forty drops of ooze from “the skull of a man that was never buried”; and, finally, had crushed stones from the intestines of an East Indian goat forced down his throat. Sadly, despite these heroic efforts, he passed away the following week. </p><p> Why did the doctors go this far?</p><p> Prof, Robin Hanson, Associate Professor of Economics at George Mason University suspects that on top of any medical beliefs they also had a hidden motive: it needed to be clear, to the king and the public, that the physicians cared enormously about saving His Royal Majesty. Only by going ‘all out’ would they be protected against accusations of negligence should the King die. </p><p> <a href="https://80000hours.org/2018/03/robin-hanson-on-lying-to-ourselves/?utm_campaign=podcast__robin-hanson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, summary, and links to articles discussed in the show.</a></p><p> If you believe Hanson, the same desire to be seen to care about our family and friends explains much of what’s perverse about our medical system today.</p><p> And not just medicine - Robin thinks we’re mostly kidding ourselves when we say our charities exist to help others, our schools exist to educate students and our politics are about choosing wise policies. </p><p> So important are hidden motives for navigating our social world that we have to deny them to ourselves, lest we accidentally reveal them to others.</p><p> Robin is a polymath economist, who has come up with surprising and novel insight in a range of fields including psychology, politics and futurology. In this extensive episode we discuss his latest book with Kevin Simler, *The Elephant in the Brain: Hidden Motives in Everyday Life*, but also:</p><p> * What was it like being part of a competitor group to the ‘World Wide Web’, and being beaten to the post?<br> * If people aren’t going to school to learn, what’s education all about?<br> * What split brain patients tell us about our ability to justify anything<br> * The hidden motivations that shape religions<br> * Why we choose the friends we do<br> * Why is our attitude to medicine mysterious?<br> * What would it look like if people were focused on doing as much good as possible? <br> * Are we better off donating now, when we’re older, or even wait until well after our deaths?<br> * How much of the behavior of ‘effective altruists’ can we assume is genuinely motivated by wanting to do as much good as possible?<br> * What does Robin mean when he refers to effective altruism as a youth movement? Is that a good or bad thing?<br> * And much more...</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>On February 2, 1685, England’s King Charles II was struck by a sudden illness. Fortunately his physicians were the best of the best. To reassure the public they kept them abreast of the King’s treatment regimen. King Charles was made to swallow a toxic metal; had blistering agents applied to his scalp; had pigeon droppings attached to his feet; was prodded with a red-hot poker; given forty drops of ooze from “the skull of a man that was never buried”; and, finally, had crushed stones from the intestines of an East Indian goat forced down his throat. Sadly, despite these heroic efforts, he passed away the following week. </p><p> Why did the doctors go this far?</p><p> Prof, Robin Hanson, Associate Professor of Economics at George Mason University suspects that on top of any medical beliefs they also had a hidden motive: it needed to be clear, to the king and the public, that the physicians cared enormously about saving His Royal Majesty. Only by going ‘all out’ would they be protected against accusations of negligence should the King die. </p><p> <a href="https://80000hours.org/2018/03/robin-hanson-on-lying-to-ourselves/?utm_campaign=podcast__robin-hanson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, summary, and links to articles discussed in the show.</a></p><p> If you believe Hanson, the same desire to be seen to care about our family and friends explains much of what’s perverse about our medical system today.</p><p> And not just medicine - Robin thinks we’re mostly kidding ourselves when we say our charities exist to help others, our schools exist to educate students and our politics are about choosing wise policies. </p><p> So important are hidden motives for navigating our social world that we have to deny them to ourselves, lest we accidentally reveal them to others.</p><p> Robin is a polymath economist, who has come up with surprising and novel insight in a range of fields including psychology, politics and futurology. In this extensive episode we discuss his latest book with Kevin Simler, *The Elephant in the Brain: Hidden Motives in Everyday Life*, but also:</p><p> * What was it like being part of a competitor group to the ‘World Wide Web’, and being beaten to the post?<br> * If people aren’t going to school to learn, what’s education all about?<br> * What split brain patients tell us about our ability to justify anything<br> * The hidden motivations that shape religions<br> * Why we choose the friends we do<br> * Why is our attitude to medicine mysterious?<br> * What would it look like if people were focused on doing as much good as possible? <br> * Are we better off donating now, when we’re older, or even wait until well after our deaths?<br> * How much of the behavior of ‘effective altruists’ can we assume is genuinely motivated by wanting to do as much good as possible?<br> * What does Robin mean when he refers to effective altruism as a youth movement? Is that a good or bad thing?<br> * And much more...</p>]]>
      </content:encoded>
      <pubDate>Wed, 28 Mar 2018 17:20:12 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/be3fa650/dc6fe161.mp3" length="153311717" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/IatrDzMVzl6Pv4lHx4vzSw9vUK9xaWA3kclYldXRWYo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDMv/MTY4MzU0NDU2NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9560</itunes:duration>
      <itunes:summary>On February 2, 1685, England’s King Charles II was struck by a sudden illness. Fortunately his physicians were the best of the best. To reassure the public they kept them abreast of  the King’s treatment regimen. King Charles was made to swallow a toxic metal; had blistering agents applied to his scalp; had pigeon droppings attached to his feet; was prodded with a red-hot poker; given forty drops of ooze from “the skull of a man that was never buried”; and, finally, had crushed stones from the intestines of an East Indian goat forced down his throat. Sadly, despite these heroic efforts, he passed away the following week. 

Why did the doctors go this far?

Prof, Robin Hanson, Associate Professor of Economics at George Mason University suspects that on top of any medical beliefs they also had a hidden motive: it needed to be clear, to the king and the public, that the physicians cared enormously about saving His Royal Majesty. Only by going ‘all out’ would they be protected against accusations of negligence should the King die. 

Full transcript, summary, and links to articles discussed in the show.

If you believe Hanson, the same desire to be seen to care about our family and friends explains much of what’s perverse about our medical system today.

And not just medicine - Robin thinks we’re mostly kidding ourselves when we say our charities exist to help others, our schools exist to educate students and our politics are about choosing wise policies. 

So important are hidden motives for navigating our social world that we have to deny them to ourselves, lest we accidentally reveal them to others.

Robin is a polymath economist, who has come up with surprising and novel insight in a range of fields including psychology, politics and futurology. In this extensive episode we discuss his latest book with Kevin Simler, *The Elephant in the Brain: Hidden Motives in Everyday Life*, but also:

* What was it like being part of a competitor group to the ‘World Wide Web’, and being beaten to the post?
* If people aren’t going to school to learn, what’s education all about?
* What split brain patients tell us about our ability to justify anything
* The hidden motivations that shape religions
* Why we choose the friends we do
* Why is our attitude to medicine mysterious?
* What would it look like if people were focused on doing as much good as possible? 
* Are we better off donating now, when we’re older, or even wait until well after our deaths?
* How much of the behavior of ‘effective altruists’ can we assume is genuinely motivated by wanting to do as much good as possible?
* What does Robin mean when he refers to effective altruism as a youth movement? Is that a good or bad thing?
* And much more...</itunes:summary>
      <itunes:subtitle>On February 2, 1685, England’s King Charles II was struck by a sudden illness. Fortunately his physicians were the best of the best. To reassure the public they kept them abreast of  the King’s treatment regimen. King Charles was made to swallow a toxic m</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#24 - Stefan Schubert on why it’s a bad idea to break the rules, even if it’s for a good cause</title>
      <itunes:title>#24 - Stefan Schubert on why it’s a bad idea to break the rules, even if it’s for a good cause</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/416404905</guid>
      <link>https://share.transistor.fm/s/c88f67a5</link>
      <description>
        <![CDATA[How honest should we be? How helpful? How friendly? If our society claims to value honesty, for instance, but in reality accepts an awful lot of lying – should we go along with those lax standards? Or, should we attempt to set a new norm for ourselves?<p>

Dr Stefan Schubert, a researcher at the Social Behaviour and Ethics Lab at Oxford University, has been modelling this in the context of the effective altruism community. He thinks people trying to improve the world should hold themselves to very high standards of integrity, because their minor sins can impose major costs on the thousands of others who share their goals.</p><p>

<a href="https://80000hours.org/2018/03/stefan-schubert-considering-considerateness/?utm_campaign=podcast__stefan-schubert&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Summary, related links and full transcript.</b></a></p><p>

In addition, when a norm is uniquely important to our situation, we should be willing to question society and come up with something different and hopefully better.</p><p>

But in other cases, we can be better off sticking with whatever our culture expects, both to save time, avoid making mistakes, and ensure others can predict our behaviour.</p><p>

In this interview Stefan offers a range of views on the projects and culture that make up ‘effective altruism’ - including where it’s going right and where it’s going wrong.</p><p>

Stefan did his PhD in formal epistemology, before moving on to a postdoc in political rationality at the London School of Economics, while working on advocacy projects to improve truthfulness among politicians. At the time the interview was recorded Stefan was a researcher at the Centre for Effective Altruism in Oxford. </p><p>

We discuss:</p><p>

* Should we trust our own judgement more than others’?<br>
* How hard is it to improve political discourse?<br>
* What should we make of well-respected academics writing articles that seem to be completely misinformed?<br>
* How is effective altruism (EA) changing? What might it be doing wrong?<br>
* How has Stefan’s view of EA changed?<br>
* Should EA get more involved in politics, or steer clear of it? Would it be a bad idea for a talented graduate to get involved in party politics?<br>
* How much should we cooperate with those with whom we have disagreements?<br>
* What good reasons are there to be inconsiderate?<br>
* Should effective altruism potentially focused on a more narrow range of problems?</p><p>

*The 80,000 Hours podcast is produced by Keiran Harris.*</p><p>

**If you subscribe to our podcast, you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.**</p>]]>
      </description>
      <content:encoded>
        <![CDATA[How honest should we be? How helpful? How friendly? If our society claims to value honesty, for instance, but in reality accepts an awful lot of lying – should we go along with those lax standards? Or, should we attempt to set a new norm for ourselves?<p>

Dr Stefan Schubert, a researcher at the Social Behaviour and Ethics Lab at Oxford University, has been modelling this in the context of the effective altruism community. He thinks people trying to improve the world should hold themselves to very high standards of integrity, because their minor sins can impose major costs on the thousands of others who share their goals.</p><p>

<a href="https://80000hours.org/2018/03/stefan-schubert-considering-considerateness/?utm_campaign=podcast__stefan-schubert&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Summary, related links and full transcript.</b></a></p><p>

In addition, when a norm is uniquely important to our situation, we should be willing to question society and come up with something different and hopefully better.</p><p>

But in other cases, we can be better off sticking with whatever our culture expects, both to save time, avoid making mistakes, and ensure others can predict our behaviour.</p><p>

In this interview Stefan offers a range of views on the projects and culture that make up ‘effective altruism’ - including where it’s going right and where it’s going wrong.</p><p>

Stefan did his PhD in formal epistemology, before moving on to a postdoc in political rationality at the London School of Economics, while working on advocacy projects to improve truthfulness among politicians. At the time the interview was recorded Stefan was a researcher at the Centre for Effective Altruism in Oxford. </p><p>

We discuss:</p><p>

* Should we trust our own judgement more than others’?<br>
* How hard is it to improve political discourse?<br>
* What should we make of well-respected academics writing articles that seem to be completely misinformed?<br>
* How is effective altruism (EA) changing? What might it be doing wrong?<br>
* How has Stefan’s view of EA changed?<br>
* Should EA get more involved in politics, or steer clear of it? Would it be a bad idea for a talented graduate to get involved in party politics?<br>
* How much should we cooperate with those with whom we have disagreements?<br>
* What good reasons are there to be inconsiderate?<br>
* Should effective altruism potentially focused on a more narrow range of problems?</p><p>

*The 80,000 Hours podcast is produced by Keiran Harris.*</p><p>

**If you subscribe to our podcast, you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.**</p>]]>
      </content:encoded>
      <pubDate>Tue, 20 Mar 2018 16:56:08 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c88f67a5/93963e38.mp3" length="52894557" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/P5FS4xnG_1QAPYbaGqVOVYrIX7OdfzSIyfc2wLxLzk4/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDIv/MTY4MzU0NDU2NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3302</itunes:duration>
      <itunes:summary>How honest should we be? How helpful? How friendly? If our society claims to value honesty, for instance, but in reality accepts an awful lot of lying – should we go along with those lax standards? Or, should we attempt to set a new norm for ourselves?

Dr Stefan Schubert, a researcher at the Social Behaviour and Ethics Lab at Oxford University, has been modelling this in the context of the effective altruism community. He thinks people trying to improve the world should hold themselves to very high standards of integrity, because their minor sins can impose major costs on the thousands of others who share their goals.

Summary, related links and full transcript.

In addition, when a norm is uniquely important to our situation, we should be willing to question society and come up with something different and hopefully better.

But in other cases, we can be better off sticking with whatever our culture expects, both to save time, avoid making mistakes, and ensure others can predict our behaviour.

In this interview Stefan offers a range of views on the projects and culture that make up ‘effective altruism’ - including where it’s going right and where it’s going wrong.

Stefan did his PhD in formal epistemology, before moving on to a postdoc in political rationality at the London School of Economics, while working on advocacy projects to improve truthfulness among politicians. At the time the interview was recorded Stefan was a researcher at the Centre for Effective Altruism in Oxford. 

We discuss:

* Should we trust our own judgement more than others’?
* How hard is it to improve political discourse?
* What should we make of well-respected academics writing articles that seem to be completely misinformed?
* How is effective altruism (EA) changing? What might it be doing wrong?
* How has Stefan’s view of EA changed?
* Should EA get more involved in politics, or steer clear of it? Would it be a bad idea for a talented graduate to get involved in party politics?
* How much should we cooperate with those with whom we have disagreements?
* What good reasons are there to be inconsiderate?
* Should effective altruism potentially focused on a more narrow range of problems?

*The 80,000 Hours podcast is produced by Keiran Harris.*

**If you subscribe to our podcast, you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.**</itunes:summary>
      <itunes:subtitle>How honest should we be? How helpful? How friendly? If our society claims to value honesty, for instance, but in reality accepts an awful lot of lying – should we go along with those lax standards? Or, should we attempt to set a new norm for ourselves?

D</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#23 - How to actually become an AI alignment researcher, according to Dr Jan Leike</title>
      <itunes:title>#23 - How to actually become an AI alignment researcher, according to Dr Jan Leike</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/414482838</guid>
      <link>https://share.transistor.fm/s/ad44f2df</link>
      <description>
        <![CDATA[Want to help steer the 21st century’s most transformative technology? First complete an undergrad degree in computer science and mathematics. Prioritize harder courses over easier ones. Publish at least one paper before you apply for a PhD. Find a supervisor who’ll have a lot of time for you. Go to the top conferences and meet your future colleagues. And finally, get yourself hired.<br><br>

That’s Dr Jan Leike’s advice on how to join him as a Research Scientist at DeepMind, the world’s leading AI team.<br><br>

Jan is also a Research Associate at the Future of Humanity Institute at the University of Oxford, and his research aims to make machine learning robustly beneficial. His current focus is getting AI systems to learn good ‘objective functions’ in cases where we can’t easily specify the outcome we actually want.<br><br>

<a href="https://80000hours.org/2018/03/jan-leike-ml-alignment/?utm_campaign=podcast__jan-leike&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" rel="nofollow"><b>Full transcript, summary and links to learn more.</b></a><p>

How might you know you’re a good fit for research? <br><br>

Jan says to check whether you get obsessed with puzzles and problems, and find yourself mulling over questions that nobody knows the answer to. To do research in a team you also have to be good at clearly and concisely explaining your new ideas to other people.<br><br>

We also discuss:<br><br>

* Where Jan's views differ from those expressed by <a href="https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/?utm_campaign=podcast__jan-leike&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Dario Amodei in episode 3</a><br>
* Why is AGI safety one of the world’s most pressing problems? <br>
* Common misconceptions about AI<br>
* What are some of the specific things DeepMind is researching?<br>
* The ways in which today’s AI systems can fail<br>
* What are the best techniques available today for teaching an AI the right objective function?<br>
* What’s it like to have some of the world’s greatest minds as coworkers?<br>
* Who should do empirical research and who should do theoretical research<br>
* What’s the DeepMind application process like?<br>
* The importance of researchers being comfortable with the unknown.<br><br>

*The 80,000 Hours Podcast is produced by Keiran Harris.*</p>]]>
      </description>
      <content:encoded>
        <![CDATA[Want to help steer the 21st century’s most transformative technology? First complete an undergrad degree in computer science and mathematics. Prioritize harder courses over easier ones. Publish at least one paper before you apply for a PhD. Find a supervisor who’ll have a lot of time for you. Go to the top conferences and meet your future colleagues. And finally, get yourself hired.<br><br>

That’s Dr Jan Leike’s advice on how to join him as a Research Scientist at DeepMind, the world’s leading AI team.<br><br>

Jan is also a Research Associate at the Future of Humanity Institute at the University of Oxford, and his research aims to make machine learning robustly beneficial. His current focus is getting AI systems to learn good ‘objective functions’ in cases where we can’t easily specify the outcome we actually want.<br><br>

<a href="https://80000hours.org/2018/03/jan-leike-ml-alignment/?utm_campaign=podcast__jan-leike&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" rel="nofollow"><b>Full transcript, summary and links to learn more.</b></a><p>

How might you know you’re a good fit for research? <br><br>

Jan says to check whether you get obsessed with puzzles and problems, and find yourself mulling over questions that nobody knows the answer to. To do research in a team you also have to be good at clearly and concisely explaining your new ideas to other people.<br><br>

We also discuss:<br><br>

* Where Jan's views differ from those expressed by <a href="https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/?utm_campaign=podcast__jan-leike&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Dario Amodei in episode 3</a><br>
* Why is AGI safety one of the world’s most pressing problems? <br>
* Common misconceptions about AI<br>
* What are some of the specific things DeepMind is researching?<br>
* The ways in which today’s AI systems can fail<br>
* What are the best techniques available today for teaching an AI the right objective function?<br>
* What’s it like to have some of the world’s greatest minds as coworkers?<br>
* Who should do empirical research and who should do theoretical research<br>
* What’s the DeepMind application process like?<br>
* The importance of researchers being comfortable with the unknown.<br><br>

*The 80,000 Hours Podcast is produced by Keiran Harris.*</p>]]>
      </content:encoded>
      <pubDate>Fri, 16 Mar 2018 17:51:02 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/ad44f2df/208b3821.mp3" length="43635749" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/VlCwr0XMg35N4wcc_1R1gzWsxFkFwsddy53-NOE9LDg/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDEv/MTY4MzU0NDU2My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2724</itunes:duration>
      <itunes:summary>Want to help steer the 21st century’s most transformative technology? First complete an undergrad degree in computer science and mathematics. Prioritize harder courses over easier ones. Publish at least one paper before you apply for a PhD. Find a supervisor who’ll have a lot of time for you. Go to the top conferences and meet your future colleagues. And finally, get yourself hired.

That’s Dr Jan Leike’s advice on how to join him as a Research Scientist at DeepMind, the world’s leading AI team.

Jan is also a Research Associate at the Future of Humanity Institute at the University of Oxford, and his research aims to make machine learning robustly beneficial. His current focus is getting AI systems to learn good ‘objective functions’ in cases where we can’t easily specify the outcome we actually want.

Full transcript, summary and links to learn more.

How might you know you’re a good fit for research? 

Jan says to check whether you get obsessed with puzzles and problems, and find yourself mulling over questions that nobody knows the answer to. To do research in a team you also have to be good at clearly and concisely explaining your new ideas to other people.

We also discuss:

* Where Jan's views differ from those expressed by Dario Amodei in episode 3
* Why is AGI safety one of the world’s most pressing problems? 
* Common misconceptions about AI
* What are some of the specific things DeepMind is researching?
* The ways in which today’s AI systems can fail
* What are the best techniques available today for teaching an AI the right objective function?
* What’s it like to have some of the world’s greatest minds as coworkers?
* Who should do empirical research and who should do theoretical research
* What’s the DeepMind application process like?
* The importance of researchers being comfortable with the unknown.

*The 80,000 Hours Podcast is produced by Keiran Harris.*</itunes:summary>
      <itunes:subtitle>Want to help steer the 21st century’s most transformative technology? First complete an undergrad degree in computer science and mathematics. Prioritize harder courses over easier ones. Publish at least one paper before you apply for a PhD. Find a supervi</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#22 - Leah Utyasheva on the non-profit that figured out how to massively cut suicide rates</title>
      <itunes:title>#22 - Leah Utyasheva on the non-profit that figured out how to massively cut suicide rates</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/409896831</guid>
      <link>https://share.transistor.fm/s/2ec8bf8f</link>
      <description>
        <![CDATA[<p>How people kill themselves varies enormously depending on which means are most easily available. In the United States, suicide by firearm stands out. In Hong Kong, where most people live in high rise buildings, jumping from a height is more common. And in some countries in Asia and Africa with many poor agricultural communities, the leading means is drinking pesticide.</p><p> There’s a good chance you’ve never heard of this issue before. And yet, of the 800,000 people who kill themselves globally each year 20% die from pesticide self-poisoning.</p><p> <a href="https://80000hours.org/2018/03/leah-utyasheva-pesticide-suicide-prevention/?utm_campaign=podcast__leah-utyasheva&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript, summary and links to articles discussed in today's show.</strong></a></p><p> Research suggests most people who try to kill themselves with pesticides reflect on the decision for less than 30 minutes, and that less than 10% of those who don't die the first time around will try again.</p><p> Unfortunately, the fatality rate from pesticide ingestion is 40% to 70%.</p><p> Having such dangerous chemicals near people's homes is therefore an enormous public health issue not only for the direct victims, but also the partners and children they leave behind.</p><p> Fortunately researchers like Dr Leah Utyasheva have figured out a very cheap way to massively reduce pesticide suicide rates.</p><p> In this episode, Leah and I discuss:</p><p> * How do you prevent pesticide suicide and what’s the evidence it works?<br> * How do you know that most people attempting suicide don’t want to die?<br> * What types of events are causing people to have the crises that lead to attempted suicide?<br> * How much money does it cost to save a life in this way?<br> * How do you estimate the probability of getting law reform passed in a particular country?<br> * Have you generally found politicians to be sympathetic to the idea of banning these pesticides? What are their greatest reservations?<br> * The comparison of getting policy change rather than helping person-by-person<br> * The importance of working with locals in places like India and Nepal, rather than coming in exclusively as outsiders<br> * What are the benefits of starting your own non-profit versus joining an existing org and persuading them of the merits of the cause?<br> * Would Leah in general recommend starting a new charity? Is it more exciting than it is scary?<br> * Is it important to have an academic leading this kind of work?<br> * How did The Centre for Pesticide Suicide Prevention get seed funding?<br> * How does the value of saving a life from suicide compare to savings someone from malaria<br> * Leah’s political campaigning for the rights of vulnerable groups in Eastern Europe <br> * What are the biggest downsides of human rights work?</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>How people kill themselves varies enormously depending on which means are most easily available. In the United States, suicide by firearm stands out. In Hong Kong, where most people live in high rise buildings, jumping from a height is more common. And in some countries in Asia and Africa with many poor agricultural communities, the leading means is drinking pesticide.</p><p> There’s a good chance you’ve never heard of this issue before. And yet, of the 800,000 people who kill themselves globally each year 20% die from pesticide self-poisoning.</p><p> <a href="https://80000hours.org/2018/03/leah-utyasheva-pesticide-suicide-prevention/?utm_campaign=podcast__leah-utyasheva&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript, summary and links to articles discussed in today's show.</strong></a></p><p> Research suggests most people who try to kill themselves with pesticides reflect on the decision for less than 30 minutes, and that less than 10% of those who don't die the first time around will try again.</p><p> Unfortunately, the fatality rate from pesticide ingestion is 40% to 70%.</p><p> Having such dangerous chemicals near people's homes is therefore an enormous public health issue not only for the direct victims, but also the partners and children they leave behind.</p><p> Fortunately researchers like Dr Leah Utyasheva have figured out a very cheap way to massively reduce pesticide suicide rates.</p><p> In this episode, Leah and I discuss:</p><p> * How do you prevent pesticide suicide and what’s the evidence it works?<br> * How do you know that most people attempting suicide don’t want to die?<br> * What types of events are causing people to have the crises that lead to attempted suicide?<br> * How much money does it cost to save a life in this way?<br> * How do you estimate the probability of getting law reform passed in a particular country?<br> * Have you generally found politicians to be sympathetic to the idea of banning these pesticides? What are their greatest reservations?<br> * The comparison of getting policy change rather than helping person-by-person<br> * The importance of working with locals in places like India and Nepal, rather than coming in exclusively as outsiders<br> * What are the benefits of starting your own non-profit versus joining an existing org and persuading them of the merits of the cause?<br> * Would Leah in general recommend starting a new charity? Is it more exciting than it is scary?<br> * Is it important to have an academic leading this kind of work?<br> * How did The Centre for Pesticide Suicide Prevention get seed funding?<br> * How does the value of saving a life from suicide compare to savings someone from malaria<br> * Leah’s political campaigning for the rights of vulnerable groups in Eastern Europe <br> * What are the biggest downsides of human rights work?</p>]]>
      </content:encoded>
      <pubDate>Wed, 07 Mar 2018 18:13:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/2ec8bf8f/32e40cbc.mp3" length="65428461" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/-TvbXiMyqEHQgeW3znorb2SXiezuLd7VWocbfPcaT9s/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3NDAv/MTY4MzU0NDU2Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4084</itunes:duration>
      <itunes:summary>How people kill themselves varies enormously depending on which means are most easily available. In the United States, suicide by firearm stands out. In Hong Kong, where most people live in high rise buildings, jumping from a height is more common. And in some countries in Asia and Africa with many poor agricultural communities, the leading means is drinking pesticide.

There’s a good chance you’ve never heard of this issue before. And yet, of the 800,000 people who kill themselves globally each year 20% die from pesticide self-poisoning.

Full transcript, summary and links to articles discussed in today's show.

Research suggests most people who try to kill themselves with pesticides reflect on the decision for less than 30 minutes, and that less than 10% of those who don't die the first time around will try again.

Unfortunately, the fatality rate from pesticide ingestion is 40% to 70%.

Having such dangerous chemicals near people's homes is therefore an enormous public health issue not only for the direct victims, but also the partners and children they leave behind.

Fortunately researchers like Dr Leah Utyasheva have figured out a very cheap way to massively reduce pesticide suicide rates.

In this episode, Leah and I discuss:

* How do you prevent pesticide suicide and what’s the evidence it works?
* How do you know that most people attempting suicide don’t want to die?
* What types of events are causing people to have the crises that lead to attempted suicide?
* How much money does it cost to save a life in this way?
* How do you estimate the probability of getting law reform passed in a particular country?
* Have you generally found politicians to be sympathetic to the idea of banning these pesticides? What are their greatest reservations?
* The comparison of getting policy change rather than helping person-by-person
* The importance of working with locals in places like India and Nepal, rather than coming in exclusively as outsiders
* What are the benefits of starting your own non-profit versus joining an existing org and persuading them of the merits of the cause?
* Would Leah in general recommend starting a new charity? Is it more exciting than it is scary?
* Is it important to have an academic leading this kind of work?
* How did The Centre for Pesticide Suicide Prevention get seed funding?
* How does the value of saving a life from suicide compare to savings someone from malaria
* Leah’s political campaigning for the rights of vulnerable groups in Eastern Europe 
* What are the biggest downsides of human rights work?</itunes:summary>
      <itunes:subtitle>How people kill themselves varies enormously depending on which means are most easily available. In the United States, suicide by firearm stands out. In Hong Kong, where most people live in high rise buildings, jumping from a height is more common. And in</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#21 - Holden Karnofsky on times philanthropy transformed the world &amp; Open Phil’s plan to do the same</title>
      <itunes:title>#21 - Holden Karnofsky on times philanthropy transformed the world &amp; Open Phil’s plan to do the same</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/406051449</guid>
      <link>https://share.transistor.fm/s/4ec8b579</link>
      <description>
        <![CDATA[The Green Revolution averted mass famine during the 20th century. The contraceptive pill gave women unprecedented freedom in planning their own lives. Both are widely recognised as scientific breakthroughs that transformed the world. But few know that those breakthroughs only happened when they did because of a philanthropist willing to take a risky bet on a new idea.<p>

Today’s guest, Holden Karnofsky, has been looking for philanthropy’s biggest success stories because he’s Executive Director of the Open Philanthropy Project, which gives away over $100 million per year - and he’s hungry for big wins.</p><p>

<a href="https://80000hours.org/2018/02/holden-karnofsky-open-philanthropy/?utm_campaign=podcast__holden-karnofsky&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, related links, job opportunities and summary of the interview.</b></a></p><p>

In the 1940s, poverty reduction overseas was not a big priority for many. But the Rockefeller Foundation decided to fund agricultural scientists to breed much better crops for the developing world - thereby massively increasing their food production.</p><p>

In the 1950s, society was a long way from demanding effective birth control. Activist Margaret Sanger had the idea for the pill, and endocrinologist Gregory Pincus the research team – but they couldn’t proceed without a $40,000 research check from biologist and women’s rights activist Katherine McCormick.</p><p>

In both cases, it was philanthropists rather than governments that led the way.</p><p>

The reason, according to Holden, is that while governments have enormous resources, they’re constrained by only being able to fund reasonably sure bets. Philanthropists can transform the world by filling the gaps government leaves - but to seize that opportunity they have to hire outstanding researchers, think long-term and be willing to fail most of the time. </p><p>

Holden knows more about this type of giving than almost anyone. As founder of GiveWell and then the Open Philanthropy Project, he has been working feverishly since 2007 to find outstanding giving opportunities. This practical experience has made him one of the most influential figures in the development of the school of thought that has come to be known as effective altruism.</p><p>

We’ve recorded this episode now because [the Open Philanthropy Project is hiring](https://www.openphilanthropy.org/get-involved/jobs) for a large number of positions, which we think would allow the right person to have a very large positive influence on the world. They’re looking for a large number of entry lever researchers to train up, 3 specialist researchers into potential risks from advanced artificial intelligence, as well as a Director of Operations, Operations Associate and General Counsel.</p><p>

But the conversation goes well beyond specifics about these jobs. We also discuss:</p><p>

* How did they pick the problems they focus on, and how will they change over time?<br>
* What would Holden do differently if he were starting Open Phil again today?<br>
* What can we learn from the history of philanthropy?<br>
* What makes a good Program Officer.<br>
* The importance of not letting hype get ahead of the science in an emerging field.<br>
* The importance of honest feedback for philanthropists, and the difficulty getting it.<br>
* How do they decide what’s above the bar to fund, and when it’s better to hold onto the money?<br>
* How philanthropic funding can most influence politics.<br>
* What Holden would say to a new billionaire who wanted to give away most of their wealth.<br>
* Why Open Phil is building a research field around the safe development of artificial intelligence<br>
* Why they invested in OpenAI.<br>
* Academia’s faulty approach to answering practical questions.<br>
* What potential utopias do people most want, according to opinion polls? </p><p>

Keiran Harris helped produce today’s episode.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[The Green Revolution averted mass famine during the 20th century. The contraceptive pill gave women unprecedented freedom in planning their own lives. Both are widely recognised as scientific breakthroughs that transformed the world. But few know that those breakthroughs only happened when they did because of a philanthropist willing to take a risky bet on a new idea.<p>

Today’s guest, Holden Karnofsky, has been looking for philanthropy’s biggest success stories because he’s Executive Director of the Open Philanthropy Project, which gives away over $100 million per year - and he’s hungry for big wins.</p><p>

<a href="https://80000hours.org/2018/02/holden-karnofsky-open-philanthropy/?utm_campaign=podcast__holden-karnofsky&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, related links, job opportunities and summary of the interview.</b></a></p><p>

In the 1940s, poverty reduction overseas was not a big priority for many. But the Rockefeller Foundation decided to fund agricultural scientists to breed much better crops for the developing world - thereby massively increasing their food production.</p><p>

In the 1950s, society was a long way from demanding effective birth control. Activist Margaret Sanger had the idea for the pill, and endocrinologist Gregory Pincus the research team – but they couldn’t proceed without a $40,000 research check from biologist and women’s rights activist Katherine McCormick.</p><p>

In both cases, it was philanthropists rather than governments that led the way.</p><p>

The reason, according to Holden, is that while governments have enormous resources, they’re constrained by only being able to fund reasonably sure bets. Philanthropists can transform the world by filling the gaps government leaves - but to seize that opportunity they have to hire outstanding researchers, think long-term and be willing to fail most of the time. </p><p>

Holden knows more about this type of giving than almost anyone. As founder of GiveWell and then the Open Philanthropy Project, he has been working feverishly since 2007 to find outstanding giving opportunities. This practical experience has made him one of the most influential figures in the development of the school of thought that has come to be known as effective altruism.</p><p>

We’ve recorded this episode now because [the Open Philanthropy Project is hiring](https://www.openphilanthropy.org/get-involved/jobs) for a large number of positions, which we think would allow the right person to have a very large positive influence on the world. They’re looking for a large number of entry lever researchers to train up, 3 specialist researchers into potential risks from advanced artificial intelligence, as well as a Director of Operations, Operations Associate and General Counsel.</p><p>

But the conversation goes well beyond specifics about these jobs. We also discuss:</p><p>

* How did they pick the problems they focus on, and how will they change over time?<br>
* What would Holden do differently if he were starting Open Phil again today?<br>
* What can we learn from the history of philanthropy?<br>
* What makes a good Program Officer.<br>
* The importance of not letting hype get ahead of the science in an emerging field.<br>
* The importance of honest feedback for philanthropists, and the difficulty getting it.<br>
* How do they decide what’s above the bar to fund, and when it’s better to hold onto the money?<br>
* How philanthropic funding can most influence politics.<br>
* What Holden would say to a new billionaire who wanted to give away most of their wealth.<br>
* Why Open Phil is building a research field around the safe development of artificial intelligence<br>
* Why they invested in OpenAI.<br>
* Academia’s faulty approach to answering practical questions.<br>
* What potential utopias do people most want, according to opinion polls? </p><p>

Keiran Harris helped produce today’s episode.</p>]]>
      </content:encoded>
      <pubDate>Tue, 27 Feb 2018 17:59:50 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/4ec8b579/ad8db648.mp3" length="149524378" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/kTiBQWO4yhQvSQScknfpVa9-Ojk0iXqXstsfNdqN2aY/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Mzkv/MTY4MzU0NDU2MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9336</itunes:duration>
      <itunes:summary>The Green Revolution averted mass famine during the 20th century. The contraceptive pill gave women unprecedented freedom in planning their own lives. Both are widely recognised as scientific breakthroughs that transformed the world. But few know that those breakthroughs only happened when they did because of a philanthropist willing to take a risky bet on a new idea.

Today’s guest, Holden Karnofsky, has been looking for philanthropy’s biggest success stories because he’s Executive Director of the Open Philanthropy Project, which gives away over $100 million per year - and he’s hungry for big wins.

Full transcript, related links, job opportunities and summary of the interview.

In the 1940s, poverty reduction overseas was not a big priority for many. But the Rockefeller Foundation decided to fund agricultural scientists to breed much better crops for the developing world - thereby massively increasing their food production.

In the 1950s, society was a long way from demanding effective birth control. Activist Margaret Sanger had the idea for the pill, and endocrinologist Gregory Pincus the research team – but they couldn’t proceed without a $40,000 research check from biologist and women’s rights activist Katherine McCormick.

In both cases, it was philanthropists rather than governments that led the way.

The reason, according to Holden, is that while governments have enormous resources, they’re constrained by only being able to fund reasonably sure bets. Philanthropists can transform the world by filling the gaps government leaves - but to seize that opportunity they have to hire outstanding researchers, think long-term and be willing to fail most of the time. 

Holden knows more about this type of giving than almost anyone. As founder of GiveWell and then the Open Philanthropy Project, he has been working feverishly since 2007 to find outstanding giving opportunities. This practical experience has made him one of the most influential figures in the development of the school of thought that has come to be known as effective altruism.

We’ve recorded this episode now because [the Open Philanthropy Project is hiring](https://www.openphilanthropy.org/get-involved/jobs) for a large number of positions, which we think would allow the right person to have a very large positive influence on the world. They’re looking for a large number of entry lever researchers to train up, 3 specialist researchers into potential risks from advanced artificial intelligence, as well as a Director of Operations, Operations Associate and General Counsel.

But the conversation goes well beyond specifics about these jobs. We also discuss:

* How did they pick the problems they focus on, and how will they change over time?
* What would Holden do differently if he were starting Open Phil again today?
* What can we learn from the history of philanthropy?
* What makes a good Program Officer.
* The importance of not letting hype get ahead of the science in an emerging field.
* The importance of honest feedback for philanthropists, and the difficulty getting it.
* How do they decide what’s above the bar to fund, and when it’s better to hold onto the money?
* How philanthropic funding can most influence politics.
* What Holden would say to a new billionaire who wanted to give away most of their wealth.
* Why Open Phil is building a research field around the safe development of artificial intelligence
* Why they invested in OpenAI.
* Academia’s faulty approach to answering practical questions.
* What potential utopias do people most want, according to opinion polls? 

Keiran Harris helped produce today’s episode.</itunes:summary>
      <itunes:subtitle>The Green Revolution averted mass famine during the 20th century. The contraceptive pill gave women unprecedented freedom in planning their own lives. Both are widely recognised as scientific breakthroughs that transformed the world. But few know that tho</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#20 - Bruce Friedrich on inventing outstanding meat substitutes to end speciesism &amp; factory farming</title>
      <itunes:title>#20 - Bruce Friedrich on inventing outstanding meat substitutes to end speciesism &amp; factory farming</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/401771421</guid>
      <link>https://share.transistor.fm/s/b988616f</link>
      <description>
        <![CDATA[Before the US Civil War, it was easier for the North to morally oppose slavery. Why? Because unlike the South they weren’t profiting much from its existence. The fight for abolition was partly won because many no longer saw themselves as having a selfish stake in its continuation.<p>

Bruce Friedrich, executive director of The Good Food Institute (GFI), thinks the same may be true in the fight against speciesism. 98% of people currently eat meat. But if eating meat stops being part of most people’s daily lives -- it should be a lot easier to convince them that farming practices are just as cruel as they look, and that the suffering of these animals really matters.</p><p>

<a href="https://80000hours.org/2018/02/bruce-friedrich-good-food-institute/?utm_campaign=podcast__bruce-friedrich&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, related links, job opportunities and summary of the interview.</b></a></p><p>

That’s why GFI is “working with scientists, investors, and entrepreneurs” to create plant-based meat, dairy and eggs as well as clean meat alternatives to animal products. In 2016, Animal Charity Evaluators named GFI one of its recommended charities.</p><p>

In this interview I’m joined by my colleague Natalie Cargill, and we ask Bruce about:</p><p>

* What’s the best meat replacement product out there right now?<br>
* How effective is meat substitute research for people who want to reduce animal suffering as much as possible?<br>
* When will we get our hands on clean meat? And why does Bruce call it clean meat, rather than in vitro meat or cultured meat?<br>
* What are the challenges of producing something structurally identical to meat?<br>
* Can clean meat be healthier than conventional meat?<br>
* Do plant-based alternatives have a better shot at success than clean meat?<br>
* Is there a concern that, even if the product is perfect, people still won’t eat it? Why might that happen?<br>
* What’s it like being a vegan in a family made up largely of hunters and meat-eaters?<br>
* What kind of pushback should be expected from the meat industry?</p><p>

Keiran Harris helped produce today’s episode.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[Before the US Civil War, it was easier for the North to morally oppose slavery. Why? Because unlike the South they weren’t profiting much from its existence. The fight for abolition was partly won because many no longer saw themselves as having a selfish stake in its continuation.<p>

Bruce Friedrich, executive director of The Good Food Institute (GFI), thinks the same may be true in the fight against speciesism. 98% of people currently eat meat. But if eating meat stops being part of most people’s daily lives -- it should be a lot easier to convince them that farming practices are just as cruel as they look, and that the suffering of these animals really matters.</p><p>

<a href="https://80000hours.org/2018/02/bruce-friedrich-good-food-institute/?utm_campaign=podcast__bruce-friedrich&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full transcript, related links, job opportunities and summary of the interview.</b></a></p><p>

That’s why GFI is “working with scientists, investors, and entrepreneurs” to create plant-based meat, dairy and eggs as well as clean meat alternatives to animal products. In 2016, Animal Charity Evaluators named GFI one of its recommended charities.</p><p>

In this interview I’m joined by my colleague Natalie Cargill, and we ask Bruce about:</p><p>

* What’s the best meat replacement product out there right now?<br>
* How effective is meat substitute research for people who want to reduce animal suffering as much as possible?<br>
* When will we get our hands on clean meat? And why does Bruce call it clean meat, rather than in vitro meat or cultured meat?<br>
* What are the challenges of producing something structurally identical to meat?<br>
* Can clean meat be healthier than conventional meat?<br>
* Do plant-based alternatives have a better shot at success than clean meat?<br>
* Is there a concern that, even if the product is perfect, people still won’t eat it? Why might that happen?<br>
* What’s it like being a vegan in a family made up largely of hunters and meat-eaters?<br>
* What kind of pushback should be expected from the meat industry?</p><p>

Keiran Harris helped produce today’s episode.</p>]]>
      </content:encoded>
      <pubDate>Mon, 19 Feb 2018 03:39:55 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b988616f/415d9309.mp3" length="74955762" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lU72cZmu9BMpZpchSWIHubOlajSCLqCnxAtZDeki4UE/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Mzgv/MTY4MzU0NDU2MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4680</itunes:duration>
      <itunes:summary>Before the US Civil War, it was easier for the North to morally oppose slavery. Why? Because unlike the South they weren’t profiting much from its existence. The fight for abolition was partly won because many no longer saw themselves as having a selfish stake in its continuation.

Bruce Friedrich, executive director of The Good Food Institute (GFI), thinks the same may be true in the fight against speciesism. 98% of people currently eat meat. But if eating meat stops being part of most people’s daily lives -- it should be a lot easier to convince them that farming practices are just as cruel as they look, and that the suffering of these animals really matters.

Full transcript, related links, job opportunities and summary of the interview.

That’s why GFI is “working with scientists, investors, and entrepreneurs” to create plant-based meat, dairy and eggs as well as clean meat alternatives to animal products. In 2016, Animal Charity Evaluators named GFI one of its recommended charities.

In this interview I’m joined by my colleague Natalie Cargill, and we ask Bruce about:

* What’s the best meat replacement product out there right now?
* How effective is meat substitute research for people who want to reduce animal suffering as much as possible?
* When will we get our hands on clean meat? And why does Bruce call it clean meat, rather than in vitro meat or cultured meat?
* What are the challenges of producing something structurally identical to meat?
* Can clean meat be healthier than conventional meat?
* Do plant-based alternatives have a better shot at success than clean meat?
* Is there a concern that, even if the product is perfect, people still won’t eat it? Why might that happen?
* What’s it like being a vegan in a family made up largely of hunters and meat-eaters?
* What kind of pushback should be expected from the meat industry?

Keiran Harris helped produce today’s episode.</itunes:summary>
      <itunes:subtitle>Before the US Civil War, it was easier for the North to morally oppose slavery. Why? Because unlike the South they weren’t profiting much from its existence. The fight for abolition was partly won because many no longer saw themselves as having a selfish </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#19 - Samantha Pitts-Kiefer on working next to the White House trying to prevent nuclear war</title>
      <itunes:title>#19 - Samantha Pitts-Kiefer on working next to the White House trying to prevent nuclear war</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/399391563</guid>
      <link>https://share.transistor.fm/s/c4fb53d2</link>
      <description>
        <![CDATA[Rogue elements within a state’s security forces enrich dozens of kilograms of uranium. It’s then assembled into a crude nuclear bomb. The bomb is transported on a civilian aircraft to Washington D.C, and loaded onto a delivery truck. The truck is driven by an American citizen midway between the White House and the Capitol Building. The driver casually steps out of the vehicle, and detonates the weapon. There are more than 80,000 instant deaths. There are also at least 100,000 seriously wounded, with nowhere left to treat them. <p>

<a href="https://80000hours.org/2018/02/samantha-pk-nuclear-security/?utm_campaign=podcast__samantha-pk&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full blog post about this episode, including a transcript, summary and links to resources mentioned in the show</b></a></p><p>

It’s likely that one of those immediately killed would be Samantha Pitts-Kiefer, who works only one block away from the White House.</p><p>

Samantha serves as Senior Director of The Global Nuclear Policy Program at the Nuclear Threat Initiative, and warns that the chances of a nuclear terrorist attack are alarmingly high. Terrorist groups have expressed a desire for nuclear weapons, and the material required to build those weapons is scattered throughout the world at a diverse range of sites – some of which lack the necessary  security. </p><p>

When you combine the massive death toll with the accompanying social panic and economic disruption – the consequences of a nuclear 9/11 would be a disasterare almost unthinkable. And yet, Samantha reminds us – we must confront the possibility.</p><p>

Clearly, this is far from the only nuclear nightmare. We also discuss:</p><p>

* In the case of nuclear war, what fraction of the world's population would die?<br>
* What is the biggest nuclear threat?<br>
* How concerned should we be about North Korea?<br>
* How often has the world experienced nuclear near misses?<br>
* How might a conflict between India and Pakistan escalate to the nuclear level?<br>
* How quickly must a president make a decision in the result of a suspected first strike?<br>
* Are global sources of nuclear material safely secured?<br>
* What role does cyber security have in preventing nuclear disasters?<br>
* How can we improve relations between nuclear armed states?<br>
* What do you think about the campaign for complete nuclear disarmament?<br>
* If you could tell the US government to do three things, what are the key priorities today? <br>
* Is it practical to get members of congress to pay attention to nuclear risks?<br>
* Could modernisation of nuclear weapons actually make the world safer?</p>]]>
      </description>
      <content:encoded>
        <![CDATA[Rogue elements within a state’s security forces enrich dozens of kilograms of uranium. It’s then assembled into a crude nuclear bomb. The bomb is transported on a civilian aircraft to Washington D.C, and loaded onto a delivery truck. The truck is driven by an American citizen midway between the White House and the Capitol Building. The driver casually steps out of the vehicle, and detonates the weapon. There are more than 80,000 instant deaths. There are also at least 100,000 seriously wounded, with nowhere left to treat them. <p>

<a href="https://80000hours.org/2018/02/samantha-pk-nuclear-security/?utm_campaign=podcast__samantha-pk&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full blog post about this episode, including a transcript, summary and links to resources mentioned in the show</b></a></p><p>

It’s likely that one of those immediately killed would be Samantha Pitts-Kiefer, who works only one block away from the White House.</p><p>

Samantha serves as Senior Director of The Global Nuclear Policy Program at the Nuclear Threat Initiative, and warns that the chances of a nuclear terrorist attack are alarmingly high. Terrorist groups have expressed a desire for nuclear weapons, and the material required to build those weapons is scattered throughout the world at a diverse range of sites – some of which lack the necessary  security. </p><p>

When you combine the massive death toll with the accompanying social panic and economic disruption – the consequences of a nuclear 9/11 would be a disasterare almost unthinkable. And yet, Samantha reminds us – we must confront the possibility.</p><p>

Clearly, this is far from the only nuclear nightmare. We also discuss:</p><p>

* In the case of nuclear war, what fraction of the world's population would die?<br>
* What is the biggest nuclear threat?<br>
* How concerned should we be about North Korea?<br>
* How often has the world experienced nuclear near misses?<br>
* How might a conflict between India and Pakistan escalate to the nuclear level?<br>
* How quickly must a president make a decision in the result of a suspected first strike?<br>
* Are global sources of nuclear material safely secured?<br>
* What role does cyber security have in preventing nuclear disasters?<br>
* How can we improve relations between nuclear armed states?<br>
* What do you think about the campaign for complete nuclear disarmament?<br>
* If you could tell the US government to do three things, what are the key priorities today? <br>
* Is it practical to get members of congress to pay attention to nuclear risks?<br>
* Could modernisation of nuclear weapons actually make the world safer?</p>]]>
      </content:encoded>
      <pubDate>Wed, 14 Feb 2018 16:56:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c4fb53d2/6af153ae.mp3" length="61978121" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Gq_uPpRT5w2Scg2QdUutd6kPOdHyEy7pFzKpUu0MbjM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Mzcv/MTY4MzU0NDU1OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3870</itunes:duration>
      <itunes:summary>Rogue elements within a state’s security forces enrich dozens of kilograms of uranium. It’s then assembled into a crude nuclear bomb. The bomb is transported on a civilian aircraft to Washington D.C, and loaded onto a delivery truck. The truck is driven by an American citizen midway between the White House and the Capitol Building. The driver casually steps out of the vehicle, and detonates the weapon. There are more than 80,000 instant deaths. There are also at least 100,000 seriously wounded, with nowhere left to treat them. 

Full blog post about this episode, including a transcript, summary and links to resources mentioned in the show

It’s likely that one of those immediately killed would be Samantha Pitts-Kiefer, who works only one block away from the White House.

Samantha serves as Senior Director of The Global Nuclear Policy Program at the Nuclear Threat Initiative, and warns that the chances of a nuclear terrorist attack are alarmingly high. Terrorist groups have expressed a desire for nuclear weapons, and the material required to build those weapons is scattered throughout the world at a diverse range of sites – some of which lack the necessary  security. 

When you combine the massive death toll with the accompanying social panic and economic disruption – the consequences of a nuclear 9/11 would be a disasterare almost unthinkable. And yet, Samantha reminds us – we must confront the possibility.

Clearly, this is far from the only nuclear nightmare. We also discuss:

* In the case of nuclear war, what fraction of the world's population would die?
* What is the biggest nuclear threat?
* How concerned should we be about North Korea?
* How often has the world experienced nuclear near misses?
* How might a conflict between India and Pakistan escalate to the nuclear level?
* How quickly must a president make a decision in the result of a suspected first strike?
* Are global sources of nuclear material safely secured?
* What role does cyber security have in preventing nuclear disasters?
* How can we improve relations between nuclear armed states?
* What do you think about the campaign for complete nuclear disarmament?
* If you could tell the US government to do three things, what are the key priorities today? 
* Is it practical to get members of congress to pay attention to nuclear risks?
* Could modernisation of nuclear weapons actually make the world safer?</itunes:summary>
      <itunes:subtitle>Rogue elements within a state’s security forces enrich dozens of kilograms of uranium. It’s then assembled into a crude nuclear bomb. The bomb is transported on a civilian aircraft to Washington D.C, and loaded onto a delivery truck. The truck is driven b</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#18 - Ofir Reich on using data science to end poverty &amp; the spurious action-inaction distinction</title>
      <itunes:title>#18 - Ofir Reich on using data science to end poverty &amp; the spurious action-inaction distinction</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/392364360</guid>
      <link>https://share.transistor.fm/s/f2806f68</link>
      <description>
        <![CDATA[Ofir Reich started out doing math in the military, before spending 8 years in tech startups - but then made a sharp turn to become a data scientist focussed on helping the global poor.<p>

At UC Berkeley’s Center for Effective Global Action he helps prevent tax evasion by identifying fake companies in India, enable Afghanistan to pay its teachers electronically, and raise yields for Ethiopian farmers by messaging them when local conditions make it ideal to apply fertiliser. Or at least that’s the hope - he’s also working on ways to test whether those interventions actually work.
</p><p>

<a href="https://80000hours.org/2018/01/ofir-reich-data-science/?utm_campaign=podcast__ofir-reich&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full post about this episode, including a transcript and relevant links to learn more.</b></a></p><p>

Why dedicate his life to helping the global poor?</p><p>

Ofir sees little moral difference between harming people and failing to help them. After all, if you had to press a button to keep all of your money from going to charity, and you pressed that button, would that be an action, or an inaction? Is there even an answer?</p><p>

After reflecting on cases like this, he decided that to not engage with a problem is an active choice, one whose consequences he is just as morally responsible for as if he were directly involved. On top of his life philosophy we also discuss:
</p><p>

* The benefits of working in a top academic environment<br>
* How best to start a career in global development<br>
* Are RCTs worth the money? Should we focus on big picture policy change instead? Or more economic theory?<br>
* How the delivery standards of nonprofits compare to top universities<br>
* Why he doesn’t enjoy living in the San Francisco bay area<br>
* How can we fix the problem of most published research being false?<br>
* How good a career path is data science?<br>
* How important is experience in development versus technical skills?<br>
* How he learned much of what he needed to know in the army<br>
* How concerned should effective altruists be about burnout?</p><p>

Keiran Harris helped produce today’s episode.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[Ofir Reich started out doing math in the military, before spending 8 years in tech startups - but then made a sharp turn to become a data scientist focussed on helping the global poor.<p>

At UC Berkeley’s Center for Effective Global Action he helps prevent tax evasion by identifying fake companies in India, enable Afghanistan to pay its teachers electronically, and raise yields for Ethiopian farmers by messaging them when local conditions make it ideal to apply fertiliser. Or at least that’s the hope - he’s also working on ways to test whether those interventions actually work.
</p><p>

<a href="https://80000hours.org/2018/01/ofir-reich-data-science/?utm_campaign=podcast__ofir-reich&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><b>Full post about this episode, including a transcript and relevant links to learn more.</b></a></p><p>

Why dedicate his life to helping the global poor?</p><p>

Ofir sees little moral difference between harming people and failing to help them. After all, if you had to press a button to keep all of your money from going to charity, and you pressed that button, would that be an action, or an inaction? Is there even an answer?</p><p>

After reflecting on cases like this, he decided that to not engage with a problem is an active choice, one whose consequences he is just as morally responsible for as if he were directly involved. On top of his life philosophy we also discuss:
</p><p>

* The benefits of working in a top academic environment<br>
* How best to start a career in global development<br>
* Are RCTs worth the money? Should we focus on big picture policy change instead? Or more economic theory?<br>
* How the delivery standards of nonprofits compare to top universities<br>
* Why he doesn’t enjoy living in the San Francisco bay area<br>
* How can we fix the problem of most published research being false?<br>
* How good a career path is data science?<br>
* How important is experience in development versus technical skills?<br>
* How he learned much of what he needed to know in the army<br>
* How concerned should effective altruists be about burnout?</p><p>

Keiran Harris helped produce today’s episode.</p>]]>
      </content:encoded>
      <pubDate>Wed, 31 Jan 2018 12:53:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/f2806f68/1dae8eb3.mp3" length="75747348" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/pq1Ylo9ILGhlGZBTHv_NPbhe4-kSq4bh9J5CRHbOEco/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzYv/MTY4MzU0NDU1OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4729</itunes:duration>
      <itunes:summary>Ofir Reich started out doing math in the military, before spending 8 years in tech startups - but then made a sharp turn to become a data scientist focussed on helping the global poor.

At UC Berkeley’s Center for Effective Global Action he helps prevent tax evasion by identifying fake companies in India, enable Afghanistan to pay its teachers electronically, and raise yields for Ethiopian farmers by messaging them when local conditions make it ideal to apply fertiliser. Or at least that’s the hope - he’s also working on ways to test whether those interventions actually work.


Full post about this episode, including a transcript and relevant links to learn more.

Why dedicate his life to helping the global poor?

Ofir sees little moral difference between harming people and failing to help them. After all, if you had to press a button to keep all of your money from going to charity, and you pressed that button, would that be an action, or an inaction? Is there even an answer?

After reflecting on cases like this, he decided that to not engage with a problem is an active choice, one whose consequences he is just as morally responsible for as if he were directly involved. On top of his life philosophy we also discuss:


* The benefits of working in a top academic environment
* How best to start a career in global development
* Are RCTs worth the money? Should we focus on big picture policy change instead? Or more economic theory?
* How the delivery standards of nonprofits compare to top universities
* Why he doesn’t enjoy living in the San Francisco bay area
* How can we fix the problem of most published research being false?
* How good a career path is data science?
* How important is experience in development versus technical skills?
* How he learned much of what he needed to know in the army
* How concerned should effective altruists be about burnout?

Keiran Harris helped produce today’s episode.</itunes:summary>
      <itunes:subtitle>Ofir Reich started out doing math in the military, before spending 8 years in tech startups - but then made a sharp turn to become a data scientist focussed on helping the global poor.

At UC Berkeley’s Center for Effective Global Action he helps prevent </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#17 - Will MacAskill on moral uncertainty, utilitarianism &amp; how to avoid being a moral monster</title>
      <itunes:title>#17 - Will MacAskill on moral uncertainty, utilitarianism &amp; how to avoid being a moral monster</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/385939754</guid>
      <link>https://share.transistor.fm/s/75fb20e5</link>
      <description>
        <![CDATA[<p>Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He <a href="https://80000hours.org/2018/01/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">also thought</a> that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races.</p><p> Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?</p><p> <a href="https://80000hours.org/2018/01/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, key points and links to articles and career guides discussed in the show.</a></p><p> If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide.</p><p> Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism community. In this interview we discuss a wide range of topics:</p><p> * How would we go about a ‘long reflection’ to fix our moral errors?<br> * Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’?<br> * If we basically solve existential risks, what does humanity do next?<br> * What are some of Will’s most unusual philosophical positions?<br> * What are the best arguments for and against utilitarianism?<br> * Given disagreements among philosophers, how much should we believe the findings of philosophy as a field?<br> * What are some the biases we should be aware of within academia?<br> * What are some of the downsides of becoming a professor?<br> * What are the merits of becoming a philosopher?<br> * How does the media image of EA differ to the actual goals of the community?<br> * What kinds of things would you like to see the EA community do differently?<br> * How much should we explore potentially controversial ideas?<br> * How focused should we be on diversity?<br> * What are the best arguments against effective altruism?</p><p> <strong>Get free, one-on-one career advice</strong></p><p> We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, <a href="https://80000hours.org/coaching/?utm_campaign=podcast__will-macaskill">find out if our coaching can help you.</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He <a href="https://80000hours.org/2018/01/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">also thought</a> that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races.</p><p> Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?</p><p> <a href="https://80000hours.org/2018/01/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, key points and links to articles and career guides discussed in the show.</a></p><p> If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide.</p><p> Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism community. In this interview we discuss a wide range of topics:</p><p> * How would we go about a ‘long reflection’ to fix our moral errors?<br> * Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’?<br> * If we basically solve existential risks, what does humanity do next?<br> * What are some of Will’s most unusual philosophical positions?<br> * What are the best arguments for and against utilitarianism?<br> * Given disagreements among philosophers, how much should we believe the findings of philosophy as a field?<br> * What are some the biases we should be aware of within academia?<br> * What are some of the downsides of becoming a professor?<br> * What are the merits of becoming a philosopher?<br> * How does the media image of EA differ to the actual goals of the community?<br> * What kinds of things would you like to see the EA community do differently?<br> * How much should we explore potentially controversial ideas?<br> * How focused should we be on diversity?<br> * What are the best arguments against effective altruism?</p><p> <strong>Get free, one-on-one career advice</strong></p><p> We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, <a href="https://80000hours.org/coaching/?utm_campaign=podcast__will-macaskill">find out if our coaching can help you.</a></p>]]>
      </content:encoded>
      <pubDate>Fri, 19 Jan 2018 12:18:49 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/75fb20e5/8eb86804.mp3" length="107843972" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/u4Dv1HjaM8Cdg1bi2UF6v-wFF_xnHYsvOL21kGLG3xA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzUv/MTY4MzU0NDU1Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6734</itunes:duration>
      <itunes:summary>Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He also thought that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races.
 
Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?

Full transcript, key points and links to articles and career guides discussed in the show.
 
If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide.
 
Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism community. In this interview we discuss a wide range of topics:
 
* How would we go about a ‘long reflection’ to fix our moral errors?
* Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’?
* If we basically solve existential risks, what does humanity do next?
* What are some of Will’s most unusual philosophical positions?
* What are the best arguments for and against utilitarianism?
* Given disagreements among philosophers, how much should we believe the findings of philosophy as a field?
* What are some the biases we should be aware of within academia?
* What are some of the downsides of becoming a professor?
* What are the merits of becoming a philosopher?
* How does the media image of EA differ to the actual goals of the community?
* What kinds of things would you like to see the EA community do differently?
* How much should we explore potentially controversial ideas?
* How focused should we be on diversity?
* What are the best arguments against effective altruism?

Get free, one-on-one career advice

We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, find out if our coaching can help you.</itunes:summary>
      <itunes:subtitle>Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He also thought that women have no place in civil society, that it was okay to kill illegi</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#16 - Michelle Hutchinson on global priorities research &amp; shaping the ideas of intellectuals</title>
      <itunes:title>#16 - Michelle Hutchinson on global priorities research &amp; shaping the ideas of intellectuals</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/373158677</guid>
      <link>https://share.transistor.fm/s/08044a85</link>
      <description>
        <![CDATA[<p>In the 40s and 50s neoliberalism was a fringe movement within economics. But by the 80s it had become a dominant school of thought in public policy, and achieved major policy changes across the English speaking world. How did this happen?</p><p> In part because its leaders invested heavily in training academics to study and develop their ideas. Whether you think neoliberalism was good or bad, its history demonstrates the impact building a strong intellectual base within universities can have.</p><p> Michelle Hutchinson is working to get a different set of ideas a hearing in academia by setting up the Global Priorities Institute (GPI) at Oxford University. The Institute, which is <a href="https://globalprioritiesinstitute.org/opportunities/">currently hiring for three roles</a>, aims to bring together outstanding philosophers and economists to research how to most improve the world. The hope is that it will spark widespread academic engagement with effective altruist thinking, which will hone the ideas and help them gradually percolate into society more broadly.</p><p> <a href="https://80000hours.org/2017/12/michelle-hutchinson-global-priorities/?utm_campaign=podcast__michelle-hutchinson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Link to the full blog post about this episode including transcript and links to learn more</a></p><p> Its <a href="https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf">research agenda</a> includes questions like:</p><p> * How do we compare the good done by focussing on really different types of causes?<br> * How does saving lives actually affect the world relative to other things we could do?<br> * What are the biggest wins governments should be focussed on getting?</p><p> Before moving to GPI, Michelle was the Executive Director of Giving What We Can and a founding figure of the effective altruism movement. She has a PhD in Applied Ethics from Oxford on prioritization and global health.</p><p> We discuss:</p><p> * What is global priorities research and why does it matter?<br> * How is effective altruism seen in academia? Is it important to convince academics of the value of your work, or is it OK to ignore them?<br> * Operating inside a university is quite expensive, so is it even worth doing? Who can pay for this kind of thing?<br> * How hard is it to do something innovative inside a university? How serious are the administrative and other barriers?<br> * Is it harder to fundraise for a new institute, or hire the right people?<br> * Have other social movements benefitted from having a prominent academic arm?<br> * How can people prepare themselves to get research roles at a place like GPI?<br> * Many people want to have roles doing this kind of research. How many are actually cut out for it? What should those who aren’t do instead?<br> * What are the odds of the Institute’s work having an effect on the real world?</p><p> <strong>Get free, one-on-one career advice</strong></p><p> We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, <a href="https://80000hours.org/coaching/?utm_campaign=podcast__michelle-hutchinson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">find out if our coaching can help you.</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>In the 40s and 50s neoliberalism was a fringe movement within economics. But by the 80s it had become a dominant school of thought in public policy, and achieved major policy changes across the English speaking world. How did this happen?</p><p> In part because its leaders invested heavily in training academics to study and develop their ideas. Whether you think neoliberalism was good or bad, its history demonstrates the impact building a strong intellectual base within universities can have.</p><p> Michelle Hutchinson is working to get a different set of ideas a hearing in academia by setting up the Global Priorities Institute (GPI) at Oxford University. The Institute, which is <a href="https://globalprioritiesinstitute.org/opportunities/">currently hiring for three roles</a>, aims to bring together outstanding philosophers and economists to research how to most improve the world. The hope is that it will spark widespread academic engagement with effective altruist thinking, which will hone the ideas and help them gradually percolate into society more broadly.</p><p> <a href="https://80000hours.org/2017/12/michelle-hutchinson-global-priorities/?utm_campaign=podcast__michelle-hutchinson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Link to the full blog post about this episode including transcript and links to learn more</a></p><p> Its <a href="https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf">research agenda</a> includes questions like:</p><p> * How do we compare the good done by focussing on really different types of causes?<br> * How does saving lives actually affect the world relative to other things we could do?<br> * What are the biggest wins governments should be focussed on getting?</p><p> Before moving to GPI, Michelle was the Executive Director of Giving What We Can and a founding figure of the effective altruism movement. She has a PhD in Applied Ethics from Oxford on prioritization and global health.</p><p> We discuss:</p><p> * What is global priorities research and why does it matter?<br> * How is effective altruism seen in academia? Is it important to convince academics of the value of your work, or is it OK to ignore them?<br> * Operating inside a university is quite expensive, so is it even worth doing? Who can pay for this kind of thing?<br> * How hard is it to do something innovative inside a university? How serious are the administrative and other barriers?<br> * Is it harder to fundraise for a new institute, or hire the right people?<br> * Have other social movements benefitted from having a prominent academic arm?<br> * How can people prepare themselves to get research roles at a place like GPI?<br> * Many people want to have roles doing this kind of research. How many are actually cut out for it? What should those who aren’t do instead?<br> * What are the odds of the Institute’s work having an effect on the real world?</p><p> <strong>Get free, one-on-one career advice</strong></p><p> We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, <a href="https://80000hours.org/coaching/?utm_campaign=podcast__michelle-hutchinson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">find out if our coaching can help you.</a></p>]]>
      </content:encoded>
      <pubDate>Fri, 22 Dec 2017 18:53:33 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/08044a85/4ebf1bb4.mp3" length="52869213" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ujt3kfB_lDw5IRirKXjX6jpWR3vTGV_y00vhff8mPAQ/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzQv/MTY4MzU0NDU1Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3301</itunes:duration>
      <itunes:summary>In the 40s and 50s neoliberalism was a fringe movement within economics. But by the 80s it had become a dominant school of thought in public policy, and achieved major policy changes across the English speaking world. How did this happen?

In part because its leaders invested heavily in training academics to study and develop their ideas. Whether you think neoliberalism was good or bad, its history demonstrates the impact building a strong intellectual base within universities can have.

Michelle Hutchinson is working to get a different set of ideas a hearing in academia by setting up the Global Priorities Institute (GPI) at Oxford University. The Institute, which is currently hiring for three roles, aims to bring together outstanding philosophers and economists to research how to most improve the world. The hope is that it will spark widespread academic engagement with effective altruist thinking, which will hone the ideas and help them gradually percolate into society more broadly.

Link to the full blog post about this episode including transcript and links to learn more

Its research agenda includes questions like:

* How do we compare the good done by focussing on really different types of causes?
* How does saving lives actually affect the world relative to other things we could do?
* What are the biggest wins governments should be focussed on getting?

Before moving to GPI, Michelle was the Executive Director of Giving What We Can and a founding figure of the effective altruism movement. She has a PhD in Applied Ethics from Oxford on prioritization and global health.

We discuss:

* What is global priorities research and why does it matter?
* How is effective altruism seen in academia? Is it important to convince academics of the value of your work, or is it OK to ignore them?
* Operating inside a university is quite expensive, so is it even worth doing? Who can pay for this kind of thing?
* How hard is it to do something innovative inside a university? How serious are the administrative and other barriers?
* Is it harder to fundraise for a new institute, or hire the right people?
* Have other social movements benefitted from having a prominent academic arm?
* How can people prepare themselves to get research roles at a place like GPI?
* Many people want to have roles doing this kind of research. How many are actually cut out for it? What should those who aren’t do instead?
* What are the odds of the Institute’s work having an effect on the real world?

Get free, one-on-one career advice

We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, find out if our coaching can help you.</itunes:summary>
      <itunes:subtitle>In the 40s and 50s neoliberalism was a fringe movement within economics. But by the 80s it had become a dominant school of thought in public policy, and achieved major policy changes across the English speaking world. How did this happen?

In part becau</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#15 - Phil Tetlock on how chimps beat Berkeley undergrads and when it’s wise to defer to the wise</title>
      <itunes:title>#15 - Phil Tetlock on how chimps beat Berkeley undergrads and when it’s wise to defer to the wise</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/358492934</guid>
      <link>https://share.transistor.fm/s/b60fc87e</link>
      <description>
        <![CDATA[<p>Prof Philip Tetlock is a social science legend. Over forty years he has researched whose predictions we can trust, whose we can’t and why - and developed methods that allow all of us to be better at predicting the future.</p><p> After the Iraq WMDs fiasco, the US intelligence services hired him to figure out how to ensure they’d never screw up that badly again. The result of that work – <em>Superforecasting</em> – was a media sensation in 2015.</p><p> <a href="https://80000hours.org/2017/11/prof-tetlock-predicting-the-future/?utm_campaign=podcast__phil-tetlock&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript, brief summary, apply for coaching and links to learn more.</strong></a></p><p> It described Tetlock’s Good Judgement Project, which found forecasting methods so accurate they beat everyone else in open competition, including thousands of people in the intelligence services with access to classified information.</p><p> Today he’s working to develop the best forecasting process ever, by combining top human and machine intelligence in the <a href="https://www.hybridforecasting.com/">Hybrid Forecasting Competition</a>, which you can sign up and participate in.</p><p> We start by describing his key findings, and then push to the edge of what is known about how to foresee the unforeseeable:</p><p> * Should people who want to be right just adopt the views of experts rather than apply their own judgement?<br> * Why are Berkeley undergrads worse forecasters than dart-throwing chimps?<br> * Should I keep my political views secret, so it will be easier to change them later?<br> * How can listeners contribute to his latest cutting-edge research?<br> * What do we know about our accuracy at predicting low-probability high-impact disasters?<br> * Does his research provide an intellectual basis for populist political movements?<br> * Was the Iraq War caused by bad politics, or bad intelligence methods?<br> * What can we learn about forecasting from the 2016 election?<br> * Can experience help people avoid overconfidence and underconfidence?<br> * When does an AI easily beat human judgement?<br> * Could more accurate forecasting methods make the world more dangerous?<br> * How much does demographic diversity line up with cognitive diversity?<br> * What are the odds we’ll go to war with China?<br> * Should we let prediction tournaments run most of the government?</p><p> Listen to it. <strong>Get free, one-on-one career advice.</strong> Want to work on important social science research like Tetlock? We’ve helped hundreds of people compare their options and get introductions. <a href="https://80000hours.org/coaching/?utm_campaign=podcast__phil-tetlock&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Find out if our coaching can help you.</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Prof Philip Tetlock is a social science legend. Over forty years he has researched whose predictions we can trust, whose we can’t and why - and developed methods that allow all of us to be better at predicting the future.</p><p> After the Iraq WMDs fiasco, the US intelligence services hired him to figure out how to ensure they’d never screw up that badly again. The result of that work – <em>Superforecasting</em> – was a media sensation in 2015.</p><p> <a href="https://80000hours.org/2017/11/prof-tetlock-predicting-the-future/?utm_campaign=podcast__phil-tetlock&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"><strong>Full transcript, brief summary, apply for coaching and links to learn more.</strong></a></p><p> It described Tetlock’s Good Judgement Project, which found forecasting methods so accurate they beat everyone else in open competition, including thousands of people in the intelligence services with access to classified information.</p><p> Today he’s working to develop the best forecasting process ever, by combining top human and machine intelligence in the <a href="https://www.hybridforecasting.com/">Hybrid Forecasting Competition</a>, which you can sign up and participate in.</p><p> We start by describing his key findings, and then push to the edge of what is known about how to foresee the unforeseeable:</p><p> * Should people who want to be right just adopt the views of experts rather than apply their own judgement?<br> * Why are Berkeley undergrads worse forecasters than dart-throwing chimps?<br> * Should I keep my political views secret, so it will be easier to change them later?<br> * How can listeners contribute to his latest cutting-edge research?<br> * What do we know about our accuracy at predicting low-probability high-impact disasters?<br> * Does his research provide an intellectual basis for populist political movements?<br> * Was the Iraq War caused by bad politics, or bad intelligence methods?<br> * What can we learn about forecasting from the 2016 election?<br> * Can experience help people avoid overconfidence and underconfidence?<br> * When does an AI easily beat human judgement?<br> * Could more accurate forecasting methods make the world more dangerous?<br> * How much does demographic diversity line up with cognitive diversity?<br> * What are the odds we’ll go to war with China?<br> * Should we let prediction tournaments run most of the government?</p><p> Listen to it. <strong>Get free, one-on-one career advice.</strong> Want to work on important social science research like Tetlock? We’ve helped hundreds of people compare their options and get introductions. <a href="https://80000hours.org/coaching/?utm_campaign=podcast__phil-tetlock&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Find out if our coaching can help you.</a></p>]]>
      </content:encoded>
      <pubDate>Mon, 20 Nov 2017 17:28:44 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/b60fc87e/864a88f1.mp3" length="80716397" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/S4HKaGYPSe0IJWUXfyy0Lh-XwOkcaKsIF-Mw4Szdk-M/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzMv/MTY4MzU0NDU1Ni1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5040</itunes:duration>
      <itunes:summary>Prof Philip Tetlock is a social science legend. Over forty years he has researched whose predictions we can trust, whose we can’t and why - and developed methods that allow all of us to be better at predicting the future.

After the Iraq WMDs fiasco, the US intelligence services hired him to figure out how to ensure they’d never screw up that badly again. The result of that work – Superforecasting – was a media sensation in 2015.

Full transcript, brief summary, apply for coaching and links to learn more.

It described Tetlock’s Good Judgement Project, which found forecasting methods so accurate they beat everyone else in open competition, including thousands of people in the intelligence services with access to classified information.

Today he’s working to develop the best forecasting process ever, by combining top human and machine intelligence in the Hybrid Forecasting Competition, which you can sign up and participate in.

We start by describing his key findings, and then push to the edge of what is known about how to foresee the unforeseeable:

* Should people who want to be right just adopt the views of experts rather than apply their own judgement?
* Why are Berkeley undergrads worse forecasters than dart-throwing chimps?
* Should I keep my political views secret, so it will be easier to change them later?
* How can listeners contribute to his latest cutting-edge research?
* What do we know about our accuracy at predicting low-probability high-impact disasters?
* Does his research provide an intellectual basis for populist political movements?
* Was the Iraq War caused by bad politics, or bad intelligence methods?
* What can we learn about forecasting from the 2016 election?
* Can experience help people avoid overconfidence and underconfidence?
* When does an AI easily beat human judgement?
* Could more accurate forecasting methods make the world more dangerous?
* How much does demographic diversity line up with cognitive diversity?
* What are the odds we’ll go to war with China?
* Should we let prediction tournaments run most of the government?

Listen to it.

Get free, one-on-one career advice. Want to work on important social science research like Tetlock? We’ve helped hundreds of people compare their options and get introductions. Find out if our coaching can help you.</itunes:summary>
      <itunes:subtitle>Prof Philip Tetlock is a social science legend. Over forty years he has researched whose predictions we can trust, whose we can’t and why - and developed methods that allow all of us to be better at predicting the future.

After the Iraq WMDs fiasco, th</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#14 - Sharon Nunez &amp; Jose Valle on going undercover to expose animal abuse</title>
      <itunes:title>#14 - Sharon Nunez &amp; Jose Valle on going undercover to expose animal abuse</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/355186310</guid>
      <link>https://share.transistor.fm/s/13de4cc8</link>
      <description>
        <![CDATA[What if you knew that ducks were being killed with pitchforks? Rabbits dumped alive into containers? Or pigs being strangled with forklifts? Would you be willing to go undercover to expose the crime?<p>

That’s a real question that confronts volunteers at Animal Equality (AE). In this episode we speak to Sharon Nunez and Jose Valle, who founded AE in 2006 and then grew it into a multi-million dollar international animal rights organisation. They’ve been chosen as one of the most effective animal protection orgs in the world by Animal Charity Evaluators for the last 3 consecutive years.</p><p>

<a href="https://80000hours.org/2017/11/animal-equality-exposing-cruelty/?utm_campaign=podcast__sharon-nunez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Blog post about the episode, including links and full transcript.</a></p><p>

<a href="https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__sharon-nunez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">A related previous episode, strongly recommended: Lewis Bollard on how to end factory farming as soon as possible.</a></p><p>

In addition to undercover investigations AE has also designed a 3D virtual-reality farm experience called iAnimal360. People get to experience being trapped in a cage – in a room designed to kill then - and can’t just look away. How big an impact is this having on users?</p><p>

Sharon Nuñez and Jose Valle also tackle:</p><p>

* How do they track their goals and metrics week to week?<br>
* How much does an undercover investigation cost?<br>
* Why don’t people donate more to factory farmed animals, given that they’re the vast majority of animals harmed directly by humans?<br>
* How risky is it to attempt to build a career in animal advocacy?<br>
* What led to a change in their focus from bullfighting in Spain to animal farming?<br>
* How does working with governments or corporate campaigns compare with early strategies like creating new vegans/vegetarians?<br>
* Has their very rapid growth been difficult to handle?<br>
* What should our listeners study or do if they want to work in this area?<br>
* How can we get across the message that horrific cases are a feature - not a bug - of factory farming?<br>
* Do the owners or workers of factory farms ever express shame at what they do?</p>]]>
      </description>
      <content:encoded>
        <![CDATA[What if you knew that ducks were being killed with pitchforks? Rabbits dumped alive into containers? Or pigs being strangled with forklifts? Would you be willing to go undercover to expose the crime?<p>

That’s a real question that confronts volunteers at Animal Equality (AE). In this episode we speak to Sharon Nunez and Jose Valle, who founded AE in 2006 and then grew it into a multi-million dollar international animal rights organisation. They’ve been chosen as one of the most effective animal protection orgs in the world by Animal Charity Evaluators for the last 3 consecutive years.</p><p>

<a href="https://80000hours.org/2017/11/animal-equality-exposing-cruelty/?utm_campaign=podcast__sharon-nunez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Blog post about the episode, including links and full transcript.</a></p><p>

<a href="https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__sharon-nunez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">A related previous episode, strongly recommended: Lewis Bollard on how to end factory farming as soon as possible.</a></p><p>

In addition to undercover investigations AE has also designed a 3D virtual-reality farm experience called iAnimal360. People get to experience being trapped in a cage – in a room designed to kill then - and can’t just look away. How big an impact is this having on users?</p><p>

Sharon Nuñez and Jose Valle also tackle:</p><p>

* How do they track their goals and metrics week to week?<br>
* How much does an undercover investigation cost?<br>
* Why don’t people donate more to factory farmed animals, given that they’re the vast majority of animals harmed directly by humans?<br>
* How risky is it to attempt to build a career in animal advocacy?<br>
* What led to a change in their focus from bullfighting in Spain to animal farming?<br>
* How does working with governments or corporate campaigns compare with early strategies like creating new vegans/vegetarians?<br>
* Has their very rapid growth been difficult to handle?<br>
* What should our listeners study or do if they want to work in this area?<br>
* How can we get across the message that horrific cases are a feature - not a bug - of factory farming?<br>
* Do the owners or workers of factory farms ever express shame at what they do?</p>]]>
      </content:encoded>
      <pubDate>Mon, 13 Nov 2017 18:12:21 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/13de4cc8/78d8a91b.mp3" length="82596166" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/4qlvzRZjF28mHF7_yBmUcAfcHSgAkqmqnlJE4mwcjtA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzIv/MTY4MzU0NDU1NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5157</itunes:duration>
      <itunes:summary>What if you knew that ducks were being killed with pitchforks? Rabbits dumped alive into containers? Or pigs being strangled with forklifts? Would you be willing to go undercover to expose the crime?

That’s a real question that confronts volunteers at Animal Equality (AE). In this episode we speak to Sharon Nunez and Jose Valle, who founded AE in 2006 and then grew it into a multi-million dollar international animal rights organisation. They’ve been chosen as one of the most effective animal protection orgs in the world by Animal Charity Evaluators for the last 3 consecutive years.

Blog post about the episode, including links and full transcript.

A related previous episode, strongly recommended: Lewis Bollard on how to end factory farming as soon as possible.

In addition to undercover investigations AE has also designed a 3D virtual-reality farm experience called iAnimal360. People get to experience being trapped in a cage – in a room designed to kill then - and can’t just look away. How big an impact is this having on users?

Sharon Nuñez and Jose Valle also tackle:

* How do they track their goals and metrics week to week?
* How much does an undercover investigation cost?
* Why don’t people donate more to factory farmed animals, given that they’re the vast majority of animals harmed directly by humans?
* How risky is it to attempt to build a career in animal advocacy?
* What led to a change in their focus from bullfighting in Spain to animal farming?
* How does working with governments or corporate campaigns compare with early strategies like creating new vegans/vegetarians?
* Has their very rapid growth been difficult to handle?
* What should our listeners study or do if they want to work in this area?
* How can we get across the message that horrific cases are a feature - not a bug - of factory farming?
* Do the owners or workers of factory farms ever express shame at what they do?</itunes:summary>
      <itunes:subtitle>What if you knew that ducks were being killed with pitchforks? Rabbits dumped alive into containers? Or pigs being strangled with forklifts? Would you be willing to go undercover to expose the crime?

That’s a real question that confronts volunteers at An</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#13 - Claire Walsh on testing which policies work &amp; how to get governments to listen to the results</title>
      <itunes:title>#13 - Claire Walsh on testing which policies work &amp; how to get governments to listen to the results</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/349509882</guid>
      <link>https://share.transistor.fm/s/c853d723</link>
      <description>
        <![CDATA[In both rich and poor countries, government policy is often based on no evidence at all and many programs don’t work. This has particularly harsh effects on the global poor - in some countries governments only spend $100 on each citizen a year so they can’t afford to waste a single dollar.<p>

Enter MIT’s Poverty Action Lab (J-PAL). Since 2003 they’ve conducted experiments to figure out what policies actually help recipients, and then tried to get them implemented by governments and non-profits.</p><p>

Claire Walsh leads J-PAL’s Government Partnership Initiative, which works to evaluate policies and programs in collaboration with developing world governments, scale policies that have been shown to work, and generally promote a culture of evidence-based policymaking.</p><p>

<a href="https://80000hours.org/2017/10/claire-walsh-evidence-in-development/?utm_campaign=podcast__claire-walsh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Summary, links to career opportunities and topics discussed in the show.</a></p><p>

We discussed (her views only, not J-PAL’s):</p><p>

* How can they get evidence backed policies adopted? Do politicians in the developing world even care whether their programs actually work? Is the norm evidence-based policy, or policy-based evidence?<br>
* Is evidence-based policy an evidence-based strategy itself?<br>
* Which policies does she think would have a particularly large impact on human welfare relative to their cost?<br>
* How did she come to lead one of J-PAL’s departments at 29?<br>
* How do you evaluate the effectiveness of energy and environment programs (Walsh’s area of expertise), and what are the standout approaches in that area?<br>
* 80,000 Hours has warned people about the downsides of starting your career in a non-profit. Walsh started her career in a non-profit and has thrived, so are we making a mistake?<br>
* Other than J-PAL, what are the best places to work in development? What are the best subjects to study? Where can you go network to break into the sector?<br>
* Is living in poverty as bad as we think?</p><p>

And plenty of other things besides.</p><p>

We haven’t run an RCT to test whether this episode will actually help your career, but I suggest you listen anyway. Trust my intuition on this one.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[In both rich and poor countries, government policy is often based on no evidence at all and many programs don’t work. This has particularly harsh effects on the global poor - in some countries governments only spend $100 on each citizen a year so they can’t afford to waste a single dollar.<p>

Enter MIT’s Poverty Action Lab (J-PAL). Since 2003 they’ve conducted experiments to figure out what policies actually help recipients, and then tried to get them implemented by governments and non-profits.</p><p>

Claire Walsh leads J-PAL’s Government Partnership Initiative, which works to evaluate policies and programs in collaboration with developing world governments, scale policies that have been shown to work, and generally promote a culture of evidence-based policymaking.</p><p>

<a href="https://80000hours.org/2017/10/claire-walsh-evidence-in-development/?utm_campaign=podcast__claire-walsh&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Summary, links to career opportunities and topics discussed in the show.</a></p><p>

We discussed (her views only, not J-PAL’s):</p><p>

* How can they get evidence backed policies adopted? Do politicians in the developing world even care whether their programs actually work? Is the norm evidence-based policy, or policy-based evidence?<br>
* Is evidence-based policy an evidence-based strategy itself?<br>
* Which policies does she think would have a particularly large impact on human welfare relative to their cost?<br>
* How did she come to lead one of J-PAL’s departments at 29?<br>
* How do you evaluate the effectiveness of energy and environment programs (Walsh’s area of expertise), and what are the standout approaches in that area?<br>
* 80,000 Hours has warned people about the downsides of starting your career in a non-profit. Walsh started her career in a non-profit and has thrived, so are we making a mistake?<br>
* Other than J-PAL, what are the best places to work in development? What are the best subjects to study? Where can you go network to break into the sector?<br>
* Is living in poverty as bad as we think?</p><p>

And plenty of other things besides.</p><p>

We haven’t run an RCT to test whether this episode will actually help your career, but I suggest you listen anyway. Trust my intuition on this one.</p>]]>
      </content:encoded>
      <pubDate>Tue, 31 Oct 2017 21:00:08 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/c853d723/e50cbf29.mp3" length="50408737" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/JIKNvvtOS_f2kNhZoYrksMDi090ZkNxrmACyk-478Qc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzEv/MTY4MzU0NDU1NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3148</itunes:duration>
      <itunes:summary>In both rich and poor countries, government policy is often based on no evidence at all and many programs don’t work. This has particularly harsh effects on the global poor - in some countries governments only spend $100 on each citizen a year so they can’t afford to waste a single dollar.

Enter MIT’s Poverty Action Lab (J-PAL). Since 2003 they’ve conducted experiments to figure out what policies actually help recipients, and then tried to get them implemented by governments and non-profits.

Claire Walsh leads J-PAL’s Government Partnership Initiative, which works to evaluate policies and programs in collaboration with developing world governments, scale policies that have been shown to work, and generally promote a culture of evidence-based policymaking.

Summary, links to career opportunities and topics discussed in the show.

We discussed (her views only, not J-PAL’s):

* How can they get evidence backed policies adopted? Do politicians in the developing world even care whether their programs actually work? Is the norm evidence-based policy, or policy-based evidence?
* Is evidence-based policy an evidence-based strategy itself?
* Which policies does she think would have a particularly large impact on human welfare relative to their cost?
* How did she come to lead one of J-PAL’s departments at 29?
* How do you evaluate the effectiveness of energy and environment programs (Walsh’s area of expertise), and what are the standout approaches in that area?
* 80,000 Hours has warned people about the downsides of starting your career in a non-profit. Walsh started her career in a non-profit and has thrived, so are we making a mistake?
* Other than J-PAL, what are the best places to work in development? What are the best subjects to study? Where can you go network to break into the sector?
* Is living in poverty as bad as we think?

And plenty of other things besides.

We haven’t run an RCT to test whether this episode will actually help your career, but I suggest you listen anyway. Trust my intuition on this one.</itunes:summary>
      <itunes:subtitle>In both rich and poor countries, government policy is often based on no evidence at all and many programs don’t work. This has particularly harsh effects on the global poor - in some countries governments only spend $100 on each citizen a year so they can</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#12 - Beth Cameron works to stop you dying in a pandemic. Here’s what keeps her up at night.</title>
      <itunes:title>#12 - Beth Cameron works to stop you dying in a pandemic. Here’s what keeps her up at night.</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/348492993</guid>
      <link>https://share.transistor.fm/s/75b13c76</link>
      <description>
        <![CDATA[<p><em>“When you're in the middle of a crisis and you have to ask for money, you're already too late.”</em></p><p> That’s Dr Beth Cameron, who leads <em>Global Biological Policy and Programs</em> at the Nuclear Threat Initiative.</p><p> Beth should know. She has years of experience preparing for and fighting the diseases of our nightmares, on the <em>White House Ebola Taskforce</em>, in the <em>National Security Council</em> staff, and as the Assistant Secretary of Defense for <em>Nuclear, Chemical and Biological Defense Programs</em>.</p><p> <a href="https://80000hours.org/2017/10/beth-cameron-pandemic-preparedness/?utm_campaign=podcast__beth-cameron&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Summary, list of career opportunities, extra links to learn more and coaching application.</a></p><p> Unfortunately, the countries of the world aren’t prepared for a crisis - and like children crowded into daycare, there’s a good chance something will make us all sick at once.</p><p> During past pandemics countries have dragged their feet over who will pay to contain them, or struggled to move people and supplies where they needed to be. At the same time advanced biotechnology threatens to make it possible for terrorists to bring back smallpox - or create something even worse.</p><p> In this interview we look at the current state of play in disease control, what needs to change, and how you can build the career capital necessary to make those changes yourself. That includes:</p><p> * What and where to study, and where to begin a career in pandemic preparedness. Below you’ll find a lengthy list of people and places mentioned in the interview, and others we’ve had recommended to us.<br> * How the Nuclear Threat Initiative, with just 50 people, collaborates with governments around the world to reduce the risk of nuclear or biological catastrophes, and whether they might want to hire you.<br> * The best strategy for containing pandemics.<br> * Why we lurch from panic, to neglect, to panic again when it comes to protecting ourselves from contagious diseases.<br> * Current reform efforts within the World Health Organisation, and attempts to prepare partial vaccines ahead of time.<br> * Which global health security groups most impress Beth, and what they’re doing.<br> * What new technologies could be invented to make us safer.<br> * Whether it’s possible to help solve the problem through mass advocacy.<br> * Much more besides.</p><p> <a href="https://80000hours.org/coaching/?utm_campaign=podcast__beth-cameron">Get free, one-on-one career advice to improve biosecurity</a></p><p> Considering a relevant grad program like a biology PhD, medicine, or security studies? Able to apply for a relevant job already? We’ve helped dozens of people plan their careers to work on pandemic preparedness and put them in touch with mentors. <strong>If you want to work on the problem discussed in this episode, you should apply for coaching:</strong></p><p> <a href="https://80000hours.org/coaching/?utm_campaign=podcast__beth-cameron&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Read more</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p><em>“When you're in the middle of a crisis and you have to ask for money, you're already too late.”</em></p><p> That’s Dr Beth Cameron, who leads <em>Global Biological Policy and Programs</em> at the Nuclear Threat Initiative.</p><p> Beth should know. She has years of experience preparing for and fighting the diseases of our nightmares, on the <em>White House Ebola Taskforce</em>, in the <em>National Security Council</em> staff, and as the Assistant Secretary of Defense for <em>Nuclear, Chemical and Biological Defense Programs</em>.</p><p> <a href="https://80000hours.org/2017/10/beth-cameron-pandemic-preparedness/?utm_campaign=podcast__beth-cameron&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Summary, list of career opportunities, extra links to learn more and coaching application.</a></p><p> Unfortunately, the countries of the world aren’t prepared for a crisis - and like children crowded into daycare, there’s a good chance something will make us all sick at once.</p><p> During past pandemics countries have dragged their feet over who will pay to contain them, or struggled to move people and supplies where they needed to be. At the same time advanced biotechnology threatens to make it possible for terrorists to bring back smallpox - or create something even worse.</p><p> In this interview we look at the current state of play in disease control, what needs to change, and how you can build the career capital necessary to make those changes yourself. That includes:</p><p> * What and where to study, and where to begin a career in pandemic preparedness. Below you’ll find a lengthy list of people and places mentioned in the interview, and others we’ve had recommended to us.<br> * How the Nuclear Threat Initiative, with just 50 people, collaborates with governments around the world to reduce the risk of nuclear or biological catastrophes, and whether they might want to hire you.<br> * The best strategy for containing pandemics.<br> * Why we lurch from panic, to neglect, to panic again when it comes to protecting ourselves from contagious diseases.<br> * Current reform efforts within the World Health Organisation, and attempts to prepare partial vaccines ahead of time.<br> * Which global health security groups most impress Beth, and what they’re doing.<br> * What new technologies could be invented to make us safer.<br> * Whether it’s possible to help solve the problem through mass advocacy.<br> * Much more besides.</p><p> <a href="https://80000hours.org/coaching/?utm_campaign=podcast__beth-cameron">Get free, one-on-one career advice to improve biosecurity</a></p><p> Considering a relevant grad program like a biology PhD, medicine, or security studies? Able to apply for a relevant job already? We’ve helped dozens of people plan their careers to work on pandemic preparedness and put them in touch with mentors. <strong>If you want to work on the problem discussed in this episode, you should apply for coaching:</strong></p><p> <a href="https://80000hours.org/coaching/?utm_campaign=podcast__beth-cameron&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Read more</a></p>]]>
      </content:encoded>
      <pubDate>Wed, 25 Oct 2017 17:13:10 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/75b13c76/4c547bfb.mp3" length="101160815" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/36y6eOaaXJKq-O4sAXKgW2gLycZ1_O9LGgTPIMzfCY0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MzAv/MTY4MzU0NDU1My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6316</itunes:duration>
      <itunes:summary>“When you're in the middle of a crisis and you have to ask for money, you're already too late.”

That’s Dr Beth Cameron, who leads Global Biological Policy and Programs at the Nuclear Threat Initiative.

Beth should know. She has years of experience preparing for and fighting the diseases of our nightmares, on the White House Ebola Taskforce, in the National Security Council staff, and as the Assistant Secretary of Defense for Nuclear, Chemical and Biological Defense Programs.

Summary, list of career opportunities, extra links to learn more and coaching application.

Unfortunately, the countries of the world aren’t prepared for a crisis - and like children crowded into daycare, there’s a good chance something will make us all sick at once.

During past pandemics countries have dragged their feet over who will pay to contain them, or struggled to move people and supplies where they needed to be. At the same time advanced biotechnology threatens to make it possible for terrorists to bring back smallpox - or create something even worse.

In this interview we look at the current state of play in disease control, what needs to change, and how you can build the career capital necessary to make those changes yourself. That includes:

* What and where to study, and where to begin a career in pandemic preparedness. Below you’ll find a lengthy list of people and places mentioned in the interview, and others we’ve had recommended to us.
* How the Nuclear Threat Initiative, with just 50 people, collaborates with governments around the world to reduce the risk of nuclear or biological catastrophes, and whether they might want to hire you.
* The best strategy for containing pandemics.
* Why we lurch from panic, to neglect, to panic again when it comes to protecting ourselves from contagious diseases.
* Current reform efforts within the World Health Organisation, and attempts to prepare partial vaccines ahead of time.
* Which global health security groups most impress Beth, and what they’re doing.
* What new technologies could be invented to make us safer.
* Whether it’s possible to help solve the problem through mass advocacy.
* Much more besides.

Get free, one-on-one career advice to improve biosecurity

Considering a relevant grad program like a biology PhD, medicine, or security studies? Able to apply for a relevant job already? We’ve helped dozens of people plan their careers to work on pandemic preparedness and put them in touch with mentors. If you want to work on the problem discussed in this episode, you should apply for coaching:

Read more</itunes:summary>
      <itunes:subtitle>“When you're in the middle of a crisis and you have to ask for money, you're already too late.”

That’s Dr Beth Cameron, who leads Global Biological Policy and Programs at the Nuclear Threat Initiative.

Beth should know. She has years of experience p</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#11 - Spencer Greenberg on speeding up social science 10-fold &amp; why plenty of startups cause harm</title>
      <itunes:title>#11 - Spencer Greenberg on speeding up social science 10-fold &amp; why plenty of startups cause harm</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/347339539</guid>
      <link>https://share.transistor.fm/s/7abed21a</link>
      <description>
        <![CDATA[<p>Do most meat eaters think it’s wrong to hurt animals? Do Americans think climate change is likely to cause human extinction? What is the best, state-of-the-art therapy for depression? How can we make academics more intellectually honest, so we can actually trust their findings? How can we speed up social science research ten-fold? Do most startups improve the world, or make it worse?</p><p> If you’re interested in these question, this interview is for you.</p><p> <a href="https://80000hours.org/2017/10/spencer-greenberg-social-science/?utm_campaign=podcast__spencer-greenberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Click for a full transcript, links discussed in the show, etc.</a></p><p> A scientist, entrepreneur, writer and mathematician, Spencer Greenberg is constantly working to create tools to speed up and improve research and critical thinking. These include:</p><p> * Rapid public opinion surveys to find out what most people actually think about animal consciousness, farm animal welfare, the impact of developing world charities and the likelihood of extinction by various different means;<br> * Tools to enable social science research to be run en masse very cheaply;<br> * ClearerThinking.org, a highly popular site for improving people’s judgement and decision-making;<br> * Ways to transform data analysis methods to ensure that papers only show true findings;<br> * Innovative research methods;<br> * Ways to decide which research projects are actually worth pursuing.</p><p> In this interview, Spencer discusses all of these and more. If you don’t feel like listening, that just shows that you have poor judgement and need to benefit from his wisdom even more!</p><p> <a href="https://80000hours.org/coaching/?utm_campaign=podcast__spencer-greenberg">Get free, one-on-one career advice</a></p><p> We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. <strong>If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</strong><br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Do most meat eaters think it’s wrong to hurt animals? Do Americans think climate change is likely to cause human extinction? What is the best, state-of-the-art therapy for depression? How can we make academics more intellectually honest, so we can actually trust their findings? How can we speed up social science research ten-fold? Do most startups improve the world, or make it worse?</p><p> If you’re interested in these question, this interview is for you.</p><p> <a href="https://80000hours.org/2017/10/spencer-greenberg-social-science/?utm_campaign=podcast__spencer-greenberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Click for a full transcript, links discussed in the show, etc.</a></p><p> A scientist, entrepreneur, writer and mathematician, Spencer Greenberg is constantly working to create tools to speed up and improve research and critical thinking. These include:</p><p> * Rapid public opinion surveys to find out what most people actually think about animal consciousness, farm animal welfare, the impact of developing world charities and the likelihood of extinction by various different means;<br> * Tools to enable social science research to be run en masse very cheaply;<br> * ClearerThinking.org, a highly popular site for improving people’s judgement and decision-making;<br> * Ways to transform data analysis methods to ensure that papers only show true findings;<br> * Innovative research methods;<br> * Ways to decide which research projects are actually worth pursuing.</p><p> In this interview, Spencer discusses all of these and more. If you don’t feel like listening, that just shows that you have poor judgement and need to benefit from his wisdom even more!</p><p> <a href="https://80000hours.org/coaching/?utm_campaign=podcast__spencer-greenberg">Get free, one-on-one career advice</a></p><p> We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. <strong>If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</strong><br></p>]]>
      </content:encoded>
      <pubDate>Tue, 17 Oct 2017 17:57:28 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/7abed21a/78875e20.mp3" length="85809302" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/ZpkOwcIiSnbdU1SLaoQe7ui_MLrKMIFdx0Xi9Jj9qhc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Mjkv/MTY4MzU0NDU1Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5358</itunes:duration>
      <itunes:summary>Do most meat eaters think it’s wrong to hurt animals? Do Americans think climate change is likely to cause human extinction? What is the best, state-of-the-art therapy for depression? How can we make academics more intellectually honest, so we can actually trust their findings? How can we speed up social science research ten-fold? Do most startups improve the world, or make it worse?

If you’re interested in these question, this interview is for you.

Click for a full transcript, links discussed in the show, etc.

A scientist, entrepreneur, writer and mathematician, Spencer Greenberg is constantly working to create tools to speed up and improve research and critical thinking. These include:

* Rapid public opinion surveys to find out what most people actually think about animal consciousness, farm animal welfare, the impact of developing world charities and the likelihood of extinction by various different means;
* Tools to enable social science research to be run en masse very cheaply;
* ClearerThinking.org, a highly popular site for improving people’s judgement and decision-making;
* Ways to transform data analysis methods to ensure that papers only show true findings;
* Innovative research methods;
* Ways to decide which research projects are actually worth pursuing.

In this interview, Spencer discusses all of these and more. If you don’t feel like listening, that just shows that you have poor judgement and need to benefit from his wisdom even more!

Get free, one-on-one career advice

We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</itunes:summary>
      <itunes:subtitle>Do most meat eaters think it’s wrong to hurt animals? Do Americans think climate change is likely to cause human extinction? What is the best, state-of-the-art therapy for depression? How can we make academics more intellectually honest, so we can actuall</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#10 - Nick Beckstead on how to spend billions of dollars preventing human extinction</title>
      <itunes:title>#10 - Nick Beckstead on how to spend billions of dollars preventing human extinction</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/346373135</guid>
      <link>https://share.transistor.fm/s/6fc8033c</link>
      <description>
        <![CDATA[<p>What if you were in a position to give away billions of dollars to improve the world? What would you do with it? This is the problem facing Program Officers at the Open Philanthropy Project - people like Dr Nick Beckstead.</p><p> Following a PhD in philosophy, Nick works to figure out where money can do the most good. He’s been involved in major grants in a wide range of areas, including ending factory farming through technological innovation, safeguarding the world from advances in biotechnology and artificial intelligence, and spreading rational compassion.</p><p> <a href="https://80000hours.org/2017/10/nick-beckstead-giving-billions/?utm_campaign=podcast__nick-beckstead&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, coaching application form, overview of the conversation, and links to resources discussed in the episode:</a></p><p> This episode is a tour through some of the toughest questions ‘effective altruists’ face when figuring out how to best improve the world, including:</p><p> * * Should we mostly try to help people currently alive, or future generations? Nick studied this question for years in his PhD thesis, <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE">On the Overwhelming Importance of Shaping the Far Future</a>. (The first 31 minutes is a snappier version of <a href="https://80k.link/toby-ord-from-nick">my conversation with Toby Ord</a>.)<br> * Is clean meat (aka *in vitro* meat) technologically feasible any time soon, or should we be looking for plant-based alternatives?<br> * What are the greatest risks to human civilisation?<br> * To stop malaria is it more cost-effective to use technology to <a href="https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support">eliminate mosquitos</a> than to distribute bed nets?<br> * Should people who want to improve the future work for changes that will be very useful in a specific scenario, or just generally try to improve how well humanity makes decisions?<br> * What specific jobs should our listeners take in order for Nick to be able to spend more money in useful ways to improve the world?<br> * Should we expect the future to be better if the economy grows more quickly - or more slowly?</p><p> <strong>Get free, one-on-one career advice</strong></p><p> We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href="https://80000hours.org/coaching/?utm_campaign=nick-beckstead-podcast">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a><br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>What if you were in a position to give away billions of dollars to improve the world? What would you do with it? This is the problem facing Program Officers at the Open Philanthropy Project - people like Dr Nick Beckstead.</p><p> Following a PhD in philosophy, Nick works to figure out where money can do the most good. He’s been involved in major grants in a wide range of areas, including ending factory farming through technological innovation, safeguarding the world from advances in biotechnology and artificial intelligence, and spreading rational compassion.</p><p> <a href="https://80000hours.org/2017/10/nick-beckstead-giving-billions/?utm_campaign=podcast__nick-beckstead&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, coaching application form, overview of the conversation, and links to resources discussed in the episode:</a></p><p> This episode is a tour through some of the toughest questions ‘effective altruists’ face when figuring out how to best improve the world, including:</p><p> * * Should we mostly try to help people currently alive, or future generations? Nick studied this question for years in his PhD thesis, <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE">On the Overwhelming Importance of Shaping the Far Future</a>. (The first 31 minutes is a snappier version of <a href="https://80k.link/toby-ord-from-nick">my conversation with Toby Ord</a>.)<br> * Is clean meat (aka *in vitro* meat) technologically feasible any time soon, or should we be looking for plant-based alternatives?<br> * What are the greatest risks to human civilisation?<br> * To stop malaria is it more cost-effective to use technology to <a href="https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support">eliminate mosquitos</a> than to distribute bed nets?<br> * Should people who want to improve the future work for changes that will be very useful in a specific scenario, or just generally try to improve how well humanity makes decisions?<br> * What specific jobs should our listeners take in order for Nick to be able to spend more money in useful ways to improve the world?<br> * Should we expect the future to be better if the economy grows more quickly - or more slowly?</p><p> <strong>Get free, one-on-one career advice</strong></p><p> We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href="https://80000hours.org/coaching/?utm_campaign=nick-beckstead-podcast">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a><br></p>]]>
      </content:encoded>
      <pubDate>Wed, 11 Oct 2017 16:41:03 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/6fc8033c/ea4f865b.mp3" length="161155677" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/jpQJXthh0gOwxxCZ4mfjHiijJAxsCi_RYpbjIqaTiBc/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Mjgv/MTY4MzU0NDU1MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6708</itunes:duration>
      <itunes:summary>What if you were in a position to give away billions of dollars to improve the world? What would you do with it? This is the problem facing Program Officers at the Open Philanthropy Project - people like Dr Nick Beckstead.

Following a PhD in philosophy, Nick works to figure out where money can do the most good. He’s been involved in major grants in a wide range of areas, including ending factory farming through technological innovation, safeguarding the world from advances in biotechnology and artificial intelligence, and spreading rational compassion.

Full transcript, coaching application form, overview of the conversation, and links to resources discussed in the episode:

This episode is a tour through some of the toughest questions ‘effective altruists’ face when figuring out how to best improve the world, including:

* * Should we mostly try to help people currently alive, or future generations? Nick studied this question for years in his PhD thesis, On the Overwhelming Importance of Shaping the Far Future. (The first 31 minutes is a snappier version of my conversation with Toby Ord.)
* Is clean meat (aka *in vitro* meat) technologically feasible any time soon, or should we be looking for plant-based alternatives?
* What are the greatest risks to human civilisation?
* To stop malaria is it more cost-effective to use technology to eliminate mosquitos than to distribute bed nets?
* Should people who want to improve the future work for changes that will be very useful in a specific scenario, or just generally try to improve how well humanity makes decisions?
* What specific jobs should our listeners take in order for Nick to be able to spend more money in useful ways to improve the world?
* Should we expect the future to be better if the economy grows more quickly - or more slowly?

Get free, one-on-one career advice

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</itunes:summary>
      <itunes:subtitle>What if you were in a position to give away billions of dollars to improve the world? What would you do with it? This is the problem facing Program Officers at the Open Philanthropy Project - people like Dr Nick Beckstead.

Following a PhD in philosophy</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#9 - Christine Peterson on how insecure computers could lead to global disaster, and how to fix it</title>
      <itunes:title>#9 - Christine Peterson on how insecure computers could lead to global disaster, and how to fix it</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/345341004</guid>
      <link>https://share.transistor.fm/s/015a87f3</link>
      <description>
        <![CDATA[Take a trip to Silicon Valley in the 70s and 80s, when going to space sounded like a good way to get around environmental limits, people started cryogenically freezing themselves, and nanotechnology looked like it might revolutionise industry – or turn us all into grey goo.<p>

<a href="https://80000hours.org/2017/10/christine-peterson-computer-security/?utm_campaign=podcast__christine-peterson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:</a></p><p>

In this episode of the 80,000 Hours Podcast Christine Peterson takes us back to her youth in the Bay Area, the ideas she encountered there, and what the dreamers she met did as they grew up. We also discuss how she came up with the term ‘open source software’ (and how she had to get someone else to propose it).</p><p>

Today Christine helps runs the Foresight Institute, which fills a gap left by for-profit technology companies – predicting how new revolutionary technologies could go wrong, and ensuring we steer clear of the downsides.</p><p>

We dive into:</p><p>

* Whether the poor security of computer systems poses a catastrophic risk for the world. Could all our essential services be taken down at once? And if so, what can be done about it?<br>
* Can technology ‘move fast and break things’ without eventually breaking the world? Would it be better for technology to advance more quickly, or more slowly?<br>
* How Christine came up with the term ‘open source software’ (and why someone else had to propose it).<br>
* Will AIs designed for wide-scale automated hacking make computers more or less secure?<br>
* Would it be good to radically extend human lifespan? Is it sensible to cryogenically freeze yourself in the hope of being resurrected in the future?<br>
* Could atomically precise manufacturing (nanotechnology) really work? Why was it initially so controversial and why did people stop worrying about it?<br>
* Should people who try to do good in their careers work long hours and take low salaries? Or should they take care of themselves first of all?<br>
* How she thinks the the effective altruism community resembles the scene she was involved with when she was wrong, and where it might be going wrong.</p><p>

<b>Get free, one-on-one career advice</b></p><p>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href="https://80000hours.org/coaching/?utm_campaign=christine-peterson-podcast" title="" class="btn btn-primary">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a></p><p></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Take a trip to Silicon Valley in the 70s and 80s, when going to space sounded like a good way to get around environmental limits, people started cryogenically freezing themselves, and nanotechnology looked like it might revolutionise industry – or turn us all into grey goo.<p>

<a href="https://80000hours.org/2017/10/christine-peterson-computer-security/?utm_campaign=podcast__christine-peterson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:</a></p><p>

In this episode of the 80,000 Hours Podcast Christine Peterson takes us back to her youth in the Bay Area, the ideas she encountered there, and what the dreamers she met did as they grew up. We also discuss how she came up with the term ‘open source software’ (and how she had to get someone else to propose it).</p><p>

Today Christine helps runs the Foresight Institute, which fills a gap left by for-profit technology companies – predicting how new revolutionary technologies could go wrong, and ensuring we steer clear of the downsides.</p><p>

We dive into:</p><p>

* Whether the poor security of computer systems poses a catastrophic risk for the world. Could all our essential services be taken down at once? And if so, what can be done about it?<br>
* Can technology ‘move fast and break things’ without eventually breaking the world? Would it be better for technology to advance more quickly, or more slowly?<br>
* How Christine came up with the term ‘open source software’ (and why someone else had to propose it).<br>
* Will AIs designed for wide-scale automated hacking make computers more or less secure?<br>
* Would it be good to radically extend human lifespan? Is it sensible to cryogenically freeze yourself in the hope of being resurrected in the future?<br>
* Could atomically precise manufacturing (nanotechnology) really work? Why was it initially so controversial and why did people stop worrying about it?<br>
* Should people who try to do good in their careers work long hours and take low salaries? Or should they take care of themselves first of all?<br>
* How she thinks the the effective altruism community resembles the scene she was involved with when she was wrong, and where it might be going wrong.</p><p>

<b>Get free, one-on-one career advice</b></p><p>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href="https://80000hours.org/coaching/?utm_campaign=christine-peterson-podcast" title="" class="btn btn-primary">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a></p><p></p>]]>
      </content:encoded>
      <pubDate>Wed, 04 Oct 2017 16:18:22 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/015a87f3/720e173d.mp3" length="101065844" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/Vk1EKYFw3xyq6nMRNg1XhjOjxKDLXHjjpeWpTBRScPA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3Mjcv/MTY4MzU0NDU1MC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6310</itunes:duration>
      <itunes:summary>Take a trip to Silicon Valley in the 70s and 80s, when going to space sounded like a good way to get around environmental limits, people started cryogenically freezing themselves, and nanotechnology looked like it might revolutionise industry – or turn us all into grey goo.

Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:

In this episode of the 80,000 Hours Podcast Christine Peterson takes us back to her youth in the Bay Area, the ideas she encountered there, and what the dreamers she met did as they grew up. We also discuss how she came up with the term ‘open source software’ (and how she had to get someone else to propose it).

Today Christine helps runs the Foresight Institute, which fills a gap left by for-profit technology companies – predicting how new revolutionary technologies could go wrong, and ensuring we steer clear of the downsides.

We dive into:

* Whether the poor security of computer systems poses a catastrophic risk for the world. Could all our essential services be taken down at once? And if so, what can be done about it?
* Can technology ‘move fast and break things’ without eventually breaking the world? Would it be better for technology to advance more quickly, or more slowly?
* How Christine came up with the term ‘open source software’ (and why someone else had to propose it).
* Will AIs designed for wide-scale automated hacking make computers more or less secure?
* Would it be good to radically extend human lifespan? Is it sensible to cryogenically freeze yourself in the hope of being resurrected in the future?
* Could atomically precise manufacturing (nanotechnology) really work? Why was it initially so controversial and why did people stop worrying about it?
* Should people who try to do good in their careers work long hours and take low salaries? Or should they take care of themselves first of all?
* How she thinks the the effective altruism community resembles the scene she was involved with when she was wrong, and where it might be going wrong.

Get free, one-on-one career advice

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</itunes:summary>
      <itunes:subtitle>Take a trip to Silicon Valley in the 70s and 80s, when going to space sounded like a good way to get around environmental limits, people started cryogenically freezing themselves, and nanotechnology looked like it might revolutionise industry – or turn us</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#8 - Lewis Bollard on how to end factory farming in our lifetimes</title>
      <itunes:title>#8 - Lewis Bollard on how to end factory farming in our lifetimes</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/344291989</guid>
      <link>https://share.transistor.fm/s/dddf88ba</link>
      <description>
        <![CDATA[Every year tens of billions of animals are raised in terrible conditions in factory farms before being killed for human consumption. Over the last two years Lewis Bollard – Project Officer for Farm Animal Welfare at the Open Philanthropy Project – has conducted extensive research into the best ways to eliminate animal suffering in farms as soon as possible. This has resulted in $30 million in grants to farm animal advocacy.<p>

<a href="https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:</a></p><p>

We covered almost every approach being taken, which ones work, and how individuals can best contribute through their careers.</p><p>

We also had time to venture into a wide range of issues that are less often discussed, including:</p><p>

* Why Lewis thinks insect farming would be worse than the status quo, and whether we should look for ‘humane’ insecticides;<br>
* How young people can set themselves up to contribute to scientific research into meat alternatives;<br>
* How genetic manipulation of chickens has caused them to suffer much more than their ancestors, but could also be used to make them better off;<br>
* Why Lewis is skeptical of vegan advocacy;<br>
* Why he doubts that much can be done to tackle factory farming through legal advocacy or electoral politics;<br>
* Which species of farm animals is best to focus on first;<br>
* Whether fish and crustaceans are conscious, and if so what can be done for them;<br>
* Many other issues listed below in the <i>Overview of the discussion</i>.</p><p>

<b>Get free, one-on-one career advice</b></p><p>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href="https://80000hours.org/coaching/?utm_campaign=lewis-bollard-podcast" title="" class="btn btn-primary">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a></p><p>

<b>Overview of the discussion</b></p><p>

**2m40s** What originally drew you to dedicate your career to helping animals and why did Open Philanthropy end up focusing on it?<br>
**5m40s** Do you have any concrete way of assessing the severity of animal suffering? <br>
**7m10s** Do you think the environmental gains are large compared to those that we might hope to get from animal welfare improvement?<br>
**7m55s** What grants have you made at Open Phil? How did you go about deciding which groups to fund and which ones not to fund?<br>
**9m50s** Why does Open Phil focus on chickens and fish? Is this the right call?<br>
<a href="https://80000hours.org/2017/09/lewis-bollard-end-factory-farming?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">More...</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Every year tens of billions of animals are raised in terrible conditions in factory farms before being killed for human consumption. Over the last two years Lewis Bollard – Project Officer for Farm Animal Welfare at the Open Philanthropy Project – has conducted extensive research into the best ways to eliminate animal suffering in farms as soon as possible. This has resulted in $30 million in grants to farm animal advocacy.<p>

<a href="https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:</a></p><p>

We covered almost every approach being taken, which ones work, and how individuals can best contribute through their careers.</p><p>

We also had time to venture into a wide range of issues that are less often discussed, including:</p><p>

* Why Lewis thinks insect farming would be worse than the status quo, and whether we should look for ‘humane’ insecticides;<br>
* How young people can set themselves up to contribute to scientific research into meat alternatives;<br>
* How genetic manipulation of chickens has caused them to suffer much more than their ancestors, but could also be used to make them better off;<br>
* Why Lewis is skeptical of vegan advocacy;<br>
* Why he doubts that much can be done to tackle factory farming through legal advocacy or electoral politics;<br>
* Which species of farm animals is best to focus on first;<br>
* Whether fish and crustaceans are conscious, and if so what can be done for them;<br>
* Many other issues listed below in the <i>Overview of the discussion</i>.</p><p>

<b>Get free, one-on-one career advice</b></p><p>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href="https://80000hours.org/coaching/?utm_campaign=lewis-bollard-podcast" title="" class="btn btn-primary">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a></p><p>

<b>Overview of the discussion</b></p><p>

**2m40s** What originally drew you to dedicate your career to helping animals and why did Open Philanthropy end up focusing on it?<br>
**5m40s** Do you have any concrete way of assessing the severity of animal suffering? <br>
**7m10s** Do you think the environmental gains are large compared to those that we might hope to get from animal welfare improvement?<br>
**7m55s** What grants have you made at Open Phil? How did you go about deciding which groups to fund and which ones not to fund?<br>
**9m50s** Why does Open Phil focus on chickens and fish? Is this the right call?<br>
<a href="https://80000hours.org/2017/09/lewis-bollard-end-factory-farming?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">More...</a></p>]]>
      </content:encoded>
      <pubDate>Wed, 27 Sep 2017 17:19:34 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/dddf88ba/f98cbdb8.mp3" length="189224605" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/4td4q2Cuy9RzysqzzC3n-_o07-I2CXFy1BjelJtSftM/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjYv/MTY4MzU0NDU0OS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>11815</itunes:duration>
      <itunes:summary>Every year tens of billions of animals are raised in terrible conditions in factory farms before being killed for human consumption. Over the last two years Lewis Bollard – Project Officer for Farm Animal Welfare at the Open Philanthropy Project – has conducted extensive research into the best ways to eliminate animal suffering in farms as soon as possible. This has resulted in $30 million in grants to farm animal advocacy.

Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:

We covered almost every approach being taken, which ones work, and how individuals can best contribute through their careers.

We also had time to venture into a wide range of issues that are less often discussed, including:

* Why Lewis thinks insect farming would be worse than the status quo, and whether we should look for ‘humane’ insecticides;
* How young people can set themselves up to contribute to scientific research into meat alternatives;
* How genetic manipulation of chickens has caused them to suffer much more than their ancestors, but could also be used to make them better off;
* Why Lewis is skeptical of vegan advocacy;
* Why he doubts that much can be done to tackle factory farming through legal advocacy or electoral politics;
* Which species of farm animals is best to focus on first;
* Whether fish and crustaceans are conscious, and if so what can be done for them;
* Many other issues listed below in the Overview of the discussion.

Get free, one-on-one career advice

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.

Overview of the discussion

**2m40s** What originally drew you to dedicate your career to helping animals and why did Open Philanthropy end up focusing on it?
**5m40s** Do you have any concrete way of assessing the severity of animal suffering? 
**7m10s** Do you think the environmental gains are large compared to those that we might hope to get from animal welfare improvement?
**7m55s** What grants have you made at Open Phil? How did you go about deciding which groups to fund and which ones not to fund?
**9m50s** Why does Open Phil focus on chickens and fish? Is this the right call?
More...</itunes:summary>
      <itunes:subtitle>Every year tens of billions of animals are raised in terrible conditions in factory farms before being killed for human consumption. Over the last two years Lewis Bollard – Project Officer for Farm Animal Welfare at the Open Philanthropy Project – has con</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#7 - Julia Galef on making humanity more rational, what EA does wrong, and why Twitter isn’t all bad</title>
      <itunes:title>#7 - Julia Galef on making humanity more rational, what EA does wrong, and why Twitter isn’t all bad</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/342196093</guid>
      <link>https://share.transistor.fm/s/8cf7a11d</link>
      <description>
        <![CDATA[The scientific revolution in the 16th century was one of the biggest societal shifts in human history, driven by the discovery of new and better methods of figuring out who was right and who was wrong. <p>

Julia Galef - a well-known writer and researcher focused on improving human judgment, especially about high stakes questions - believes that if we could again develop new techniques to predict the future, resolve disagreements and make sound decisions together, it could dramatically improve the world across the board. We brought her in to talk about her ideas.</p><p>

<b>This interview complements a <a href="https://80000hours.org/problem-profiles/improving-institutional-decision-making/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">new detailed review</a> of whether and how to follow Julia’s career path.</b>

<a href="https://80000hours.org/2017/09/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a></p><p>

Julia has been host of the Rationally Speaking podcast since 2010, co-founder of the Center for Applied Rationality in 2012, and is currently working for the Open Philanthropy Project on an investigation of expert disagreements.</p><p>

In our conversation we ended up speaking about a wide range of topics, including:</p><p>

* Her research on how people can have productive intellectual disagreements.<br>
* Why she once planned to become an urban designer.<br>
* Why she doubts people are more rational than 200 years ago.<br>
* What makes her a fan of Twitter (while I think it’s dystopian).<br>
* Whether people should write more books.<br>
* Whether it’s a good idea to run a podcast, and how she grew her audience.<br>
* Why saying you don’t believe X often won’t convince people you don’t.<br>
* Why she started a PhD in economics but then stopped.<br>
* Whether she would recommend an unconventional career like her own.<br>
* Whether the incentives in the intelligence community actually support sound thinking.<br>
* Whether big institutions will actually pick up new tools for improving decision-making if they are developed.<br>
* How to start out pursuing a career in which you enhance human judgement and foresight.</p><p>

<b>Get free, one-on-one career advice to help you improve judgement and decision-making</b></p><p>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. **<b>If you want to work on any of the problems discussed in this episode, find out if our coaching can help you:</b>**</p><p>

<a href="https://80000hours.org/coaching/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" title="" class="btn btn-primary">APPLY FOR COACHING</a></p><p>

<b>Overview of the conversation</b></p><p>

**1m30s** So what projects are you working on at the moment?<br>
**3m50s** How are you working on the problem of expert disagreement?<br>
**6m0s** Is this the same method as the double crux process that was developed at the Center for Applied Rationality?<br>
**10m** Why did the Open Philanthropy Project decide this was a very valuable project to fund?<br>
**13m** Is the double crux process actually that effective?<br>
**14m50s** Is Facebook dangerous?<br>
**17m** What makes for a good life? Can you be mistaken about having a good life?<br>
**19m** Should more people write books?<br>
<a href="https://80000hours.org/2017/09/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Read more...</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[The scientific revolution in the 16th century was one of the biggest societal shifts in human history, driven by the discovery of new and better methods of figuring out who was right and who was wrong. <p>

Julia Galef - a well-known writer and researcher focused on improving human judgment, especially about high stakes questions - believes that if we could again develop new techniques to predict the future, resolve disagreements and make sound decisions together, it could dramatically improve the world across the board. We brought her in to talk about her ideas.</p><p>

<b>This interview complements a <a href="https://80000hours.org/problem-profiles/improving-institutional-decision-making/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">new detailed review</a> of whether and how to follow Julia’s career path.</b>

<a href="https://80000hours.org/2017/09/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a></p><p>

Julia has been host of the Rationally Speaking podcast since 2010, co-founder of the Center for Applied Rationality in 2012, and is currently working for the Open Philanthropy Project on an investigation of expert disagreements.</p><p>

In our conversation we ended up speaking about a wide range of topics, including:</p><p>

* Her research on how people can have productive intellectual disagreements.<br>
* Why she once planned to become an urban designer.<br>
* Why she doubts people are more rational than 200 years ago.<br>
* What makes her a fan of Twitter (while I think it’s dystopian).<br>
* Whether people should write more books.<br>
* Whether it’s a good idea to run a podcast, and how she grew her audience.<br>
* Why saying you don’t believe X often won’t convince people you don’t.<br>
* Why she started a PhD in economics but then stopped.<br>
* Whether she would recommend an unconventional career like her own.<br>
* Whether the incentives in the intelligence community actually support sound thinking.<br>
* Whether big institutions will actually pick up new tools for improving decision-making if they are developed.<br>
* How to start out pursuing a career in which you enhance human judgement and foresight.</p><p>

<b>Get free, one-on-one career advice to help you improve judgement and decision-making</b></p><p>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. **<b>If you want to work on any of the problems discussed in this episode, find out if our coaching can help you:</b>**</p><p>

<a href="https://80000hours.org/coaching/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" title="" class="btn btn-primary">APPLY FOR COACHING</a></p><p>

<b>Overview of the conversation</b></p><p>

**1m30s** So what projects are you working on at the moment?<br>
**3m50s** How are you working on the problem of expert disagreement?<br>
**6m0s** Is this the same method as the double crux process that was developed at the Center for Applied Rationality?<br>
**10m** Why did the Open Philanthropy Project decide this was a very valuable project to fund?<br>
**13m** Is the double crux process actually that effective?<br>
**14m50s** Is Facebook dangerous?<br>
**17m** What makes for a good life? Can you be mistaken about having a good life?<br>
**19m** Should more people write books?<br>
<a href="https://80000hours.org/2017/09/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/?utm_campaign=podcast__julia-galef&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Read more...</a></p>]]>
      </content:encoded>
      <pubDate>Wed, 13 Sep 2017 16:25:41 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/8cf7a11d/20ba65a9.mp3" length="71385749" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/6OQiJJUdDEgiIkVJkJIEl6S3cQymOv7RC80zCR9-YNA/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjUv/MTY4MzU0NDU0OC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>4457</itunes:duration>
      <itunes:summary>The scientific revolution in the 16th century was one of the biggest societal shifts in human history, driven by the discovery of new and better methods of figuring out who was right and who was wrong. 

Julia Galef - a well-known writer and researcher focused on improving human judgment, especially about high stakes questions - believes that if we could again develop new techniques to predict the future, resolve disagreements and make sound decisions together, it could dramatically improve the world across the board. We brought her in to talk about her ideas.

This interview complements a new detailed review of whether and how to follow Julia’s career path.

Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.

Julia has been host of the Rationally Speaking podcast since 2010, co-founder of the Center for Applied Rationality in 2012, and is currently working for the Open Philanthropy Project on an investigation of expert disagreements.

In our conversation we ended up speaking about a wide range of topics, including:

* Her research on how people can have productive intellectual disagreements.
* Why she once planned to become an urban designer.
* Why she doubts people are more rational than 200 years ago.
* What makes her a fan of Twitter (while I think it’s dystopian).
* Whether people should write more books.
* Whether it’s a good idea to run a podcast, and how she grew her audience.
* Why saying you don’t believe X often won’t convince people you don’t.
* Why she started a PhD in economics but then stopped.
* Whether she would recommend an unconventional career like her own.
* Whether the incentives in the intelligence community actually support sound thinking.
* Whether big institutions will actually pick up new tools for improving decision-making if they are developed.
* How to start out pursuing a career in which you enhance human judgement and foresight.

Get free, one-on-one career advice to help you improve judgement and decision-making

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. **If you want to work on any of the problems discussed in this episode, find out if our coaching can help you:**

APPLY FOR COACHING

Overview of the conversation

**1m30s** So what projects are you working on at the moment?
**3m50s** How are you working on the problem of expert disagreement?
**6m0s** Is this the same method as the double crux process that was developed at the Center for Applied Rationality?
**10m** Why did the Open Philanthropy Project decide this was a very valuable project to fund?
**13m** Is the double crux process actually that effective?
**14m50s** Is Facebook dangerous?
**17m** What makes for a good life? Can you be mistaken about having a good life?
**19m** Should more people write books?
Read more...</itunes:summary>
      <itunes:subtitle>The scientific revolution in the 16th century was one of the biggest societal shifts in human history, driven by the discovery of new and better methods of figuring out who was right and who was wrong. 

Julia Galef - a well-known writer and researcher fo</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#6 - Toby Ord on why the long-term future matters more than anything else &amp; what to do about it</title>
      <itunes:title>#6 - Toby Ord on why the long-term future matters more than anything else &amp; what to do about it</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/341193732</guid>
      <link>https://share.transistor.fm/s/5e1d3f96</link>
      <description>
        <![CDATA[<p>Of all the people whose well-being we should care about, only a small fraction are alive today. The rest are members of future generations who are yet to exist. Whether they’ll be born into a world that is flourishing or disintegrating – and indeed, whether they will ever be born at all – is in large part up to us. As such, the welfare of future generations should be our number one moral concern.</p><p> This conclusion holds true regardless of whether your moral framework is based on common sense, consequences, rules of ethical conduct, cooperating with others, virtuousness, keeping options open – or just a sense of wonder about the universe we find ourselves in.</p><p> That’s the view of Dr Toby Ord, a philosophy Fellow at the University of Oxford and co-founder of the effective altruism community. In this episode of the 80,000 Hours Podcast Dr Ord makes the case that aiming for a positive long-term future is likely the best way to improve the world.</p><p> <a href="https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a></p><p> We then discuss common objections to long-termism, such as the idea that benefits to future generations are less valuable than those to people alive now, or that we can’t meaningfully benefit future generations beyond taking the usual steps to improve the present.</p><p> Later the conversation turns to how individuals can and have changed the course of history, what could go wrong and why, and whether plans to colonise Mars would actually put humanity in a safer position than it is today.</p><p> This episode goes deep into the most distinctive features of our advice. It’s likely the most in-depth discussion of how 80,000 Hours and the effective altruism community think about the long term future and why - and why we so often give it top priority. </p><p> It’s best to subscribe, so you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.</p><p> <strong>Want to help ensure humanity has a positive future instead of destroying itself? We want to help.</strong></p><p> We’ve helped 100s of people compare between their options, get introductions, and jobs important for the the long-run future. <strong>If you want to work on any of the problems discussed in this episode, such as artificial intelligence or biosecurity, </strong><a href="https://80000hours.org/coaching/?utm_campaign=podcast__toby-ord&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">find out if our coaching can help you.</a></p><p> <strong>Overview of the discussion</strong></p><p> <strong>3m30s</strong> - Why is the long-term future of humanity such a big deal, and perhaps the most important issue for us to be thinking about?<br> <strong>9m05s</strong> - Five arguments that future generations matter<br> <strong>21m50s</strong> - How bad would it be if humanity went extinct or civilization collapses? <br> <strong>26m40s</strong> - Why do people start saying such strange things when this topic comes up?<br> <strong>30m30s</strong> - Are there any other reasons to prioritize thinking about the long-term future of humanity that you wanted to raise before we move to objections?<br> <strong>36m10s</strong> - What is this school of thought called?<br> <a href="%20https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Read more...</a><br></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Of all the people whose well-being we should care about, only a small fraction are alive today. The rest are members of future generations who are yet to exist. Whether they’ll be born into a world that is flourishing or disintegrating – and indeed, whether they will ever be born at all – is in large part up to us. As such, the welfare of future generations should be our number one moral concern.</p><p> This conclusion holds true regardless of whether your moral framework is based on common sense, consequences, rules of ethical conduct, cooperating with others, virtuousness, keeping options open – or just a sense of wonder about the universe we find ourselves in.</p><p> That’s the view of Dr Toby Ord, a philosophy Fellow at the University of Oxford and co-founder of the effective altruism community. In this episode of the 80,000 Hours Podcast Dr Ord makes the case that aiming for a positive long-term future is likely the best way to improve the world.</p><p> <a href="https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a></p><p> We then discuss common objections to long-termism, such as the idea that benefits to future generations are less valuable than those to people alive now, or that we can’t meaningfully benefit future generations beyond taking the usual steps to improve the present.</p><p> Later the conversation turns to how individuals can and have changed the course of history, what could go wrong and why, and whether plans to colonise Mars would actually put humanity in a safer position than it is today.</p><p> This episode goes deep into the most distinctive features of our advice. It’s likely the most in-depth discussion of how 80,000 Hours and the effective altruism community think about the long term future and why - and why we so often give it top priority. </p><p> It’s best to subscribe, so you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.</p><p> <strong>Want to help ensure humanity has a positive future instead of destroying itself? We want to help.</strong></p><p> We’ve helped 100s of people compare between their options, get introductions, and jobs important for the the long-run future. <strong>If you want to work on any of the problems discussed in this episode, such as artificial intelligence or biosecurity, </strong><a href="https://80000hours.org/coaching/?utm_campaign=podcast__toby-ord&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">find out if our coaching can help you.</a></p><p> <strong>Overview of the discussion</strong></p><p> <strong>3m30s</strong> - Why is the long-term future of humanity such a big deal, and perhaps the most important issue for us to be thinking about?<br> <strong>9m05s</strong> - Five arguments that future generations matter<br> <strong>21m50s</strong> - How bad would it be if humanity went extinct or civilization collapses? <br> <strong>26m40s</strong> - Why do people start saying such strange things when this topic comes up?<br> <strong>30m30s</strong> - Are there any other reasons to prioritize thinking about the long-term future of humanity that you wanted to raise before we move to objections?<br> <strong>36m10s</strong> - What is this school of thought called?<br> <a href="%20https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Read more...</a><br></p>]]>
      </content:encoded>
      <pubDate>Wed, 06 Sep 2017 17:59:24 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/5e1d3f96/9c2dd08c.mp3" length="61911773" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/zi7J-XzDM7ZsN0xraMBRjua3FqRr2PfHrUe6t7pcuAI/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjQv/MTY4MzU0NDU0Ny1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>7730</itunes:duration>
      <itunes:summary>Of all the people whose well-being we should care about, only a small fraction are alive today. The rest are members of future generations who are yet to exist. Whether they’ll be born into a world that is flourishing or disintegrating – and indeed, whether they will ever be born at all – is in large part up to us. As such, the welfare of future generations should be our number one moral concern.

This conclusion holds true regardless of whether your moral framework is based on common sense, consequences, rules of ethical conduct, cooperating with others, virtuousness, keeping options open – or just a sense of wonder about the universe we find ourselves in.

That’s the view of Dr Toby Ord, a philosophy Fellow at the University of Oxford and co-founder of the effective altruism community. In this episode of the 80,000 Hours Podcast Dr Ord makes the case that aiming for a positive long-term future is likely the best way to improve the world.

Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.

We then discuss common objections to long-termism, such as the idea that benefits to future generations are less valuable than those to people alive now, or that we can’t meaningfully benefit future generations beyond taking the usual steps to improve the present.

Later the conversation turns to how individuals can and have changed the course of history, what could go wrong and why, and whether plans to colonise Mars would actually put humanity in a safer position than it is today.

This episode goes deep into the most distinctive features of our advice. It’s likely the most in-depth discussion of how 80,000 Hours and the effective altruism community think about the long term future and why - and why we so often give it top priority. 

It’s best to subscribe, so you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.

Want to help ensure humanity has a positive future instead of destroying itself? We want to help.

We’ve helped 100s of people compare between their options, get introductions, and jobs important for the the long-run future. If you want to work on any of the problems discussed in this episode, such as artificial intelligence or biosecurity, find out if our coaching can help you.

Overview of the discussion

3m30s - Why is the long-term future of humanity such a big deal, and perhaps the most important issue for us to be thinking about?
9m05s - Five arguments that future generations matter
21m50s - How bad would it be if humanity went extinct or civilization collapses? 
26m40s - Why do people start saying such strange things when this topic comes up?
30m30s - Are there any other reasons to prioritize thinking about the long-term future of humanity that you wanted to raise before we move to objections?
36m10s - What is this school of thought called?
Read more...</itunes:summary>
      <itunes:subtitle>Of all the people whose well-being we should care about, only a small fraction are alive today. The rest are members of future generations who are yet to exist. Whether they’ll be born into a world that is flourishing or disintegrating – and indeed, wheth</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#5 - Alex Gordon-Brown on how to donate millions in your 20s working in quantitative trading</title>
      <itunes:title>#5 - Alex Gordon-Brown on how to donate millions in your 20s working in quantitative trading</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/339872675</guid>
      <link>https://share.transistor.fm/s/cd965070</link>
      <description>
        <![CDATA[<p>Quantitative financial trading is one of the highest paying parts of the world’s highest paying industry. 25 to 30 year olds with outstanding maths skills can earn millions a year in an obscure set of ‘quant trading’ firms, where they program computers with predefined algorithms to allow them to trade very quickly and effectively.</p><p>

<b>Update: we're headhunting people for quant trading roles</b></p><p>
 
Want to be kept up to date about particularly promising roles we're aware of for earning to give in quantitative finance?</p><p> <a href="https://80000hours.typeform.com/to/JiDHyM?utm_campaign=blog-post__agb-interview" title="" class="btn btn-primary"><b>Get notified by letting us know here.</b></a></p><p>

This makes it an attractive place to work for people who want to ‘earn to give’, and we know several people who are able to donate over a million dollars a year to effective charities by working in quant trading. Who are these people? What is the job like? And is there a risk that their work harms the world in other ways? </p><p>

<a href="https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a></p><p>

I spoke at length with Alexander Gordon-Brown, who has worked as a quant trader in London for the last three and a half years and donated hundreds of thousands of pounds. We covered:</p><p>

* What quant traders do and how much they earn.<br>
* Whether their work is beneficial or harmful for the world.<br>
* How to figure out if you’re a good personal fit for quant trading, and if so how to break into the industry.<br>
* Whether he enjoys the work and finds it motivating, and what other careers he considered.<br>
* What variety of positions are on offer, and what the culture is like in different firms.<br>
* How he decides where to donate, and whether he has persuaded his colleagues to join him.</p><p>

<b>Want to earn to give for effective charities in quantitative trading? We want to help.</b></p><p>
 
We’ve helped dozens of people plan their earning to give careers, and put them in touch with mentors. If you want to work in quant trading, apply for our free coaching service.</p><p>

<b><a href="https://80000hours.org/coaching/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" title="" class="btn btn-primary">APPLY FOR COACHING</a></b></p><p>

<b>What questions are asked when?</b></p><p>

1m30s - What is quant trading and how much do they earn?<br>
4m45s - How do quant trading firms manage the risks they face and avoid bankruptcy?<br>
7m05s - Do other traders also donate to charity and has Alex convinced them?<br>
9m45s - How do they track the performance of each trader?<br>
13m00s - What does the daily schedule of a quant trader look like? What do you do in the morning, afternoon, etc?<br>
<a href="https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">More...</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Quantitative financial trading is one of the highest paying parts of the world’s highest paying industry. 25 to 30 year olds with outstanding maths skills can earn millions a year in an obscure set of ‘quant trading’ firms, where they program computers with predefined algorithms to allow them to trade very quickly and effectively.</p><p>

<b>Update: we're headhunting people for quant trading roles</b></p><p>
 
Want to be kept up to date about particularly promising roles we're aware of for earning to give in quantitative finance?</p><p> <a href="https://80000hours.typeform.com/to/JiDHyM?utm_campaign=blog-post__agb-interview" title="" class="btn btn-primary"><b>Get notified by letting us know here.</b></a></p><p>

This makes it an attractive place to work for people who want to ‘earn to give’, and we know several people who are able to donate over a million dollars a year to effective charities by working in quant trading. Who are these people? What is the job like? And is there a risk that their work harms the world in other ways? </p><p>

<a href="https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a></p><p>

I spoke at length with Alexander Gordon-Brown, who has worked as a quant trader in London for the last three and a half years and donated hundreds of thousands of pounds. We covered:</p><p>

* What quant traders do and how much they earn.<br>
* Whether their work is beneficial or harmful for the world.<br>
* How to figure out if you’re a good personal fit for quant trading, and if so how to break into the industry.<br>
* Whether he enjoys the work and finds it motivating, and what other careers he considered.<br>
* What variety of positions are on offer, and what the culture is like in different firms.<br>
* How he decides where to donate, and whether he has persuaded his colleagues to join him.</p><p>

<b>Want to earn to give for effective charities in quantitative trading? We want to help.</b></p><p>
 
We’ve helped dozens of people plan their earning to give careers, and put them in touch with mentors. If you want to work in quant trading, apply for our free coaching service.</p><p>

<b><a href="https://80000hours.org/coaching/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast" title="" class="btn btn-primary">APPLY FOR COACHING</a></b></p><p>

<b>What questions are asked when?</b></p><p>

1m30s - What is quant trading and how much do they earn?<br>
4m45s - How do quant trading firms manage the risks they face and avoid bankruptcy?<br>
7m05s - Do other traders also donate to charity and has Alex convinced them?<br>
9m45s - How do they track the performance of each trader?<br>
13m00s - What does the daily schedule of a quant trader look like? What do you do in the morning, afternoon, etc?<br>
<a href="https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">More...</a></p>]]>
      </content:encoded>
      <pubDate>Mon, 28 Aug 2017 18:59:26 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/cd965070/5b956d23.mp3" length="101221154" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/42de0qNR1ejW_l2O5lXL6Fi1cj4neZdHv3ykdKvWZ8w/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjMv/MTY4MzU0NDU0NS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>6320</itunes:duration>
      <itunes:summary>Quantitative financial trading is one of the highest paying parts of the world’s highest paying industry. 25 to 30 year olds with outstanding maths skills can earn millions a year in an obscure set of ‘quant trading’ firms, where they program computers with predefined algorithms to allow them to trade very quickly and effectively.

Update: we're headhunting people for quant trading roles
 
Want to be kept up to date about particularly promising roles we're aware of for earning to give in quantitative finance? Get notified by letting us know here.

This makes it an attractive place to work for people who want to ‘earn to give’, and we know several people who are able to donate over a million dollars a year to effective charities by working in quant trading. Who are these people? What is the job like? And is there a risk that their work harms the world in other ways? 

Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.

I spoke at length with Alexander Gordon-Brown, who has worked as a quant trader in London for the last three and a half years and donated hundreds of thousands of pounds. We covered:

* What quant traders do and how much they earn.
* Whether their work is beneficial or harmful for the world.
* How to figure out if you’re a good personal fit for quant trading, and if so how to break into the industry.
* Whether he enjoys the work and finds it motivating, and what other careers he considered.
* What variety of positions are on offer, and what the culture is like in different firms.
* How he decides where to donate, and whether he has persuaded his colleagues to join him.

Want to earn to give for effective charities in quantitative trading? We want to help.
 
We’ve helped dozens of people plan their earning to give careers, and put them in touch with mentors. If you want to work in quant trading, apply for our free coaching service.

APPLY FOR COACHING

What questions are asked when?

1m30s - What is quant trading and how much do they earn?
4m45s - How do quant trading firms manage the risks they face and avoid bankruptcy?
7m05s - Do other traders also donate to charity and has Alex convinced them?
9m45s - How do they track the performance of each trader?
13m00s - What does the daily schedule of a quant trader look like? What do you do in the morning, afternoon, etc?
More...</itunes:summary>
      <itunes:subtitle>Quantitative financial trading is one of the highest paying parts of the world’s highest paying industry. 25 to 30 year olds with outstanding maths skills can earn millions a year in an obscure set of ‘quant trading’ firms, where they program computers wi</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#4 - Howie Lempel on pandemics that kill hundreds of millions and how to stop them</title>
      <itunes:title>#4 - Howie Lempel on pandemics that kill hundreds of millions and how to stop them</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/339164985</guid>
      <link>https://share.transistor.fm/s/eafb4dbd</link>
      <description>
        <![CDATA[What disaster is most likely to kill more than 10 million human beings in the next 20 years? Terrorism? Famine? An asteroid?<p>

Actually it’s probably a pandemic: a deadly new disease that spreads out of control. We’ve recently seen the risks with Ebola and swine flu, but they pale in comparison to the Spanish flu which killed 3% of the world’s population in 1918 to 1920. A pandemic of that scale today would kill 200 million.</p><p>

In this in-depth interview I speak to Howie Lempel, who spent years studying pandemic preparedness for the Open Philanthropy Project. We spend the first 20 minutes covering his work at the foundation, then discuss how bad the pandemic problem is, why it’s probably getting worse, and what can be done about it.</p><p>

<a href="https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/?utm_campaign=podcast__howie-lempel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, apply for personalised coaching to help you work on pandemic preparedness, see what questions are asked when, and read extra resources to learn more.</a></p><p> 

In the second half we go through where you personally could study and work to tackle one of the worst threats facing humanity.</p><p>

<em>Want to help ensure we have no severe pandemics in the 21st century? We want to help.</em></p><p>

We’ve helped dozens of people formulate their plans, and put them in touch with academic mentors. If you want to work on pandemic preparedness safety, apply for our free coaching service.</p><p>

<a href="https://80000hours.org/coaching/?utm_campaign=podcast__howie-lempel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">APPLY FOR COACHING</a></p><p>

<b>2m</b> - What does the Open Philanthropy Project do? What’s it like to work there?<br>
<b>16m27s</b> - What grants did OpenPhil make in pandemic preparedness? Did they work out?<br>
<b>22m56s</b> - Why is pandemic preparedness such an important thing to work on?<br>
<b>31m23s</b> - How many people could die in a global pandemic? Is Contagion a realistic movie?<br>
<b>37m05s</b> - Why the risk is getting worse due to scientific discoveries<br>
<b>40m10s</b> - How would dangerous pathogens get released?<br>
<b>45m27s</b> - Would society collapse if a billion people die in a pandemic?<br>
<b>49m25s</b> - The plague, Spanish flu, smallpox, and other historical pandemics<br>
<b>58m30s</b> - How are risks affected by sloppy research security or the existence of factory farming?<br>
<b>1h7m30s</b> - What's already being done? Why institutions for dealing with pandemics are really insufficient.<br>
<b>1h14m30s</b> - What the World Health Organisation should do but can’t.<br>
<b>1h21m51s</b> - What charities do about pandemics and why they aren’t able to fix things<br>
<b>1h25m50s</b> - How long would it take to make vaccines?<br>
<b>1h30m40s</b> - What does the US government do to protect Americans? It’s a mess.<br>
<b>1h37m20s</b> - What kind of people do you know work on this problem and what are they doing?<br>
<b>1h46m30s</b> - Are there things that we ought to be banning or technologies that we should be trying not to develop because we're just better off not having them?<br>
<b>1h49m35s</b> - What kind of reforms are needed at the international level?<br>
<b>1h54m40s</b> - Where should people who want to tackle this problem go to work?<br>
<b>1h59m50s</b> - Are there any technologies we need to urgently develop?<br>
<b>2h04m20s</b> - What about trying to stop humans from having contact with wild animals?<br>
<b>2h08m5s</b> - What should people study if they're young and choosing their major; what should they do a PhD in? Where should they study, and with who?<br>
<a href="https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/?utm_campaign=podcast__howie-lempel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">More...</a></p>]]>
      </description>
      <content:encoded>
        <![CDATA[What disaster is most likely to kill more than 10 million human beings in the next 20 years? Terrorism? Famine? An asteroid?<p>

Actually it’s probably a pandemic: a deadly new disease that spreads out of control. We’ve recently seen the risks with Ebola and swine flu, but they pale in comparison to the Spanish flu which killed 3% of the world’s population in 1918 to 1920. A pandemic of that scale today would kill 200 million.</p><p>

In this in-depth interview I speak to Howie Lempel, who spent years studying pandemic preparedness for the Open Philanthropy Project. We spend the first 20 minutes covering his work at the foundation, then discuss how bad the pandemic problem is, why it’s probably getting worse, and what can be done about it.</p><p>

<a href="https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/?utm_campaign=podcast__howie-lempel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, apply for personalised coaching to help you work on pandemic preparedness, see what questions are asked when, and read extra resources to learn more.</a></p><p> 

In the second half we go through where you personally could study and work to tackle one of the worst threats facing humanity.</p><p>

<em>Want to help ensure we have no severe pandemics in the 21st century? We want to help.</em></p><p>

We’ve helped dozens of people formulate their plans, and put them in touch with academic mentors. If you want to work on pandemic preparedness safety, apply for our free coaching service.</p><p>

<a href="https://80000hours.org/coaching/?utm_campaign=podcast__howie-lempel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">APPLY FOR COACHING</a></p><p>

<b>2m</b> - What does the Open Philanthropy Project do? What’s it like to work there?<br>
<b>16m27s</b> - What grants did OpenPhil make in pandemic preparedness? Did they work out?<br>
<b>22m56s</b> - Why is pandemic preparedness such an important thing to work on?<br>
<b>31m23s</b> - How many people could die in a global pandemic? Is Contagion a realistic movie?<br>
<b>37m05s</b> - Why the risk is getting worse due to scientific discoveries<br>
<b>40m10s</b> - How would dangerous pathogens get released?<br>
<b>45m27s</b> - Would society collapse if a billion people die in a pandemic?<br>
<b>49m25s</b> - The plague, Spanish flu, smallpox, and other historical pandemics<br>
<b>58m30s</b> - How are risks affected by sloppy research security or the existence of factory farming?<br>
<b>1h7m30s</b> - What's already being done? Why institutions for dealing with pandemics are really insufficient.<br>
<b>1h14m30s</b> - What the World Health Organisation should do but can’t.<br>
<b>1h21m51s</b> - What charities do about pandemics and why they aren’t able to fix things<br>
<b>1h25m50s</b> - How long would it take to make vaccines?<br>
<b>1h30m40s</b> - What does the US government do to protect Americans? It’s a mess.<br>
<b>1h37m20s</b> - What kind of people do you know work on this problem and what are they doing?<br>
<b>1h46m30s</b> - Are there things that we ought to be banning or technologies that we should be trying not to develop because we're just better off not having them?<br>
<b>1h49m35s</b> - What kind of reforms are needed at the international level?<br>
<b>1h54m40s</b> - Where should people who want to tackle this problem go to work?<br>
<b>1h59m50s</b> - Are there any technologies we need to urgently develop?<br>
<b>2h04m20s</b> - What about trying to stop humans from having contact with wild animals?<br>
<b>2h08m5s</b> - What should people study if they're young and choosing their major; what should they do a PhD in? Where should they study, and with who?<br>
<a href="https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/?utm_campaign=podcast__howie-lempel&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">More...</a></p>]]>
      </content:encoded>
      <pubDate>Wed, 23 Aug 2017 16:37:25 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/eafb4dbd/ffb058a6.mp3" length="149331742" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/fNQjfWXTVqS_u6oWdpwh9dM7Qntg-V9TcSpKFYTdbx0/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjIv/MTY4MzU0NDU0NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>9324</itunes:duration>
      <itunes:summary>What disaster is most likely to kill more than 10 million human beings in the next 20 years? Terrorism? Famine? An asteroid?

Actually it’s probably a pandemic: a deadly new disease that spreads out of control. We’ve recently seen the risks with Ebola and swine flu, but they pale in comparison to the Spanish flu which killed 3% of the world’s population in 1918 to 1920. A pandemic of that scale today would kill 200 million.

In this in-depth interview I speak to Howie Lempel, who spent years studying pandemic preparedness for the Open Philanthropy Project. We spend the first 20 minutes covering his work at the foundation, then discuss how bad the pandemic problem is, why it’s probably getting worse, and what can be done about it.

Full transcript, apply for personalised coaching to help you work on pandemic preparedness, see what questions are asked when, and read extra resources to learn more. 

In the second half we go through where you personally could study and work to tackle one of the worst threats facing humanity.

Want to help ensure we have no severe pandemics in the 21st century? We want to help.

We’ve helped dozens of people formulate their plans, and put them in touch with academic mentors. If you want to work on pandemic preparedness safety, apply for our free coaching service.

APPLY FOR COACHING

2m - What does the Open Philanthropy Project do? What’s it like to work there?
16m27s - What grants did OpenPhil make in pandemic preparedness? Did they work out?
22m56s - Why is pandemic preparedness such an important thing to work on?
31m23s - How many people could die in a global pandemic? Is Contagion a realistic movie?
37m05s - Why the risk is getting worse due to scientific discoveries
40m10s - How would dangerous pathogens get released?
45m27s - Would society collapse if a billion people die in a pandemic?
49m25s - The plague, Spanish flu, smallpox, and other historical pandemics
58m30s - How are risks affected by sloppy research security or the existence of factory farming?
1h7m30s - What's already being done? Why institutions for dealing with pandemics are really insufficient.
1h14m30s - What the World Health Organisation should do but can’t.
1h21m51s - What charities do about pandemics and why they aren’t able to fix things
1h25m50s - How long would it take to make vaccines?
1h30m40s - What does the US government do to protect Americans? It’s a mess.
1h37m20s - What kind of people do you know work on this problem and what are they doing?
1h46m30s - Are there things that we ought to be banning or technologies that we should be trying not to develop because we're just better off not having them?
1h49m35s - What kind of reforms are needed at the international level?
1h54m40s - Where should people who want to tackle this problem go to work?
1h59m50s - Are there any technologies we need to urgently develop?
2h04m20s - What about trying to stop humans from having contact with wild animals?
2h08m5s - What should people study if they're young and choosing their major; what should they do a PhD in? Where should they study, and with who?
More...</itunes:summary>
      <itunes:subtitle>What disaster is most likely to kill more than 10 million human beings in the next 20 years? Terrorism? Famine? An asteroid?

Actually it’s probably a pandemic: a deadly new disease that spreads out of control. We’ve recently seen the risks with Ebola and</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#3 - Dario Amodei on OpenAI and how AI will change the world for good and ill</title>
      <itunes:title>#3 - Dario Amodei on OpenAI and how AI will change the world for good and ill</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/334142741</guid>
      <link>https://share.transistor.fm/s/175ef076</link>
      <description>
        <![CDATA[<p>Just two years ago OpenAI didn’t exist. It’s now among the most elite groups of machine learning researchers. They’re trying to make an AI that’s smarter than humans and have $1b at their disposal.</p><p> Even stranger for a Silicon Valley start-up, it’s not a business, but rather a non-profit founded by Elon Musk and Sam Altman among others, to ensure the benefits of AI are distributed broadly to all of society. </p><p> I did a long interview with one of its first machine learning researchers, Dr Dario Amodei, to learn about:</p><p> * OpenAI’s latest plans and research progress.<br> * His paper *Concrete Problems in AI Safety*, which outlines five specific ways machine learning algorithms can act in dangerous ways their designers don’t intend - something OpenAI has to work to avoid.<br> * How listeners can best go about pursuing a career in machine learning and AI development themselves.</p><p> <a href="https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/?utm_campaign=podcast__amodei&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, apply for personalised coaching to work on AI safety, see what questions are asked when, and read extra resources to learn more.</a></p><p> 1m33s - What OpenAI is doing, Dario’s research and why AI is important <br> 13m - Why OpenAI scaled back its Universe project <br> 15m50s - Why AI could be dangerous <br> 24m20s - Would smarter than human AI solve most of the world’s problems? <br> 29m - Paper on five concrete problems in AI safety <br> 43m48s - Has OpenAI made progress? <br> 49m30s - What this back flipping noodle can teach you about AI safety <br> 55m30s - How someone can pursue a career in AI safety and get a job at OpenAI <br> 1h02m30s - Where and what should people study? <br> 1h4m15s - What other paradigms for AI are there? <br> 1h7m55s - How do you go from studying to getting a job? What places are there to work? <br> 1h13m30s - If there's a 17-year-old listening here what should they start reading first? <br> 1h19m - Is this a good way to develop your broader career options? Is it a safe move? <br> 1h21m10s - What if you’re older and haven’t studied machine learning? How do you break in? <br> 1h24m - What about doing this work in academia? <br> 1h26m50s - Is the work frustrating because solutions may not exist? <br> 1h31m35s - How do we prevent a dangerous arms race? <br> 1h36m30s - Final remarks on how to get into doing useful work in machine learning</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Just two years ago OpenAI didn’t exist. It’s now among the most elite groups of machine learning researchers. They’re trying to make an AI that’s smarter than humans and have $1b at their disposal.</p><p> Even stranger for a Silicon Valley start-up, it’s not a business, but rather a non-profit founded by Elon Musk and Sam Altman among others, to ensure the benefits of AI are distributed broadly to all of society. </p><p> I did a long interview with one of its first machine learning researchers, Dr Dario Amodei, to learn about:</p><p> * OpenAI’s latest plans and research progress.<br> * His paper *Concrete Problems in AI Safety*, which outlines five specific ways machine learning algorithms can act in dangerous ways their designers don’t intend - something OpenAI has to work to avoid.<br> * How listeners can best go about pursuing a career in machine learning and AI development themselves.</p><p> <a href="https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/?utm_campaign=podcast__amodei&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, apply for personalised coaching to work on AI safety, see what questions are asked when, and read extra resources to learn more.</a></p><p> 1m33s - What OpenAI is doing, Dario’s research and why AI is important <br> 13m - Why OpenAI scaled back its Universe project <br> 15m50s - Why AI could be dangerous <br> 24m20s - Would smarter than human AI solve most of the world’s problems? <br> 29m - Paper on five concrete problems in AI safety <br> 43m48s - Has OpenAI made progress? <br> 49m30s - What this back flipping noodle can teach you about AI safety <br> 55m30s - How someone can pursue a career in AI safety and get a job at OpenAI <br> 1h02m30s - Where and what should people study? <br> 1h4m15s - What other paradigms for AI are there? <br> 1h7m55s - How do you go from studying to getting a job? What places are there to work? <br> 1h13m30s - If there's a 17-year-old listening here what should they start reading first? <br> 1h19m - Is this a good way to develop your broader career options? Is it a safe move? <br> 1h21m10s - What if you’re older and haven’t studied machine learning? How do you break in? <br> 1h24m - What about doing this work in academia? <br> 1h26m50s - Is the work frustrating because solutions may not exist? <br> 1h31m35s - How do we prevent a dangerous arms race? <br> 1h36m30s - Final remarks on how to get into doing useful work in machine learning</p>]]>
      </content:encoded>
      <pubDate>Fri, 21 Jul 2017 06:55:28 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/175ef076/34af6879.mp3" length="146624241" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/hLi-LNjh_G_cXVPlk077ArSMHqkXcHmr-0SoimjpXGo/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjEv/MTY4MzU0NDU0NC1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>5902</itunes:duration>
      <itunes:summary>Just two years ago OpenAI didn’t exist. It’s now among the most elite groups of machine learning researchers. They’re trying to make an AI that’s smarter than humans and have $1b at their disposal.
    
Even stranger for a Silicon Valley start-up, it’s not a business, but rather a non-profit founded by Elon Musk and Sam Altman among others, to ensure the benefits of AI are distributed broadly to all of society.    
    
I did a long interview with one of its first machine learning researchers, Dr Dario Amodei, to learn about:
    
* OpenAI’s latest plans and research progress.
* His paper *Concrete Problems in AI Safety*, which outlines five specific ways machine learning algorithms can act in dangerous ways their designers don’t intend - something OpenAI has to work to avoid.
* How listeners can best go about pursuing a career in machine learning and AI development themselves.

Full transcript, apply for personalised coaching to work on AI safety, see what questions are asked when, and read extra resources to learn more.
    
1m33s - What OpenAI is doing, Dario’s research and why AI is important    
13m - Why OpenAI scaled back its Universe project    
15m50s - Why AI could be dangerous    
24m20s - Would smarter than human AI solve most of the world’s problems?    
29m - Paper on five concrete problems in AI safety    
43m48s - Has OpenAI made progress?    
49m30s - What this back flipping noodle can teach you about AI safety    
55m30s - How someone can pursue a career in AI safety and get a job at OpenAI    
1h02m30s - Where and what should people study?    
1h4m15s - What other paradigms for AI are there?    
1h7m55s - How do you go from studying to getting a job? What places are there to work?    
1h13m30s - If there's a 17-year-old listening here what should they start reading first?    
1h19m - Is this a good way to develop your broader career options? Is it a safe move?    
1h21m10s - What if you’re older and haven’t studied machine learning? How do you break in?    
1h24m - What about doing this work in academia?    
1h26m50s - Is the work frustrating because solutions may not exist?    
1h31m35s - How do we prevent a dangerous arms race?    
1h36m30s - Final remarks on how to get into doing useful work in machine learning</itunes:summary>
      <itunes:subtitle>Just two years ago OpenAI didn’t exist. It’s now among the most elite groups of machine learning researchers. They’re trying to make an AI that’s smarter than humans and have $1b at their disposal.
    
Even stranger for a Silicon Valley start-up, it’s </itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#2 - David Spiegelhalter on risk, stats and improving understanding of science</title>
      <itunes:title>#2 - David Spiegelhalter on risk, stats and improving understanding of science</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/329202537</guid>
      <link>https://share.transistor.fm/s/1ae2e39a</link>
      <description>
        <![CDATA[<p>Recorded in 2015 by Robert Wiblin with colleague Jess Whittlestone at the Centre for Effective Altruism, and recovered from the dusty 80,000 Hours archives.</p><p> David Spiegelhalter is a statistician at the University of Cambridge and something of an academic celebrity in the UK.</p><p> Part of his role is to improve the public understanding of risk - especially everyday risks we face like getting cancer or dying in a car crash. As a result he’s regularly in the media explaining numbers in the news, trying to assist both ordinary people and politicians focus on the important risks we face, and avoid being distracted by flashy risks that don’t actually have much impact.</p><p> <a href="https://80000hours.org/2017/06/podcast-prof-david-spiegelhalter-on-risk-statistics-and-improving-the-public-understanding-of-science/?utm_campaign=podcast__spiegelhalter&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Summary, full transcript and extra links to learn more.</a></p><p> To help make sense of the uncertainties we face in life he has had to invent concepts like the microlife, or a 30-minute change in life expectancy. (<a href="https://en.wikipedia.org/wiki/Microlife">https://en.wikipedia.org/wiki/Microlife</a>)</p><p> We wanted to learn whether he thought a lifetime of work communicating science had actually had much impact on the world, and what advice he might have for people planning their careers today.</p>]]>
      </description>
      <content:encoded>
        <![CDATA[<p>Recorded in 2015 by Robert Wiblin with colleague Jess Whittlestone at the Centre for Effective Altruism, and recovered from the dusty 80,000 Hours archives.</p><p> David Spiegelhalter is a statistician at the University of Cambridge and something of an academic celebrity in the UK.</p><p> Part of his role is to improve the public understanding of risk - especially everyday risks we face like getting cancer or dying in a car crash. As a result he’s regularly in the media explaining numbers in the news, trying to assist both ordinary people and politicians focus on the important risks we face, and avoid being distracted by flashy risks that don’t actually have much impact.</p><p> <a href="https://80000hours.org/2017/06/podcast-prof-david-spiegelhalter-on-risk-statistics-and-improving-the-public-understanding-of-science/?utm_campaign=podcast__spiegelhalter&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Summary, full transcript and extra links to learn more.</a></p><p> To help make sense of the uncertainties we face in life he has had to invent concepts like the microlife, or a 30-minute change in life expectancy. (<a href="https://en.wikipedia.org/wiki/Microlife">https://en.wikipedia.org/wiki/Microlife</a>)</p><p> We wanted to learn whether he thought a lifetime of work communicating science had actually had much impact on the world, and what advice he might have for people planning their careers today.</p>]]>
      </content:encoded>
      <pubDate>Wed, 21 Jun 2017 04:58:08 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/1ae2e39a/5d126845.mp3" length="21084413" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/lhnmYx6ihVjPDZneTpFxIMXWRQC87-OosxdGQQHRUjU/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MjAv/MTY4MzU0NDU0My1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>2023</itunes:duration>
      <itunes:summary>Recorded in 2015 by Robert Wiblin with colleague Jess Whittlestone at the Centre for Effective Altruism, and recovered from the dusty 80,000 Hours archives.

David Spiegelhalter is a statistician at the University of Cambridge and something of an academic celebrity in the UK.
 
Part of his role is to improve the public understanding of risk - especially everyday risks we face like getting cancer or dying in a car crash. As a result he’s regularly in the media explaining numbers in the news, trying to assist both ordinary people and politicians focus on the important risks we face, and avoid being distracted by flashy risks that don’t actually have much impact. 

Summary, full transcript and extra links to learn more.

To help make sense of the uncertainties we face in life he has had to invent concepts like the microlife, or a 30-minute change in life expectancy.
(https://en.wikipedia.org/wiki/Microlife)

We wanted to learn whether he thought a lifetime of work communicating science had actually had much impact on the world, and what advice he might have for people planning their careers today.</itunes:summary>
      <itunes:subtitle>Recorded in 2015 by Robert Wiblin with colleague Jess Whittlestone at the Centre for Effective Altruism, and recovered from the dusty 80,000 Hours archives.

David Spiegelhalter is a statistician at the University of Cambridge and something of an academ</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#1 - Miles Brundage on the world's desperate need for AI strategists and policy experts</title>
      <itunes:title>#1 - Miles Brundage on the world's desperate need for AI strategists and policy experts</itunes:title>
      <itunes:episodeType>full</itunes:episodeType>
      <guid isPermaLink="false">tag:soundcloud,2010:tracks/326365514</guid>
      <link>https://share.transistor.fm/s/a045212e</link>
      <description>
        <![CDATA[Robert Wiblin, Director of Research at 80,000 Hours speaks with Miles Brundage, research fellow at the University of Oxford's Future of Humanity Institute. Miles studies the social implications surrounding the development of new technologies and has a particular interest in artificial general intelligence, that is, an AI system that could do most or all of the tasks humans could do.<p>

This interview complements <a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/?utm_campaign=podcast__miles_brundage&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our profile of the importance of positively shaping artificial intelligence</a> and <a href="https://80000hours.org/articles/ai-policy-guide/?utm_campaign=podcast__miles_brundage&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our guide to careers in AI policy and strategy</a></p><p>

<a href="https://80000hours.org/2017/06/the-world-desperately-needs-ai-strategists-heres-how-to-become-one/?utm_campaign=podcast__miles_brundage&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, apply for personalised coaching to work on AI strategy, see what questions are asked when, and read extra resources to learn more. </a></p><p></p>]]>
      </description>
      <content:encoded>
        <![CDATA[Robert Wiblin, Director of Research at 80,000 Hours speaks with Miles Brundage, research fellow at the University of Oxford's Future of Humanity Institute. Miles studies the social implications surrounding the development of new technologies and has a particular interest in artificial general intelligence, that is, an AI system that could do most or all of the tasks humans could do.<p>

This interview complements <a href="https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/?utm_campaign=podcast__miles_brundage&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our profile of the importance of positively shaping artificial intelligence</a> and <a href="https://80000hours.org/articles/ai-policy-guide/?utm_campaign=podcast__miles_brundage&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">our guide to careers in AI policy and strategy</a></p><p>

<a href="https://80000hours.org/2017/06/the-world-desperately-needs-ai-strategists-heres-how-to-become-one/?utm_campaign=podcast__miles_brundage&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast">Full transcript, apply for personalised coaching to work on AI strategy, see what questions are asked when, and read extra resources to learn more. </a></p><p></p>]]>
      </content:encoded>
      <pubDate>Mon, 05 Jun 2017 23:56:05 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/a045212e/e7a1023d.mp3" length="39547346" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/IPllE2rU4uCp7Uvvn414VF41cPkGAYU1iDG_HM1-85k/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MTkv/MTY4MzU0NDU0Mi1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>3316</itunes:duration>
      <itunes:summary>Robert Wiblin, Director of Research at 80,000 Hours speaks with Miles Brundage, research fellow at the University of Oxford's Future of Humanity Institute. Miles studies the social implications surrounding the development of new technologies and has a particular interest in artificial general intelligence, that is, an AI system that could do most or all of the tasks humans could do.

This interview complements our profile of the importance of positively shaping artificial intelligence and our guide to careers in AI policy and strategy

Full transcript, apply for personalised coaching to work on AI strategy, see what questions are asked when, and read extra resources to learn more. </itunes:summary>
      <itunes:subtitle>Robert Wiblin, Director of Research at 80,000 Hours speaks with Miles Brundage, research fellow at the University of Oxford's Future of Humanity Institute. Miles studies the social implications surrounding the development of new technologies and has a par</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
    </item>
    <item>
      <title>#0 – Introducing the 80,000 Hours Podcast</title>
      <itunes:title>#0 – Introducing the 80,000 Hours Podcast</itunes:title>
      <itunes:episodeType>trailer</itunes:episodeType>
      <guid isPermaLink="false">4030c850-a3b3-11eb-94ae-124af427119b</guid>
      <link>https://share.transistor.fm/s/db72a630</link>
      <description>
        <![CDATA[80,000 Hours is a non-profit that provides research and other support to help people switch into careers that effectively tackle the world's most pressing problems. This podcast is just one of many things we offer, the others of which you can find at <a href="https://80000hours.org/"><b>80000hours.org</b></a>.<p> 

Since 2017 this show has been putting out interviews about the world's most pressing problems and how to solve them — which some people enjoy because they love to learn about important things, and others are using to figure out what they want to do with their careers or with their charitable giving.</p><p> 

If you haven't yet spent a lot of time with 80,000 Hours or our general style of thinking, called effective altruism, it's probably really helpful to first go through the episodes that set the scene, explain our overall perspective on things, and generally offer all the background information you need to get the most out of the episodes we're making now.</p><p> 

That's why we've made a new feed with ten carefully selected episodes from the show's archives, called 'Effective Altruism: An Introduction'.</p><p> 

You can find it by searching for 'Effective Altruism' in your podcasting app or at 
<a href="https://80000hours.org/intro"><b>80000hours.org/intro</b></a>.</p><p>  

Or, if you’d rather listen on this feed, here are the ten episodes we recommend you listen to first:</p><p> 

• <a href="https://80k.link/HK"><b>#21 – Holden Karnofsky on the world's most intellectual foundation and how philanthropy can have maximum impact by taking big risks</b></a></p><p> 

• <a href="https://80k.link/TO1"><b>#6 – Toby Ord on why the long-term future of humanity matters more than anything else and what we should do about it</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=eaei&amp;utm_medium=podcast"><b>#17 – Will MacAskill on why our descendants might view us as moral monsters</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/?utm_campaign=podcast__spencer-greenberg-2&amp;utm_source=eaai&amp;utm_medium=podcast"><b>#39 – Spencer Greenberg on the scientific approach to updating your beliefs when you get new evidence</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=eaai&amp;utm_medium=podcast"><b>#44 – Paul Christiano on developing real solutions to the 'AI alignment problem'</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/philip-tetlock-forecasting-research/?utm_campaign=podcast__philip-tetlock-2&amp;utm_source=eaai&amp;utm_medium=podcast"><b>#60 – What Professor Tetlock learned from 40 years studying how to predict the future</b></a></p><p> 


• <a href="https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>#46 – Hilary Greaves on moral cluelessness, population ethics and tackling global issues in academia</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/ben-todd-key-ideas-of-80000hours/?utm_campaign=podcast__ben-todd&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>#71 – Benjamin Todd on the key ideas of 80,000 Hours</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/?utm_campaign=podcast__david-denkenberger&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>#50 – Dave Denkenberger on how we might feed all 8 billion people through a nuclear winter</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/?utm_campaign=podcast__ben-todd&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>80,000 Hours Team chat #3 – Koehler and Todd on the core idea of effective altruism and how to argue for it</b></a></p><p></p>]]>
      </description>
      <content:encoded>
        <![CDATA[80,000 Hours is a non-profit that provides research and other support to help people switch into careers that effectively tackle the world's most pressing problems. This podcast is just one of many things we offer, the others of which you can find at <a href="https://80000hours.org/"><b>80000hours.org</b></a>.<p> 

Since 2017 this show has been putting out interviews about the world's most pressing problems and how to solve them — which some people enjoy because they love to learn about important things, and others are using to figure out what they want to do with their careers or with their charitable giving.</p><p> 

If you haven't yet spent a lot of time with 80,000 Hours or our general style of thinking, called effective altruism, it's probably really helpful to first go through the episodes that set the scene, explain our overall perspective on things, and generally offer all the background information you need to get the most out of the episodes we're making now.</p><p> 

That's why we've made a new feed with ten carefully selected episodes from the show's archives, called 'Effective Altruism: An Introduction'.</p><p> 

You can find it by searching for 'Effective Altruism' in your podcasting app or at 
<a href="https://80000hours.org/intro"><b>80000hours.org/intro</b></a>.</p><p>  

Or, if you’d rather listen on this feed, here are the ten episodes we recommend you listen to first:</p><p> 

• <a href="https://80k.link/HK"><b>#21 – Holden Karnofsky on the world's most intellectual foundation and how philanthropy can have maximum impact by taking big risks</b></a></p><p> 

• <a href="https://80k.link/TO1"><b>#6 – Toby Ord on why the long-term future of humanity matters more than anything else and what we should do about it</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=eaei&amp;utm_medium=podcast"><b>#17 – Will MacAskill on why our descendants might view us as moral monsters</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/?utm_campaign=podcast__spencer-greenberg-2&amp;utm_source=eaai&amp;utm_medium=podcast"><b>#39 – Spencer Greenberg on the scientific approach to updating your beliefs when you get new evidence</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=eaai&amp;utm_medium=podcast"><b>#44 – Paul Christiano on developing real solutions to the 'AI alignment problem'</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/philip-tetlock-forecasting-research/?utm_campaign=podcast__philip-tetlock-2&amp;utm_source=eaai&amp;utm_medium=podcast"><b>#60 – What Professor Tetlock learned from 40 years studying how to predict the future</b></a></p><p> 


• <a href="https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>#46 – Hilary Greaves on moral cluelessness, population ethics and tackling global issues in academia</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/ben-todd-key-ideas-of-80000hours/?utm_campaign=podcast__ben-todd&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>#71 – Benjamin Todd on the key ideas of 80,000 Hours</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/?utm_campaign=podcast__david-denkenberger&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>#50 – Dave Denkenberger on how we might feed all 8 billion people through a nuclear winter</b></a></p><p> 

• <a href="https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/?utm_campaign=podcast__ben-todd&amp;utm_source=learn+ea&amp;utm_medium=podcast"><b>80,000 Hours Team chat #3 – Koehler and Todd on the core idea of effective altruism and how to argue for it</b></a></p><p></p>]]>
      </content:encoded>
      <pubDate>Mon, 01 May 2017 21:39:00 +0000</pubDate>
      <author>Rob, Luisa, and the 80,000 Hours team</author>
      <enclosure url="https://media.transistor.fm/db72a630/7f68fba4.mp3" length="1945265" type="audio/mpeg"/>
      <itunes:author>Rob, Luisa, and the 80,000 Hours team</itunes:author>
      <itunes:image href="https://img.transistor.fm/MakdCN1zEHie3_HoXTuOQWUA3EYdDZx-6dtkET8sq8E/rs:fill:3000:3000:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9lcGlz/b2RlLzEzMjQ3MTgv/MTY4MzU0NDU0MS1h/cnR3b3JrLmpwZw.jpg"/>
      <itunes:duration>234</itunes:duration>
      <itunes:summary>80,000 Hours is a non-profit that provides research and other support to help people switch into careers that effectively tackle the world's most pressing problems. This podcast is just one of many things we offer, the others of which you can find at 80000hours.org. 

Since 2017 this show has been putting out interviews about the world's most pressing problems and how to solve them — which some people enjoy because they love to learn about important things, and others are using to figure out what they want to do with their careers or with their charitable giving. 

If you haven't yet spent a lot of time with 80,000 Hours or our general style of thinking, called effective altruism, it's probably really helpful to first go through the episodes that set the scene, explain our overall perspective on things, and generally offer all the background information you need to get the most out of the episodes we're making now. 

That's why we've made a new feed with ten carefully selected episodes from the show's archives, called 'Effective Altruism: An Introduction'. 

You can find it by searching for 'Effective Altruism' in your podcasting app or at 
80000hours.org/intro.  

Or, if you’d rather listen on this feed, here are the ten episodes we recommend you listen to first: 

• #21 – Holden Karnofsky on the world's most intellectual foundation and how philanthropy can have maximum impact by taking big risks 

• #6 – Toby Ord on why the long-term future of humanity matters more than anything else and what we should do about it 

• #17 – Will MacAskill on why our descendants might view us as moral monsters 

• #39 – Spencer Greenberg on the scientific approach to updating your beliefs when you get new evidence 

• #44 – Paul Christiano on developing real solutions to the 'AI alignment problem' 

• #60 – What Professor Tetlock learned from 40 years studying how to predict the future 


• #46 – Hilary Greaves on moral cluelessness, population ethics and tackling global issues in academia 

• #71 – Benjamin Todd on the key ideas of 80,000 Hours 

• #50 – Dave Denkenberger on how we might feed all 8 billion people through a nuclear winter 

• 80,000 Hours Team chat #3 – Koehler and Todd on the core idea of effective altruism and how to argue for it</itunes:summary>
      <itunes:subtitle>80,000 Hours is a non-profit that provides research and other support to help people switch into careers that effectively tackle the world's most pressing problems. This podcast is just one of many things we offer, the others of which you can find at 8000</itunes:subtitle>
      <itunes:keywords></itunes:keywords>
      <itunes:explicit>No</itunes:explicit>
      <podcast:chapters url="https://share.transistor.fm/s/db72a630/chapters.json" type="application/json+chapters"/>
    </item>
  </channel>
</rss>
