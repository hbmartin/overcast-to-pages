<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:media="http://search.yahoo.com/mrss/" xmlns:podcast="https://podcastindex.org/namespace/1.0">
  <channel>
    <atom:link href="https://feeds.simplecast.com/Hb_IuXOo" rel="self" title="MP3 Audio" type="application/atom+xml"/>
    <atom:link href="https://simplecast.superfeedr.com" rel="hub" xmlns="http://www.w3.org/2005/Atom"/>
    <generator>https://simplecast.com</generator>
    <title>AI + a16z</title>
    <description>Artificial intelligence is changing everything from art to enterprise IT, and a16z is watching all of it with a close eye. This podcast features discussions with leading AI engineers, founders, and experts, as well as our general partners, about where the technology and industry are heading.</description>
    <language>en</language>
    <pubDate>Fri, 20 Jun 2025 14:00:00 +0000</pubDate>
    <lastBuildDate>Fri, 20 Jun 2025 14:00:11 +0000</lastBuildDate>
    <image>
      <link>https://ai-a16z.simplecast.com</link>
      <title>AI + a16z</title>
      <url>https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/80ad2223-d8fa-4bb2-9645-8e2c1168a372/3000x3000/ai-a16z-podcast-cover-art-2.jpg?aid=rss_feed</url>
    </image>
    <link>https://ai-a16z.simplecast.com</link>
    <itunes:type>episodic</itunes:type>
    <itunes:summary>Artificial intelligence is changing everything from art to enterprise IT, and a16z is watching all of it with a close eye. This podcast features discussions with leading AI engineers, founders, and experts, as well as our general partners, about where the technology and industry are heading.</itunes:summary>
    <itunes:author>a16z</itunes:author>
    <itunes:explicit>false</itunes:explicit>
    <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/80ad2223-d8fa-4bb2-9645-8e2c1168a372/3000x3000/ai-a16z-podcast-cover-art-2.jpg?aid=rss_feed"/>
    <itunes:new-feed-url>https://feeds.simplecast.com/Hb_IuXOo</itunes:new-feed-url>
    <itunes:keywords>ai, ai/ml, artificial intelligence, enterprise it, entrepreneurship, machine learning, startups, venture capital</itunes:keywords>
    <itunes:owner>
      <itunes:name>Derrick Harris</itunes:name>
      <itunes:email>dharris@a16z.com</itunes:email>
    </itunes:owner>
    <itunes:category text="Technology"/>
    <itunes:category text="Business">
      <itunes:category text="Entrepreneurship"/>
    </itunes:category>
    <item>
      <guid isPermaLink="false">299245d1-de5e-4908-b191-b5cae20730a1</guid>
      <title>AI, Data Engineering, and the Modern Data Stack</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.getdbt.com/" target="_blank">dbt Labs</a> co-founder and CEO Tristan Handy sits down with a16z's Jennifer Li and Matt Bornstein to explore the next chapter of data engineering — from <a href="https://roundup.getdbt.com/p/is-the-modern-data-stack-still-a" target="_blank">the rise (and plateau)</a> of the <a href="https://a16z.com/emerging-architectures-for-modern-data-infrastructure/" target="_blank">modern data stack</a> to the growing role of AI in analytics and data engineering. As they sum up the impact of AI on data workflows: <i>The interesting question here is human-in-the-loop versus human-not-in-the-loop. AI isn’t about replacing analysts — it’s about enabling self-service across the company. But without a human to verify the result, that’s a very scary thing.</i></p><p>Among other specific topics, they also discuss how automation and tooling like SQL compilers are reshaping how engineers work with data; dbt's new <a href="https://www.getdbt.com/product/fusion" target="_blank">Fusion Engine</a> and what it means for developer workflows; and what to make of the spate of recent data-industry acquisitions and ambitious product launches.</p><p>Follow everyone on X:</p><p><a href="https://x.com/jthandy" target="_blank">Tristan Handy</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 20 Jun 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Tristan Handy, Matt Bornstein, Jennifer Li)</author>
      <link>https://ai-a16z.simplecast.com/episodes/ai-data-engineering-and-the-modern-data-stack-wXXhHLo7</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.getdbt.com/" target="_blank">dbt Labs</a> co-founder and CEO Tristan Handy sits down with a16z's Jennifer Li and Matt Bornstein to explore the next chapter of data engineering — from <a href="https://roundup.getdbt.com/p/is-the-modern-data-stack-still-a" target="_blank">the rise (and plateau)</a> of the <a href="https://a16z.com/emerging-architectures-for-modern-data-infrastructure/" target="_blank">modern data stack</a> to the growing role of AI in analytics and data engineering. As they sum up the impact of AI on data workflows: <i>The interesting question here is human-in-the-loop versus human-not-in-the-loop. AI isn’t about replacing analysts — it’s about enabling self-service across the company. But without a human to verify the result, that’s a very scary thing.</i></p><p>Among other specific topics, they also discuss how automation and tooling like SQL compilers are reshaping how engineers work with data; dbt's new <a href="https://www.getdbt.com/product/fusion" target="_blank">Fusion Engine</a> and what it means for developer workflows; and what to make of the spate of recent data-industry acquisitions and ambitious product launches.</p><p>Follow everyone on X:</p><p><a href="https://x.com/jthandy" target="_blank">Tristan Handy</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="33725587" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/27863aa4-9767-4c51-a1b9-7d220bb0af69/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=27863aa4-9767-4c51-a1b9-7d220bb0af69&amp;feed=Hb_IuXOo"/>
      <itunes:title>AI, Data Engineering, and the Modern Data Stack</itunes:title>
      <itunes:author>Tristan Handy, Matt Bornstein, Jennifer Li</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/8c278489-15b1-4101-8a43-168143f973f0/3000x3000/ai-20-20a16z-20pod-20-modern-20data-20stack-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:35:07</itunes:duration>
      <itunes:summary>dbt Labs co-founder and CEO Tristan Handy sits down with a16z&apos;s Jennifer Li and Matt Bornstein to explore the next chapter of data engineering — from the rise (and plateau) of the modern data stack to the growing role of AI in analytics and data engineering. </itunes:summary>
      <itunes:subtitle>dbt Labs co-founder and CEO Tristan Handy sits down with a16z&apos;s Jennifer Li and Matt Bornstein to explore the next chapter of data engineering — from the rise (and plateau) of the modern data stack to the growing role of AI in analytics and data engineering. </itunes:subtitle>
      <itunes:keywords>big data, llms, data lake, artificial intelligence, data engineering, ai, large language models, data analytics, analytics, data science, data</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>48</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">e7bd91b4-ee1f-4bd2-bd83-7dc5f471ca88</guid>
      <title>Enabling Agents and Battling Bots on an AI-Centric Web</title>
      <description><![CDATA[<p><a href="https://arcjet.com/" target="_blank">Arcjet</a> CEO David Mytton sits down with a16z partner Joel de la Garza to discuss the increasing complexity of managing who can access websites, and other web apps, and what they can do there. A primary challenge is determining whether automated traffic is coming from bad actors and troublesome bots, or perhaps AI agents trying to buy a product on behalf of a real customer.Joel and David dive into the challenge of analyzing every request without adding latency, and how faster inference at the edge opens up new possibilities for fraud prevention, content filtering, and even ad tech.Topics include:</p><ul><li>Why traditional threat analysis won’t work for the AI-powered web</li><li>The need for full-context security checks</li><li>How to perform sub-second, cost-effective inference</li><li>The wide range of potential actors and actions behind any given visit</li></ul><p>As David puts it, lower inference costs are key to letting apps act on the full context window — everything you know about the user, the session, and your application.</p><p>Follow everyone on social media:</p><p><a href="https://x.com/davidmytton" target="_blank">David Mytton</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 13 Jun 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Joel de la Garza, David Mytton)</author>
      <link>https://ai-a16z.simplecast.com/episodes/enabling-agents-and-battling-bots-on-an-ai-centric-web-LysFuQCF</link>
      <content:encoded><![CDATA[<p><a href="https://arcjet.com/" target="_blank">Arcjet</a> CEO David Mytton sits down with a16z partner Joel de la Garza to discuss the increasing complexity of managing who can access websites, and other web apps, and what they can do there. A primary challenge is determining whether automated traffic is coming from bad actors and troublesome bots, or perhaps AI agents trying to buy a product on behalf of a real customer.Joel and David dive into the challenge of analyzing every request without adding latency, and how faster inference at the edge opens up new possibilities for fraud prevention, content filtering, and even ad tech.Topics include:</p><ul><li>Why traditional threat analysis won’t work for the AI-powered web</li><li>The need for full-context security checks</li><li>How to perform sub-second, cost-effective inference</li><li>The wide range of potential actors and actions behind any given visit</li></ul><p>As David puts it, lower inference costs are key to letting apps act on the full context window — everything you know about the user, the session, and your application.</p><p>Follow everyone on social media:</p><p><a href="https://x.com/davidmytton" target="_blank">David Mytton</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="25008283" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/de183c8a-5038-4a0c-adc0-ecaacee35533/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=de183c8a-5038-4a0c-adc0-ecaacee35533&amp;feed=Hb_IuXOo"/>
      <itunes:title>Enabling Agents and Battling Bots on an AI-Centric Web</itunes:title>
      <itunes:author>Joel de la Garza, David Mytton</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/84d238d5-feab-46e9-a364-e355fc9ae32f/3000x3000/ai-20-20a16z-20pod-20-20enabling-20agents-20and-20battling-20bots-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:26:02</itunes:duration>
      <itunes:summary>Arcjet CEO David Mytton on the increasing complexity of managing who — including bots, crawlers, and AI agents — can access websites, and other web apps, and what they can do there.</itunes:summary>
      <itunes:subtitle>Arcjet CEO David Mytton on the increasing complexity of managing who — including bots, crawlers, and AI agents — can access websites, and other web apps, and what they can do there.</itunes:subtitle>
      <itunes:keywords>llms, generative agents, ai agents, ai/ml, security, ai, large language models, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>47</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">1ec03c8b-6789-4080-9097-a69f43bff972</guid>
      <title>Giving New Life to Unstructured Data with LLMs and Agents</title>
      <description><![CDATA[<p><a href="https://instabase.com/" target="_blank">Instabase</a> founder and CEO Anant Bhardwaj joins a16z Infra partner Guido Appenzeller to discuss the revolutionary impact of LLMs on analyzing unstructured data and documents (like letting banks verify identity and approve loans via WhatsApp) and shares his vision for how AI agents could take things even further (by automating actions based on those documents). In more detail, they discuss:</p><ul><li>Why legacy robotic process automation (RPA) struggles with unstructured inputs.</li><li>How Instabase developed layout-aware models to extract insights from PDFs and complex documents.</li><li>Why predictability, not perfection, is the key metric for generative AI in the enterprise.</li><li>The growing role of AI agents at <i>compile time</i> (not runtime).</li><li>A vision for decentralized, federated AI systems that scale automation across complex workflows.</li></ul><p>Follow everyone on X:</p><p><a href="https://x.com/anantpb" target="_blank">Anant Bhardwaj</a></p><p><a href="https://x.com/appenz" target="_blank">Guido Appenzeller</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 6 Jun 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Anant Bhardwaj, Guido Appenzeller)</author>
      <link>https://ai-a16z.simplecast.com/episodes/giving-new-life-to-unstructured-data-with-llms-and-agents-uk2KZglN</link>
      <content:encoded><![CDATA[<p><a href="https://instabase.com/" target="_blank">Instabase</a> founder and CEO Anant Bhardwaj joins a16z Infra partner Guido Appenzeller to discuss the revolutionary impact of LLMs on analyzing unstructured data and documents (like letting banks verify identity and approve loans via WhatsApp) and shares his vision for how AI agents could take things even further (by automating actions based on those documents). In more detail, they discuss:</p><ul><li>Why legacy robotic process automation (RPA) struggles with unstructured inputs.</li><li>How Instabase developed layout-aware models to extract insights from PDFs and complex documents.</li><li>Why predictability, not perfection, is the key metric for generative AI in the enterprise.</li><li>The growing role of AI agents at <i>compile time</i> (not runtime).</li><li>A vision for decentralized, federated AI systems that scale automation across complex workflows.</li></ul><p>Follow everyone on X:</p><p><a href="https://x.com/anantpb" target="_blank">Anant Bhardwaj</a></p><p><a href="https://x.com/appenz" target="_blank">Guido Appenzeller</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="34389377" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/fdc66310-c7e9-408f-b9de-9c0deacb56c8/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=fdc66310-c7e9-408f-b9de-9c0deacb56c8&amp;feed=Hb_IuXOo"/>
      <itunes:title>Giving New Life to Unstructured Data with LLMs and Agents</itunes:title>
      <itunes:author>Anant Bhardwaj, Guido Appenzeller</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/29c0b054-d26c-455d-ba42-4e0800f3eb44/3000x3000/ai-20-20a16z-20pod-20-20unstructured-20data-20instabase-20-201080x1080-20-20rss.jpg?aid=rss_feed"/>
      <itunes:duration>00:35:49</itunes:duration>
      <itunes:summary>Instabase founder and CEO Anant Bhardwaj discusses the revolutionary impact of LLMs on analyzing unstructured data and documents, and shares his vision for how AI agents could take things even further.</itunes:summary>
      <itunes:subtitle>Instabase founder and CEO Anant Bhardwaj discusses the revolutionary impact of LLMs on analyzing unstructured data and documents, and shares his vision for how AI agents could take things even further.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, ai, database, large language models, data architecture, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>46</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">9426e567-aad4-4b8a-aa43-5c7795ecfcf6</guid>
      <title>Beyond Leaderboards: LMArena’s Mission to Make AI Reliable</title>
      <description><![CDATA[<p><a href="https://lmarena.ai/" target="_blank">LMArena</a> cofounders Anastasios N. Angelopoulos, Wei-Lin Chiang, and Ion Stoica sit down with a16z general partner Anjney Midha to talk about the future of AI evaluation. As benchmarks struggle to keep up with the pace of real-world deployment, LMArena is reframing the problem: what if the best way to test AI models is to put them in front of millions of users and let them vote? The team discusses how Arena evolved from a research side project into a key part of the AI stack, why fresh and subjective data is crucial for reliability, and what it means to build a CI/CD pipeline for large models.</p><p>They also explore:</p><ul><li>Why expert-only benchmarks are no longer enough.</li><li>How user preferences reveal model capabilities — and their limits.</li><li>What it takes to build personalized leaderboards and evaluation SDKs.</li><li>Why real-time testing is foundational for mission-critical AI.</li></ul><p>Follow everyone on X:</p><p><a href="https://x.com/ml_angelopoulos" target="_blank">Anastasios N. Angelopoulos</a></p><p><a href="https://x.com/infwinston" target="_blank">Wei-Lin Chiang</a></p><p><a href="https://x.com/istoica05" target="_blank">Ion Stoica</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><h2>Timestamps</h2><p>0:04 -  LLM evaluation: From consumer chatbots to mission-critical systems</p><p>6:04 -  Style and substance: Crowdsourcing expertise</p><p>18:51 -  Building immunity to overfitting and gaming the system</p><p>29:49 -  The roots of LMArena</p><p>41:29 -   Proving the value of academic AI research</p><p>48:28 -  Scaling LMArena and starting a company</p><p>59:59 -  Benchmarks, evaluations, and the value of ranking LLMs</p><p>1:12:13 -  The challenges of measuring AI reliability</p><p>1:17:57 -  Expanding beyond binary rankings as models evolve</p><p>1:28:07 -  A leaderboard for each prompt</p><p>1:31:28 -  The LMArena roadmap</p><p>1:34:29 -  The importance of open source and openness</p><p>1:43:10 -  Adapting to agents (and other AI evolutions)</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 30 May 2025 15:46:18 +0000</pubDate>
      <author>dharris@a16z.com (Anastasios N. Angelopoulos, Wei-Lin Chiang, Ion Stoica, Anjney Midha)</author>
      <link>https://ai-a16z.simplecast.com/episodes/beyond-leaderboards-lmarenas-mission-to-make-ai-reliable-FXSI58J_</link>
      <content:encoded><![CDATA[<p><a href="https://lmarena.ai/" target="_blank">LMArena</a> cofounders Anastasios N. Angelopoulos, Wei-Lin Chiang, and Ion Stoica sit down with a16z general partner Anjney Midha to talk about the future of AI evaluation. As benchmarks struggle to keep up with the pace of real-world deployment, LMArena is reframing the problem: what if the best way to test AI models is to put them in front of millions of users and let them vote? The team discusses how Arena evolved from a research side project into a key part of the AI stack, why fresh and subjective data is crucial for reliability, and what it means to build a CI/CD pipeline for large models.</p><p>They also explore:</p><ul><li>Why expert-only benchmarks are no longer enough.</li><li>How user preferences reveal model capabilities — and their limits.</li><li>What it takes to build personalized leaderboards and evaluation SDKs.</li><li>Why real-time testing is foundational for mission-critical AI.</li></ul><p>Follow everyone on X:</p><p><a href="https://x.com/ml_angelopoulos" target="_blank">Anastasios N. Angelopoulos</a></p><p><a href="https://x.com/infwinston" target="_blank">Wei-Lin Chiang</a></p><p><a href="https://x.com/istoica05" target="_blank">Ion Stoica</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><h2>Timestamps</h2><p>0:04 -  LLM evaluation: From consumer chatbots to mission-critical systems</p><p>6:04 -  Style and substance: Crowdsourcing expertise</p><p>18:51 -  Building immunity to overfitting and gaming the system</p><p>29:49 -  The roots of LMArena</p><p>41:29 -   Proving the value of academic AI research</p><p>48:28 -  Scaling LMArena and starting a company</p><p>59:59 -  Benchmarks, evaluations, and the value of ranking LLMs</p><p>1:12:13 -  The challenges of measuring AI reliability</p><p>1:17:57 -  Expanding beyond binary rankings as models evolve</p><p>1:28:07 -  A leaderboard for each prompt</p><p>1:31:28 -  The LMArena roadmap</p><p>1:34:29 -  The importance of open source and openness</p><p>1:43:10 -  Adapting to agents (and other AI evolutions)</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="97654952" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/2de38043-a555-4028-90e4-89014628d728/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=2de38043-a555-4028-90e4-89014628d728&amp;feed=Hb_IuXOo"/>
      <itunes:title>Beyond Leaderboards: LMArena’s Mission to Make AI Reliable</itunes:title>
      <itunes:author>Anastasios N. Angelopoulos, Wei-Lin Chiang, Ion Stoica, Anjney Midha</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/b823465b-51a8-4f23-add5-564332da2f48/3000x3000/ai-20-20a16z-20pod-20-20beyond-20leaderboards-20lmarena-e2-80-99s-20mission-20to-20make-20ai-20reliable-20ai-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>01:41:43</itunes:duration>
      <itunes:summary>LMArena cofounders Anastasios N. Angelopoulos, Wei-Lin Chiang, and Ion Stoica sit down with a16z general partner Anjney Midha to talk about the future of AI evaluation.</itunes:summary>
      <itunes:subtitle>LMArena cofounders Anastasios N. Angelopoulos, Wei-Lin Chiang, and Ion Stoica sit down with a16z general partner Anjney Midha to talk about the future of AI evaluation.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, ai/ml, ai programming, ai, large language models, machine learning, open source, ai coding</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>45</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">d898c1af-15ba-4d9b-9616-7820f99bcc7f</guid>
      <title>Building AI Systems You Can Trust</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.distributional.com/" target="_blank">Distributional</a> cofounder and CEO Scott Clark, and a16z partner Matt Bornstein, explore why building trust in AI systems matters more than just optimizing performance metrics. From understanding the hidden complexities of generative AI behavior to addressing the challenges of reliability and consistency, they discuss how to confidently deploy AI in production. </p><p>Why is trust becoming a critical factor in enterprise AI adoption? How do traditional performance metrics fail to capture crucial behavioral nuances in generative AI systems? Scott and Matt dive into these questions, examining non-deterministic outcomes, shifting model behaviors, and the growing importance of robust testing frameworks. </p><p>Among other topics, they cover: </p><ul><li>The limitations of conventional AI evaluation methods and the need for behavioral testing. </li><li>How centralized AI platforms help enterprises manage complexity and ensure responsible AI use. </li><li>The rise of "shadow AI" and its implications for security and compliance. </li><li>Practical strategies for scaling AI confidently from prototypes to real-world applications.</li></ul><p>Follow everyone:</p><p><a href="https://www.linkedin.com/in/sc932/" target="_blank">Scott Clark</a></p><p><a href="https://x.com/dbnlAI" target="_blank">Distributional</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 23 May 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Scott Clark, Matt Bornstein)</author>
      <link>https://ai-a16z.simplecast.com/episodes/building-ai-systems-you-can-trust-AEtnbIY6</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.distributional.com/" target="_blank">Distributional</a> cofounder and CEO Scott Clark, and a16z partner Matt Bornstein, explore why building trust in AI systems matters more than just optimizing performance metrics. From understanding the hidden complexities of generative AI behavior to addressing the challenges of reliability and consistency, they discuss how to confidently deploy AI in production. </p><p>Why is trust becoming a critical factor in enterprise AI adoption? How do traditional performance metrics fail to capture crucial behavioral nuances in generative AI systems? Scott and Matt dive into these questions, examining non-deterministic outcomes, shifting model behaviors, and the growing importance of robust testing frameworks. </p><p>Among other topics, they cover: </p><ul><li>The limitations of conventional AI evaluation methods and the need for behavioral testing. </li><li>How centralized AI platforms help enterprises manage complexity and ensure responsible AI use. </li><li>The rise of "shadow AI" and its implications for security and compliance. </li><li>Practical strategies for scaling AI confidently from prototypes to real-world applications.</li></ul><p>Follow everyone:</p><p><a href="https://www.linkedin.com/in/sc932/" target="_blank">Scott Clark</a></p><p><a href="https://x.com/dbnlAI" target="_blank">Distributional</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="45762455" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/5a75f867-78df-453a-9698-5190fff73f97/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=5a75f867-78df-453a-9698-5190fff73f97&amp;feed=Hb_IuXOo"/>
      <itunes:title>Building AI Systems You Can Trust</itunes:title>
      <itunes:author>Scott Clark, Matt Bornstein</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/b6df411f-0a8c-4526-913d-3daf66b36953/3000x3000/ai-20-20a16z-20pod-20-20building-20ai-20systems-20you-20can-20trust-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:47:40</itunes:duration>
      <itunes:summary>Distributional CEO Scott Clark discusses why it&apos;s critical to continuously test generative AI systems to ensure expected behavior. In inherently chaotic systems, small upstream changes can have big downstream effects.</itunes:summary>
      <itunes:subtitle>Distributional CEO Scott Clark discusses why it&apos;s critical to continuously test generative AI systems to ensure expected behavior. In inherently chaotic systems, small upstream changes can have big downstream effects.</itunes:subtitle>
      <itunes:keywords>al/ml, generative ai, llms, artificial intelligence, ai, large language models, machine learning, software engineering</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>44</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">09173876-d309-466f-b524-5dc4c21c6960</guid>
      <title>Who&apos;s Coding Now? AI and the Future of Software Development</title>
      <description><![CDATA[<p>In this episode of the a16z AI podcast, a16z Infra partners Guido Appenzeller, Matt Bornstein, and Yoko Li explore how generative AI is reshaping software development. From its potential as a new high-level programming abstraction to its current practical impacts, they discuss whether AI coding tools will redefine what it means to be a developer.</p><p>Why has coding emerged as one of AI's most powerful use cases? How much can AI truly boost developer productivity, and will it fundamentally change traditional computer science education? Guido, Yoko, and Matt dive deep into these questions, addressing the dynamics of "vibe coding," the enduring role of formal programming languages, and the critical challenge of managing non-deterministic behavior in AI-driven applications.Among other things, they discuss:</p><ul><li>The enormous market potential of AI-generated code, projected to deliver trillions in productivity gains.</li><li>How "prompt-based programming" is evolving from Stack Overflow replacements into sophisticated development assistants.</li><li>Why formal languages like Python and Java are here to stay, even as natural language interactions become common.</li><li>The shifting landscape of programming education, and why understanding foundational abstractions remains essential.</li><li>The unique complexities of integrating AI into enterprise software, from managing uncertainty to ensuring reliability.</li></ul>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 16 May 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Guido Appenzeller, Matt Bornstein, Yoko Li)</author>
      <link>https://ai-a16z.simplecast.com/episodes/whos-coding-now-ai-and-the-future-of-software-development-kII71_li</link>
      <content:encoded><![CDATA[<p>In this episode of the a16z AI podcast, a16z Infra partners Guido Appenzeller, Matt Bornstein, and Yoko Li explore how generative AI is reshaping software development. From its potential as a new high-level programming abstraction to its current practical impacts, they discuss whether AI coding tools will redefine what it means to be a developer.</p><p>Why has coding emerged as one of AI's most powerful use cases? How much can AI truly boost developer productivity, and will it fundamentally change traditional computer science education? Guido, Yoko, and Matt dive deep into these questions, addressing the dynamics of "vibe coding," the enduring role of formal programming languages, and the critical challenge of managing non-deterministic behavior in AI-driven applications.Among other things, they discuss:</p><ul><li>The enormous market potential of AI-generated code, projected to deliver trillions in productivity gains.</li><li>How "prompt-based programming" is evolving from Stack Overflow replacements into sophisticated development assistants.</li><li>Why formal languages like Python and Java are here to stay, even as natural language interactions become common.</li><li>The shifting landscape of programming education, and why understanding foundational abstractions remains essential.</li><li>The unique complexities of integrating AI into enterprise software, from managing uncertainty to ensuring reliability.</li></ul>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="42722158" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/7fa364b7-1a84-4448-a605-110217d7fba3/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=7fa364b7-1a84-4448-a605-110217d7fba3&amp;feed=Hb_IuXOo"/>
      <itunes:title>Who&apos;s Coding Now? AI and the Future of Software Development</itunes:title>
      <itunes:author>Guido Appenzeller, Matt Bornstein, Yoko Li</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/e46cb13d-6527-4e18-9e10-d75953fd1644/3000x3000/ai-20-20a16z-20pod-20-20who-20is-20coding-20now-20-20-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:44:30</itunes:duration>
      <itunes:summary>a16z Infra partners Guido Appenzeller, Matt Bornstein, and Yoko Li explore how generative AI is reshaping software development — from its potential as a new high-level programming abstraction to its current practical impacts.</itunes:summary>
      <itunes:subtitle>a16z Infra partners Guido Appenzeller, Matt Bornstein, and Yoko Li explore how generative AI is reshaping software development — from its potential as a new high-level programming abstraction to its current practical impacts.</itunes:subtitle>
      <itunes:keywords>software development, coding, generative ai, llms, ai/ml, ai, large language models, programming</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>43</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">7cfcf8fd-9213-4c5a-9b2d-9769c64fce3e</guid>
      <title>MCP Co-Creator on the Next Wave of LLM Innovation</title>
      <description><![CDATA[<p>In this episode of AI + a16z, Anthropic's David Soria Parra — who created MCP (Model Context Protocol) along with Justin Spahr-Summers — sits down with a16z's Yoko Li to discuss the project's inception, exciting use cases for connecting LLMs to external sources, and what's coming next for the project. If you're unfamiliar with the wildly popular MCP project, this edited passage from their discussion is a great starting point to learn:</p><p><strong>David: </strong>"MCP tries to enable building AI applications in such a way that they can be extended by everyone else that is not part of the original development team through these MCP servers, and really bring the workflows you care about, the things you want to do, to these AI applications. It's a protocol that just defines how whatever you are building as a developer for that integration piece, and that AI application, talk to each other. </p><p>"It's a very boring specification, but what it enables is hopefully ... something that looks like the current API ecosystem, but for LLM interactions."</p><p><strong>Yoko: "</strong>I really love the analogy with the API ecosystem, because they give people a mental model of how the ecosystem evolves ... Before, you may have needed a different spec to query Salesforce versus query HubSpot. Now you can use similarly defined API schema to do that.</p><p>"And then when I saw MCP earlier in the year, it was very interesting in that it almost felt like a standard interface for the agent to interface with LLMs. It's like, 'What are the set of things that the agent wants to execute on that it has never seen before? What kind of context does it need to make these things happen?' When I tried it out, it was just super powerful and I no longer have to build one tool per client. I now can build just one MCP server, for example, for sending emails, and I use it for everything on Cursor, on Claude Desktop, on Goose."</p><p>Learn more:</p><p><a href="https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/" target="_blank">A Deep Dive Into MCP and the Future of AI Tooling</a></p><p><a href="https://a16z.com/podcast/what-is-an-ai-agent/" target="_blank">What Is an AI Agent?</a></p><p><a href="https://a16z.com/podcast/benchmarking-ai-agents-on-full-stack-coding/" target="_blank">Benchmarking AI Agents on Full-Stack Coding</a></p><p><a href="https://a16z.com/podcast/agent-experience-building-an-open-web-for-the-ai-era/" target="_blank">Agent Experience: Building an Open Web for the AI Era</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/dsp_" target="_blank">David Soria Parra</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 2 May 2025 14:30:00 +0000</pubDate>
      <author>dharris@a16z.com (David Soria Parra, Yoko Li)</author>
      <link>https://ai-a16z.simplecast.com/episodes/mcp-creator-on-the-next-wave-of-llm-innovation-RBH9eGzD</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, Anthropic's David Soria Parra — who created MCP (Model Context Protocol) along with Justin Spahr-Summers — sits down with a16z's Yoko Li to discuss the project's inception, exciting use cases for connecting LLMs to external sources, and what's coming next for the project. If you're unfamiliar with the wildly popular MCP project, this edited passage from their discussion is a great starting point to learn:</p><p><strong>David: </strong>"MCP tries to enable building AI applications in such a way that they can be extended by everyone else that is not part of the original development team through these MCP servers, and really bring the workflows you care about, the things you want to do, to these AI applications. It's a protocol that just defines how whatever you are building as a developer for that integration piece, and that AI application, talk to each other. </p><p>"It's a very boring specification, but what it enables is hopefully ... something that looks like the current API ecosystem, but for LLM interactions."</p><p><strong>Yoko: "</strong>I really love the analogy with the API ecosystem, because they give people a mental model of how the ecosystem evolves ... Before, you may have needed a different spec to query Salesforce versus query HubSpot. Now you can use similarly defined API schema to do that.</p><p>"And then when I saw MCP earlier in the year, it was very interesting in that it almost felt like a standard interface for the agent to interface with LLMs. It's like, 'What are the set of things that the agent wants to execute on that it has never seen before? What kind of context does it need to make these things happen?' When I tried it out, it was just super powerful and I no longer have to build one tool per client. I now can build just one MCP server, for example, for sending emails, and I use it for everything on Cursor, on Claude Desktop, on Goose."</p><p>Learn more:</p><p><a href="https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/" target="_blank">A Deep Dive Into MCP and the Future of AI Tooling</a></p><p><a href="https://a16z.com/podcast/what-is-an-ai-agent/" target="_blank">What Is an AI Agent?</a></p><p><a href="https://a16z.com/podcast/benchmarking-ai-agents-on-full-stack-coding/" target="_blank">Benchmarking AI Agents on Full-Stack Coding</a></p><p><a href="https://a16z.com/podcast/agent-experience-building-an-open-web-for-the-ai-era/" target="_blank">Agent Experience: Building an Open Web for the AI Era</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/dsp_" target="_blank">David Soria Parra</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="51510586" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/a1a907ae-0c13-4cc3-984c-f7d8a21b63cd/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=a1a907ae-0c13-4cc3-984c-f7d8a21b63cd&amp;feed=Hb_IuXOo"/>
      <itunes:title>MCP Co-Creator on the Next Wave of LLM Innovation</itunes:title>
      <itunes:author>David Soria Parra, Yoko Li</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/e6630d1d-d967-4642-9240-bb11e99d2286/3000x3000/ai-20-20a16z-20pod-20-20mcp-20x-20anthropic-20bw-201080x1080at2x.jpg?aid=rss_feed"/>
      <itunes:duration>00:53:39</itunes:duration>
      <itunes:summary>Anthropic&apos;s David Soria Parra — who created MCP (Model Context Protocol) along with Justin Spahr-Summers — sits down with a16z&apos;s Yoko Li to discuss the project&apos;s inception, exciting use cases for connecting LLMs to external sources, and what&apos;s coming next for the project.</itunes:summary>
      <itunes:subtitle>Anthropic&apos;s David Soria Parra — who created MCP (Model Context Protocol) along with Justin Spahr-Summers — sits down with a16z&apos;s Yoko Li to discuss the project&apos;s inception, exciting use cases for connecting LLMs to external sources, and what&apos;s coming next for the project.</itunes:subtitle>
      <itunes:keywords>anthropic, generative ai, llms, artificial intelligence, ai agents, ai/ml, mcp, ai, large language models, machine learning, a16z, open source</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>42</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">34bc9137-cd0a-403b-b2a2-ae25e2740bfc</guid>
      <title>What Is an AI Agent?</title>
      <description><![CDATA[<p>In this episode of AI + a16z, a16z Infra partners Guido Appenzeller, Matt Bornstein, and Yoko Li discuss and debate one of the tech industry's buzziest words right now: AI agents. The trio digs into the topic from a number of angles, including:</p><ul><li>Whether a uniform definition of agent actually exists</li><li>How to distinguish between agents, LLMs, and functions</li><li>How to think about pricing agents</li><li>Whether agents can actually replace humans, and</li><li>The effects of data siloes on agents that can access the web.</li></ul><p>They don't claim to have all the answers, but they raise many questions and insights that should interest anybody building, buying, and even marketing AI agents.</p><p>Learn more:</p><p><a href="https://a16z.com/podcast/benchmarking-ai-agents-on-full-stack-coding/" target="_blank">Benchmarking AI Agents on Full-Stack Coding</a></p><p><a href="https://a16z.com/podcast/automating-developer-email-with-mcp-and-al-agents/" target="_blank">Automating Developer Email with MCP and Al Agents</a></p><p><a href="https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/" target="_blank">A Deep Dive Into MCP and the Future of AI Tooling</a></p><p><a href="https://a16z.com/podcast/agent-experience-building-an-open-web-for-the-ai-era/" target="_blank">Agent Experience: Building an Open Web for the AI Era</a></p><p><a href="https://a16z.com/podcast/deepseek-reasoning-models-and-the-future-of-llms/" target="_blank">DeepSeek, Reasoning Models, and the Future of LLMs</a></p><p><a href="https://a16z.com/podcast/agents-lawyers-and-llms/" target="_blank">Agents, Lawyers, and LLMs</a></p><p><a href="https://a16z.com/podcast/reasoning-models-are-remaking-professional-services/" target="_blank">Reasoning Models Are Remaking Professional Services</a></p><p><a href="https://a16z.com/podcast/from-nlp-to-llms-the-quest-for-a-reliable-chatbot/" target="_blank">From NLP to LLMs: The Quest for a Reliable Chatbot</a></p><p><a href="https://a16z.com/podcast/can-ai-agents-finally-fix-customer-support/" target="_blank">Can AI Agents Finally Fix Customer Support?</a></p><p>Follow everybody on X:</p><p><a href="https://x.com/appenz" target="_blank">Guido Appenzeller</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Mon, 28 Apr 2025 20:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Guido Appenzeller, Matt Bornstein, Yoko Li)</author>
      <link>https://ai-a16z.simplecast.com/episodes/what-is-an-ai-agent-PIccwshu</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, a16z Infra partners Guido Appenzeller, Matt Bornstein, and Yoko Li discuss and debate one of the tech industry's buzziest words right now: AI agents. The trio digs into the topic from a number of angles, including:</p><ul><li>Whether a uniform definition of agent actually exists</li><li>How to distinguish between agents, LLMs, and functions</li><li>How to think about pricing agents</li><li>Whether agents can actually replace humans, and</li><li>The effects of data siloes on agents that can access the web.</li></ul><p>They don't claim to have all the answers, but they raise many questions and insights that should interest anybody building, buying, and even marketing AI agents.</p><p>Learn more:</p><p><a href="https://a16z.com/podcast/benchmarking-ai-agents-on-full-stack-coding/" target="_blank">Benchmarking AI Agents on Full-Stack Coding</a></p><p><a href="https://a16z.com/podcast/automating-developer-email-with-mcp-and-al-agents/" target="_blank">Automating Developer Email with MCP and Al Agents</a></p><p><a href="https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/" target="_blank">A Deep Dive Into MCP and the Future of AI Tooling</a></p><p><a href="https://a16z.com/podcast/agent-experience-building-an-open-web-for-the-ai-era/" target="_blank">Agent Experience: Building an Open Web for the AI Era</a></p><p><a href="https://a16z.com/podcast/deepseek-reasoning-models-and-the-future-of-llms/" target="_blank">DeepSeek, Reasoning Models, and the Future of LLMs</a></p><p><a href="https://a16z.com/podcast/agents-lawyers-and-llms/" target="_blank">Agents, Lawyers, and LLMs</a></p><p><a href="https://a16z.com/podcast/reasoning-models-are-remaking-professional-services/" target="_blank">Reasoning Models Are Remaking Professional Services</a></p><p><a href="https://a16z.com/podcast/from-nlp-to-llms-the-quest-for-a-reliable-chatbot/" target="_blank">From NLP to LLMs: The Quest for a Reliable Chatbot</a></p><p><a href="https://a16z.com/podcast/can-ai-agents-finally-fix-customer-support/" target="_blank">Can AI Agents Finally Fix Customer Support?</a></p><p>Follow everybody on X:</p><p><a href="https://x.com/appenz" target="_blank">Guido Appenzeller</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="34980301" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/2d89d785-a779-461d-8ad2-9be79e23cd8f/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=2d89d785-a779-461d-8ad2-9be79e23cd8f&amp;feed=Hb_IuXOo"/>
      <itunes:title>What Is an AI Agent?</itunes:title>
      <itunes:author>Guido Appenzeller, Matt Bornstein, Yoko Li</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/9652a996-995d-4df4-9164-df49c83eec9b/3000x3000/ai-20-20a16z-20pod-20-20what-20is-20an-20agent-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:36:26</itunes:duration>
      <itunes:summary>Partners on the a16z Infra team discuss and debate one of the tech industry&apos;s buzziest words right now: AI agents. Guido Appenzeller, Matt Bornstein, and Yoko Li dive into everything from how to distinguish between agents, LLMs, and functions, to how to think about pricing agents.</itunes:summary>
      <itunes:subtitle>Partners on the a16z Infra team discuss and debate one of the tech industry&apos;s buzziest words right now: AI agents. Guido Appenzeller, Matt Bornstein, and Yoko Li dive into everything from how to distinguish between agents, LLMs, and functions, to how to think about pricing agents.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, generative agents, ai agents, ai/ml, ai, large language models, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>41</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">f044b72b-55d8-465a-9afa-1965b9e8240d</guid>
      <title>Benchmarking AI Agents on Full-Stack Coding</title>
      <description><![CDATA[<p>In this episode, a16z General Partner Martin Casado sits down with Sujay Jayakar, co-founder and Chief Scientist at <a href="https://www.convex.dev/">Convex</a>, to talk about his team’s latest work benchmarking AI agents on full-stack coding tasks. From designing Fullstack Bench to the quirks of agent behavior, the two dig into what’s actually hard about autonomous software development, and why robust evals—and guardrails like type safety—matter more than ever. They also get tactical: which models perform best for real-world app building? How should developers think about trajectory management and variance across runs? And what changes when you treat your toolchain like part of the prompt? Whether you're a hobbyist developer or building the next generation of AI-powered devtools, Sujay’s systems-level insights are not to be missed.</p><p>Drawing from Sujay’s work developing the Fullstack-Bench, they cover:</p><ul><li>Why full-stack coding is still a frontier task for autonomous agents</li><li>How type safety and other “guardrails” can significantly reduce variance and failure</li><li>What makes a good eval—and why evals might matter more than clever prompts</li><li>How different models perform on real-world app-building tasks (and what to watch out for)</li><li>Why your toolchain might be the most underrated part of the prompt</li><li>And what all of this means for devs—from hobbyists to infra teams building with AI in the loop</li></ul><p>Learn More:</p><p><a href="https://stack.convex.dev/introducing-fullstack-bench">Introducing Fullstack-Bench</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/sujayakar314">Sujay Jayakar</a></p><p><a href="https://x.com/martin_casado">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 28 Mar 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Sujay Jayakar, Martin Casado)</author>
      <link>https://ai-a16z.simplecast.com/episodes/benchmarking-ai-agents-on-full-stack-coding-QGpiWJ91</link>
      <media:thumbnail height="720" url="https://image.simplecastcdn.com/images/c816dcf3-bf6b-4383-bfd0-ce36db7a9d83/498d7bee-0772-41b8-a997-347258d78d15/ai-20-20a16z-20pod-20-20benchmarking-20ai-20agents-20on-20full-stack-20coding-209-16.jpg" width="1280"/>
      <content:encoded><![CDATA[<p>In this episode, a16z General Partner Martin Casado sits down with Sujay Jayakar, co-founder and Chief Scientist at <a href="https://www.convex.dev/">Convex</a>, to talk about his team’s latest work benchmarking AI agents on full-stack coding tasks. From designing Fullstack Bench to the quirks of agent behavior, the two dig into what’s actually hard about autonomous software development, and why robust evals—and guardrails like type safety—matter more than ever. They also get tactical: which models perform best for real-world app building? How should developers think about trajectory management and variance across runs? And what changes when you treat your toolchain like part of the prompt? Whether you're a hobbyist developer or building the next generation of AI-powered devtools, Sujay’s systems-level insights are not to be missed.</p><p>Drawing from Sujay’s work developing the Fullstack-Bench, they cover:</p><ul><li>Why full-stack coding is still a frontier task for autonomous agents</li><li>How type safety and other “guardrails” can significantly reduce variance and failure</li><li>What makes a good eval—and why evals might matter more than clever prompts</li><li>How different models perform on real-world app-building tasks (and what to watch out for)</li><li>Why your toolchain might be the most underrated part of the prompt</li><li>And what all of this means for devs—from hobbyists to infra teams building with AI in the loop</li></ul><p>Learn More:</p><p><a href="https://stack.convex.dev/introducing-fullstack-bench">Introducing Fullstack-Bench</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/sujayakar314">Sujay Jayakar</a></p><p><a href="https://x.com/martin_casado">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="32139915" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/8d45a574-ae4a-494e-846f-18d238b2850f/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=8d45a574-ae4a-494e-846f-18d238b2850f&amp;feed=Hb_IuXOo"/>
      <itunes:title>Benchmarking AI Agents on Full-Stack Coding</itunes:title>
      <itunes:author>Sujay Jayakar, Martin Casado</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/c816dcf3-bf6b-4383-bfd0-ce36db7a9d83/41f119e9-df47-4c08-b4bb-cf83b7c20576/3000x3000/ai-20-20a16z-20pod-20-20benchmarking-20ai-20agents-20on-20full-stack-20coding-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>00:33:28</itunes:duration>
      <itunes:summary>In this episode, a16z General Partner Martin Casado sits down with Sujay Jayakar, co-founder and Chief Scientist at Convex, to talk about his team’s latest work benchmarking AI agents on full-stack coding tasks.</itunes:summary>
      <itunes:subtitle>In this episode, a16z General Partner Martin Casado sits down with Sujay Jayakar, co-founder and Chief Scientist at Convex, to talk about his team’s latest work benchmarking AI agents on full-stack coding tasks.</itunes:subtitle>
      <itunes:keywords>evals, full-stack, agents, ai agents, reinforcement learning, benchmarking, guardrails, full-stack-coding, benchmarks</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>40</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">e65c6423-f891-4032-accf-7cc256ea954e</guid>
      <title>Automating Developer Email with MCP and Al Agents</title>
      <description><![CDATA[<p>In this episode of AI + a16z,  <a href="https://resend.com/" target="_blank">Resend</a> founder and CEO Zeno Rocha sits down with a16z partner Yoko Li to discuss:</p><ul><li>How generative AI — powered by agents and, now, MCP — is reshaping the email experience for developers, as well as the overall world of programming. </li><li>Zeno's obsession with developer experience has evolved into designing for "agent experience" — a new frontier where LLM-powered agents are not only building products but also operating within them. </li><li>How email, one of the most ubiquitous tools for developers and end users alike, is being reimagined for a future where agents send, parse, and optimize communication. </li><li>What it means to build agent-friendly APIs. </li><li>The emerging MCP protocol, and how AI is collapsing the creative loop for prosumers and developers alike.</li></ul><p>Learn more:</p><p><a href="https://resend.com/blog/agent-experience" target="_blank">What is AX (agent experience) and how to improve it</a></p><p><a href="https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/" target="_blank">A deep dive into MCP and the future of AI tooling</a></p><p><a href="https://draculatheme.com/" target="_blank">Dracula theme</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/zenorocha" target="_blank">Zeno Rocha</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 21 Mar 2025 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Yoko Li, Zeno Rocha)</author>
      <link>https://ai-a16z.simplecast.com/episodes/automating-developer-email-with-mcp-and-al-agents-cDVmKE0u</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z,  <a href="https://resend.com/" target="_blank">Resend</a> founder and CEO Zeno Rocha sits down with a16z partner Yoko Li to discuss:</p><ul><li>How generative AI — powered by agents and, now, MCP — is reshaping the email experience for developers, as well as the overall world of programming. </li><li>Zeno's obsession with developer experience has evolved into designing for "agent experience" — a new frontier where LLM-powered agents are not only building products but also operating within them. </li><li>How email, one of the most ubiquitous tools for developers and end users alike, is being reimagined for a future where agents send, parse, and optimize communication. </li><li>What it means to build agent-friendly APIs. </li><li>The emerging MCP protocol, and how AI is collapsing the creative loop for prosumers and developers alike.</li></ul><p>Learn more:</p><p><a href="https://resend.com/blog/agent-experience" target="_blank">What is AX (agent experience) and how to improve it</a></p><p><a href="https://a16z.com/a-deep-dive-into-mcp-and-the-future-of-ai-tooling/" target="_blank">A deep dive into MCP and the future of AI tooling</a></p><p><a href="https://draculatheme.com/" target="_blank">Dracula theme</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/zenorocha" target="_blank">Zeno Rocha</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="42874780" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/a44ad6c4-8db4-4b47-8ad1-fdcbc7b9b06e/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=a44ad6c4-8db4-4b47-8ad1-fdcbc7b9b06e&amp;feed=Hb_IuXOo"/>
      <itunes:title>Automating Developer Email with MCP and Al Agents</itunes:title>
      <itunes:author>Yoko Li, Zeno Rocha</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/05d9f805-62d4-419a-9f43-a2ba70a14b00/3000x3000/ai-20-20a16z-20pod-20-20automating-20-e2-80-a8developer-20email-20with-20mcp-20and-20ai-20agents-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>00:44:39</itunes:duration>
      <itunes:summary>a16z&apos;s Yoko Li and Resend&apos;s Zeno Rocha discuss how generative AI — powered by agents and, now, MCP — is reshaping the email experience for developers, as well as the overall world of programming.</itunes:summary>
      <itunes:subtitle>a16z&apos;s Yoko Li and Resend&apos;s Zeno Rocha discuss how generative AI — powered by agents and, now, MCP — is reshaping the email experience for developers, as well as the overall world of programming.</itunes:subtitle>
      <itunes:keywords>coding, generative ai, apis, llms, ai agents, ai/ml, mcp, ai, programming, software developer</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>39</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">973e96d6-5331-4597-b337-305cbf4caec6</guid>
      <title>The Future of Digital Workers</title>
      <description><![CDATA[<p>In this episode of <i>AI + a16z</i>, a16z Partner Joe Schmidt sits down with 11x CTO Prabhav Jain for an inside look at how AI-powered digital workers are reshaping sales and revenue operations. They discuss the evolution of agentic AI, the trade-offs between orchestration and autonomy, and the technical innovations driving 11x’s products, Alice and Mike.</p><p>Prabhav breaks down the challenges of real-time voice AI, the complexities of multimodal agent interactions, and why the future of enterprise AI is about delivering measurable customer outcomes—not just automation. They also dive into the fast-moving landscape of model providers, the impact of open-source AI, and how startups can stay ahead in an environment of constant technological change.</p><p>Plus, they explore 11x’s bold decision to re-architect its platform from the ground up, the lessons learned from scaling AI-powered sales automation, and what it takes to build truly effective digital workers.</p><h3>Key Takeaways:</h3><ul><li>The difference between true AI agents and complex orchestrations—and why it matters.</li><li>How 11x built Alice and Mike to deliver human-like sales performance at scale.</li><li>The cutting-edge advancements shaping AI voice assistants and real-time multimodal interactions.</li><li>Lessons from rebuilding an AI platform while supporting a fast-growing customer base.</li><li>How AI startups can balance rapid iteration with long-term strategic bets.</li></ul><p>For anyone interested in AI-powered automation, enterprise sales, or the future of digital work, this episode offers a front-row seat to the latest innovations pushing the boundaries of AI agents.</p><p><strong>Learn more:</strong></p><p><a href="https://www.11x.ai/demo"><strong>11x</strong></a><br /><br /><strong>Follow everybody on X:</strong></p><p><a href="https://x.com/prabhavjain">Prabhav Jain</a></p><p><a href="https://x.com/joeschmidtiv?lang=en">Joe Schmidt</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Thu, 20 Mar 2025 17:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Prabhav Jain, Joe Schmidt)</author>
      <link>https://ai-a16z.simplecast.com/episodes/the-future-of-digital-workers-x2kUEf_N</link>
      <media:thumbnail height="720" url="https://image.simplecastcdn.com/images/c816dcf3-bf6b-4383-bfd0-ce36db7a9d83/c3de7e75-488b-4440-a4ba-fa87821697b4/ai-20-20a16z-20pod-20-20the-20future-20of-20-e2-80-a8digital-20workers-209-16.jpg" width="1280"/>
      <content:encoded><![CDATA[<p>In this episode of <i>AI + a16z</i>, a16z Partner Joe Schmidt sits down with 11x CTO Prabhav Jain for an inside look at how AI-powered digital workers are reshaping sales and revenue operations. They discuss the evolution of agentic AI, the trade-offs between orchestration and autonomy, and the technical innovations driving 11x’s products, Alice and Mike.</p><p>Prabhav breaks down the challenges of real-time voice AI, the complexities of multimodal agent interactions, and why the future of enterprise AI is about delivering measurable customer outcomes—not just automation. They also dive into the fast-moving landscape of model providers, the impact of open-source AI, and how startups can stay ahead in an environment of constant technological change.</p><p>Plus, they explore 11x’s bold decision to re-architect its platform from the ground up, the lessons learned from scaling AI-powered sales automation, and what it takes to build truly effective digital workers.</p><h3>Key Takeaways:</h3><ul><li>The difference between true AI agents and complex orchestrations—and why it matters.</li><li>How 11x built Alice and Mike to deliver human-like sales performance at scale.</li><li>The cutting-edge advancements shaping AI voice assistants and real-time multimodal interactions.</li><li>Lessons from rebuilding an AI platform while supporting a fast-growing customer base.</li><li>How AI startups can balance rapid iteration with long-term strategic bets.</li></ul><p>For anyone interested in AI-powered automation, enterprise sales, or the future of digital work, this episode offers a front-row seat to the latest innovations pushing the boundaries of AI agents.</p><p><strong>Learn more:</strong></p><p><a href="https://www.11x.ai/demo"><strong>11x</strong></a><br /><br /><strong>Follow everybody on X:</strong></p><p><a href="https://x.com/prabhavjain">Prabhav Jain</a></p><p><a href="https://x.com/joeschmidtiv?lang=en">Joe Schmidt</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="25456741" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/3b96c967-9367-4e4d-a162-cd851754b902/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=3b96c967-9367-4e4d-a162-cd851754b902&amp;feed=Hb_IuXOo"/>
      <itunes:title>The Future of Digital Workers</itunes:title>
      <itunes:author>Prabhav Jain, Joe Schmidt</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/c816dcf3-bf6b-4383-bfd0-ce36db7a9d83/6f6a43fd-d840-457f-8db4-cd2392e321b4/3000x3000/ai-20-20a16z-20pod-20-20the-20future-20of-20-e2-80-a8digital-20workers-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>00:26:31</itunes:duration>
      <itunes:summary></itunes:summary>
      <itunes:subtitle></itunes:subtitle>
      <itunes:keywords>agents, digital workers, llms, agent workers, ai agents, large language models, agent experience</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>38</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">02ecf2c3-b3f8-48af-8110-e97a42b1b1c3</guid>
      <title>Building the Next Generation of Conversational AI</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.sesame.com/" target="_blank">Sesame</a> Cofounder and CTO Ankit Kumar joins a16z general partner Anjney Midha for a deep dive into the research and engineering behind their voice technology. They discuss the technical challenges of real-time speech generation, the trade-offs in balancing personality with efficiency, and why the team is open-sourcing key components of their model. Ankit breaks down the complexities of multimodal AI, full-duplex conversation modeling, and the computational optimizations that enable low-latency interactions. </p><p>They also explore the evolution of natural language as a user interface and its potential to redefine human-computer interaction.<br />Plus, we take audience questions on everything from scaling laws in speech synthesis to the role of in-context learning in making AI voices more expressive.</p><p><strong>Key Takeaways:</strong><br />How Sesame AI achieves natural voice interactions through real-time speech generation.</p><ul><li>The impact of open-sourcing their speech model and what it means for AI research.</li><li>The role of full-duplex modeling in improving AI responsiveness.</li><li>How computational efficiency and system latency shape AI conversation quality.</li><li>The growing role of natural language as a user interface in AI-driven experiences.</li></ul><p>For anyone interested in AI and voice technology, this episode offers an in-depth look at the latest advancements pushing the boundaries of human-computer interaction.</p><p><strong>Learn more:</strong></p><p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo">The Maya + Miles demo</a></p><p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank">Crossing the uncanny valley of conversational voice</a></p><p><a href="https://huggingface.co/sesame/csm-1b" target="_blank">Sesame CSM 1B model</a></p><p>Follow everybody on X:</p><p><a href="https://x.com/_apkumar" target="_blank">Ankit Kumar</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 14 Mar 2025 16:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Ankit Kumar, Anjney Midha)</author>
      <link>https://ai-a16z.simplecast.com/episodes/building-the-next-generation-of-conversational-ai-gN5BfFT9</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.sesame.com/" target="_blank">Sesame</a> Cofounder and CTO Ankit Kumar joins a16z general partner Anjney Midha for a deep dive into the research and engineering behind their voice technology. They discuss the technical challenges of real-time speech generation, the trade-offs in balancing personality with efficiency, and why the team is open-sourcing key components of their model. Ankit breaks down the complexities of multimodal AI, full-duplex conversation modeling, and the computational optimizations that enable low-latency interactions. </p><p>They also explore the evolution of natural language as a user interface and its potential to redefine human-computer interaction.<br />Plus, we take audience questions on everything from scaling laws in speech synthesis to the role of in-context learning in making AI voices more expressive.</p><p><strong>Key Takeaways:</strong><br />How Sesame AI achieves natural voice interactions through real-time speech generation.</p><ul><li>The impact of open-sourcing their speech model and what it means for AI research.</li><li>The role of full-duplex modeling in improving AI responsiveness.</li><li>How computational efficiency and system latency shape AI conversation quality.</li><li>The growing role of natural language as a user interface in AI-driven experiences.</li></ul><p>For anyone interested in AI and voice technology, this episode offers an in-depth look at the latest advancements pushing the boundaries of human-computer interaction.</p><p><strong>Learn more:</strong></p><p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice#demo">The Maya + Miles demo</a></p><p><a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice" target="_blank">Crossing the uncanny valley of conversational voice</a></p><p><a href="https://huggingface.co/sesame/csm-1b" target="_blank">Sesame CSM 1B model</a></p><p>Follow everybody on X:</p><p><a href="https://x.com/_apkumar" target="_blank">Ankit Kumar</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="97555060" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/da821d5f-f7b7-41d6-a979-771c8e5bf5d4/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=da821d5f-f7b7-41d6-a979-771c8e5bf5d4&amp;feed=Hb_IuXOo"/>
      <itunes:title>Building the Next Generation of Conversational AI</itunes:title>
      <itunes:author>Ankit Kumar, Anjney Midha</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/38786c8a-ff99-4433-8671-624db49b18d5/3000x3000/ai-20-20a16z-20pod-20-20building-20the-20next-20generation-20of-20conversational-20ai-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>01:41:37</itunes:duration>
      <itunes:summary></itunes:summary>
      <itunes:subtitle></itunes:subtitle>
      <itunes:keywords>generative ai, llms, ai/ml, ai, machine learning, natural language processing, large language model</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>37</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">aa96d88b-0261-43ac-9b56-b07abef23f63</guid>
      <title>Agent Experience: Building an Open Web for the AI Era</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.netlify.com/" target="_blank">Netlify</a> CEO and Cofounder Matt Biilmann joins a16z General Partner Martin Casado to explore how AI is reshaping web development — not just through faster code generation, but by fundamentally shifting how we think about building for the web. At the center of this shift is <strong>Agent Experience (AX)</strong>, a new paradigm where AI agents aren’t just tools, but active participants in development, shaping both the creative process and the underlying infrastructure.</p><p>Matt shares how Netlify is evolving to meet this future, why the next 100 million web developers will collaborate with AI, and what’s at stake if the web doesn’t adapt — will we see a thriving, open, AI-powered internet, or a future dominated by walled gardens?</p><p>Learn more:</p><p><a href="https://biilmann.blog/articles/introducing-ax/" target="_blank">Introducing AX: Why Agent Experience Matters</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/biilmann">Matt Biilmann</a></p><p><a href="https://x.com/martin_casado">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 7 Mar 2025 15:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Matt Biilmann, Martin Casado)</author>
      <link>https://ai-a16z.simplecast.com/episodes/agent-experience-building-an-open-web-for-the-ai-era-ssRRM1rQ</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.netlify.com/" target="_blank">Netlify</a> CEO and Cofounder Matt Biilmann joins a16z General Partner Martin Casado to explore how AI is reshaping web development — not just through faster code generation, but by fundamentally shifting how we think about building for the web. At the center of this shift is <strong>Agent Experience (AX)</strong>, a new paradigm where AI agents aren’t just tools, but active participants in development, shaping both the creative process and the underlying infrastructure.</p><p>Matt shares how Netlify is evolving to meet this future, why the next 100 million web developers will collaborate with AI, and what’s at stake if the web doesn’t adapt — will we see a thriving, open, AI-powered internet, or a future dominated by walled gardens?</p><p>Learn more:</p><p><a href="https://biilmann.blog/articles/introducing-ax/" target="_blank">Introducing AX: Why Agent Experience Matters</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/biilmann">Matt Biilmann</a></p><p><a href="https://x.com/martin_casado">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="39294475" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/9733bcdf-b3e3-4e7d-b75b-896f32d9e906/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=9733bcdf-b3e3-4e7d-b75b-896f32d9e906&amp;feed=Hb_IuXOo"/>
      <itunes:title>Agent Experience: Building an Open Web for the AI Era</itunes:title>
      <itunes:author>Matt Biilmann, Martin Casado</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/7e498b1f-16e0-4c28-b5d8-e0e1e135f811/3000x3000/ai-20-20a16z-20pod-20-20agent-20experience-20building-20an-20open-20web-20for-20the-20ai-20era.jpg?aid=rss_feed"/>
      <itunes:duration>00:40:55</itunes:duration>
      <itunes:summary>Netlify CEO and Cofounder Matt Biilmann joins a16z General Partner Martin Casado to explore how AI is reshaping web development .</itunes:summary>
      <itunes:subtitle>Netlify CEO and Cofounder Matt Biilmann joins a16z General Partner Martin Casado to explore how AI is reshaping web development .</itunes:subtitle>
      <itunes:keywords>coding, generative ai, web development, artificial intelligence, ai agents, ai/ml, software engineering</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>36</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">9df42864-e6ea-4c1a-acda-f8ed9614c0c2</guid>
      <title>What DeepSeek Means for Cybersecurity</title>
      <description><![CDATA[<p>In this episode of AI + a16z, a trio of security experts join a16z partner Joel de la Garza to discuss the security implications of the DeepSeek reasoning model that made waves recently. It's three separate discussions, focusing on different aspects of DeepSeek and the fast-moving world of generative AI.</p><p>The first segment, with Ian Webster of <a href="https://www.promptfoo.dev/">Promptfoo</a>, focuses on vulnerabilities within DeepSeek itself, and how users can protect themselves against backdoors, jailbreaks, and censorship. </p><p>The second segment, with Dylan Ayrey of Truffle Security, focuses on the advent of AI-generated code and how developers and security teams can ensure it's safe. As Dylan explains, many problem lie in how the underlying models were trained and how their security alignment was carried out.</p><p>The final segment features Brian Long of <a href="https://www.adaptivesecurity.com/">Adaptive</a>, who highlights a growing list of risk vectors for deepfakes and other threats that generative AI can exacerbate. In his view, it's up to individuals and organizations to keep sharp about what's possible — while the the arms race between hackers and white-hat AI agents kicks into gear.</p><p>Learn more: </p><p><a href="https://www.promptfoo.dev/blog/deepseek-redteam/">What Are the Security Risks of Deploying DeepSeek-R1?</a></p><p><a href="https://trufflesecurity.com/blog/research-finds-12-000-live-api-keys-and-passwords-in-deepseek-s-training-data" target="_blank">Research finds 12,000 ‘Live’ API Keys and Passwords in DeepSeek's Training Data</a></p><p>Follow everybody on social media:</p><p><a href="https://x.com/iwebst" target="_blank">Ian Webster</a></p><p><a href="https://x.com/insecurenature" target="_blank">Dylan Ayrey</a></p><p><a href="https://x.com/BrianCLong/" target="_blank">Brian Long</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 28 Feb 2025 15:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Joel de la Garza, Dylan Ayrey, Ian Webster, Brian Long)</author>
      <link>https://ai-a16z.simplecast.com/episodes/what-deepseek-means-for-cybersecurity-bzFt_hoW</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, a trio of security experts join a16z partner Joel de la Garza to discuss the security implications of the DeepSeek reasoning model that made waves recently. It's three separate discussions, focusing on different aspects of DeepSeek and the fast-moving world of generative AI.</p><p>The first segment, with Ian Webster of <a href="https://www.promptfoo.dev/">Promptfoo</a>, focuses on vulnerabilities within DeepSeek itself, and how users can protect themselves against backdoors, jailbreaks, and censorship. </p><p>The second segment, with Dylan Ayrey of Truffle Security, focuses on the advent of AI-generated code and how developers and security teams can ensure it's safe. As Dylan explains, many problem lie in how the underlying models were trained and how their security alignment was carried out.</p><p>The final segment features Brian Long of <a href="https://www.adaptivesecurity.com/">Adaptive</a>, who highlights a growing list of risk vectors for deepfakes and other threats that generative AI can exacerbate. In his view, it's up to individuals and organizations to keep sharp about what's possible — while the the arms race between hackers and white-hat AI agents kicks into gear.</p><p>Learn more: </p><p><a href="https://www.promptfoo.dev/blog/deepseek-redteam/">What Are the Security Risks of Deploying DeepSeek-R1?</a></p><p><a href="https://trufflesecurity.com/blog/research-finds-12-000-live-api-keys-and-passwords-in-deepseek-s-training-data" target="_blank">Research finds 12,000 ‘Live’ API Keys and Passwords in DeepSeek's Training Data</a></p><p>Follow everybody on social media:</p><p><a href="https://x.com/iwebst" target="_blank">Ian Webster</a></p><p><a href="https://x.com/insecurenature" target="_blank">Dylan Ayrey</a></p><p><a href="https://x.com/BrianCLong/" target="_blank">Brian Long</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="50143860" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/0a928bae-cad8-4bbd-b520-485baefe3d60/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=0a928bae-cad8-4bbd-b520-485baefe3d60&amp;feed=Hb_IuXOo"/>
      <itunes:title>What DeepSeek Means for Cybersecurity</itunes:title>
      <itunes:author>Joel de la Garza, Dylan Ayrey, Ian Webster, Brian Long</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/edd57c7d-cda6-4555-b4e5-e2fa645a0c90/3000x3000/ai-20-20a16z-20pod-20-20what-20deepseek-20means-20for-20cybersecurity.jpg?aid=rss_feed"/>
      <itunes:duration>00:52:13</itunes:duration>
      <itunes:summary>A trio of security experts — Ian Webster of Promptfoo, Dylan Ayrey of Truffle Security, and Brian Long of Adaptive — discuss the security implications of the DeepSeek reasoning model that made waves recently.</itunes:summary>
      <itunes:subtitle>A trio of security experts — Ian Webster of Promptfoo, Dylan Ayrey of Truffle Security, and Brian Long of Adaptive — discuss the security implications of the DeepSeek reasoning model that made waves recently.</itunes:subtitle>
      <itunes:keywords>deepseek, generative ai, artificial intelligence, deepfakes, ai/ml, ai, machine learning, hacking, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>35</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">fc376253-ca3c-4432-8251-671823201be2</guid>
      <title>Agents, Lawyers, and LLMs</title>
      <description><![CDATA[<p>In this episode of AI + a16z, Aatish Nayak, head of product at <a href="https://www.harvey.ai/" target="_blank">Harvey</a>, sits down with a16z partner Kimberly Tan to share his experience building AI products for enterprises — including the legal profession — and how to address areas like UX, trust, and customer engagement. Importantly, Aatish explains, industries like law don't need AGI or even the latest and greatest models; they need products that augment their existing workflows so they can better serve clients and still make it home for dinner.</p><p>Learn more:</p><p><a href="https://github.com/harveyai/biglaw-bench" target="_blank">BigLaw Bench</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/nayakkayak" target="_blank">Aatish Nayak</a></p><p><a href="https://x.com/kimberlywtan" target="_blank">Kimberly Tan</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 21 Feb 2025 15:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Aatish Nayak, Kimberly Tan)</author>
      <link>https://ai-a16z.simplecast.com/episodes/agents-lawyers-and-llms-CV33MiPf</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, Aatish Nayak, head of product at <a href="https://www.harvey.ai/" target="_blank">Harvey</a>, sits down with a16z partner Kimberly Tan to share his experience building AI products for enterprises — including the legal profession — and how to address areas like UX, trust, and customer engagement. Importantly, Aatish explains, industries like law don't need AGI or even the latest and greatest models; they need products that augment their existing workflows so they can better serve clients and still make it home for dinner.</p><p>Learn more:</p><p><a href="https://github.com/harveyai/biglaw-bench" target="_blank">BigLaw Bench</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/nayakkayak" target="_blank">Aatish Nayak</a></p><p><a href="https://x.com/kimberlywtan" target="_blank">Kimberly Tan</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="38637443" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/62b3993f-4c95-4d9e-9bee-1fa26f41211d/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=62b3993f-4c95-4d9e-9bee-1fa26f41211d&amp;feed=Hb_IuXOo"/>
      <itunes:title>Agents, Lawyers, and LLMs</itunes:title>
      <itunes:author>Aatish Nayak, Kimberly Tan</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/95b4f9c5-9889-48b0-9fe7-dad6d8263334/3000x3000/ai-20-20a16z-20pod-20-20agents-20lawyers-20and-20llms-20ai-20goes-20white-collar-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>00:40:14</itunes:duration>
      <itunes:summary>Aatish Nayak, head of product at Harvey, shares his experience building AI products for enterprises — including the legal profession — and how to address areas like UX, trust, and customer engagement.</itunes:summary>
      <itunes:subtitle>Aatish Nayak, head of product at Harvey, shares his experience building AI products for enterprises — including the legal profession — and how to address areas like UX, trust, and customer engagement.</itunes:subtitle>
      <itunes:keywords>llms, legal, ai/ml, ai, large language models</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>34</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">8c1c22bc-2e12-4b9f-ba31-bb61f8692638</guid>
      <title>Reasoning Models Are Remaking Professional Services</title>
      <description><![CDATA[<p>In this episode of AI + a16z, a16z partner Alex Immerman sits down with <a href="https://www.hebbia.com/" target="_blank">Hebbia </a>founder and CEO George Sivulka to discuss the potential for reasoning models and AI agents to supercharge knowledge-worker productivity — and the global economy along with it. As George explains, his customers are already saving significant time and and effort on important, but monotonous, tasks, and improved models paired with savvy users will continue to reshape how industries including finance, law, and other professional services operate.</p><p>Follow everyone on X:</p><p><a href="https://x.com/gsivulka" target="_blank">George Sivulka</a></p><p><a href="https://x.com/aleximm" target="_blank">Alex Immerman</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 14 Feb 2025 18:38:36 +0000</pubDate>
      <author>dharris@a16z.com (George Sivulka, Alex Immerman)</author>
      <link>https://ai-a16z.simplecast.com/episodes/reasoning-models-are-remaking-professional-services-Kt3C1oXU</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, a16z partner Alex Immerman sits down with <a href="https://www.hebbia.com/" target="_blank">Hebbia </a>founder and CEO George Sivulka to discuss the potential for reasoning models and AI agents to supercharge knowledge-worker productivity — and the global economy along with it. As George explains, his customers are already saving significant time and and effort on important, but monotonous, tasks, and improved models paired with savvy users will continue to reshape how industries including finance, law, and other professional services operate.</p><p>Follow everyone on X:</p><p><a href="https://x.com/gsivulka" target="_blank">George Sivulka</a></p><p><a href="https://x.com/aleximm" target="_blank">Alex Immerman</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="36289548" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/c34f4dad-9d16-455a-9593-1fb0224a6de0/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=c34f4dad-9d16-455a-9593-1fb0224a6de0&amp;feed=Hb_IuXOo"/>
      <itunes:title>Reasoning Models Are Remaking Professional Services</itunes:title>
      <itunes:author>George Sivulka, Alex Immerman</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/1b029183-df16-46c6-9135-1040b4047b61/3000x3000/ai-20-20a16z-20pod-20-20reasoning-20models-20are-20remaking-20professional-20services-201-1.jpg?aid=rss_feed"/>
      <itunes:duration>00:37:48</itunes:duration>
      <itunes:summary>Hebbia founder and CEO George Sivulka discusses the potential for reasoning models and AI agents to supercharge knowledge-worker productivity — and the global economy along with it.</itunes:summary>
      <itunes:subtitle>Hebbia founder and CEO George Sivulka discusses the potential for reasoning models and AI agents to supercharge knowledge-worker productivity — and the global economy along with it.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, generative agents, ai/ml, ai, large language models, machine learning, financial services</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>33</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">cfb6bb26-7fe0-4d7c-996d-9a0e3a65fa3b</guid>
      <title>Data Management for Enterprise LLMs</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://fivetran.com" target="_blank">Fivetran</a> cofounder and CEO George Fraser and a16z partner Guido Appenzeller discuss how LLMs fit into the data management picture within large enterprises. In order to take advantage of a potentially revolutionary technology, organizations don't need to rip out their existing infrastructure, but they do need to rethink their data hygiene so language models can understand it.</p><p>Follow everyone on X:</p><p><a href="https://x.com/frasergeorgew" target="_blank">George Fraser</a></p><p><a href="https://x.com/appenz" target="_blank">Guido Appenzeller</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 7 Feb 2025 15:00:00 +0000</pubDate>
      <author>dharris@a16z.com (George Fraser, Guido Appenzeller, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/data-management-for-enterprise-llms-qbA99Sv6</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://fivetran.com" target="_blank">Fivetran</a> cofounder and CEO George Fraser and a16z partner Guido Appenzeller discuss how LLMs fit into the data management picture within large enterprises. In order to take advantage of a potentially revolutionary technology, organizations don't need to rip out their existing infrastructure, but they do need to rethink their data hygiene so language models can understand it.</p><p>Follow everyone on X:</p><p><a href="https://x.com/frasergeorgew" target="_blank">George Fraser</a></p><p><a href="https://x.com/appenz" target="_blank">Guido Appenzeller</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="36674354" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/cc86bd42-1ba0-4509-a9ec-da5e73287cd4/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=cc86bd42-1ba0-4509-a9ec-da5e73287cd4&amp;feed=Hb_IuXOo"/>
      <itunes:title>Data Management for Enterprise LLMs</itunes:title>
      <itunes:author>George Fraser, Guido Appenzeller, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/e9186ed9-2b45-43dd-a77a-2b571411e16d/3000x3000/ai-20-20a16z-20pod-20-20data-20management-20for-20enterprise-20llms-20-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:38:12</itunes:duration>
      <itunes:summary>Fivetran cofounder and CEO George Fraser discusses how LLMs fit into the data management picture within large enterprises — including what needs to change and what does not.</itunes:summary>
      <itunes:subtitle>Fivetran cofounder and CEO George Fraser discusses how LLMs fit into the data management picture within large enterprises — including what needs to change and what does not.</itunes:subtitle>
      <itunes:keywords>big data, llms, artificial intelligence, ai/ml, ai, large language models, databases, data science</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>32</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">7544421e-e7e3-4fe2-9654-d7846054a191</guid>
      <title>From NLP to LLMs: The Quest for a Reliable Chatbot</title>
      <description><![CDATA[<p>In this episode of AI + a16z, a16z General Partner Martin Casado and <a href="https://rasa.com/">Rasa</a> cofounder and CEO Alan Nichol discuss the past, present, and future of AI agents and chatbots. Alan shares his history working to solve this problem with traditional natural language processing (NLP), expounds on how large language models (LLMs) are helping to dull the many sharp corners of natural-language interactions, and explains how pairing them with inflexible business logic is a great combination.</p><p>Learn more:</p><p><a href="https://arxiv.org/abs/2402.12234">Task-Oriented Dialogue with In-Context Learning</a></p><p><a href="https://arxiv.org/abs/2404.06921" target="_blank">GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Application</a></p><p><a href="https://calmsummit24.com/" target="_blank">CALM Summit</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/alanmnichol/" target="_blank">Alan Nichol</a></p><p><a href="https://x.com/martin_casado" target="_blank">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 10 Jan 2025 17:30:00 +0000</pubDate>
      <author>dharris@a16z.com (Martin Casado, Alan Nichol)</author>
      <link>https://ai-a16z.simplecast.com/episodes/from-nlp-to-llms-the-quest-for-a-reliable-chatbot-3iZNGUgQ</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, a16z General Partner Martin Casado and <a href="https://rasa.com/">Rasa</a> cofounder and CEO Alan Nichol discuss the past, present, and future of AI agents and chatbots. Alan shares his history working to solve this problem with traditional natural language processing (NLP), expounds on how large language models (LLMs) are helping to dull the many sharp corners of natural-language interactions, and explains how pairing them with inflexible business logic is a great combination.</p><p>Learn more:</p><p><a href="https://arxiv.org/abs/2402.12234">Task-Oriented Dialogue with In-Context Learning</a></p><p><a href="https://arxiv.org/abs/2404.06921" target="_blank">GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Application</a></p><p><a href="https://calmsummit24.com/" target="_blank">CALM Summit</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/alanmnichol/" target="_blank">Alan Nichol</a></p><p><a href="https://x.com/martin_casado" target="_blank">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="36820157" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/93f356bf-cdfc-4adf-bdc6-42ffe41c7dd9/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=93f356bf-cdfc-4adf-bdc6-42ffe41c7dd9&amp;feed=Hb_IuXOo"/>
      <itunes:title>From NLP to LLMs: The Quest for a Reliable Chatbot</itunes:title>
      <itunes:author>Martin Casado, Alan Nichol</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/7c2e93c6-3294-44b5-8d9d-f31984c6ad37/3000x3000/ai-20-20a16z-20pod-20-20from-20nlp-20to-20llms-20the-20quest-20for-20a-20reliable-20chatbot-20-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:38:21</itunes:duration>
      <itunes:summary>a16z General Partner Martin Casado and Rasa cofounder and CEO Alan Nichol discuss the past, present, and future of AI agents and chatbots. </itunes:summary>
      <itunes:subtitle>a16z General Partner Martin Casado and Rasa cofounder and CEO Alan Nichol discuss the past, present, and future of AI agents and chatbots. </itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, chatbot, ai, large language models, machine learning, natural language processing</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>31</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">1bfcc384-ae14-4659-8642-50e2d668fcc6</guid>
      <title>Best of the Year: Building AI Companies</title>
      <description><![CDATA[<p>A 2024 highlight reel, featuring founders sharing their insights, advice, and experiences building AI companies — from foundation-model labs to vertical applications. Topics include:</p><ul><li>Building AI tools for developers</li><li>Getting into AI as a systems expert</li><li>The researcher-to-founder journey</li><li>Founding AI companies in specific industries</li><li>Early lessons from selling AI agents</li><li>And more</li></ul><p>Companies include:</p><ul><li>Ambience</li><li>Anyscale</li><li>Black Forest Labs</li><li>CommandZero</li><li>Databricks</li><li>Decagon</li><li>Ideogram</li><li>Inngest</li><li>Replicate</li><li>Socket</li></ul>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 27 Dec 2024 17:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Derrick Harris, Jesse Zhang, Dean de Beer, Nikhil Buduma, Ben Firshman, Tony Holdstock-Brown, Robert Nishihara, Anjney Midha, Jennifer Li, Robin Rombach, Patrick Esser, Andreas Blattmann, Mohammad Norouzi, Feross Aboukhadijeh, Naveen Rao)</author>
      <link>https://ai-a16z.simplecast.com/episodes/best-of-the-year-building-ai-companies-i0QzGcKn</link>
      <content:encoded><![CDATA[<p>A 2024 highlight reel, featuring founders sharing their insights, advice, and experiences building AI companies — from foundation-model labs to vertical applications. Topics include:</p><ul><li>Building AI tools for developers</li><li>Getting into AI as a systems expert</li><li>The researcher-to-founder journey</li><li>Founding AI companies in specific industries</li><li>Early lessons from selling AI agents</li><li>And more</li></ul><p>Companies include:</p><ul><li>Ambience</li><li>Anyscale</li><li>Black Forest Labs</li><li>CommandZero</li><li>Databricks</li><li>Decagon</li><li>Ideogram</li><li>Inngest</li><li>Replicate</li><li>Socket</li></ul>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="44414712" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/c3c8210f-72bc-4239-a4f3-4adb40271ecc/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=c3c8210f-72bc-4239-a4f3-4adb40271ecc&amp;feed=Hb_IuXOo"/>
      <itunes:title>Best of the Year: Building AI Companies</itunes:title>
      <itunes:author>Derrick Harris, Jesse Zhang, Dean de Beer, Nikhil Buduma, Ben Firshman, Tony Holdstock-Brown, Robert Nishihara, Anjney Midha, Jennifer Li, Robin Rombach, Patrick Esser, Andreas Blattmann, Mohammad Norouzi, Feross Aboukhadijeh, Naveen Rao</itunes:author>
      <itunes:duration>00:46:15</itunes:duration>
      <itunes:summary>A 2024 highlight reel, featuring founders sharing their insights, advice, and experiences building AI companies — from foundation-model labs to vertical applications.</itunes:summary>
      <itunes:subtitle>A 2024 highlight reel, featuring founders sharing their insights, advice, and experiences building AI companies — from foundation-model labs to vertical applications.</itunes:subtitle>
      <itunes:keywords>generative ai, artificial intelligence, ai/ml, llmls, ai, entrepreneurship, large language models, machine learning, startups</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>30</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">ac9904e4-0950-4c7c-86e9-767174fb2c4a</guid>
      <title>Can AI Agents Finally Fix Customer Support?</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://decagon.ai/" target="_blank">Decagon</a> cofounder/CEO Jesse Zhang and a16z partner Kimberly Tan discuss how LLMs are reshaping customer support, the strong market demand for AI agents, and how AI agents give startups a a new pricing model to help disrupt incumbents.</p><p>Here's an excerpt of Jesse explaining how conversation-based pricing can win over customers who are used to traditional seat-based pricing:</p><p>"Our view on this is that, in the past, software is based per seat because it's roughly scaled based on the number of people that can take advantage of the software.</p><p>"With most AI agents, the value . . . doesn't really scale in terms of the number of people that are maintaining it; it's just the amount of work output. . . . The pricing that you want to provide has to be a model where the more work you do, the more that gets paid.  </p><p>"So for us, there's two obvious ways to do that: you can pay per conversation, or you can pay per resolution. One fun learning for us has been that most people have opted into the per-conversation model . . .  It just creates a lot more simplicity and predictability.</p><p>. . .</p><p>"It's a little bit tricky for incumbents if they're trying to launch agents because it just cannibalizes their seat-based model. . . . Incumbents have less risk tolerance, naturally, because they have a ton of customers. And if they're iterating quickly and something doesn't go well, that's a big loss for them. Whereas, younger companies can always iterate a lot faster, and the iteration process just inherently leads to better product. . .  </p><p>"We always want to pride ourselves on shipping speed, quality of the product, and just how hardcore our team is in terms of delivering things."</p><p>Learn more:</p><p><a href="https://a16z.com/rip-to-rpa-the-rise-of-intelligent-automation/" target="_blank">RIP to RPA: The Rise of Intelligent Automation</a></p><p><a href="https://a16z.com/big-ideas-in-tech-2025/" target="_blank">Big Ideas in Tech for 2025</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/thejessezhang" target="_blank">Jesse Zhang</a></p><p><a href="https://x.com/kimberlywtan" target="_blank">Kimberly Tan</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Wed, 18 Dec 2024 17:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Jesse Zhang, Kimberly Tan, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/can-ai-agents-finally-fix-customer-support-Gsu0K0AX</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://decagon.ai/" target="_blank">Decagon</a> cofounder/CEO Jesse Zhang and a16z partner Kimberly Tan discuss how LLMs are reshaping customer support, the strong market demand for AI agents, and how AI agents give startups a a new pricing model to help disrupt incumbents.</p><p>Here's an excerpt of Jesse explaining how conversation-based pricing can win over customers who are used to traditional seat-based pricing:</p><p>"Our view on this is that, in the past, software is based per seat because it's roughly scaled based on the number of people that can take advantage of the software.</p><p>"With most AI agents, the value . . . doesn't really scale in terms of the number of people that are maintaining it; it's just the amount of work output. . . . The pricing that you want to provide has to be a model where the more work you do, the more that gets paid.  </p><p>"So for us, there's two obvious ways to do that: you can pay per conversation, or you can pay per resolution. One fun learning for us has been that most people have opted into the per-conversation model . . .  It just creates a lot more simplicity and predictability.</p><p>. . .</p><p>"It's a little bit tricky for incumbents if they're trying to launch agents because it just cannibalizes their seat-based model. . . . Incumbents have less risk tolerance, naturally, because they have a ton of customers. And if they're iterating quickly and something doesn't go well, that's a big loss for them. Whereas, younger companies can always iterate a lot faster, and the iteration process just inherently leads to better product. . .  </p><p>"We always want to pride ourselves on shipping speed, quality of the product, and just how hardcore our team is in terms of delivering things."</p><p>Learn more:</p><p><a href="https://a16z.com/rip-to-rpa-the-rise-of-intelligent-automation/" target="_blank">RIP to RPA: The Rise of Intelligent Automation</a></p><p><a href="https://a16z.com/big-ideas-in-tech-2025/" target="_blank">Big Ideas in Tech for 2025</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/thejessezhang" target="_blank">Jesse Zhang</a></p><p><a href="https://x.com/kimberlywtan" target="_blank">Kimberly Tan</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="42442544" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/1a73d06e-0d7c-4c39-aef8-b6be315a55e0/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=1a73d06e-0d7c-4c39-aef8-b6be315a55e0&amp;feed=Hb_IuXOo"/>
      <itunes:title>Can AI Agents Finally Fix Customer Support?</itunes:title>
      <itunes:author>Jesse Zhang, Kimberly Tan, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/61c96754-aef9-4b6c-ba37-070afc772cce/3000x3000/ai-20-20a16z-20pod-20-20can-20ai-20agents-20finally-20fix-20customer-20support-1.jpg?aid=rss_feed"/>
      <itunes:duration>00:44:12</itunes:duration>
      <itunes:summary>Decagon CEO Jesse Zhang and a16z partner Kimberly Tan discuss how LLMs are reshaping customer support, the strong market demand for AI agents, and how AI agents give startups a a new pricing model to help disrupt incumbents.</itunes:summary>
      <itunes:subtitle>Decagon CEO Jesse Zhang and a16z partner Kimberly Tan discuss how LLMs are reshaping customer support, the strong market demand for AI agents, and how AI agents give startups a a new pricing model to help disrupt incumbents.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, large language models, startups</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>29</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">9a0070e9-c9a3-4c90-b4f2-90842ac08b3f</guid>
      <title>REPLAY: Scoping the Enterprise LLM Market</title>
      <description><![CDATA[<p>This is a replay of our first episode from April 12, featuring Databricks VP of AI Naveen Rao and a16z partner Matt Bornstein discussing enterprise LLM adoption, hardware platforms, and what it means for AI to be mainstream. If you're unfamiliar with Naveen, he has been in the AI space for more than decade working on everything from custom hardware to LLMs, and has founded two successful startups — Nervana Systems and MosaicML.</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Sat, 30 Nov 2024 15:00:46 +0000</pubDate>
      <author>dharris@a16z.com (Naveen Rao, Matt Bornstein, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/replay-scoping-the-enterprise-llm-market-AkytHnbF</link>
      <content:encoded><![CDATA[<p>This is a replay of our first episode from April 12, featuring Databricks VP of AI Naveen Rao and a16z partner Matt Bornstein discussing enterprise LLM adoption, hardware platforms, and what it means for AI to be mainstream. If you're unfamiliar with Naveen, he has been in the AI space for more than decade working on everything from custom hardware to LLMs, and has founded two successful startups — Nervana Systems and MosaicML.</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="41479148" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/21f6e2f2-86c8-4b36-829f-f6c4306f1e44/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=21f6e2f2-86c8-4b36-829f-f6c4306f1e44&amp;feed=Hb_IuXOo"/>
      <itunes:title>REPLAY: Scoping the Enterprise LLM Market</itunes:title>
      <itunes:author>Naveen Rao, Matt Bornstein, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/eb9f53a1-26c0-4daa-b9fc-9f2ba824ce14/3000x3000/ai-20-20a16z-20pod-20scoping-20the-20enterprise-20llm-20market-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:43:12</itunes:duration>
      <itunes:summary>This is a replay of our first episode from April 12, featuring Databricks VP of AI Naveen Rao and a16z partner Matt Bornstein discussing enterprise LLM adoption, hardware platforms, and what it means for AI to be mainstream.</itunes:summary>
      <itunes:subtitle>This is a replay of our first episode from April 12, featuring Databricks VP of AI Naveen Rao and a16z partner Matt Bornstein discussing enterprise LLM adoption, hardware platforms, and what it means for AI to be mainstream.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, foundation models, ai, large language models, machine learning, enterprise tech</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>28</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">b5b3e6ce-4683-4c9c-89d4-b2b52db0014e</guid>
      <title>Building Developers Tools, From Docker to Diffusion Models</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://replicate.com/" target="_blank">Replicate</a> cofounder and CEO Ben Firshman, and a16z partner Matt Bornstein, discuss the art of building products and companies that appeal to software developers. Ben was the creator of Docker Compose, and Replicate has a thriving community of developers hosting and fine-tuning their own models to power AI-based applications.</p><p>Here's an excerpt of Ben and Matt discussing the difference in the variety of applications built using multimedia models compared with language models:</p><p><strong>Matt: </strong>"I've noticed there's a lot of really diverse multimedia AI apps out there. Meaning that when you give someone an amazing primitive, like a FLUX API call or a Stable Diffusion API call, and Replicate, there's so many things they can do with it. And we actually see that happening — versus with language, where all LLM apps look kind of the same if you squint a little bit.</p><p>"It's like you chat with something — there's obviously code, there's language, there's a few different things — but I've been surprised that even today we don't see as many apps built on language models as we do based on, say, image models."</p><p><strong>Ben: </strong>"It certainly maps with what we're seeing, as well. I think these language models, beyond just chat apps, are particularly good at turning unstructured information into structured information. Which is actually kind of magical. And computers haven't been very good at that before. That is really a kind of cool use case for it. </p><p>"But with these image models and video models and things like that, people are creating lots of new products that were not possible before — things that were just impossible for computers to do. So yeah, I'm certainly more excited by all the magical things these multimedia models can make."</p><p>"But with these image models and video models and things like that, people are creating lots of new products that were just not possible before — things that were just impossible for computers to do. So yeah, I'm certainly more excited by all the magical things these multimedia models can make."</p><p>Follow everyone on X:</p><p><a href="https://x.com/bfirsh" target="_blank">Ben Firshman</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p><p>Learn more:</p><p><a href="https://replicate.com/explore" target="_blank">Replicate's AI model hub</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 15 Nov 2024 15:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Ben Firshman, Matt Bornstein, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/building-developers-tools-from-docker-to-diffusion-models-XJC7uYVj</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://replicate.com/" target="_blank">Replicate</a> cofounder and CEO Ben Firshman, and a16z partner Matt Bornstein, discuss the art of building products and companies that appeal to software developers. Ben was the creator of Docker Compose, and Replicate has a thriving community of developers hosting and fine-tuning their own models to power AI-based applications.</p><p>Here's an excerpt of Ben and Matt discussing the difference in the variety of applications built using multimedia models compared with language models:</p><p><strong>Matt: </strong>"I've noticed there's a lot of really diverse multimedia AI apps out there. Meaning that when you give someone an amazing primitive, like a FLUX API call or a Stable Diffusion API call, and Replicate, there's so many things they can do with it. And we actually see that happening — versus with language, where all LLM apps look kind of the same if you squint a little bit.</p><p>"It's like you chat with something — there's obviously code, there's language, there's a few different things — but I've been surprised that even today we don't see as many apps built on language models as we do based on, say, image models."</p><p><strong>Ben: </strong>"It certainly maps with what we're seeing, as well. I think these language models, beyond just chat apps, are particularly good at turning unstructured information into structured information. Which is actually kind of magical. And computers haven't been very good at that before. That is really a kind of cool use case for it. </p><p>"But with these image models and video models and things like that, people are creating lots of new products that were not possible before — things that were just impossible for computers to do. So yeah, I'm certainly more excited by all the magical things these multimedia models can make."</p><p>"But with these image models and video models and things like that, people are creating lots of new products that were just not possible before — things that were just impossible for computers to do. So yeah, I'm certainly more excited by all the magical things these multimedia models can make."</p><p>Follow everyone on X:</p><p><a href="https://x.com/bfirsh" target="_blank">Ben Firshman</a></p><p><a href="https://x.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p><p>Learn more:</p><p><a href="https://replicate.com/explore" target="_blank">Replicate's AI model hub</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="40145858" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/43767f7b-31f3-441f-8628-7f55150fd7f3/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=43767f7b-31f3-441f-8628-7f55150fd7f3&amp;feed=Hb_IuXOo"/>
      <itunes:title>Building Developers Tools, From Docker to Diffusion Models</itunes:title>
      <itunes:author>Ben Firshman, Matt Bornstein, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/dc93babd-2e1c-4e58-b415-2aa0e2bf1a83/3000x3000/ai-20-20a16z-20pod-20-20building-20developer-20products-20from-20docker-20to-20diffusion-20models-20-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:41:49</itunes:duration>
      <itunes:summary>Replicate cofounder and CEO Ben Firshman, and a16z partner Matt Bornstein, discuss the art of building products and companies that appeal to software developers. Ben was the creator of Docker Compose, and Replicate has a thriving community of developers hosting and fine-tuning their own models to power AI-based applications.</itunes:summary>
      <itunes:subtitle>Replicate cofounder and CEO Ben Firshman, and a16z partner Matt Bornstein, discuss the art of building products and companies that appeal to software developers. Ben was the creator of Docker Compose, and Replicate has a thriving community of developers hosting and fine-tuning their own models to power AI-based applications.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, ai, large language models, machine learning, programming, open source, software engineering, software developer</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>27</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">7a5d9cd6-8331-400c-b90c-4edcb08cc695</guid>
      <title>The Best Way to Achieve AGI Is to Invent It</title>
      <description><![CDATA[<p>Longtime machine-learning researcher, and University of Washington Professor Emeritus, Pedro Domingos joins a16z General Partner Martin Casado to discuss the state of artificial intelligence, whether we're really on a path toward AGI, and the value of expressing unpopular opinions.  It's a very insightful discussion as we head into an era of mainstream AI adoption, and ask big questions about how to ramp up progress and diversify research directions.</p><p>Here's an excerpt of Pedro sharing his thoughts on the increasing cost of frontier models and whether that's the right direction:</p><p><i>"if you believe the scaling laws hold and the scaling laws will take us to human-level intelligence, then, hey, it's worth a lot of investment. That's one part, but that may be wrong. The other part, however, is that to do that, we need exploding amounts of compute. </i></p><p><i>"If if I had to predict what's going to happen, it's that we do not need a trillion dollars to reach AGI at all. So if you spend a trillion dollars reaching AGI, this is a very bad investment."</i></p><p>Learn more:</p><p><a href="https://www.basicbooks.com/titles/pedro-domingos/the-master-algorithm/9780465061921/" target="_blank">The Master Algorithm</a></p><p><a href="https://www.amazon.com/dp/B0D894594T/" target="_blank">2040: A Silicon Valley Satire</a></p><p><a href="https://a16z.com/the-economic-case-for-generative-ai-and-foundation-models/" target="_blank">The Economic Case for Generative AI and Foundation Models</a></p><p>Follow everyone on Z:</p><p><a href="http://twitter.com/pmddomingos" target="_blank">Pedro Domingos</a></p><p><a href="https://x.com/martin_casado" target="_blank">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Mon, 4 Nov 2024 15:30:00 +0000</pubDate>
      <author>dharris@a16z.com (Martin Casado, Pedro Domingos)</author>
      <link>https://ai-a16z.simplecast.com/episodes/the-best-way-to-achieve-agi-is-to-invent-it-IOxN7syq</link>
      <content:encoded><![CDATA[<p>Longtime machine-learning researcher, and University of Washington Professor Emeritus, Pedro Domingos joins a16z General Partner Martin Casado to discuss the state of artificial intelligence, whether we're really on a path toward AGI, and the value of expressing unpopular opinions.  It's a very insightful discussion as we head into an era of mainstream AI adoption, and ask big questions about how to ramp up progress and diversify research directions.</p><p>Here's an excerpt of Pedro sharing his thoughts on the increasing cost of frontier models and whether that's the right direction:</p><p><i>"if you believe the scaling laws hold and the scaling laws will take us to human-level intelligence, then, hey, it's worth a lot of investment. That's one part, but that may be wrong. The other part, however, is that to do that, we need exploding amounts of compute. </i></p><p><i>"If if I had to predict what's going to happen, it's that we do not need a trillion dollars to reach AGI at all. So if you spend a trillion dollars reaching AGI, this is a very bad investment."</i></p><p>Learn more:</p><p><a href="https://www.basicbooks.com/titles/pedro-domingos/the-master-algorithm/9780465061921/" target="_blank">The Master Algorithm</a></p><p><a href="https://www.amazon.com/dp/B0D894594T/" target="_blank">2040: A Silicon Valley Satire</a></p><p><a href="https://a16z.com/the-economic-case-for-generative-ai-and-foundation-models/" target="_blank">The Economic Case for Generative AI and Foundation Models</a></p><p>Follow everyone on Z:</p><p><a href="http://twitter.com/pmddomingos" target="_blank">Pedro Domingos</a></p><p><a href="https://x.com/martin_casado" target="_blank">Martin Casado</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="36517972" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/956722fc-aa85-4efd-bab0-d098de2e0fa3/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=956722fc-aa85-4efd-bab0-d098de2e0fa3&amp;feed=Hb_IuXOo"/>
      <itunes:title>The Best Way to Achieve AGI Is to Invent It</itunes:title>
      <itunes:author>Martin Casado, Pedro Domingos</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/e9bac886-1f78-4e25-b547-71feffa3e202/3000x3000/ai-20-20a16z-20pod-20-20the-20best-20way-20to-20achieve-20agi-20is-20to-20invent-20it-20-201080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:38:02</itunes:duration>
      <itunes:summary>Longtime machine-learning researcher Pedro Domingos joins a16z General Partner Martin Casado to discuss the state of artificial intelligence, whether we&apos;re really on a path toward AGI, and the value of expressing unpopular opinions.</itunes:summary>
      <itunes:subtitle>Longtime machine-learning researcher Pedro Domingos joins a16z General Partner Martin Casado to discuss the state of artificial intelligence, whether we&apos;re really on a path toward AGI, and the value of expressing unpopular opinions.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, ai, large language models, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>26</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">8088377e-db43-4e1f-ba18-879b0719668d</guid>
      <title>Neural Nets and Nobel Prizes: AI&apos;s 40-Year Journey from the Lab to Ubiquity</title>
      <description><![CDATA[<p>In this episode of AI + a16z, General Partner Anjney Midha shares his perspective on the recent collection of Nobel Prizes awarded to AI researchers in both Physics and Chemistry. He talks through how early work on neural networks in the 1980s spurred continuous advancement in the field — even through the "AI winter" — which resulted in today's extremely useful AI technologies.</p><p>Here's a sample of the discussion, in response to a question about whether we will see more high-quality research emerge from sources beyond large universities and commercial labs:</p><p><i>"It can be easy to conclude that the most impactful AI research still requires resources beyond the reach of most individuals or small teams. And that open source contributions, while valuable, are  unlikely to match the breakthroughs from well-funded labs. I've even heard heard some dismissive folks call it cute, and undermine the value of those.</i></p><p><i>"But on the other hand, I think that you could argue that open source and individual contributions are becoming increasingly more important in AI development. I think that the democratization of AI will lead probably to more diverse and innovative applications. And I think, in particular, the reason we should expect an explosion in home scientists — folks who aren't necessarily affiliated with a top-tier academic, or for that matter,  industry lab — is that as open source models get more and more accessible, the rate limiter really is on the creativity of somebody who's willing to apply the power of that model's computational ability to a novel domain. And there are just a ton of domains and combinatorial intersections of different disciplines.</i></p><p><i>"Our blind spot for traditional academia [is that] it's not particularly rewarding to veer off the publish-or-perish conference circuit. And if you're at a large industry lab and you're not contributing directly to the next model release, it's not that clear how you get rewarded. And so being an independent actually frees you up from the incentive misstructure, I think, of some of the larger labs. And if you get to leverage the millions of dollars that the Llama team spent on pre-training, applying it to data sets that nobody else has perused before, it results in pretty big breakthroughs."</i></p><p>Learn more:</p><p><a href="https://www.nobelprize.org/prizes/physics/2024/press-release/" target="_blank">They trained artificial neural networks using physics</a></p><p><a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/" target="_blank">They cracked the code for proteins’ amazing structures</a></p><p><a href="https://epochai.org/data/notable-ai-models?categorization=null#explore-the-data" target="_blank">Notable AI models by year</a></p><p>Follow on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 25 Oct 2024 18:30:00 +0000</pubDate>
      <author>dharris@a16z.com (Anjney Midha, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/neural-nets-and-nobel-prizes-9z3A0pnK</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, General Partner Anjney Midha shares his perspective on the recent collection of Nobel Prizes awarded to AI researchers in both Physics and Chemistry. He talks through how early work on neural networks in the 1980s spurred continuous advancement in the field — even through the "AI winter" — which resulted in today's extremely useful AI technologies.</p><p>Here's a sample of the discussion, in response to a question about whether we will see more high-quality research emerge from sources beyond large universities and commercial labs:</p><p><i>"It can be easy to conclude that the most impactful AI research still requires resources beyond the reach of most individuals or small teams. And that open source contributions, while valuable, are  unlikely to match the breakthroughs from well-funded labs. I've even heard heard some dismissive folks call it cute, and undermine the value of those.</i></p><p><i>"But on the other hand, I think that you could argue that open source and individual contributions are becoming increasingly more important in AI development. I think that the democratization of AI will lead probably to more diverse and innovative applications. And I think, in particular, the reason we should expect an explosion in home scientists — folks who aren't necessarily affiliated with a top-tier academic, or for that matter,  industry lab — is that as open source models get more and more accessible, the rate limiter really is on the creativity of somebody who's willing to apply the power of that model's computational ability to a novel domain. And there are just a ton of domains and combinatorial intersections of different disciplines.</i></p><p><i>"Our blind spot for traditional academia [is that] it's not particularly rewarding to veer off the publish-or-perish conference circuit. And if you're at a large industry lab and you're not contributing directly to the next model release, it's not that clear how you get rewarded. And so being an independent actually frees you up from the incentive misstructure, I think, of some of the larger labs. And if you get to leverage the millions of dollars that the Llama team spent on pre-training, applying it to data sets that nobody else has perused before, it results in pretty big breakthroughs."</i></p><p>Learn more:</p><p><a href="https://www.nobelprize.org/prizes/physics/2024/press-release/" target="_blank">They trained artificial neural networks using physics</a></p><p><a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/" target="_blank">They cracked the code for proteins’ amazing structures</a></p><p><a href="https://epochai.org/data/notable-ai-models?categorization=null#explore-the-data" target="_blank">Notable AI models by year</a></p><p>Follow on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="38599827" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/9d1e2204-1fa8-4038-b34a-eb3cf0e3ce42/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=9d1e2204-1fa8-4038-b34a-eb3cf0e3ce42&amp;feed=Hb_IuXOo"/>
      <itunes:title>Neural Nets and Nobel Prizes: AI&apos;s 40-Year Journey from the Lab to Ubiquity</itunes:title>
      <itunes:author>Anjney Midha, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/2afa11a1-5c34-4968-a722-ad6b7e2ede43/3000x3000/ai-a16z-pod-neural-nets-and-nobel-prizes-ais-40-year-journey-from-the-lab-to-ubiquity-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:40:12</itunes:duration>
      <itunes:summary>a16z General Partner Anjney Midha shares his perspective on the recent collection of Nobel Prizes awarded to AI researchers in both Physics and Chemistry. He talks through how early work on neural networks in the 1980s spurred continuous advancement in the field — even through the &quot;AI winter&quot; — which resulted in today&apos;s extremely useful AI technologies.</itunes:summary>
      <itunes:subtitle>a16z General Partner Anjney Midha shares his perspective on the recent collection of Nobel Prizes awarded to AI researchers in both Physics and Chemistry. He talks through how early work on neural networks in the 1980s spurred continuous advancement in the field — even through the &quot;AI winter&quot; — which resulted in today&apos;s extremely useful AI technologies.</itunes:subtitle>
      <itunes:keywords>generative ai, artificial intelligence, ai/ml, ai, machine learning, open source</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>25</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">a238f049-c33a-4939-9df4-e3319c4a4cc7</guid>
      <title>How GPU Access Helps AI Startups Be Agile</title>
      <description><![CDATA[<p>In this episode of AI + a16z, General Partner Anjney Midha explains the forces that lead to GPU shortages and price spikes, and how the firm mitigates these concerns for portfolio companies by supplying them with the GPUs they need through a program called Oxygen. The TL;DR version of the problem is that competition for GPU access favors large incumbents who can afford to outbid startups and commit to long contracts; when startups do buy or rent in bulk, they can be stuck with lots of GPUs and — absent training runs or ample customer demand for inference workloads — nothing to do with them. </p><p>Here is an excerpt of Anjney explaining how training versus inference workloads affect what level of resources a company needs at any given time:</p><p>"It comes down to whether the customer that's using them . . .  has a use that can really optimize the efficiency of those chips. As an example, if you happen to be an image model company or a video model company and you put a long-term contract on H100s this year, and you trained and put out a really good model and a product that a lot of people want to use, even though you're not training on the best and latest cluster next year, that's OK. Because you can essentially swap out your training workloads for your inference workloads on those H100s.</p><p>"The H100s are actually incredibly powerful chips that you can run really good inference workloads on. So as long as you have customers who want to run inference of your model on your infrastructure, then you can just redirect that capacity to them and then buy new [Nvidia] Blackwells for your training runs.</p><p>"Who it becomes really tricky for is people who bought a bunch, don't have demand from their customers for inference, and therefore are stuck doing training runs on that last-generation hardware. That's a tough place to be."</p><p>Learn more:</p><p><a href="https://a16z.com/navigating-the-high-cost-of-ai-compute/" target="_blank">Navigating the High Cost of GPU Compute</a></p><p><a href="https://a16z.com/podcast/chasing-silicon-the-race-for-gpus/" target="_blank">Chasing Silicon: The Race for GPUs</a></p><p><a href="https://a16z.com/podcast/remaking-the-ui-for-ai/" target="_blank">Remaking the UI for AI</a></p><p>Follow on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Wed, 23 Oct 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Anjney Midha, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/how-gpu-access-helps-ai-startups-be-agile-kyoI6Qvk</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, General Partner Anjney Midha explains the forces that lead to GPU shortages and price spikes, and how the firm mitigates these concerns for portfolio companies by supplying them with the GPUs they need through a program called Oxygen. The TL;DR version of the problem is that competition for GPU access favors large incumbents who can afford to outbid startups and commit to long contracts; when startups do buy or rent in bulk, they can be stuck with lots of GPUs and — absent training runs or ample customer demand for inference workloads — nothing to do with them. </p><p>Here is an excerpt of Anjney explaining how training versus inference workloads affect what level of resources a company needs at any given time:</p><p>"It comes down to whether the customer that's using them . . .  has a use that can really optimize the efficiency of those chips. As an example, if you happen to be an image model company or a video model company and you put a long-term contract on H100s this year, and you trained and put out a really good model and a product that a lot of people want to use, even though you're not training on the best and latest cluster next year, that's OK. Because you can essentially swap out your training workloads for your inference workloads on those H100s.</p><p>"The H100s are actually incredibly powerful chips that you can run really good inference workloads on. So as long as you have customers who want to run inference of your model on your infrastructure, then you can just redirect that capacity to them and then buy new [Nvidia] Blackwells for your training runs.</p><p>"Who it becomes really tricky for is people who bought a bunch, don't have demand from their customers for inference, and therefore are stuck doing training runs on that last-generation hardware. That's a tough place to be."</p><p>Learn more:</p><p><a href="https://a16z.com/navigating-the-high-cost-of-ai-compute/" target="_blank">Navigating the High Cost of GPU Compute</a></p><p><a href="https://a16z.com/podcast/chasing-silicon-the-race-for-gpus/" target="_blank">Chasing Silicon: The Race for GPUs</a></p><p><a href="https://a16z.com/podcast/remaking-the-ui-for-ai/" target="_blank">Remaking the UI for AI</a></p><p>Follow on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="37574991" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/14056164-378f-4d5e-9c00-4941686dbe82/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=14056164-378f-4d5e-9c00-4941686dbe82&amp;feed=Hb_IuXOo"/>
      <itunes:title>How GPU Access Helps AI Startups Be Agile</itunes:title>
      <itunes:author>Anjney Midha, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/1e8f8936-1d6e-4e6a-80a9-dd35ec070d8c/3000x3000/ai-a16z-pod-how-gpu-access-helps-ai-startups-be-agile-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:39:08</itunes:duration>
      <itunes:summary>GPU access is like oxygen to AI startups: without it, they can&apos;t breathe. Here, a16z General Partner Anjney Midha explains the forces that lead to GPU shortages and price spikes, and how the firm mitigates these concerns for portfolio companies.</itunes:summary>
      <itunes:subtitle>GPU access is like oxygen to AI startups: without it, they can&apos;t breathe. Here, a16z General Partner Anjney Midha explains the forces that lead to GPU shortages and price spikes, and how the firm mitigates these concerns for portfolio companies.</itunes:subtitle>
      <itunes:keywords>gpus, generative ai, llms, artificial intelligence, ai/ml, foundation models, ai, large language models, machine learning, venture capital, startups</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>24</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">8de37443-4990-4027-9fc3-3f14328f1b93</guid>
      <title>DisTrO and the Quest for Community-Trained AI Models</title>
      <description><![CDATA[<p>In this episode of AI + a16z, Bowen Peng and Jeffrey Quesnelle of Nous Research join a16z General Partner Anjney Midha to discuss their mission to keep open source AI research alive and activate the community of independent builders. The focus is on a recent project called DisTrO, which demonstrates it's possible to train AI models across the public internet much faster than previously thought possible. However, Nous is behind a number of other successful open source AI projects, including the popular Hermes family of "neutral" and guardrail-free language models.</p><p>Here's an excerpt of Jeffrey explaining how DisTrO was inspired by the possibility that major open source AI providers could turn their efforts back inward:</p><p>"What if we don't get Llama 4? That's like an actual existential threat because the closed providers will continue to get better and we would be dead in the water, in a sense. </p><p>"So we asked, 'Is there any real reason we can't make Llama 4 ourselves?' And there is a real reason, which is that we don't have 20,000 H100s. . . . God willing and the creek don't rise, maybe we will one day, but we don't have that right now. </p><p>"So we said, 'But what do we have?' We have a giant activated community who's passionate about wanting to do this and would be willing to contribute their GPUs, their power, to it, if only they could . . . but we don't have the ability to activate that willingness into actual action. . . . The only way people are connected is over the internet, and so anything that isn't sharing over the internet is not gonna work. </p><p>"And so that was the initial premise: What if we don't get Llama 4? And then, what do we have that we could use to create Llama 4? And,  if we can't, what are the technical problems that, if only we slayed that one technical problem, the dam of our community can now flow and actually solve the problem?"</p><p>Learn more:</p><p><a href="https://github.com/NousResearch/DisTrO/raw/main/A_Preliminary_Report_on_DisTrO.pdf" target="_blank">DisTrO paper</a></p><p><a href="https://nousresearch.com/" target="_blank">Nous Research</a></p><p><a href="https://github.com/NousResearch" target="_blank">Nous Research GitHub</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/bloc97_" target="_blank">Bowen Peng</a></p><p><a href="https://x.com/theemozilla" target="_blank">Jeffrey Quesnelle</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 27 Sep 2024 18:39:16 +0000</pubDate>
      <author>dharris@a16z.com (Anjney Midha, Jeffrey Quesnelle, Bowen Peng)</author>
      <link>https://ai-a16z.simplecast.com/episodes/distro-and-the-quest-for-community-trained-ai-models-vRIP4M42</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, Bowen Peng and Jeffrey Quesnelle of Nous Research join a16z General Partner Anjney Midha to discuss their mission to keep open source AI research alive and activate the community of independent builders. The focus is on a recent project called DisTrO, which demonstrates it's possible to train AI models across the public internet much faster than previously thought possible. However, Nous is behind a number of other successful open source AI projects, including the popular Hermes family of "neutral" and guardrail-free language models.</p><p>Here's an excerpt of Jeffrey explaining how DisTrO was inspired by the possibility that major open source AI providers could turn their efforts back inward:</p><p>"What if we don't get Llama 4? That's like an actual existential threat because the closed providers will continue to get better and we would be dead in the water, in a sense. </p><p>"So we asked, 'Is there any real reason we can't make Llama 4 ourselves?' And there is a real reason, which is that we don't have 20,000 H100s. . . . God willing and the creek don't rise, maybe we will one day, but we don't have that right now. </p><p>"So we said, 'But what do we have?' We have a giant activated community who's passionate about wanting to do this and would be willing to contribute their GPUs, their power, to it, if only they could . . . but we don't have the ability to activate that willingness into actual action. . . . The only way people are connected is over the internet, and so anything that isn't sharing over the internet is not gonna work. </p><p>"And so that was the initial premise: What if we don't get Llama 4? And then, what do we have that we could use to create Llama 4? And,  if we can't, what are the technical problems that, if only we slayed that one technical problem, the dam of our community can now flow and actually solve the problem?"</p><p>Learn more:</p><p><a href="https://github.com/NousResearch/DisTrO/raw/main/A_Preliminary_Report_on_DisTrO.pdf" target="_blank">DisTrO paper</a></p><p><a href="https://nousresearch.com/" target="_blank">Nous Research</a></p><p><a href="https://github.com/NousResearch" target="_blank">Nous Research GitHub</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/bloc97_" target="_blank">Bowen Peng</a></p><p><a href="https://x.com/theemozilla" target="_blank">Jeffrey Quesnelle</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="70001518" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/e59f4957-319b-468b-a457-bf917e9eeb04/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=e59f4957-319b-468b-a457-bf917e9eeb04&amp;feed=Hb_IuXOo"/>
      <itunes:title>DisTrO and the Quest for Community-Trained AI Models</itunes:title>
      <itunes:author>Anjney Midha, Jeffrey Quesnelle, Bowen Peng</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/4a88d967-6101-4721-b030-107fb27af7bf/3000x3000/ai-a16z-pod-distro-and-the-quest-for-community-trained-ai-models-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>01:12:55</itunes:duration>
      <itunes:summary>Bowen Peng and Jeffrey Quesnelle of Nous Research join a16z General Partner Anjney Midha to discuss their mission to keep open source AI research alive and activate the community of independent builders. The focus is on a recent project called DisTrO, which demonstrates it&apos;s possible to train AI models across the public internet much faster than previously thought possible.</itunes:summary>
      <itunes:subtitle>Bowen Peng and Jeffrey Quesnelle of Nous Research join a16z General Partner Anjney Midha to discuss their mission to keep open source AI research alive and activate the community of independent builders. The focus is on a recent project called DisTrO, which demonstrates it&apos;s possible to train AI models across the public internet much faster than previously thought possible.</itunes:subtitle>
      <itunes:keywords>llms, artificial intelligence, ai/ml, foundation models, ai, large language models, machine learning, open source</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>23</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">23e24a91-e831-4849-95ea-c54b06e87325</guid>
      <title>Balancing AI Expertise and Industry Acumen in Vertical Applications</title>
      <description><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.ambiencehealthcare.com/" target="_blank">Ambience</a> cofounder and chief scientist Nikhil Buduma joins Derrick Harris to discuss the nuances of using AI models to build vertical applications (including in his space, health care), and why industry acumen is at least as important as technical expertise. Nikhil also shares his experience of having a first-row seat to key advances in AI — including the transformer architecture — which not only allowed his company to be an early adopter, but also gave him insight into the types of problems that AI could solve in the future.</p><p>Here's an excerpt of Nikhil explaining the importance of understanding your buyer:</p><p>"If you believe that the most valuable companies are going to fall out of some level of vertical integration between the app layer and the model layer, [that] this next generation of incredibly valuable companies is going to be built by founders who've spent years just obsessively becoming experts in an industry, I would recommend that someone actually know how to map out the most valuable use cases and have a clear story for how those use cases have synergistic, compounding value when you solve those problems increasingly in concert together. </p><p>"I think the founding team is going to have to have the right ML chops to actually build out the right live learning loops, build out the ML ops loops to measure and to close the gap on model quality for those use cases. [But] the model is actually just one part of solving the problem. </p><p>"You actually need to be thoughtful about the product, the design, the delivery competencies to make sure that what you build is integrated with the right sources of the enterprise data that fits into the right workflows in the right way. And you're going to have to invest heavily in the change management to make sure that customers realize the full value of what they're buying from you. That's all actually way more important than people realize."</p><p>Learn more:</p><p><a href="https://www.oreilly.com/library/view/fundamentals-of-deep/9781492082170/" target="_blank">Fundamentals of Deep Learning</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/nkbuduma" target="_blank">Nikhil Buduma</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 13 Sep 2024 16:15:00 +0000</pubDate>
      <author>dharris@a16z.com (Nikhil Buduma, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/balancing-ai-expertise-and-industry-acumen-when-building-vertical-applications-B9iQiL0o</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, <a href="https://www.ambiencehealthcare.com/" target="_blank">Ambience</a> cofounder and chief scientist Nikhil Buduma joins Derrick Harris to discuss the nuances of using AI models to build vertical applications (including in his space, health care), and why industry acumen is at least as important as technical expertise. Nikhil also shares his experience of having a first-row seat to key advances in AI — including the transformer architecture — which not only allowed his company to be an early adopter, but also gave him insight into the types of problems that AI could solve in the future.</p><p>Here's an excerpt of Nikhil explaining the importance of understanding your buyer:</p><p>"If you believe that the most valuable companies are going to fall out of some level of vertical integration between the app layer and the model layer, [that] this next generation of incredibly valuable companies is going to be built by founders who've spent years just obsessively becoming experts in an industry, I would recommend that someone actually know how to map out the most valuable use cases and have a clear story for how those use cases have synergistic, compounding value when you solve those problems increasingly in concert together. </p><p>"I think the founding team is going to have to have the right ML chops to actually build out the right live learning loops, build out the ML ops loops to measure and to close the gap on model quality for those use cases. [But] the model is actually just one part of solving the problem. </p><p>"You actually need to be thoughtful about the product, the design, the delivery competencies to make sure that what you build is integrated with the right sources of the enterprise data that fits into the right workflows in the right way. And you're going to have to invest heavily in the change management to make sure that customers realize the full value of what they're buying from you. That's all actually way more important than people realize."</p><p>Learn more:</p><p><a href="https://www.oreilly.com/library/view/fundamentals-of-deep/9781492082170/" target="_blank">Fundamentals of Deep Learning</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/nkbuduma" target="_blank">Nikhil Buduma</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="41274766" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/20ff9545-60ea-4d1d-ae4c-61726b3daafc/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=20ff9545-60ea-4d1d-ae4c-61726b3daafc&amp;feed=Hb_IuXOo"/>
      <itunes:title>Balancing AI Expertise and Industry Acumen in Vertical Applications</itunes:title>
      <itunes:author>Nikhil Buduma, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/5a2da42f-71df-4e50-9bb7-aa08addad25c/3000x3000/ai-a16z-pod-balancing-ai-expertise-and-industry-acumen-in-vertical-applications-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:42:59</itunes:duration>
      <itunes:summary>Ambience cofounder and chief scientist Nikhil Buduma discusses the nuances of using AI models to build vertical applications (including in his space, health care), and why industry acumen is at least as important as technical expertise.</itunes:summary>
      <itunes:subtitle>Ambience cofounder and chief scientist Nikhil Buduma discusses the nuances of using AI models to build vertical applications (including in his space, health care), and why industry acumen is at least as important as technical expertise.</itunes:subtitle>
      <itunes:keywords>healthcare, generative ai, llms, artificial intelligence, ai/ml, ai, large language models, machine learning, startups</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>22</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">3f47a257-9f46-46a2-a606-39dc5b230808</guid>
      <title>AI, SQL, and the End of Big Data</title>
      <description><![CDATA[<p>In this episode of AI + a16z, a16z General Partner Jennifer Li joins <a href="https://motherduck.com/" target="_blank">MotherDuck</a> Cofounder and CEO Jordan Tigani to discuss DuckDB's spiking popularity as the era of big data wanes, as well as the applicability of SQL-based systems for AI workloads and the prospect of text-to-SQL for analyzing data.</p><p>Here's an excerpt of Jordan discussing an early win when it comes to applying generative AI to data analysis:</p><p>"Everybody forgets syntax for various SQL calls. And it's just like  in coding. So there's some people that memorize . . . all of the code base, and so they don't need auto-complete. They don't need any copilot. . . . They don't need an ID; they can just type in Notepad. But for the rest of us, I think these tools are super useful. And I think we have seen that these tools have already changed how people are interacting with their data, how they're writing their SQL queries.</p><p>"One of the things that we've done . . .  is we focused on improving the experience of writing queries. Something we found is actually really useful is when somebody runs a query and there's an error, we basically feed the line of the error into GPT 4 and ask it to fix it. And it turns out to be really good. </p><p>". . . It's a great way of letting you stay in the flow of writing your queries and having true interactivity."</p><p>Learn more:</p><p><a href="https://www.smalldatasf.com/2024/" target="_blank">Small Data SF conference</a></p><p><a href="https://duckdb.org/" target="_blank">DuckDB</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/jrdntgn" target="_blank">Jordan Tigani</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 30 Aug 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Jennifer Li, Jordan Tigani, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/ai-sql-and-the-end-of-big-data-1f3hQwYf</link>
      <content:encoded><![CDATA[<p>In this episode of AI + a16z, a16z General Partner Jennifer Li joins <a href="https://motherduck.com/" target="_blank">MotherDuck</a> Cofounder and CEO Jordan Tigani to discuss DuckDB's spiking popularity as the era of big data wanes, as well as the applicability of SQL-based systems for AI workloads and the prospect of text-to-SQL for analyzing data.</p><p>Here's an excerpt of Jordan discussing an early win when it comes to applying generative AI to data analysis:</p><p>"Everybody forgets syntax for various SQL calls. And it's just like  in coding. So there's some people that memorize . . . all of the code base, and so they don't need auto-complete. They don't need any copilot. . . . They don't need an ID; they can just type in Notepad. But for the rest of us, I think these tools are super useful. And I think we have seen that these tools have already changed how people are interacting with their data, how they're writing their SQL queries.</p><p>"One of the things that we've done . . .  is we focused on improving the experience of writing queries. Something we found is actually really useful is when somebody runs a query and there's an error, we basically feed the line of the error into GPT 4 and ask it to fix it. And it turns out to be really good. </p><p>". . . It's a great way of letting you stay in the flow of writing your queries and having true interactivity."</p><p>Learn more:</p><p><a href="https://www.smalldatasf.com/2024/" target="_blank">Small Data SF conference</a></p><p><a href="https://duckdb.org/" target="_blank">DuckDB</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/jrdntgn" target="_blank">Jordan Tigani</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="31811334" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/2d7a2c9d-2d24-4054-b4cb-66f92e776212/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=2d7a2c9d-2d24-4054-b4cb-66f92e776212&amp;feed=Hb_IuXOo"/>
      <itunes:title>AI, SQL, and the End of Big Data</itunes:title>
      <itunes:author>Jennifer Li, Jordan Tigani, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/c309aa23-cb54-484f-b8c9-d508ed293951/3000x3000/ai-a16z-pod-ai-sql-and-the-end-of-big-data-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:33:08</itunes:duration>
      <itunes:summary>MotherDuck cofounder and CEO Jordan Tigani discusses DuckDB&apos;s spiking popularity as the era of big data wanes, as well as the applicability of SQL-based systems for AI workloads and the prospect of text-to-SQL for analyzing data.</itunes:summary>
      <itunes:subtitle>MotherDuck cofounder and CEO Jordan Tigani discusses DuckDB&apos;s spiking popularity as the era of big data wanes, as well as the applicability of SQL-based systems for AI workloads and the prospect of text-to-SQL for analyzing data.</itunes:subtitle>
      <itunes:keywords>big data, generative ai, sql, artificial intelligence, ai/ml, foundation models, ai, databases, data science, data analysis</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>21</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">cdd9816d-99ad-4121-a06f-5c2e565b478e</guid>
      <title>The Researcher to Founder Journey, and the Power of Open Models</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://blackforestlabs.ai/" target="_blank">Black Forest Labs</a> founders Robin Rombach, Andreas Blattmann, and Patrick Esser sit down with a16z general partner Anjney Midha to discuss their journey from PhD researchers to Stability AI, and now to launching their own company building state-of-the-art image and video models. They also delve into the topic of openness in AI, explaining the benefits of releasing open models and sharing research findings with the field.</p><p>Learn more:</p><p><a href="https://blackforestlabs.ai/#get-flux" target="_blank">Flux</a></p><p><a href="https://www.economist.com/by-invitation/2024/07/29/keep-the-code-behind-ai-open-say-two-entrepreneurs" target="_blank">Keep the code to AI open, say two entrepreneurs</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/robrombach" target="_blank">Robin Rombach</a></p><p><a href="https://x.com/andi_blatt" target="_blank">Andreas Blattmann</a></p><p><a href="https://x.com/pess_r" target="_blank">Patrick Esser</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 16 Aug 2024 23:51:40 +0000</pubDate>
      <author>dharris@a16z.com (Anjney Midha, Robin Rombach, Andreas Blattmann, Patrick Esser)</author>
      <link>https://ai-a16z.simplecast.com/episodes/the-researcher-to-founder-journey-and-the-power-of-open-models-dPtNB0So</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://blackforestlabs.ai/" target="_blank">Black Forest Labs</a> founders Robin Rombach, Andreas Blattmann, and Patrick Esser sit down with a16z general partner Anjney Midha to discuss their journey from PhD researchers to Stability AI, and now to launching their own company building state-of-the-art image and video models. They also delve into the topic of openness in AI, explaining the benefits of releasing open models and sharing research findings with the field.</p><p>Learn more:</p><p><a href="https://blackforestlabs.ai/#get-flux" target="_blank">Flux</a></p><p><a href="https://www.economist.com/by-invitation/2024/07/29/keep-the-code-behind-ai-open-say-two-entrepreneurs" target="_blank">Keep the code to AI open, say two entrepreneurs</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/robrombach" target="_blank">Robin Rombach</a></p><p><a href="https://x.com/andi_blatt" target="_blank">Andreas Blattmann</a></p><p><a href="https://x.com/pess_r" target="_blank">Patrick Esser</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="35786183" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/2706c7c1-5f62-47cd-8cf1-707007a5a41e/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=2706c7c1-5f62-47cd-8cf1-707007a5a41e&amp;feed=Hb_IuXOo"/>
      <itunes:title>The Researcher to Founder Journey, and the Power of Open Models</itunes:title>
      <itunes:author>Anjney Midha, Robin Rombach, Andreas Blattmann, Patrick Esser</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/0e06edf5-3d17-40ee-892a-9e2ed39a212d/3000x3000/ai-a16z-pod-the-researcher-to-founder-journey-and-the-power-of-open-models-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:37:16</itunes:duration>
      <itunes:summary>Black Forest Labs founders Robin Rombach, Andreas Blattmann, and Patrick Esser sit down with a16z general partner Anjney Midha to discuss their journey from PhD researchers to Stability AI, and now to launching their own company that emphasizes open models.</itunes:summary>
      <itunes:subtitle>Black Forest Labs founders Robin Rombach, Andreas Blattmann, and Patrick Esser sit down with a16z general partner Anjney Midha to discuss their journey from PhD researchers to Stability AI, and now to launching their own company that emphasizes open models.</itunes:subtitle>
      <itunes:keywords>image models, open sour, generative ai, artificial intelligence, ai/ml, foundation models, ai, entrepreneurship, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>20</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">1f222824-bd51-4b1a-960b-08840ed4a8ac</guid>
      <title>Why Computer Science Subsumed Biotech</title>
      <description><![CDATA[<p>In this episode, a16z General Partner Vijay Pande walks us through the past two decades of applying software engineering to the life sciences — from the Folding@Home project that he launched, through AlphaFold and more. He also discusses the major opportunities for AI to transform medicine and health care, as well as some pitfalls that founders in that space need to watch out for.</p><p>Here's an excerpt of Vijay discussing how AlphaFold and other projects revolutionized biology research not just because of their algorithms, but because of how they introduced software engineering into the field:</p><p>"I think the key thing about AlphaFold that really got people excited was not just the AI part, because people have been using machine learning. And so that part was there. I think it was how fast, at least to me, an engineering approach could make a big jump in this field. Because this was a field largely addressed by academics, and academics would have a lab of maybe 20 [or] 30 people — some of the bigger ones, maybe slightly bigger. And of that, these are graduate students working on their PhDs. It's very different than having a team of professional programmers and engineers going after the problem. </p><p>"And so that jump in team ability, plus the technology, I think was very critical for the jump in results. And also, finally, I think having a company like Google say, 'You know, this is a problem we're excited about and we're interested in,' and that AI and biology is something that is an area of great interest to them . . . was a huge flag to plant."</p><p>Learn more:</p><p><a href="https://a16z.com/bio-health/" target="_blank">a16z Bio + Health</a></p><p><a href="https://foldingathome.org/" target="_blank">Folding@Home</a></p><p><a href="https://alphafold.ebi.ac.uk/" target="_blank">AlphaFold</a></p><p><a href="https://a16z.com/podcasts/raising-health/" target="_blank">Raising Health podcast</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/vijaypande" target="_blank">Vijay Pande</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 9 Aug 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Derrick Harris, Vijay Pande)</author>
      <link>https://ai-a16z.simplecast.com/episodes/why-computer-science-subsumed-biotech-xfxkSRNU</link>
      <content:encoded><![CDATA[<p>In this episode, a16z General Partner Vijay Pande walks us through the past two decades of applying software engineering to the life sciences — from the Folding@Home project that he launched, through AlphaFold and more. He also discusses the major opportunities for AI to transform medicine and health care, as well as some pitfalls that founders in that space need to watch out for.</p><p>Here's an excerpt of Vijay discussing how AlphaFold and other projects revolutionized biology research not just because of their algorithms, but because of how they introduced software engineering into the field:</p><p>"I think the key thing about AlphaFold that really got people excited was not just the AI part, because people have been using machine learning. And so that part was there. I think it was how fast, at least to me, an engineering approach could make a big jump in this field. Because this was a field largely addressed by academics, and academics would have a lab of maybe 20 [or] 30 people — some of the bigger ones, maybe slightly bigger. And of that, these are graduate students working on their PhDs. It's very different than having a team of professional programmers and engineers going after the problem. </p><p>"And so that jump in team ability, plus the technology, I think was very critical for the jump in results. And also, finally, I think having a company like Google say, 'You know, this is a problem we're excited about and we're interested in,' and that AI and biology is something that is an area of great interest to them . . . was a huge flag to plant."</p><p>Learn more:</p><p><a href="https://a16z.com/bio-health/" target="_blank">a16z Bio + Health</a></p><p><a href="https://foldingathome.org/" target="_blank">Folding@Home</a></p><p><a href="https://alphafold.ebi.ac.uk/" target="_blank">AlphaFold</a></p><p><a href="https://a16z.com/podcasts/raising-health/" target="_blank">Raising Health podcast</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/vijaypande" target="_blank">Vijay Pande</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="45286756" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/cd5df37d-2712-41a5-955e-cb1714a91ed1/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=cd5df37d-2712-41a5-955e-cb1714a91ed1&amp;feed=Hb_IuXOo"/>
      <itunes:title>Why Computer Science Subsumed Biotech</itunes:title>
      <itunes:author>Derrick Harris, Vijay Pande</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/3247a77a-3c2b-4b41-a5fd-baf6e4597599/3000x3000/ai-a16z-pod-why-computer-science-subsumed-biotech-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:47:10</itunes:duration>
      <itunes:summary>a16z General Partner Vijay Pande walks us through the past two decades of applying software engineering to the life sciences — from the Folding@Home project that he launched, through AlphaFold and more. He also discusses the major opportunities for AI to transform medicine and health care, as well as some pitfalls that founders in that space need to watch out for.</itunes:summary>
      <itunes:subtitle>a16z General Partner Vijay Pande walks us through the past two decades of applying software engineering to the life sciences — from the Folding@Home project that he launched, through AlphaFold and more. He also discusses the major opportunities for AI to transform medicine and health care, as well as some pitfalls that founders in that space need to watch out for.</itunes:subtitle>
      <itunes:keywords>life sciences, generative ai, llms, biotech, artificial intelligence, ai/ml, ai, large language models, machine learning, health care</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>19</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">dd0f530c-89c2-4229-955b-a6c166ebf4ec</guid>
      <title>Democratizing Generative AI Red Teams</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, a16z General Partner Anjney Midha speaks with <a href="https://www.promptfoo.dev/" target="_blank">PromptFoo</a> founder and CEO Ian Webster about the importance of red-teaming for AI safety and security, and how bringing those capabilities to more organizations will lead to safer, more predictable generative AI applications. They also delve into lessons they learned about this during their time together as early large language model adopters at Discord, and why attempts to regulate AI should focus on applications and use cases rather than models themselves.</p><p>Here's an excerpt of Ian laying out his take on AI governance:</p><p>"The reason why I think that the future of AI safety is open source is that I think there's been a lot of high-level discussion about what AI safety is, and some of the existential threats, and all of these scenarios. But what I'm really hoping to do is focus the conversation on the here and now. Like, what are the harms and the safety and security issues that we see in the wild right now with AI? And the reality is that there's a very large set of <i>practical</i> security considerations that we should be thinking about. </p><p>"And the reason why I think that open source is really important here is because you have the large AI labs, which have the resources to employ specialized red teams and start to find these problems, but there are only, let's say, five big AI labs that are doing this. And the rest of us are left in the dark. So I think that it's not acceptable to just have safety in the domain of the foundation model labs, because I don't think that's an effective way to solve the real problems that we see today.</p><p>"So my stance here is that we really need open source solutions that are available to all developers and all companies and enterprises to identify and eliminate a lot of these real safety issues."</p><p>Learn more:</p><p><a href="" target="_blank">Securing the Black Box: OpenAI, Anthropic, and GDM Discuss</a></p><p><a href="https://a16z.com/podcast/security-founders-talk-shop-about-generative-ai/" target="_blank">Security Founders Talk Shop About Generative AI</a></p><p><a href="https://a16z.com/podcast/californias-senate-bill-1047-what-you-need-to-know/" target="_blank">California's Senate Bill 1047: What You Need to Know</a></p><p>Follow everybody on X:</p><p><a href="https://x.com/iwebst" target="_blank">Ian Webster</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 2 Aug 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Ian Webster, Anjney Midha)</author>
      <link>https://ai-a16z.simplecast.com/episodes/securing-ai-by-democratizing-red-teams-ML60QMBJ</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, a16z General Partner Anjney Midha speaks with <a href="https://www.promptfoo.dev/" target="_blank">PromptFoo</a> founder and CEO Ian Webster about the importance of red-teaming for AI safety and security, and how bringing those capabilities to more organizations will lead to safer, more predictable generative AI applications. They also delve into lessons they learned about this during their time together as early large language model adopters at Discord, and why attempts to regulate AI should focus on applications and use cases rather than models themselves.</p><p>Here's an excerpt of Ian laying out his take on AI governance:</p><p>"The reason why I think that the future of AI safety is open source is that I think there's been a lot of high-level discussion about what AI safety is, and some of the existential threats, and all of these scenarios. But what I'm really hoping to do is focus the conversation on the here and now. Like, what are the harms and the safety and security issues that we see in the wild right now with AI? And the reality is that there's a very large set of <i>practical</i> security considerations that we should be thinking about. </p><p>"And the reason why I think that open source is really important here is because you have the large AI labs, which have the resources to employ specialized red teams and start to find these problems, but there are only, let's say, five big AI labs that are doing this. And the rest of us are left in the dark. So I think that it's not acceptable to just have safety in the domain of the foundation model labs, because I don't think that's an effective way to solve the real problems that we see today.</p><p>"So my stance here is that we really need open source solutions that are available to all developers and all companies and enterprises to identify and eliminate a lot of these real safety issues."</p><p>Learn more:</p><p><a href="" target="_blank">Securing the Black Box: OpenAI, Anthropic, and GDM Discuss</a></p><p><a href="https://a16z.com/podcast/security-founders-talk-shop-about-generative-ai/" target="_blank">Security Founders Talk Shop About Generative AI</a></p><p><a href="https://a16z.com/podcast/californias-senate-bill-1047-what-you-need-to-know/" target="_blank">California's Senate Bill 1047: What You Need to Know</a></p><p>Follow everybody on X:</p><p><a href="https://x.com/iwebst" target="_blank">Ian Webster</a></p><p><a href="https://x.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="43016820" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/ad5eb780-51f4-499d-b15d-cfa1b900529b/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=ad5eb780-51f4-499d-b15d-cfa1b900529b&amp;feed=Hb_IuXOo"/>
      <itunes:title>Democratizing Generative AI Red Teams</itunes:title>
      <itunes:author>Ian Webster, Anjney Midha</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/7a7dbf2f-0844-4ed4-80d4-71d55fd8654f/3000x3000/ai-a16z-pod-securing-ai-by-democratizing-red-teams-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:44:48</itunes:duration>
      <itunes:summary>PromptFoo founder and CEO Ian Webster discusses the importance of red-teaming for AI safety and security, and how bringing those capabilities to more organizations will lead to safer, more predictable generative AI applications.</itunes:summary>
      <itunes:subtitle>PromptFoo founder and CEO Ian Webster discusses the importance of red-teaming for AI safety and security, and how bringing those capabilities to more organizations will lead to safer, more predictable generative AI applications.</itunes:subtitle>
      <itunes:keywords>ai security, ai safety, generative ai, llms, artificial intelligence, ai/ml, ai, large language models, machine learning, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>18</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">511b8d61-deee-41c1-b5d2-3990799da73f</guid>
      <title>Augmenting Incident Response with LLMs</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://www.cmdzero.io/" target="_blank">Command Zero</a> cofounder and CTO Dean de Beer joins a16z's Joel de la Garza and Derrick Harris to discuss the benefits of training large language models on security data, as well as the myriad factors product teams need to consider when building on LLMs. </p><p>Here's an excerpt of Dean discussing the challenges and concerns around scaling up LLMs:</p><p>"Scaling out infrastructure has a lot of limitations: the APIs you're using, tokens, inbound and outbound, the cost associated with that — the nuances of the models, if you will. And not all models are created equal, and they oftentimes are very good for specific use cases and they might not be appropriate for your use case, which is why we tend to use a lot of different models for our use cases . . .</p><p>"So your use cases will heavily determine the models that you're going to use. Very quickly, you'll find that you'll be spending more time on the adjacent technologies or infrastructure. So, memory management for models. How do you go beyond the context window for a model? How do you maintain the context of the data, when given back to the model? How do you do entity extraction so that the model understands that there are certain entities that it needs to prioritize when looking at new data? How do you leverage semantic search as something to augment the capabilities of the model and the data that you're ingesting? </p><p>"That's where we have found that we spend a lot more of our time today than on the models themselves. We have found a good combination of models that run our use cases; we augment them with those adjacent technologies."</p><p>Learn more:</p><p><a href="https://en.wikipedia.org/wiki/The_Cuckoo%27s_Egg_(book)" target="_blank">The Cuckoo's Egg</a></p><p><a href="https://www.latimes.com/archives/la-xpm-1995-08-19-fi-36656-story.html" target="_blank">1995 Citigroup hack</a></p><p>Follow everyone on social media:</p><p><a href="https://x.com/deanzero" target="_blank">Dean de Beer</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 26 Jul 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Joel de la Garza, Derrick Harris, Dean De Beer)</author>
      <link>https://ai-a16z.simplecast.com/episodes/augmenting-incident-response-with-llms-h7juJCvd</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://www.cmdzero.io/" target="_blank">Command Zero</a> cofounder and CTO Dean de Beer joins a16z's Joel de la Garza and Derrick Harris to discuss the benefits of training large language models on security data, as well as the myriad factors product teams need to consider when building on LLMs. </p><p>Here's an excerpt of Dean discussing the challenges and concerns around scaling up LLMs:</p><p>"Scaling out infrastructure has a lot of limitations: the APIs you're using, tokens, inbound and outbound, the cost associated with that — the nuances of the models, if you will. And not all models are created equal, and they oftentimes are very good for specific use cases and they might not be appropriate for your use case, which is why we tend to use a lot of different models for our use cases . . .</p><p>"So your use cases will heavily determine the models that you're going to use. Very quickly, you'll find that you'll be spending more time on the adjacent technologies or infrastructure. So, memory management for models. How do you go beyond the context window for a model? How do you maintain the context of the data, when given back to the model? How do you do entity extraction so that the model understands that there are certain entities that it needs to prioritize when looking at new data? How do you leverage semantic search as something to augment the capabilities of the model and the data that you're ingesting? </p><p>"That's where we have found that we spend a lot more of our time today than on the models themselves. We have found a good combination of models that run our use cases; we augment them with those adjacent technologies."</p><p>Learn more:</p><p><a href="https://en.wikipedia.org/wiki/The_Cuckoo%27s_Egg_(book)" target="_blank">The Cuckoo's Egg</a></p><p><a href="https://www.latimes.com/archives/la-xpm-1995-08-19-fi-36656-story.html" target="_blank">1995 Citigroup hack</a></p><p>Follow everyone on social media:</p><p><a href="https://x.com/deanzero" target="_blank">Dean de Beer</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="38628666" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/1888cea2-74eb-42d2-8216-b2b4e8ae180a/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=1888cea2-74eb-42d2-8216-b2b4e8ae180a&amp;feed=Hb_IuXOo"/>
      <itunes:title>Augmenting Incident Response with LLMs</itunes:title>
      <itunes:author>Joel de la Garza, Derrick Harris, Dean De Beer</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/e2903c6e-a91b-4022-9ab9-e731c3d257ef/3000x3000/ai-a16z-pod-augmenting-incident-responders-with-llms-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:40:14</itunes:duration>
      <itunes:summary>Command Zero cofounder and CTO Dean de Beer discusses the benefits of training large language models on security data, as well as the myriad factors product teams need to consider when building on LLMs.</itunes:summary>
      <itunes:subtitle>Command Zero cofounder and CTO Dean de Beer discusses the benefits of training large language models on security data, as well as the myriad factors product teams need to consider when building on LLMs.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, ai, large language models, machine learning, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>17</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">f79e5084-030b-4c07-a357-a1cccd31b5fc</guid>
      <title>Scaling AI for the Coming Data Deluge</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://www.anyscale.com/" target="_blank">Anyscale</a> cofounder and CEO Robert Nishihara joins a16z's Jennifer Li and Derrick Harris to discuss the challenges of training and running AI models at scale; how a focus on video models — and the huge amount of data involved — will change generative AI models and infrastructure; and the unique experience of launching a company out of the UC-Berkeley <a href="https://sky.cs.berkeley.edu/" target="_blank">Sky Computing Lab</a> (the successor to RISElab and AMPLab).</p><p>Here's a sample of the discussion, where Robert explains how generative AI has turbocharged the appetite for AI capabilities within enterprise customers:</p><p>"Two years ago, we would talk to companies, prospective customers, and AI just wasn't a priority. It certainly wasn't a company-level priority in the way that it is today. And generative AI is the reason a lot of companies now reach out to us . . . because they know that succeeding with AI is essential for their businesses, it's essential for their competitive advantage.</p><p>"And time to market matters for them. They don't want to spend a year hiring an AI infrastructure team, building up a 20-person team to build all of the internal infrastructure, just to be able to start to use generative AI. That's something they want to do today."</p><p>At another point in the discussion, he notes on this same topic:</p><p>"One dimension where we try to go really deep is on the developer experience and just enabling developers to be more productive. This is a complaint we hear all the time with machine learning teams or infrastructure teams: They'll say that they hired all these machine learning people, but then the machine learning people are spending all of their time managing clusters or working on the infrastructure. Or they'll say that it takes 6 weeks or 12 weeks to get a model to transition from development to production . . . Or moving from a laptop to the cloud, and to go from single machine to scaling — these are expensive handoffs often involve rewriting a bunch of code."</p><p>Learn more:</p><p><a href="https://www.anyscale.com/" target="_blank">Anyscale</a></p><p><a href="https://sky.cs.berkeley.edu/" target="_blank">Sky Computing Lab</a></p><p><a href="https://www.ray.io/" target="_blank">Ray</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/robertnishihara" target="_blank">Robert Nishihara</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 19 Jul 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Robert Nishihara, Derrick Harris, Jennifer Li)</author>
      <link>https://ai-a16z.simplecast.com/episodes/scaling-ai-for-the-coming-data-deluge-5F5qykAS</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://www.anyscale.com/" target="_blank">Anyscale</a> cofounder and CEO Robert Nishihara joins a16z's Jennifer Li and Derrick Harris to discuss the challenges of training and running AI models at scale; how a focus on video models — and the huge amount of data involved — will change generative AI models and infrastructure; and the unique experience of launching a company out of the UC-Berkeley <a href="https://sky.cs.berkeley.edu/" target="_blank">Sky Computing Lab</a> (the successor to RISElab and AMPLab).</p><p>Here's a sample of the discussion, where Robert explains how generative AI has turbocharged the appetite for AI capabilities within enterprise customers:</p><p>"Two years ago, we would talk to companies, prospective customers, and AI just wasn't a priority. It certainly wasn't a company-level priority in the way that it is today. And generative AI is the reason a lot of companies now reach out to us . . . because they know that succeeding with AI is essential for their businesses, it's essential for their competitive advantage.</p><p>"And time to market matters for them. They don't want to spend a year hiring an AI infrastructure team, building up a 20-person team to build all of the internal infrastructure, just to be able to start to use generative AI. That's something they want to do today."</p><p>At another point in the discussion, he notes on this same topic:</p><p>"One dimension where we try to go really deep is on the developer experience and just enabling developers to be more productive. This is a complaint we hear all the time with machine learning teams or infrastructure teams: They'll say that they hired all these machine learning people, but then the machine learning people are spending all of their time managing clusters or working on the infrastructure. Or they'll say that it takes 6 weeks or 12 weeks to get a model to transition from development to production . . . Or moving from a laptop to the cloud, and to go from single machine to scaling — these are expensive handoffs often involve rewriting a bunch of code."</p><p>Learn more:</p><p><a href="https://www.anyscale.com/" target="_blank">Anyscale</a></p><p><a href="https://sky.cs.berkeley.edu/" target="_blank">Sky Computing Lab</a></p><p><a href="https://www.ray.io/" target="_blank">Ray</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/robertnishihara" target="_blank">Robert Nishihara</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="36446396" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/5bedf3c3-5533-4a3f-94ed-66c179d6125a/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=5bedf3c3-5533-4a3f-94ed-66c179d6125a&amp;feed=Hb_IuXOo"/>
      <itunes:title>Scaling AI for the Coming Data Deluge</itunes:title>
      <itunes:author>Robert Nishihara, Derrick Harris, Jennifer Li</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/48ae5dc4-889c-421c-8cfc-15175aa81523/3000x3000/ai-a16z-pod-scaling-ai-infrastructure-for-the-coming-data-deluge-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:37:56</itunes:duration>
      <itunes:summary>Anyscale cofounder and CEO Robert Nishihara discusses the challenges of training and running AI models at scale; how a focus on video models — and the huge amount of data involved — will change generative AI models and infrastructure; and the unique experience of launching a company out of the UC-Berkeley Sky Computing Lab (the successor to RISElab and AMPLab).</itunes:summary>
      <itunes:subtitle>Anyscale cofounder and CEO Robert Nishihara discusses the challenges of training and running AI models at scale; how a focus on video models — and the huge amount of data involved — will change generative AI models and infrastructure; and the unique experience of launching a company out of the UC-Berkeley Sky Computing Lab (the successor to RISElab and AMPLab).</itunes:subtitle>
      <itunes:keywords>big data, generative ai, llms, artificial intelligence, ai/ml, large language models, machine learning, startups</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>16</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">4e9bb0d0-581c-484d-a954-9465e512732d</guid>
      <title>ARCHIVE: The Dream of AI Is Alive in AlphaGo</title>
      <description><![CDATA[<p>In this archive episode from 2015, a16z's Sonal Chokshi, Frank Chen, and Steven Sinofsky discuss DeepMind's breakthrough AlphaGo system, which mastered the ancient Chinese game Go and introduced the public to reinforcement learning.</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 5 Jul 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Sonal Chokshi, Frank Chen, Steven Sinofsky)</author>
      <link>https://ai-a16z.simplecast.com/episodes/archive-the-dream-of-ai-is-alive-in-alphago-0MEOO8xc</link>
      <content:encoded><![CDATA[<p>In this archive episode from 2015, a16z's Sonal Chokshi, Frank Chen, and Steven Sinofsky discuss DeepMind's breakthrough AlphaGo system, which mastered the ancient Chinese game Go and introduced the public to reinforcement learning.</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="31701001" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/4e3a6928-9316-4199-a18f-fea52595f1cb/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=4e3a6928-9316-4199-a18f-fea52595f1cb&amp;feed=Hb_IuXOo"/>
      <itunes:title>ARCHIVE: The Dream of AI Is Alive in AlphaGo</itunes:title>
      <itunes:author>Sonal Chokshi, Frank Chen, Steven Sinofsky</itunes:author>
      <itunes:duration>00:33:00</itunes:duration>
      <itunes:summary>In this archive episode from 2015, a16z&apos;s Sonal Chokshi, Frank Chen, and Steven Sinofsky discuss DeepMind&apos;s breakthrough AlphaGo system, which mastered the ancient Chinese game Go and introduced the public to reinforcement learning.</itunes:summary>
      <itunes:subtitle>In this archive episode from 2015, a16z&apos;s Sonal Chokshi, Frank Chen, and Steven Sinofsky discuss DeepMind&apos;s breakthrough AlphaGo system, which mastered the ancient Chinese game Go and introduced the public to reinforcement learning.</itunes:subtitle>
      <itunes:keywords>deep learning, artificial intelligence, ai/ml, ai, machine learning, reinforcement learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>15</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">26632793-5fc6-4cb7-ae3d-8e76c277640d</guid>
      <title>Beyond Language: Inside a Hundred-Trillion-Token Video Model</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://lumalabs.ai/" target="_blank">Luma</a> Chief Scientist Jiaming Song joins a16z General Partner Anjney MIdha to discuss Jiaming's esteemed career in video models, culminating thus far in Luma's recently released Dream Machine 3D model that shows abilities to reason about the world across a variety of aspects. Jiaming covers the history of image and video models,  shares his vision for the future of multimodal models, and explains why he thinks Dream Machine demonstrates its emergent reasoning capabilities. In short: Because it was trained on a volume of high-quality video data that, if measured in relation to language data, would amount to hundreds of trillions of tokens.</p><p>Here's a sample of the discussion, where Jiaming explains the "bitter lesson" as applied to training generative models, and in the process sums up a big component of why Dream Machine can do what it does by using context-rich video data:</p><p>"For a lot of the problems related to artificial intelligence, it is often more productive in the long run to use methods that are simpler but use more compute, [rather] than trying to develop priors, and then trying to leverage the priors so that you can use less compute.</p><p>"Cases in this question first happened in language, where people were initially working on language understanding, trying to use grammar or semantic parsing, these kinds of techniques. But eventually these tasks began to be replaced by large language models. And a similar case is happening in the vision domain, as well . . . and now people have been using deep learning features for almost all the tasks. This is a clear demonstration of how using more compute and having less priors is good.</p><p>"But how does it work with language? Language by itself is also a human construct. Of course, it is a very good and highly compressed kind of knowledge, but it's definitely a lot less data than what humans take in day to day from the real world . . . </p><p>"[And] it is a vastly smaller data set size than visual signals. And we are already almost exhausting the . . . high-quality language sources that we have in the world. The speed at which humans can produce language is definitely not enough to keep up with the demands of the scaling laws. So even if we have a world where we can scale up the compute infrastructure for that, we don't really have the infrastructure to scale up the data efforts . . .</p><p>"Even though people would argue that the emergence of large language models is already evidence of the scaling law . . . against the rule-based methods in language understanding, we are arguing that language by itself is also a prior in the face of more of the richer data signal that is happening in the physical world."</p><p>Learn more:</p><p><a href="https://lumalabs.ai/dream-machine/creations" target="_blank">Dream Machine</a></p><p><a href="https://tsong.me/" target="_blank">Jiaming's personal site</a></p><p><a href="https://lumalabs.ai/join" target="_blank">Luma careers</a></p><p><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank">The bitter lesson</a></p><p>Follow everyone on X:</p><p><a href="https://twitter.com/baaadas" target="_blank">Jiaming Song</a></p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Wed, 3 Jul 2024 23:06:22 +0000</pubDate>
      <author>dharris@a16z.com (Jiaming Song, Anjney Midha)</author>
      <link>https://ai-a16z.simplecast.com/episodes/beyond-language-inside-a-hundred-trillion-token-video-model-48FKpX9k</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, <a href="https://lumalabs.ai/" target="_blank">Luma</a> Chief Scientist Jiaming Song joins a16z General Partner Anjney MIdha to discuss Jiaming's esteemed career in video models, culminating thus far in Luma's recently released Dream Machine 3D model that shows abilities to reason about the world across a variety of aspects. Jiaming covers the history of image and video models,  shares his vision for the future of multimodal models, and explains why he thinks Dream Machine demonstrates its emergent reasoning capabilities. In short: Because it was trained on a volume of high-quality video data that, if measured in relation to language data, would amount to hundreds of trillions of tokens.</p><p>Here's a sample of the discussion, where Jiaming explains the "bitter lesson" as applied to training generative models, and in the process sums up a big component of why Dream Machine can do what it does by using context-rich video data:</p><p>"For a lot of the problems related to artificial intelligence, it is often more productive in the long run to use methods that are simpler but use more compute, [rather] than trying to develop priors, and then trying to leverage the priors so that you can use less compute.</p><p>"Cases in this question first happened in language, where people were initially working on language understanding, trying to use grammar or semantic parsing, these kinds of techniques. But eventually these tasks began to be replaced by large language models. And a similar case is happening in the vision domain, as well . . . and now people have been using deep learning features for almost all the tasks. This is a clear demonstration of how using more compute and having less priors is good.</p><p>"But how does it work with language? Language by itself is also a human construct. Of course, it is a very good and highly compressed kind of knowledge, but it's definitely a lot less data than what humans take in day to day from the real world . . . </p><p>"[And] it is a vastly smaller data set size than visual signals. And we are already almost exhausting the . . . high-quality language sources that we have in the world. The speed at which humans can produce language is definitely not enough to keep up with the demands of the scaling laws. So even if we have a world where we can scale up the compute infrastructure for that, we don't really have the infrastructure to scale up the data efforts . . .</p><p>"Even though people would argue that the emergence of large language models is already evidence of the scaling law . . . against the rule-based methods in language understanding, we are arguing that language by itself is also a prior in the face of more of the richer data signal that is happening in the physical world."</p><p>Learn more:</p><p><a href="https://lumalabs.ai/dream-machine/creations" target="_blank">Dream Machine</a></p><p><a href="https://tsong.me/" target="_blank">Jiaming's personal site</a></p><p><a href="https://lumalabs.ai/join" target="_blank">Luma careers</a></p><p><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" target="_blank">The bitter lesson</a></p><p>Follow everyone on X:</p><p><a href="https://twitter.com/baaadas" target="_blank">Jiaming Song</a></p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="62637078" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/c4d20aa4-03f6-4b64-8535-2b8216e56d35/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=c4d20aa4-03f6-4b64-8535-2b8216e56d35&amp;feed=Hb_IuXOo"/>
      <itunes:title>Beyond Language: Inside a Hundred-Trillion-Token Video Model</itunes:title>
      <itunes:author>Jiaming Song, Anjney Midha</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/a654e5fd-586b-4e86-a7dd-c20375b06c4e/3000x3000/ai-a16z-pod-beyond-language-inside-a-hundred-trillion-token-video-model-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>01:05:14</itunes:duration>
      <itunes:summary>Luma Chief Scientist Jiaming Song joins a16z General Partner Anjney MIdha to discuss Jiaming&apos;s esteemed career in video models, culminating thus far in Luma&apos;s recently released Dream Machine 3D model that shows abilities to reason about the world across a variety of aspects.</itunes:summary>
      <itunes:subtitle>Luma Chief Scientist Jiaming Song joins a16z General Partner Anjney MIdha to discuss Jiaming&apos;s esteemed career in video models, culminating thus far in Luma&apos;s recently released Dream Machine 3D model that shows abilities to reason about the world across a variety of aspects.</itunes:subtitle>
      <itunes:keywords>generative ai, deep learning, llms, artificial intelligence, ai/ml, ai, large language models, 3d, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>14</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">a61e5b6a-7e6c-441a-868f-35c2f320869e</guid>
      <title>Developer Tool UX in the Age of Generative AI</title>
      <description><![CDATA[<p>In this episode, design engineer Alasdair Monk joins a16z's Yoko Li and Derrick Harris to discuss how generative AI is changing how developers — and the those building for developers — interact with the tools of their trade. Alasdair’s journey includes stints at dev-centric companies such as Heroku/Salesforce, and he's presently designing the user experience for Poolside, an AI programming startup. </p><p>Here's a sample of Alasdair discussing the future of the prompt bar in generative coding tools:</p><p> "When interacting with machine learning models, we've almost thrown away 30 years of human-computer interaction knowledge and kind of reverted to using a terminal circa 1980 to interact with computer, or the prompt bar. This very plain-text way to interact with AI is really interesting. </p><p>"I think it's very different when you can't predict what a user interface is going to look like. What an LLM can spit out is basically unpredictable or non-deterministic, and so how do you design for that or how do you design around the guardrails for that are the really interesting things that I think everyone who works in the industry right now is trying to figure out. And I think it's pretty clear to a lot of people that sometimes you want to chat to the computer as if it's like the rubber duck.</p><p>"I think a lot of where AI is going to really help us, particularly with engineering, is going to be in the interactions that aren't that at all, and will actually probably look much more like interacting with traditional software today, where I interact with it via windows and buttons and all sorts of GUI elements."</p><p>Follow everyone on X:</p><p><a href="https://twitter.com/almonk" target="_blank">Alasdair Monk</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 21 Jun 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Alasdair Monk, Yoko Li, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/developer-tool-ux-in-the-age-of-generative-ai-ylZ7o5cj</link>
      <content:encoded><![CDATA[<p>In this episode, design engineer Alasdair Monk joins a16z's Yoko Li and Derrick Harris to discuss how generative AI is changing how developers — and the those building for developers — interact with the tools of their trade. Alasdair’s journey includes stints at dev-centric companies such as Heroku/Salesforce, and he's presently designing the user experience for Poolside, an AI programming startup. </p><p>Here's a sample of Alasdair discussing the future of the prompt bar in generative coding tools:</p><p> "When interacting with machine learning models, we've almost thrown away 30 years of human-computer interaction knowledge and kind of reverted to using a terminal circa 1980 to interact with computer, or the prompt bar. This very plain-text way to interact with AI is really interesting. </p><p>"I think it's very different when you can't predict what a user interface is going to look like. What an LLM can spit out is basically unpredictable or non-deterministic, and so how do you design for that or how do you design around the guardrails for that are the really interesting things that I think everyone who works in the industry right now is trying to figure out. And I think it's pretty clear to a lot of people that sometimes you want to chat to the computer as if it's like the rubber duck.</p><p>"I think a lot of where AI is going to really help us, particularly with engineering, is going to be in the interactions that aren't that at all, and will actually probably look much more like interacting with traditional software today, where I interact with it via windows and buttons and all sorts of GUI elements."</p><p>Follow everyone on X:</p><p><a href="https://twitter.com/almonk" target="_blank">Alasdair Monk</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="36367954" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/c5c6aeea-3dee-4b29-a5d9-13fd84410a6e/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=c5c6aeea-3dee-4b29-a5d9-13fd84410a6e&amp;feed=Hb_IuXOo"/>
      <itunes:title>Developer Tool UX in the Age of Generative AI</itunes:title>
      <itunes:author>Alasdair Monk, Yoko Li, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/b70a6178-0412-4e3f-971a-7ebaec2d3e1d/3000x3000/ai-a16z-pod-alasdair-monk-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:37:48</itunes:duration>
      <itunes:summary>Design engineer Alasdair Monk shares his takes on how generative AI and LLMs are changing the way software developers — and the people building tools for them — do their jobs.</itunes:summary>
      <itunes:subtitle>Design engineer Alasdair Monk shares his takes on how generative AI and LLMs are changing the way software developers — and the people building tools for them — do their jobs.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, ai, large language models, programming, software developer</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>13</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">d850ad98-40bf-4658-b7b5-fb49cfc20231</guid>
      <title>Building Production Workflows for AI Applications</title>
      <description><![CDATA[<p>In this episode, <a href="https://www.inngest.com/" target="_blank">Inngest</a> cofounder and CEO Tony Holdstock-Brown joins a16z partner Yoko Li, as well as Derrick Harris, to discuss the reality and complexity of running AI agents and other multistep AI workflows in production. Tony also why developer tools for generative AI — and their founders — might look very similar to previous generations of these products, and where there are opportunities for improvement.</p><p>Here's a sample of the discussion, where Tony shares some advice for engineers looking to build for AI:<br /><br />"We almost have two parallel tracks right now as, as engineers. We've got the CPU track in which we're all like, 'Oh yeah, CPU-bound, big O notation. What are we doing on the application-level side?' And then we've got the GPU side, in which people are doing like crazy things in order to make numbers faster, in order to make differentiation better and smoother, in order to do  gradient descent in a nicer and more powerful way. The two disciplines right now are working together, but are also very, very, very different from an engineering point of view. </p><p>"This is one interesting part to think about for like new engineers, people that are just thinking about what to do if they want to go into the engineering field overall. Do you want to be on the side using AI, in which you take all of these models, do all of this stuff, build the application-level stuff, and chain things together to build products? Or do you want to be on the math side of things, in which you do really low-level things in order to make compilers work better, so that your AI things can run faster and more efficiently? Both are engineering, just completely different applications of it."</p><p>Learn more:</p><p><a href="https://a16z.com/the-modern-transactional-stack/" target="_blank">The Modern Transactional Stack</a></p><p><a href="https://github.com/a16z-infra/llm-app-stack" target="_blank">The LLM App Stack</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/itstonyhb" target="_blank">Tony Holdstock-Brown</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 14 Jun 2024 19:25:45 +0000</pubDate>
      <author>dharris@a16z.com (Tony Holdstock-Brown, Yoko Li, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/building-production-workflows-for-ai-applications-mw_yqRkR</link>
      <content:encoded><![CDATA[<p>In this episode, <a href="https://www.inngest.com/" target="_blank">Inngest</a> cofounder and CEO Tony Holdstock-Brown joins a16z partner Yoko Li, as well as Derrick Harris, to discuss the reality and complexity of running AI agents and other multistep AI workflows in production. Tony also why developer tools for generative AI — and their founders — might look very similar to previous generations of these products, and where there are opportunities for improvement.</p><p>Here's a sample of the discussion, where Tony shares some advice for engineers looking to build for AI:<br /><br />"We almost have two parallel tracks right now as, as engineers. We've got the CPU track in which we're all like, 'Oh yeah, CPU-bound, big O notation. What are we doing on the application-level side?' And then we've got the GPU side, in which people are doing like crazy things in order to make numbers faster, in order to make differentiation better and smoother, in order to do  gradient descent in a nicer and more powerful way. The two disciplines right now are working together, but are also very, very, very different from an engineering point of view. </p><p>"This is one interesting part to think about for like new engineers, people that are just thinking about what to do if they want to go into the engineering field overall. Do you want to be on the side using AI, in which you take all of these models, do all of this stuff, build the application-level stuff, and chain things together to build products? Or do you want to be on the math side of things, in which you do really low-level things in order to make compilers work better, so that your AI things can run faster and more efficiently? Both are engineering, just completely different applications of it."</p><p>Learn more:</p><p><a href="https://a16z.com/the-modern-transactional-stack/" target="_blank">The Modern Transactional Stack</a></p><p><a href="https://github.com/a16z-infra/llm-app-stack" target="_blank">The LLM App Stack</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/itstonyhb" target="_blank">Tony Holdstock-Brown</a></p><p><a href="https://x.com/stuffyokodraws" target="_blank">Yoko Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="42081427" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/9e405d17-1a79-407d-a334-6cce39d47fdc/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=9e405d17-1a79-407d-a334-6cce39d47fdc&amp;feed=Hb_IuXOo"/>
      <itunes:title>Building Production Workflows for AI Applications</itunes:title>
      <itunes:author>Tony Holdstock-Brown, Yoko Li, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/d4237a1b-fd34-424f-9d54-0b9d604cbbb8/3000x3000/ai-a16z-pod-building-production-workflows-for-ai-applications-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:43:50</itunes:duration>
      <itunes:summary>Inngest cofounder and CEO Tony Holdstock-Brown discusses the reality and complexity of running AI agents and other multistep AI workflows in production, and why developer tools for generative AI — and their founders — might look very similar to previous generations of these products.</itunes:summary>
      <itunes:subtitle>Inngest cofounder and CEO Tony Holdstock-Brown discusses the reality and complexity of running AI agents and other multistep AI workflows in production, and why developer tools for generative AI — and their founders — might look very similar to previous generations of these products.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, ai, entrepreneurship, large language models, machine learning, startups, software engineers, software developer</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>12</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">525a4e61-f7cd-4a77-b216-ce6c6bc1413a</guid>
      <title>The Future of Image Models Is Multimodal</title>
      <description><![CDATA[<p>In this episode, <a href="https://ideogram.ai/" target="_blank">Ideogram</a> CEO Mohammad Norouzi joins a16z General Partner Jennifer Li, as well as Derrick Harris, to share his story of growing up in Iran, helping build influential text-to-image models at Google, and ultimately cofounding and running Ideogram. He also breaks down the differences between transformer models and diffusion models, as well as the transition from researcher to startup CEO.</p><p>Here's an excerpt where Mohammad discusses the reaction to the original transformer architecture paper, <a href="https://arxiv.org/abs/1706.03762" target="_blank">"Attention Is All You Need,</a>" within Google's AI team:<br /><br />"I think [lead author Asish Vaswani] knew right after the paper was submitted that this is a very important piece of the technology. And he was telling me in the hallway how it works and how much improvement it gives to translation. Translation was a testbed for the transformer paper at the time, and it helped in two ways. One is the speed of training and the other is the quality of translation. </p><p>"To be fair, I don't think anybody had a very crystal clear idea of how big this would become. And I guess the interesting thing is, now, it's the founding architecture for computer vision, too, not only for language. And then we also went far beyond language translation as a task, and we are talking about general-purpose assistants and the idea of building general-purpose intelligent machines. And it's really humbling to see how big of a role the transformer is playing into this."</p><p>Learn more:<br /><a href="https://a16z.com/announcement/investing-in-ideogram/" target="_blank">Investing in Ideogram</a></p><p><a href="https://imagen.research.google/" target="_blank">Imagen</a></p><p><a href="https://arxiv.org/abs/2006.11239" target="_blank">Denoising Diffusion Probabilistic Models</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/mo_norouzi" target="_blank">Mohammad Norouzi</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 7 Jun 2024 16:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Jennifer Li, Mohammad Norouzi, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/the-future-of-image-models-is-multimodal-gSij1jNf</link>
      <content:encoded><![CDATA[<p>In this episode, <a href="https://ideogram.ai/" target="_blank">Ideogram</a> CEO Mohammad Norouzi joins a16z General Partner Jennifer Li, as well as Derrick Harris, to share his story of growing up in Iran, helping build influential text-to-image models at Google, and ultimately cofounding and running Ideogram. He also breaks down the differences between transformer models and diffusion models, as well as the transition from researcher to startup CEO.</p><p>Here's an excerpt where Mohammad discusses the reaction to the original transformer architecture paper, <a href="https://arxiv.org/abs/1706.03762" target="_blank">"Attention Is All You Need,</a>" within Google's AI team:<br /><br />"I think [lead author Asish Vaswani] knew right after the paper was submitted that this is a very important piece of the technology. And he was telling me in the hallway how it works and how much improvement it gives to translation. Translation was a testbed for the transformer paper at the time, and it helped in two ways. One is the speed of training and the other is the quality of translation. </p><p>"To be fair, I don't think anybody had a very crystal clear idea of how big this would become. And I guess the interesting thing is, now, it's the founding architecture for computer vision, too, not only for language. And then we also went far beyond language translation as a task, and we are talking about general-purpose assistants and the idea of building general-purpose intelligent machines. And it's really humbling to see how big of a role the transformer is playing into this."</p><p>Learn more:<br /><a href="https://a16z.com/announcement/investing-in-ideogram/" target="_blank">Investing in Ideogram</a></p><p><a href="https://imagen.research.google/" target="_blank">Imagen</a></p><p><a href="https://arxiv.org/abs/2006.11239" target="_blank">Denoising Diffusion Probabilistic Models</a></p><p>Follow everyone on X:</p><p><a href="https://x.com/mo_norouzi" target="_blank">Mohammad Norouzi</a></p><p><a href="https://x.com/JenniferHli" target="_blank">Jennifer Li</a></p><p><a href="https://x.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="35799501" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/506819d3-eb50-4544-b3fd-ee38a8a5b919/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=506819d3-eb50-4544-b3fd-ee38a8a5b919&amp;feed=Hb_IuXOo"/>
      <itunes:title>The Future of Image Models Is Multimodal</itunes:title>
      <itunes:author>Jennifer Li, Mohammad Norouzi, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/c344666b-b0a4-48e1-a798-d4c719a093a6/3000x3000/ai-a16z-pod-mohammad-norouzi-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:37:17</itunes:duration>
      <itunes:summary>Ideogram CEO Mohammad Norouzi shares his story of growing up in Iran, helping build influential text-to-image models at Google, and ultimately cofounding and running an AI startup. He breaks down the differences between transformer models and diffusion models, as well as the transition from researcher to startup CEO.</itunes:summary>
      <itunes:subtitle>Ideogram CEO Mohammad Norouzi shares his story of growing up in Iran, helping build influential text-to-image models at Google, and ultimately cofounding and running an AI startup. He breaks down the differences between transformer models and diffusion models, as well as the transition from researcher to startup CEO.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, entrepreneurship, large language models, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>11</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">98a31230-a5cc-43b4-b48e-bedbd786a44c</guid>
      <title>ARCHIVE: Open Models (with Arthur Mensch) and Video Models (with Stefano Ermon)</title>
      <description><![CDATA[<p>For this holiday weekend (in the United States) episode, we've stitched together two archived episodes from the <a href="https://a16z.com/podcasts/a16z-podcast/" target="_blank">a16z Podcast</a>, both featuring General Partner Anjney Midha. In the first half, from December, he speaks with Mistral cofounder and CEO Arthur Mensch about the importance of open foundation models, as well as Mistral's approach to building them. In the second half (at 34:40), from February, he speaks with Stanford's Stefano Ermon about the state of the art in video models, including how OpenAI's Sora might work under the hood.</p><p>Here's a sample of what Arthur had to say about the debate over how to regulate AI models:</p><p>"I think the battle is for the neutrality of the technology. Like a technology, by a sense, is something neutral. You can use it for bad purposes. You can use it for good purposes. If you look at what an LLM does, it's not really different from a programming language. . . .</p><p>"So we should regulate the function, the mathematics behind it. But, really, you never use a large language model itself. You  always use it in an application, in a way, with a user interface. And so,  that's the one thing you want to regulate. And what it means is that companies like us, like foundational model companies, will obviously make the model as controllable as possible so that the applications on top of it can be compliant, can be safe. We'll also build the tools that allow you to measure the compliance and the safety of the application, because that's super useful for the application makers. It's actually needed.  </p><p>"But there's no point in regulating something that is neutral in itself, that is just a mathematical tool. I think that's the one thing that we've been hammering a lot, which is good, but there's still a lot of effort in making this strong distinction, which is super important to understand what's going on."</p><p>Follow everyone on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://x.com/arthurmensch" target="_blank">Arthur Mensch</a></p><p><a href="" target="_blank">Stefano Ermon</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 24 May 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Anjney Midha, Arthur Mensch, Stefano Ermon)</author>
      <link>https://ai-a16z.simplecast.com/episodes/archive-open-models-with-arthur-mensch-and-video-models-with-stefano-ermon-Gyvsu16j</link>
      <content:encoded><![CDATA[<p>For this holiday weekend (in the United States) episode, we've stitched together two archived episodes from the <a href="https://a16z.com/podcasts/a16z-podcast/" target="_blank">a16z Podcast</a>, both featuring General Partner Anjney Midha. In the first half, from December, he speaks with Mistral cofounder and CEO Arthur Mensch about the importance of open foundation models, as well as Mistral's approach to building them. In the second half (at 34:40), from February, he speaks with Stanford's Stefano Ermon about the state of the art in video models, including how OpenAI's Sora might work under the hood.</p><p>Here's a sample of what Arthur had to say about the debate over how to regulate AI models:</p><p>"I think the battle is for the neutrality of the technology. Like a technology, by a sense, is something neutral. You can use it for bad purposes. You can use it for good purposes. If you look at what an LLM does, it's not really different from a programming language. . . .</p><p>"So we should regulate the function, the mathematics behind it. But, really, you never use a large language model itself. You  always use it in an application, in a way, with a user interface. And so,  that's the one thing you want to regulate. And what it means is that companies like us, like foundational model companies, will obviously make the model as controllable as possible so that the applications on top of it can be compliant, can be safe. We'll also build the tools that allow you to measure the compliance and the safety of the application, because that's super useful for the application makers. It's actually needed.  </p><p>"But there's no point in regulating something that is neutral in itself, that is just a mathematical tool. I think that's the one thing that we've been hammering a lot, which is good, but there's still a lot of effort in making this strong distinction, which is super important to understand what's going on."</p><p>Follow everyone on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://x.com/arthurmensch" target="_blank">Arthur Mensch</a></p><p><a href="" target="_blank">Stefano Ermon</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="63127001" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/834172e2-0d2e-429b-8a96-0238fce82346/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=834172e2-0d2e-429b-8a96-0238fce82346&amp;feed=Hb_IuXOo"/>
      <itunes:title>ARCHIVE: Open Models (with Arthur Mensch) and Video Models (with Stefano Ermon)</itunes:title>
      <itunes:author>Anjney Midha, Arthur Mensch, Stefano Ermon</itunes:author>
      <itunes:duration>01:05:43</itunes:duration>
      <itunes:summary>For this holiday weekend (in the United States) episode, we&apos;ve stitched together two archived episodes from the a16z Podcast, both featuring General Partner Anjney Midha. In the first half, from December, he speaks with Mistral cofounder and CEO Arthur Mensch about the importance of open foundation models, as well as Mistral&apos;s approach to building them. In the second half (at 34:40), from February, he speaks with Stanford&apos;s Stefano Ermon about the state of the art in video models, including how OpenAI&apos;s Sora might work under the hood.</itunes:summary>
      <itunes:subtitle>For this holiday weekend (in the United States) episode, we&apos;ve stitched together two archived episodes from the a16z Podcast, both featuring General Partner Anjney Midha. In the first half, from December, he speaks with Mistral cofounder and CEO Arthur Mensch about the importance of open foundation models, as well as Mistral&apos;s approach to building them. In the second half (at 34:40), from February, he speaks with Stanford&apos;s Stefano Ermon about the state of the art in video models, including how OpenAI&apos;s Sora might work under the hood.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, large language models, machine learning, open source</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>10</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">5bb00edf-4622-4978-afec-5818e00e5739</guid>
      <title>Open Models and Maturation: Assessing the Generative AI Market</title>
      <description><![CDATA[<p>a16z partners Guido Appenzeller and Matt Bornstein join Derrick Harris to discuss the state of the generative AI market, about 18 months after it really kicked into high gear with the release of ChatGPT — everything from the emergence of powerful open source LLMs to the excitement around AI-generated music.</p><p>If there's one major lesson to learn, it's that although we've made some very impressive technological strides and companies are generating meaningful revenue, this is still a a very fluid space. As Matt puts it during the discussion:<br /><br />"For nearly all AI applications and most model providers,  growth is kind of a sawtooth pattern, meaning when there's a big new amazing thing announced, you see very fast growth.  And when it's been a while since the last release, growth kind of can flatten off. And you can imagine retention can be  all over the place, too . . .</p><p>"I think every time we're in a flat period, people start to think, 'Oh, it's mature now,  the, the gold rush is over. What happens next?' But then a new spike almost always comes, or at least has over the last 18 months or so. So a lot of this depends on your time horizon, and I think we're still in this period of, like, if you think growth has slowed, wait a month  and see it change."</p><p>Follow everyone on X:</p><p><a href="https://twitter.com/appenz" target="_blank">Guido Appenzeller</a></p><p><a href="BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 17 May 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Guido Appenzeller, Matt Bornstein, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/open-models-and-maturation-assessing-the-generative-ai-market-DvN9dXg6</link>
      <content:encoded><![CDATA[<p>a16z partners Guido Appenzeller and Matt Bornstein join Derrick Harris to discuss the state of the generative AI market, about 18 months after it really kicked into high gear with the release of ChatGPT — everything from the emergence of powerful open source LLMs to the excitement around AI-generated music.</p><p>If there's one major lesson to learn, it's that although we've made some very impressive technological strides and companies are generating meaningful revenue, this is still a a very fluid space. As Matt puts it during the discussion:<br /><br />"For nearly all AI applications and most model providers,  growth is kind of a sawtooth pattern, meaning when there's a big new amazing thing announced, you see very fast growth.  And when it's been a while since the last release, growth kind of can flatten off. And you can imagine retention can be  all over the place, too . . .</p><p>"I think every time we're in a flat period, people start to think, 'Oh, it's mature now,  the, the gold rush is over. What happens next?' But then a new spike almost always comes, or at least has over the last 18 months or so. So a lot of this depends on your time horizon, and I think we're still in this period of, like, if you think growth has slowed, wait a month  and see it change."</p><p>Follow everyone on X:</p><p><a href="https://twitter.com/appenz" target="_blank">Guido Appenzeller</a></p><p><a href="BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="38736918" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/bdce7e44-2090-45e2-91e5-e7e5fee5c081/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=bdce7e44-2090-45e2-91e5-e7e5fee5c081&amp;feed=Hb_IuXOo"/>
      <itunes:title>Open Models and Maturation: Assessing the Generative AI Market</itunes:title>
      <itunes:author>Guido Appenzeller, Matt Bornstein, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/0ef8491e-7d47-4022-94df-b65ea6227e06/3000x3000/ai-a16z-pod-vector-databases-and-the-power-of-rag-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:40:21</itunes:duration>
      <itunes:summary>a16z&apos;s Guido Appenzeller and Matt Bornstein discuss the state of the generative AI market, about 18 months after it really kicked into high gear with the release of ChatGPT — everything from the emergence of powerful open source LLMs to the excitement around AI-generated music.</itunes:summary>
      <itunes:subtitle>a16z&apos;s Guido Appenzeller and Matt Bornstein discuss the state of the generative AI market, about 18 months after it really kicked into high gear with the release of ChatGPT — everything from the emergence of powerful open source LLMs to the excitement around AI-generated music.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, large language models, machine learning, startups</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>9</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">a92a5fb5-96e5-4522-a530-0f53e56724e8</guid>
      <title>Security Founders Talk Shop About Generative AI</title>
      <description><![CDATA[<p>In this bonus episode, recorded live at our San Francisco office, security-startup founders Dean De Beer (Command Zero), Kevin Tian (Doppel), and Travis McPeak (Resourcely) share their thoughts on generative AI, as well as their experiences building with LLMs and dealing with LLM-based threats.</p><p>Here's a sample of what Dean had to say about the myriad considerations when choosing, and operating, a large language model:</p><p>"The more advanced your use case is, the more requirements you have, the more data you attach to it, the more complex your prompts — ll this is going to change your inference time. </p><p>"I liken this to perceived waiting time for an elevator. There's data scientists at places like Otis that actually work on that problem. You know, no one wants to wait 45 seconds for an elevator, but taking the stairs will take them half an hour if they're going to the top floor of . . . something. Same thing here: If I can generate an outcome in 90 seconds, it's still too long from the user's perspective, even if them building out and figuring out the data and building that report [would have] took them four hours . . . two days."</p><p>Follow everyone:</p><p><a href="" target="_blank">Dean De Beer</a></p><p><a href="" target="_blank">Kevin Tian</a></p><p><a href="" target="_blank">Travis McPeak</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Wed, 15 May 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Dean De Beer, Kevin Tian, Travis McPeak, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/security-founders-talk-shop-about-generative-ai-JeSoaIOH</link>
      <content:encoded><![CDATA[<p>In this bonus episode, recorded live at our San Francisco office, security-startup founders Dean De Beer (Command Zero), Kevin Tian (Doppel), and Travis McPeak (Resourcely) share their thoughts on generative AI, as well as their experiences building with LLMs and dealing with LLM-based threats.</p><p>Here's a sample of what Dean had to say about the myriad considerations when choosing, and operating, a large language model:</p><p>"The more advanced your use case is, the more requirements you have, the more data you attach to it, the more complex your prompts — ll this is going to change your inference time. </p><p>"I liken this to perceived waiting time for an elevator. There's data scientists at places like Otis that actually work on that problem. You know, no one wants to wait 45 seconds for an elevator, but taking the stairs will take them half an hour if they're going to the top floor of . . . something. Same thing here: If I can generate an outcome in 90 seconds, it's still too long from the user's perspective, even if them building out and figuring out the data and building that report [would have] took them four hours . . . two days."</p><p>Follow everyone:</p><p><a href="" target="_blank">Dean De Beer</a></p><p><a href="" target="_blank">Kevin Tian</a></p><p><a href="" target="_blank">Travis McPeak</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="21629013" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/3cfec823-1c99-4b68-afe1-224583ff1ec8/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=3cfec823-1c99-4b68-afe1-224583ff1ec8&amp;feed=Hb_IuXOo"/>
      <itunes:title>Security Founders Talk Shop About Generative AI</itunes:title>
      <itunes:author>Dean De Beer, Kevin Tian, Travis McPeak, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/2eefff26-e5d0-491f-934e-b5b34aa01067/3000x3000/ai-a16z-podsecurity-founders-talk-shop-about-generative-ai-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:22:31</itunes:duration>
      <itunes:summary>Founders Dean De Beer (Command Zero), Kevin Tian (Doppel), and Travis McPeak (Resourcely) share their thoughts on generative AI, as well as their experiences building with LLMs and dealing with LLM-based threats.</itunes:summary>
      <itunes:subtitle>Founders Dean De Beer (Command Zero), Kevin Tian (Doppel), and Travis McPeak (Resourcely) share their thoughts on generative AI, as well as their experiences building with LLMs and dealing with LLM-based threats.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, security, large language models, machine learning, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>8</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">2d2d73c9-189a-44d2-85dd-c55ecf5c729a</guid>
      <title>How to Think About Foundation Models for Cybersecurity</title>
      <description><![CDATA[<p>In this episode of the AI + a16z podcast, a16z General Partner Zane Lackey and a16z Partner Joel de la Garza sit down with Derrick Harris to discuss how generative AI — LLMs, in particular — and foundation models could effect profound change in cybersecurity. After years of AI-washing by security vendors, they explain why the hype is legitimate this time as AI provides  a real opportunity to help security teams cut through the noise and automate away the types of drudgery that lead to mistakes.</p><p>"Often when you're running a security team, you're not only drowning in noise, but you're drowning in just the volume of things going on," Zane explains. "And so I think a lot of security teams are excited about, 'Can we utilize AI and LLMs to really take at least some of that off of our plate?'</p><p>"I think it's still very much an open question of how far they go in helping us, but even taking some meaningful percentage off of our plate in terms of overall work is going to really help security teams overall."</p><p>Follow everyone:</p><p><a href="https://twitter.com/zanelackey" target="_blank">Zane Lackey</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 10 May 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Zane Lackey, Joel de la Garza, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/how-to-think-about-foundation-models-for-cybersecurity-MOVCREQh</link>
      <content:encoded><![CDATA[<p>In this episode of the AI + a16z podcast, a16z General Partner Zane Lackey and a16z Partner Joel de la Garza sit down with Derrick Harris to discuss how generative AI — LLMs, in particular — and foundation models could effect profound change in cybersecurity. After years of AI-washing by security vendors, they explain why the hype is legitimate this time as AI provides  a real opportunity to help security teams cut through the noise and automate away the types of drudgery that lead to mistakes.</p><p>"Often when you're running a security team, you're not only drowning in noise, but you're drowning in just the volume of things going on," Zane explains. "And so I think a lot of security teams are excited about, 'Can we utilize AI and LLMs to really take at least some of that off of our plate?'</p><p>"I think it's still very much an open question of how far they go in helping us, but even taking some meaningful percentage off of our plate in terms of overall work is going to really help security teams overall."</p><p>Follow everyone:</p><p><a href="https://twitter.com/zanelackey" target="_blank">Zane Lackey</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="35620197" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/cbcad1cc-5698-4370-9283-bc87743697a2/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=cbcad1cc-5698-4370-9283-bc87743697a2&amp;feed=Hb_IuXOo"/>
      <itunes:title>How to Think About Foundation Models for Cybersecurity</itunes:title>
      <itunes:author>Zane Lackey, Joel de la Garza, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/ae4bcedf-a416-4b31-bade-58e26bc6f8ca/3000x3000/ai-a16z-pod-foundation-models-for-cybersecurity-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:37:06</itunes:duration>
      <itunes:summary>a16z&apos;s Zane Lackey and Joel de la Garza discuss the state of the cybersecurity market vis a vis generative AI, foundation models, and large language models — and explain why 2024 could be a watershed year for security teams.</itunes:summary>
      <itunes:subtitle>a16z&apos;s Zane Lackey and Joel de la Garza discuss the state of the cybersecurity market vis a vis generative AI, foundation models, and large language models — and explain why 2024 could be a watershed year for security teams.</itunes:subtitle>
      <itunes:keywords>generative ai, llms, artificial intelligence, ai/ml, security, large language models, machine learning, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>7</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">94a44d54-1757-4d9b-8acc-33023955c1cd</guid>
      <title>Securing the Software Supply Chain with LLMs</title>
      <description><![CDATA[<p>Socket Founder and CEO Feross Aboukhadijeh joins a16z's Joel de la Garza and Derrick Harris to discuss the open-source software supply chain. Feross and Joel share their thoughts and insights on topics ranging from the recent XZutils attack to how large language models can help overcome understaffed security teams and overwhelmed developers. </p><p>Despite some increasingly sophisticated attacks making headlines and compromising countless systems, they're optimistic that LLMs, in particular, could be a turning point for security blue teams. As Feross sums up one possibility:</p><p>"The way we think about gen AI on the defensive side is that it's not as good as a human looking at the code, but it's something. . . . Our challenge is that we want to scan all the open source code that exists out there. That is not something you can pay humans to do. That is not scalable at all. But, with the right techniques, with the right pre-filtering stages, you can actually put a lot of that stuff through LLMs and out the other side will pop a list of of risky packages.</p><p>"And then that's a much smaller number that you can have humans take a look at. And so we're using it as a tool . . . to find the needle in the haystack, what is worth looking at. It's not perfect, but it can help cut down on the noise and it can even make this problem tractable, which previously wasn't even tractable."</p><p>More about Socket and  cybersecurity:</p><p><a href="" target="_blank">Socket</a></p><p><a href="https://a16z.com/announcement/investing-in-socket/" target="_blank">Investing in Socket</a></p><p><a href="https://a16z.com/hiring-a-chief-information-security-officer/" target="_blank">Hiring a CISO</a></p><p>Follow everyone :</p><p><a href="https://twitter.com/feross" target="_blank">Feross Aboukhadijeh</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 3 May 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Feross Aboukhadijeh, Joel de la Garza, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/securing-the-software-supply-chain-with-llms-3552jvU_</link>
      <content:encoded><![CDATA[<p>Socket Founder and CEO Feross Aboukhadijeh joins a16z's Joel de la Garza and Derrick Harris to discuss the open-source software supply chain. Feross and Joel share their thoughts and insights on topics ranging from the recent XZutils attack to how large language models can help overcome understaffed security teams and overwhelmed developers. </p><p>Despite some increasingly sophisticated attacks making headlines and compromising countless systems, they're optimistic that LLMs, in particular, could be a turning point for security blue teams. As Feross sums up one possibility:</p><p>"The way we think about gen AI on the defensive side is that it's not as good as a human looking at the code, but it's something. . . . Our challenge is that we want to scan all the open source code that exists out there. That is not something you can pay humans to do. That is not scalable at all. But, with the right techniques, with the right pre-filtering stages, you can actually put a lot of that stuff through LLMs and out the other side will pop a list of of risky packages.</p><p>"And then that's a much smaller number that you can have humans take a look at. And so we're using it as a tool . . . to find the needle in the haystack, what is worth looking at. It's not perfect, but it can help cut down on the noise and it can even make this problem tractable, which previously wasn't even tractable."</p><p>More about Socket and  cybersecurity:</p><p><a href="" target="_blank">Socket</a></p><p><a href="https://a16z.com/announcement/investing-in-socket/" target="_blank">Investing in Socket</a></p><p><a href="https://a16z.com/hiring-a-chief-information-security-officer/" target="_blank">Hiring a CISO</a></p><p>Follow everyone :</p><p><a href="https://twitter.com/feross" target="_blank">Feross Aboukhadijeh</a></p><p><a href="https://www.linkedin.com/in/3448827723723234/" target="_blank">Joel de la Garza</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="37403629" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/016a8ebd-f786-4d9c-93e6-3a458bfc816c/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=016a8ebd-f786-4d9c-93e6-3a458bfc816c&amp;feed=Hb_IuXOo"/>
      <itunes:title>Securing the Software Supply Chain with LLMs</itunes:title>
      <itunes:author>Feross Aboukhadijeh, Joel de la Garza, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/2166ecd8-a3b9-40bf-a37d-739da804a44a/3000x3000/ai-a16z-pod-securing-the-software-supply-chain-with-llms-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:38:57</itunes:duration>
      <itunes:summary>Socket&apos;s Feross Aboukhadijeh joins a16z&apos;s Joel de la Garza to discuss the state of security for the open-source software supply chain, including how large language models can help both developers and security teams identify issues.</itunes:summary>
      <itunes:subtitle>Socket&apos;s Feross Aboukhadijeh joins a16z&apos;s Joel de la Garza to discuss the state of security for the open-source software supply chain, including how large language models can help both developers and security teams identify issues.</itunes:subtitle>
      <itunes:keywords>llms, artificial intelligence, ai/ml, large language models, machine learning, open source, cybersecurity</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>6</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">17ff6959-c9b1-4c9e-9121-765483f4ffb3</guid>
      <title>ARCHIVE: GPT-3 Hype</title>
      <description><![CDATA[<p>In this episode, though, we’re traveling back in time to distant — in AI years, at least — past of 2020. Because amid all the news over the past 18 or so months, it’s easy to forget that generative AI — and LLMs, in particular — have been around for a while. OpenAI released its GPT-2 paper in late 2018, which excited the AI research community, and in 2020 made GPT-3 (as well as other capabilities) publicly available for the first time via its API. This episode dates back to that point in time (it was published in July 2020), when GPT-3 piqued the interest of the broader developer community and people really started testing what was possible.</p><p>And although it doesn’t predict the precambrian explosion of multimodal models, regulatory and copyright debate, and entrepreneurial activity that would hit a couple of years later — and who could have? — it does set the table for some of the bigger — and still unanswered — questions about what tools like LLMs actually mean from a business perspective. And, perhaps more importantly, what they ultimately mean for how we define intelligence.</p><p>So set your wayback machine to the seemingly long ago summer of 2020 and enjoy a16z’s Sonal Chokshi and Frank Chen discussing the advent of commercially available LLMs.</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Wed, 1 May 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Frank Chen, Sonal Chokshi, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/archive-gpt-3-hype-KhECv3pm</link>
      <content:encoded><![CDATA[<p>In this episode, though, we’re traveling back in time to distant — in AI years, at least — past of 2020. Because amid all the news over the past 18 or so months, it’s easy to forget that generative AI — and LLMs, in particular — have been around for a while. OpenAI released its GPT-2 paper in late 2018, which excited the AI research community, and in 2020 made GPT-3 (as well as other capabilities) publicly available for the first time via its API. This episode dates back to that point in time (it was published in July 2020), when GPT-3 piqued the interest of the broader developer community and people really started testing what was possible.</p><p>And although it doesn’t predict the precambrian explosion of multimodal models, regulatory and copyright debate, and entrepreneurial activity that would hit a couple of years later — and who could have? — it does set the table for some of the bigger — and still unanswered — questions about what tools like LLMs actually mean from a business perspective. And, perhaps more importantly, what they ultimately mean for how we define intelligence.</p><p>So set your wayback machine to the seemingly long ago summer of 2020 and enjoy a16z’s Sonal Chokshi and Frank Chen discussing the advent of commercially available LLMs.</p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="32201475" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/bf6c7ec6-5533-4b64-a11a-b236d4afce14/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=bf6c7ec6-5533-4b64-a11a-b236d4afce14&amp;feed=Hb_IuXOo"/>
      <itunes:title>ARCHIVE: GPT-3 Hype</itunes:title>
      <itunes:author>Frank Chen, Sonal Chokshi, Derrick Harris</itunes:author>
      <itunes:duration>00:33:29</itunes:duration>
      <itunes:summary>In this bonus episode from the a16z Podcast archives, set your wayback machine to the seemingly long ago summer of 2020, and enjoy a16z’s Sonal Chokshi and Frank Chen discussing GPT-3 and the advent of commercially available LLMs.</itunes:summary>
      <itunes:subtitle>In this bonus episode from the a16z Podcast archives, set your wayback machine to the seemingly long ago summer of 2020, and enjoy a16z’s Sonal Chokshi and Frank Chen discussing GPT-3 and the advent of commercially available LLMs.</itunes:subtitle>
      <itunes:keywords>llms, artificial intelligence, ai/ml, large language models, machine learning, gpt-3</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>5</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">452b487a-b980-45ae-82aa-403bccfb24c9</guid>
      <title>Vector Databases and the Power of RAG</title>
      <description><![CDATA[<p><a href="https://www.pinecone.io/" target="_blank">Pinecone</a> Founder and CEO Edo Liberty joins a16z's Satish Talluri and Derrick Harris to discuss the promises, challenges, and opportunities for vector databases and retrieval augmented generation (RAG). He also shares insights and highlights from a decades-long career in machine learning, which includes stints running research teams at both Yahoo and Amazon Web Services.</p><p>Because he's been at this a long time,  and despite its utility, Edo understands that RAG — like most of today's popular AI concepts — is still very much a progress:</p><p>"I think RAG  today is where transformers were in 2017. It's clunky and weird and hard to get right. And it  has a lot of sharp edges, but it already does something amazing. Sometimes, most of the time, the very early adopters and the very advanced users are already picking it up and running with it and lovingly deal with all the sharp edges ...</p><p>"Making progress on RAG, making progress on information retrieval, and making progress on making AI more knowledgeable and less hallucinatory and more dependable, is a complete greenfield today. There's an infinite amount of innovation that will have to go into it."</p><p><strong>More about Pinecone and RAG:</strong></p><p><a href="https://a16z.com/announcement/investing-in-pinecone/" target="_blank">Investing in Pinecone</a></p><p><a href="https://www.pinecone.io/learn/retrieval-augmented-generation/" target="_blank">Retrieval Augmented Generation (RAG)</a></p><p><a href="https://a16z.com/emerging-architectures-for-llm-applications/" target="_blank">Emerging Architectures for LLM Applications</a></p><p><strong>Follow everyone on X:</strong></p><p><a href="https://twitter.com/EdoLiberty" target="_blank">Edo Liberty</a></p><p><a href="https://twitter.com/satishtalluri" target="_blank">Satish Talluri</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 26 Apr 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Edo Liberty, Satish Talluri, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/vector-databases-and-the-power-of-rag-5cAv48Af</link>
      <content:encoded><![CDATA[<p><a href="https://www.pinecone.io/" target="_blank">Pinecone</a> Founder and CEO Edo Liberty joins a16z's Satish Talluri and Derrick Harris to discuss the promises, challenges, and opportunities for vector databases and retrieval augmented generation (RAG). He also shares insights and highlights from a decades-long career in machine learning, which includes stints running research teams at both Yahoo and Amazon Web Services.</p><p>Because he's been at this a long time,  and despite its utility, Edo understands that RAG — like most of today's popular AI concepts — is still very much a progress:</p><p>"I think RAG  today is where transformers were in 2017. It's clunky and weird and hard to get right. And it  has a lot of sharp edges, but it already does something amazing. Sometimes, most of the time, the very early adopters and the very advanced users are already picking it up and running with it and lovingly deal with all the sharp edges ...</p><p>"Making progress on RAG, making progress on information retrieval, and making progress on making AI more knowledgeable and less hallucinatory and more dependable, is a complete greenfield today. There's an infinite amount of innovation that will have to go into it."</p><p><strong>More about Pinecone and RAG:</strong></p><p><a href="https://a16z.com/announcement/investing-in-pinecone/" target="_blank">Investing in Pinecone</a></p><p><a href="https://www.pinecone.io/learn/retrieval-augmented-generation/" target="_blank">Retrieval Augmented Generation (RAG)</a></p><p><a href="https://a16z.com/emerging-architectures-for-llm-applications/" target="_blank">Emerging Architectures for LLM Applications</a></p><p><strong>Follow everyone on X:</strong></p><p><a href="https://twitter.com/EdoLiberty" target="_blank">Edo Liberty</a></p><p><a href="https://twitter.com/satishtalluri" target="_blank">Satish Talluri</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="35226479" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/3248c4c4-1e56-4cb9-89f2-fe0f6da51544/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=3248c4c4-1e56-4cb9-89f2-fe0f6da51544&amp;feed=Hb_IuXOo"/>
      <itunes:title>Vector Databases and the Power of RAG</itunes:title>
      <itunes:author>Edo Liberty, Satish Talluri, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/b8f9d80f-fe37-498c-81ae-3f527460db5b/3000x3000/ai-a16z-pod-vector-databases-and-the-power-of-rag-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:36:41</itunes:duration>
      <itunes:summary>Pinecone Founder and CEO Edo Liberty discusses the promises, challenges, and opportunities for vector databases and retrieval augmented generation (RAG). He also shares insights and highlights from his decades-long career in machine learning.</itunes:summary>
      <itunes:subtitle>Pinecone Founder and CEO Edo Liberty discusses the promises, challenges, and opportunities for vector databases and retrieval augmented generation (RAG). He also shares insights and highlights from his decades-long career in machine learning.</itunes:subtitle>
      <itunes:keywords>generative ai, deep learning, llms, artificial intelligence, ai/ml, large language models, machine learning, data</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>4</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">9d7fc21d-d3e2-44b1-854e-40e8cf441390</guid>
      <title>Remaking the UI for AI</title>
      <description><![CDATA[<p>a16z General Partner Anjney Midha joins the podcast to discuss what's happening with hardware for artificial intelligence. Nvidia might have cornered the market on training workloads for now, but he believes there's a big opportunity at the inference layer — especially for wearable or similar devices that can become a natural part of our everyday interactions. </p><p>Here's one small passage that speaks to his larger thesis on where we're heading:</p><p>"I think why we're seeing so many developers flock to Ollama is because there is a lot of demand from consumers to interact with language models in private ways. And that means that they're going to have to figure out how to get the models to run locally without ever leaving without ever the user's context, and data leaving the user's device. And that's going to result, I think, in a renaissance of new kinds of chips that are capable of handling massive workloads of inference on device.</p><p>"We are yet to see those unlocked, but the good news is that open source models are phenomenal at unlocking efficiency.  The open source language model ecosystem is just so ravenous."</p><p>More from Anjney:</p><p><a href="https://a16z.com/podcast/the-quest-for-agi-q-self-play-and-synthetic-data/">The Quest for AGI: Q*, Self-Play, and Synthetic Data</a></p><p><a href="https://a16z.com/podcast/making-the-most-of-open-source-ai/" target="_blank"><strong>Making the Most of Open Source AI</strong></a></p><p><a href="https://a16z.com/podcast/safety-in-numbers-keeping-ai-open/" target="_blank"><strong>Safety in Numbers: Keeping AI Open</strong></a></p><p><a href="" target="_blank"><strong>Investing in Luma AI</strong></a></p><p>Follow everyone on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 19 Apr 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Anjney Midha, Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/remaking-the-ui-for-ai-fjtNyoWs</link>
      <content:encoded><![CDATA[<p>a16z General Partner Anjney Midha joins the podcast to discuss what's happening with hardware for artificial intelligence. Nvidia might have cornered the market on training workloads for now, but he believes there's a big opportunity at the inference layer — especially for wearable or similar devices that can become a natural part of our everyday interactions. </p><p>Here's one small passage that speaks to his larger thesis on where we're heading:</p><p>"I think why we're seeing so many developers flock to Ollama is because there is a lot of demand from consumers to interact with language models in private ways. And that means that they're going to have to figure out how to get the models to run locally without ever leaving without ever the user's context, and data leaving the user's device. And that's going to result, I think, in a renaissance of new kinds of chips that are capable of handling massive workloads of inference on device.</p><p>"We are yet to see those unlocked, but the good news is that open source models are phenomenal at unlocking efficiency.  The open source language model ecosystem is just so ravenous."</p><p>More from Anjney:</p><p><a href="https://a16z.com/podcast/the-quest-for-agi-q-self-play-and-synthetic-data/">The Quest for AGI: Q*, Self-Play, and Synthetic Data</a></p><p><a href="https://a16z.com/podcast/making-the-most-of-open-source-ai/" target="_blank"><strong>Making the Most of Open Source AI</strong></a></p><p><a href="https://a16z.com/podcast/safety-in-numbers-keeping-ai-open/" target="_blank"><strong>Safety in Numbers: Keeping AI Open</strong></a></p><p><a href="" target="_blank"><strong>Investing in Luma AI</strong></a></p><p>Follow everyone on X:</p><p><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="37149510" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/d24e8f3e-f5f6-4b34-a6bf-ec17b7223d5a/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=d24e8f3e-f5f6-4b34-a6bf-ec17b7223d5a&amp;feed=Hb_IuXOo"/>
      <itunes:title>Remaking the UI for AI</itunes:title>
      <itunes:author>Anjney Midha, Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/3bca26e5-9db5-4a03-be5d-31ad2040f549/3000x3000/ai-a16z-pod-remaking-the-ui-for-ai-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:38:41</itunes:duration>
      <itunes:summary>a16z General Partner Anjney Midha shares his perspective on how hardware for artificial intelligence will improve —especially at the inference layer — and what that means for how we&apos;ll build, train, and interact with AI models.</itunes:summary>
      <itunes:subtitle>a16z General Partner Anjney Midha shares his perspective on how hardware for artificial intelligence will improve —especially at the inference layer — and what that means for how we&apos;ll build, train, and interact with AI models.</itunes:subtitle>
      <itunes:keywords>gpus, generative ai, llms, artificial intelligence, ai/ml, large language models, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>3</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">531fe7a0-94a9-46b1-8dc0-0d73160516da</guid>
      <title>Making the Most of Open Source in AI</title>
      <description><![CDATA[<p>There are few terms in the world of AI — if any — that invoke more of a reaction than a simple four-letter word: Open. Whether it’s industry debates over business models and the actual definition of open, or the US government actively discussing how to regulate open models, seemingly everyone has an opinion on what it means for AI models to be open. The good, the bad, and the ugly.</p><p>But to be fair, there’s good reason for this. In a world where many developers have come to expect open source tools at every level of the stack, the idea of powerful models locked behind enterprise licenses and corporate ethics can be disconcerting — especially for a technology as game-changing as AI promises to be. It’s a matter of who has the ability to innovate in the space, and whose release schedules and guardrails they’re beholden to.</p><p>This is why, back in February, a16z convened a panel of experts to discuss the state — and future — of open source AI models.</p><p>Featuring:</p><ul><li>Jim Zemlin (Executive Director, Linux Foundation)</li><li>Mitchell Baker (Executive Chair, Mozilla Corp.)</li><li><a href="https://twitter.com/percyliang" target="_blank">Percy Liang</a> (Associate Professor, Stanford; Cofounder, Together AI)</li><li><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a> (General Partner, a16z)</li><li><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a> (Editorial Partner, a16z)</li></ul>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 12 Apr 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Jim Zemlin, Mitchell Baker, Percy Liang, Anjney Midha)</author>
      <link>https://ai-a16z.simplecast.com/episodes/making-the-most-of-open-source-in-ai-tGGTz6m_</link>
      <content:encoded><![CDATA[<p>There are few terms in the world of AI — if any — that invoke more of a reaction than a simple four-letter word: Open. Whether it’s industry debates over business models and the actual definition of open, or the US government actively discussing how to regulate open models, seemingly everyone has an opinion on what it means for AI models to be open. The good, the bad, and the ugly.</p><p>But to be fair, there’s good reason for this. In a world where many developers have come to expect open source tools at every level of the stack, the idea of powerful models locked behind enterprise licenses and corporate ethics can be disconcerting — especially for a technology as game-changing as AI promises to be. It’s a matter of who has the ability to innovate in the space, and whose release schedules and guardrails they’re beholden to.</p><p>This is why, back in February, a16z convened a panel of experts to discuss the state — and future — of open source AI models.</p><p>Featuring:</p><ul><li>Jim Zemlin (Executive Director, Linux Foundation)</li><li>Mitchell Baker (Executive Chair, Mozilla Corp.)</li><li><a href="https://twitter.com/percyliang" target="_blank">Percy Liang</a> (Associate Professor, Stanford; Cofounder, Together AI)</li><li><a href="https://twitter.com/AnjneyMidha" target="_blank">Anjney Midha</a> (General Partner, a16z)</li><li><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a> (Editorial Partner, a16z)</li></ul>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="39318300" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/15933aa3-2155-40dd-988f-205fe19ac008/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=15933aa3-2155-40dd-988f-205fe19ac008&amp;feed=Hb_IuXOo"/>
      <itunes:title>Making the Most of Open Source in AI</itunes:title>
      <itunes:author>Jim Zemlin, Mitchell Baker, Percy Liang, Anjney Midha</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/9c471bfa-f6f2-488d-9e84-0759e4a72102/3000x3000/ai-a16z-pod-making-the-most-of-open-source-in-ai-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:40:57</itunes:duration>
      <itunes:summary>a16z General Partner Anjney Midha leads an insightful panel discussion about what the AI community can learn from past open source efforts. Panelists include Jim Zemlin (Linux Foundation), Mitchell Baker (Mozilla), and Percy Liang (Stanford / Together AI).</itunes:summary>
      <itunes:subtitle>a16z General Partner Anjney Midha leads an insightful panel discussion about what the AI community can learn from past open source efforts. Panelists include Jim Zemlin (Linux Foundation), Mitchell Baker (Mozilla), and Percy Liang (Stanford / Together AI).</itunes:subtitle>
      <itunes:keywords>artificial intelligence, ai/ml, ai, entrepreneurship, machine learning, open source</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>2</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">403ed693-4b94-4102-81b3-ae0540ec29c1</guid>
      <title>Scoping the Enterprise LLM Market</title>
      <description><![CDATA[<p>Naveen Rao, vice president of generative AI at Databricks, joins a16z's Matt Bornstein and Derrick Harris to discuss enterprise usage of LLMs and generative AI. Naveen is particularly knowledgeable about the space, having spent years building AI chips first at Qualcomm and then as the founder of AI chip startup Nervana Systems back in 2014. Intel acquired Nervana in 2016.</p><p>After a stint at Intel, Rao re-emerged with MosaicML in 2021. This time, he focused on the software side of things, helping customers train their own LLMs, and also fine-tune foundation models, on top of an optimized tech stack. Databricks acquired Mosaic in July of 2023.</p><p>This discussion covers the gamut of generative AI topics — from basic theory to specialized chips — to  although we focus on how the enterprise LLM market is shaping up. Naveen also shares his thoughts on why he prefers finally being part of the technology in-crowd, even if it means he can’t escape talking about AI outside of work.</p><p>More information:</p><p><a href="https://www.databricks.com/product/machine-learning/large-language-models" target="_blank">LLMs at Databricks</a></p><p><a href="https://www.databricks.com/research/mosaic" target="_blank">Mosaic Research</a></p><p><a href="https://a16z.com/category/enterprise/enterprise-x-ai/" target="_blank">More AI content from a16z</a></p><p>Follow everyone on X:</p><p><a href="https://twitter.com/NaveenGRao" target="_blank">Naveen Rao</a></p><p><a href="https://twitter.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></description>
      <pubDate>Fri, 12 Apr 2024 14:00:00 +0000</pubDate>
      <author>dharris@a16z.com (Derrick Harris, Matt Bornstein, Naveen Rao)</author>
      <link>https://ai-a16z.simplecast.com/episodes/scoping-the-enterprise-llm-market-9d41HJPD</link>
      <content:encoded><![CDATA[<p>Naveen Rao, vice president of generative AI at Databricks, joins a16z's Matt Bornstein and Derrick Harris to discuss enterprise usage of LLMs and generative AI. Naveen is particularly knowledgeable about the space, having spent years building AI chips first at Qualcomm and then as the founder of AI chip startup Nervana Systems back in 2014. Intel acquired Nervana in 2016.</p><p>After a stint at Intel, Rao re-emerged with MosaicML in 2021. This time, he focused on the software side of things, helping customers train their own LLMs, and also fine-tune foundation models, on top of an optimized tech stack. Databricks acquired Mosaic in July of 2023.</p><p>This discussion covers the gamut of generative AI topics — from basic theory to specialized chips — to  although we focus on how the enterprise LLM market is shaping up. Naveen also shares his thoughts on why he prefers finally being part of the technology in-crowd, even if it means he can’t escape talking about AI outside of work.</p><p>More information:</p><p><a href="https://www.databricks.com/product/machine-learning/large-language-models" target="_blank">LLMs at Databricks</a></p><p><a href="https://www.databricks.com/research/mosaic" target="_blank">Mosaic Research</a></p><p><a href="https://a16z.com/category/enterprise/enterprise-x-ai/" target="_blank">More AI content from a16z</a></p><p>Follow everyone on X:</p><p><a href="https://twitter.com/NaveenGRao" target="_blank">Naveen Rao</a></p><p><a href="https://twitter.com/BornsteinMatt" target="_blank">Matt Bornstein</a></p><p><a href="https://twitter.com/derrickharris" target="_blank">Derrick Harris</a></p>
<p><p>Check out everything a16z is doing with artificial intelligence <a href="https://a16z.com/ai/" target="_blank">here</a>, including articles, projects, and more podcasts.</p></p>]]></content:encoded>
      <enclosure length="42666989" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/f7b6d81a-5e78-4c8c-8f0b-3aeef459130e/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=f7b6d81a-5e78-4c8c-8f0b-3aeef459130e&amp;feed=Hb_IuXOo"/>
      <itunes:title>Scoping the Enterprise LLM Market</itunes:title>
      <itunes:author>Derrick Harris, Matt Bornstein, Naveen Rao</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/e1246b30-bca2-46ea-a72c-e508cb2293d9/3000x3000/ai-a16z-pod-scoping-the-enterprise-llm-market-1080x1080.jpg?aid=rss_feed"/>
      <itunes:duration>00:44:26</itunes:duration>
      <itunes:summary>Naveen Rao of Databricks — and former founder of Nervana Systems and MosaicML — shares his thoughts on the state of large language models (LLMs), how they&apos;ll evolve, and how enterprises will adopt them.  </itunes:summary>
      <itunes:subtitle>Naveen Rao of Databricks — and former founder of Nervana Systems and MosaicML — shares his thoughts on the state of large language models (LLMs), how they&apos;ll evolve, and how enterprises will adopt them.  </itunes:subtitle>
      <itunes:keywords>big data, generative ai, artificial intelligence, ai, entrepreneurship, large language models, machine learning, enterprise tech</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>1</itunes:episode>
    </item>
    <item>
      <guid isPermaLink="false">f1b90b59-6039-4650-bec6-ca2149eaea9a</guid>
      <title>Welcome to the AI + a16z podcast</title>
      <description><![CDATA[The AI + a16z podcast captures our thinking on AI across a broad swath of areas, from the infrastructure that powers today’s foundation models to how specific tools, like LLMs, are reshaping the hiring process. Looking forward, you can expect to hear about a list of topics that includes the latest advances in generative AI, cybersecurity, and the emerging stack of tools for building and running LLMs.Check out everything a16z is doing with artificial intelligence here,
including articles, projects, and more podcasts.
]]></description>
      <pubDate>Mon, 8 Apr 2024 19:17:33 +0000</pubDate>
      <author>dharris@a16z.com (Derrick Harris)</author>
      <link>https://ai-a16z.simplecast.com/episodes/welcome-to-the-ai-a16z-podcast-XkLsxrUq</link>
      <enclosure length="2301016" type="audio/mpeg" url="https://mgln.ai/e/1344/afp-848985-injected.calisto.simplecastaudio.com/112866f3-1a50-4a8d-b12e-850b73e71b33/episodes/c2c34a28-964e-4572-a29e-33c43bfe78f6/audio/128/default.mp3?aid=rss_feed&amp;awCollectionId=112866f3-1a50-4a8d-b12e-850b73e71b33&amp;awEpisodeId=c2c34a28-964e-4572-a29e-33c43bfe78f6&amp;feed=Hb_IuXOo"/>
      <itunes:title>Welcome to the AI + a16z podcast</itunes:title>
      <itunes:author>Derrick Harris</itunes:author>
      <itunes:image href="https://image.simplecastcdn.com/images/89450696-5007-4713-8168-eea085e49626/7409510f-eabf-481d-92e4-c3747040c5eb/3000x3000/ai-a16z-podcast-cover-art-2.jpg?aid=rss_feed"/>
      <itunes:duration>00:02:22</itunes:duration>
      <itunes:summary>The AI + a16z podcast captures our thinking on AI across a broad swath of areas, from the infrastructure that powers today’s foundation models to how specific tools, like LLMs, are reshaping the hiring process. Looking forward, you can expect to hear about a list of topics that includes the latest advances in generative AI, cybersecurity, and the emerging stack of tools for building and running LLMs.</itunes:summary>
      <itunes:subtitle>The AI + a16z podcast captures our thinking on AI across a broad swath of areas, from the infrastructure that powers today’s foundation models to how specific tools, like LLMs, are reshaping the hiring process. Looking forward, you can expect to hear about a list of topics that includes the latest advances in generative AI, cybersecurity, and the emerging stack of tools for building and running LLMs.</itunes:subtitle>
      <itunes:keywords>artificial intelligence, technology, ai, machine learning</itunes:keywords>
      <itunes:explicit>false</itunes:explicit>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:episode>0</itunes:episode>
    </item>
  </channel>
</rss>