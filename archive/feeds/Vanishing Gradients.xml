<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" encoding="UTF-8" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:admin="http://webns.net/mvcb/" xmlns:atom="http://www.w3.org/2005/Atom/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:podcast="https://podcastindex.org/namespace/1.0" xmlns:fireside="https://fireside.fm/modules/rss/fireside">
  <channel>
    <fireside:hostname>feed01.fireside.fm</fireside:hostname>
    <fireside:genDate>Thu, 26 Jun 2025 12:46:45 -0500</fireside:genDate>
    <generator>Fireside (https://fireside.fm)</generator>
    <title>Vanishing Gradients</title>
    <link>https://vanishinggradients.fireside.fm</link>
    <pubDate>Fri, 27 Jun 2025 03:45:05 +1000</pubDate>
    <description>A podcast about all things data, brought to you by data scientist Hugo Bowne-Anderson.
It's time for more critical conversations about the challenges in our industry in order to build better compasses for the solution space! To this end, this podcast will consist of long-format conversations between Hugo and other people who work broadly in the data science, machine learning, and AI spaces. We'll dive deep into all the moving parts of the data world, so if you're new to the space, you'll have an opportunity to learn from the experts. And if you've been around for a while, you'll find out what's happening in many other parts of the data world.
</description>
    <language>en-us</language>
    <copyright>© 2025 Hugo Bowne-Anderson</copyright>
    <itunes:type>episodic</itunes:type>
    <itunes:subtitle>a data podcast with hugo bowne-anderson</itunes:subtitle>
    <itunes:author>Hugo Bowne-Anderson</itunes:author>
    <itunes:summary>A podcast about all things data, brought to you by data scientist Hugo Bowne-Anderson.
It's time for more critical conversations about the challenges in our industry in order to build better compasses for the solution space! To this end, this podcast will consist of long-format conversations between Hugo and other people who work broadly in the data science, machine learning, and AI spaces. We'll dive deep into all the moving parts of the data world, so if you're new to the space, you'll have an opportunity to learn from the experts. And if you've been around for a while, you'll find out what's happening in many other parts of the data world.
</itunes:summary>
    <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
    <itunes:explicit>no</itunes:explicit>
    <itunes:keywords>data science, machine learning, AI</itunes:keywords>
    <itunes:owner>
      <itunes:name>Hugo Bowne-Anderson</itunes:name>
      <itunes:email>hugobowne@hey.com</itunes:email>
    </itunes:owner>
<itunes:category text="Technology"/>
    <item>
      <title>Episode 1: Introducing Vanishing Gradients</title>
      <link>https://vanishinggradients.fireside.fm/1</link>
      <guid isPermaLink="false">a77d732e-f7be-4b71-be2f-fd09a392bd86</guid>
      <pubDate>Wed, 16 Feb 2022 20:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/a77d732e-f7be-4b71-be2f-fd09a392bd86.mp3" length="5270212" type="audio/mpeg"/>
      <itunes:episodeType>trailer</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>In this episode, Hugo introduces the new data science podcast Vanishing Gradients. </itunes:subtitle>
      <itunes:duration>5:29</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://assets.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/a/a77d732e-f7be-4b71-be2f-fd09a392bd86/cover.jpg?v=1"/>
      <description>In this brief introduction, Hugo introduces the rationale behind launching a new data science podcast and gets excited about his upcoming guests: Jeremy Howard, Rachael Tatman, and Heather Nolis!
Original music, bleeps, and blops by local Sydney legend PlaneFace (https://planeface.bandcamp.com/album/fishing-from-an-asteroid)! 
</description>
      <itunes:keywords>data science, machine learning, AI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>In this brief introduction, Hugo introduces the rationale behind launching a new data science podcast and gets excited about his upcoming guests: Jeremy Howard, Rachael Tatman, and Heather Nolis!</p>

<p>Original music, bleeps, and blops by local Sydney legend <a href="https://planeface.bandcamp.com/album/fishing-from-an-asteroid" rel="nofollow">PlaneFace</a>!</p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>In this brief introduction, Hugo introduces the rationale behind launching a new data science podcast and gets excited about his upcoming guests: Jeremy Howard, Rachael Tatman, and Heather Nolis!</p>

<p>Original music, bleeps, and blops by local Sydney legend <a href="https://planeface.bandcamp.com/album/fishing-from-an-asteroid" rel="nofollow">PlaneFace</a>!</p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+_XPsJfIe</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+_XPsJfIe" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 51: Why We Built an MCP Server and What Broke First</title>
      <link>https://vanishinggradients.fireside.fm/51</link>
      <guid isPermaLink="false">c45cdd9e-56a6-4b90-8ccf-3acd0c697415</guid>
      <pubDate>Fri, 27 Jun 2025 03:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/c45cdd9e-56a6-4b90-8ccf-3acd0c697415.mp3" length="45788781" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>What does it take to actually ship LLM-powered features, and what breaks when you connect them to real production data?

In this episode, we hear from Philip Carter — then a Principal PM at Honeycomb and now a Product Management Director at Salesforce. In early 2023, he helped build one of the first LLM-powered SaaS features to ship to real users. More recently, he and his team built a production-ready MCP server.</itunes:subtitle>
      <itunes:duration>47:41</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>What does it take to actually ship LLM-powered features, and what breaks when you connect them to real production data?
In this episode, we hear from Philip Carter — then a Principal PM at Honeycomb and now a Product Management Director at Salesforce. In early 2023, he helped build one of the first LLM-powered SaaS features to ship to real users. More recently, he and his team built a production-ready MCP server.
We cover:
    • How to evaluate LLM systems using human-aligned judges
    • The spreadsheet-driven process behind shipping Honeycomb’s first LLM feature
    • The challenges of tool usage, prompt templates, and flaky model behavior
    • Where MCP shows promise, and where it breaks in the real world
If you’re working on LLMs in production, this one’s for you!
LINKS
So We Shipped an AI Product: Did it Work? by Philip Carter (https://www.honeycomb.io/blog/we-shipped-ai-product)
Vanishing Gradients YouTube Channel (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)  
Upcoming Events on Luma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
Hugo's recent newsletter about upcoming events and more! (https://hugobowne.substack.com/p/ai-as-a-civilizational-technology)
🎓 Learn more:
Hugo's course: Building LLM Applications for Data Scientists and Software Engineers (https://maven.com/s/course/d56067f338) — next cohort starts July 8: https://maven.com/s/course/d56067f338
📺 Watch the video version on YouTube: YouTube link (https://youtu.be/JDMzdaZh9Ig) 
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>What does it take to actually ship LLM-powered features, and what breaks when you connect them to real production data?</p>

<p>In this episode, we hear from Philip Carter — then a Principal PM at Honeycomb and now a Product Management Director at Salesforce. In early 2023, he helped build one of the first LLM-powered SaaS features to ship to real users. More recently, he and his team built a production-ready MCP server.</p>

<p>We cover:<br>
    • How to evaluate LLM systems using human-aligned judges<br>
    • The spreadsheet-driven process behind shipping Honeycomb’s first LLM feature<br>
    • The challenges of tool usage, prompt templates, and flaky model behavior<br>
    • Where MCP shows promise, and where it breaks in the real world</p>

<p>If you’re working on LLMs in production, this one’s for you!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.honeycomb.io/blog/we-shipped-ai-product" rel="nofollow">So We Shipped an AI Product: Did it Work? by Philip Carter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
</ul>

<p>🎓 Learn more:</p>

<ul>
<li><strong>Hugo&#39;s course:</strong> <a href="https://maven.com/s/course/d56067f338" rel="nofollow">Building LLM Applications for Data Scientists and Software Engineers</a> — next cohort starts July 8: <a href="https://maven.com/s/course/d56067f338" rel="nofollow">https://maven.com/s/course/d56067f338</a></li>
</ul>

<p>📺 <strong>Watch the video version on YouTube:</strong> <a href="https://youtu.be/JDMzdaZh9Ig" rel="nofollow">YouTube link</a></p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>What does it take to actually ship LLM-powered features, and what breaks when you connect them to real production data?</p>

<p>In this episode, we hear from Philip Carter — then a Principal PM at Honeycomb and now a Product Management Director at Salesforce. In early 2023, he helped build one of the first LLM-powered SaaS features to ship to real users. More recently, he and his team built a production-ready MCP server.</p>

<p>We cover:<br>
    • How to evaluate LLM systems using human-aligned judges<br>
    • The spreadsheet-driven process behind shipping Honeycomb’s first LLM feature<br>
    • The challenges of tool usage, prompt templates, and flaky model behavior<br>
    • Where MCP shows promise, and where it breaks in the real world</p>

<p>If you’re working on LLMs in production, this one’s for you!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.honeycomb.io/blog/we-shipped-ai-product" rel="nofollow">So We Shipped an AI Product: Did it Work? by Philip Carter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
</ul>

<p>🎓 Learn more:</p>

<ul>
<li><strong>Hugo&#39;s course:</strong> <a href="https://maven.com/s/course/d56067f338" rel="nofollow">Building LLM Applications for Data Scientists and Software Engineers</a> — next cohort starts July 8: <a href="https://maven.com/s/course/d56067f338" rel="nofollow">https://maven.com/s/course/d56067f338</a></li>
</ul>

<p>📺 <strong>Watch the video version on YouTube:</strong> <a href="https://youtu.be/JDMzdaZh9Ig" rel="nofollow">YouTube link</a></p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+zPA1jXm4</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+zPA1jXm4" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 50: A Field Guide to Rapidly Improving AI Products -- With Hamel Husain</title>
      <link>https://vanishinggradients.fireside.fm/50</link>
      <guid isPermaLink="false">3851d92b-389c-4690-90c3-8a54ad73b7d8</guid>
      <pubDate>Tue, 17 Jun 2025 18:30:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/3851d92b-389c-4690-90c3-8a54ad73b7d8.mp3" length="54176426" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo talks with Hamel Hussain (ex-Airbnb, GitHub, DataRobot) about how to improve AI products through evaluation, error analysis, and iteration. They discuss why most teams overlook debugging LLM systems, how to prioritize what to fix, and why evals are not just metrics—but a full development process.</itunes:subtitle>
      <itunes:duration>27:42</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>If we want AI systems that actually work, we need to get much better at evaluating them, not just building more pipelines, agents, and frameworks.
In this episode, Hugo talks with Hamel Hussain (ex-Airbnb, GitHub, DataRobot) about how teams can improve AI products by focusing on error analysis, data inspection, and systematic iteration. The conversation is based on Hamel’s blog post A Field Guide to Rapidly Improving AI Products, which he joined Hugo’s class to discuss.
They cover:
🔍 Why most teams struggle to measure whether their systems are actually improving  
📊 How error analysis helps you prioritize what to fix (and when to write evals)  
🧮 Why evaluation isn’t just a metric — but a full development process  
⚠️ Common mistakes when debugging LLM and agent systems  
🛠️ How to think about the tradeoffs in adding more evals vs. fixing obvious issues  
👥 Why enabling domain experts — not just engineers — can accelerate iteration
If you’ve ever built an AI system and found yourself unsure how to make it better, this conversation is for you.
LINKS
* A Field Guide to Rapidly Improving AI Products by Hamel Husain (https://hamel.dev/blog/posts/field-guide/)
* Vanishing Gradients YouTube Channel (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)  
* Upcoming Events on Luma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
* Hugo's recent newsletter about upcoming events and more! (https://hugobowne.substack.com/p/ai-as-a-civilizational-technology)
🎓 Learn more:
Hugo's course: Building LLM Applications for Data Scientists and Software Engineers (https://maven.com/s/course/d56067f338) — next cohort starts July 8: https://maven.com/s/course/d56067f338
Hamel &amp; Shreya's course: AI Evals For Engineers &amp; PMs (https://maven.com/parlance-labs/evals?promoCode=GOHUGORGOHOME) — use code GOHUGORGOHOME for $800 off
📺 Watch the video version on YouTube: YouTube link (https://youtu.be/rWToRi2_SeY) 
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs, evas</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>If we want AI systems that actually work, we need to get much better at evaluating them, not just building more pipelines, agents, and frameworks.</p>

<p>In this episode, Hugo talks with Hamel Hussain (ex-Airbnb, GitHub, DataRobot) about how teams can improve AI products by focusing on error analysis, data inspection, and systematic iteration. The conversation is based on Hamel’s blog post <em>A Field Guide to Rapidly Improving AI Products</em>, which he joined Hugo’s class to discuss.</p>

<p>They cover:<br>
🔍 Why most teams struggle to measure whether their systems are actually improving<br><br>
📊 How error analysis helps you prioritize what to fix (and when to write evals)<br><br>
🧮 Why evaluation isn’t just a metric — but a full development process<br><br>
⚠️ Common mistakes when debugging LLM and agent systems<br><br>
🛠️ How to think about the tradeoffs in adding more evals vs. fixing obvious issues<br><br>
👥 Why enabling domain experts — not just engineers — can accelerate iteration</p>

<p>If you’ve ever built an AI system and found yourself unsure how to make it better, this conversation is for you.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://hamel.dev/blog/posts/field-guide/" rel="nofollow">A Field Guide to Rapidly Improving AI Products by Hamel Husain</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
</ul>

<hr>

<p>🎓 Learn more:</p>

<ul>
<li><strong>Hugo&#39;s course:</strong> <a href="https://maven.com/s/course/d56067f338" rel="nofollow">Building LLM Applications for Data Scientists and Software Engineers</a> — next cohort starts July 8: <a href="https://maven.com/s/course/d56067f338" rel="nofollow">https://maven.com/s/course/d56067f338</a></li>
<li><strong>Hamel &amp; Shreya&#39;s course:</strong> <a href="https://maven.com/parlance-labs/evals?promoCode=GOHUGORGOHOME" rel="nofollow">AI Evals For Engineers &amp; PMs</a> — use code <code>GOHUGORGOHOME</code> for $800 off</li>
</ul>

<p>📺 <strong>Watch the video version on YouTube:</strong> <a href="https://youtu.be/rWToRi2_SeY" rel="nofollow">YouTube link</a></p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>If we want AI systems that actually work, we need to get much better at evaluating them, not just building more pipelines, agents, and frameworks.</p>

<p>In this episode, Hugo talks with Hamel Hussain (ex-Airbnb, GitHub, DataRobot) about how teams can improve AI products by focusing on error analysis, data inspection, and systematic iteration. The conversation is based on Hamel’s blog post <em>A Field Guide to Rapidly Improving AI Products</em>, which he joined Hugo’s class to discuss.</p>

<p>They cover:<br>
🔍 Why most teams struggle to measure whether their systems are actually improving<br><br>
📊 How error analysis helps you prioritize what to fix (and when to write evals)<br><br>
🧮 Why evaluation isn’t just a metric — but a full development process<br><br>
⚠️ Common mistakes when debugging LLM and agent systems<br><br>
🛠️ How to think about the tradeoffs in adding more evals vs. fixing obvious issues<br><br>
👥 Why enabling domain experts — not just engineers — can accelerate iteration</p>

<p>If you’ve ever built an AI system and found yourself unsure how to make it better, this conversation is for you.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://hamel.dev/blog/posts/field-guide/" rel="nofollow">A Field Guide to Rapidly Improving AI Products by Hamel Husain</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
</ul>

<hr>

<p>🎓 Learn more:</p>

<ul>
<li><strong>Hugo&#39;s course:</strong> <a href="https://maven.com/s/course/d56067f338" rel="nofollow">Building LLM Applications for Data Scientists and Software Engineers</a> — next cohort starts July 8: <a href="https://maven.com/s/course/d56067f338" rel="nofollow">https://maven.com/s/course/d56067f338</a></li>
<li><strong>Hamel &amp; Shreya&#39;s course:</strong> <a href="https://maven.com/parlance-labs/evals?promoCode=GOHUGORGOHOME" rel="nofollow">AI Evals For Engineers &amp; PMs</a> — use code <code>GOHUGORGOHOME</code> for $800 off</li>
</ul>

<p>📺 <strong>Watch the video version on YouTube:</strong> <a href="https://youtu.be/rWToRi2_SeY" rel="nofollow">YouTube link</a></p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+v7GLgxDe</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+v7GLgxDe" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 49: Why Data and AI Still Break at Scale (and What to Do About It)</title>
      <link>https://vanishinggradients.fireside.fm/49</link>
      <guid isPermaLink="false">309762f9-59cd-4f24-bea5-8e692a0d870f</guid>
      <pubDate>Thu, 05 Jun 2025 14:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/309762f9-59cd-4f24-bea5-8e692a0d870f.mp3" length="117738811" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo talks with Akshay Agrawal (Marimo, ex-Google Brain, Netflix, Stanford) about why data and AI systems still break at scale—and what it takes to fix them. They dive into the limits of existing workflows, the importance of reproducibility and reactive execution, and how Marimo reimagines notebooks for modern software development.</itunes:subtitle>
      <itunes:duration>1:21:45</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/3/309762f9-59cd-4f24-bea5-8e692a0d870f/transcript.txt" type="text/plain"/>
      <description>If we want AI systems that actually work in production, we need better infrastructure—not just better models.
In this episode, Hugo talks with Akshay Agrawal (Marimo, ex-Google Brain, Netflix, Stanford) about why data and AI pipelines still break down at scale, and how we can fix the fundamentals: reproducibility, composability, and reliable execution.
They discuss:
🔁 Why reactive execution matters—and how current tools fall short
🛠️ The design goals behind Marimo, a new kind of Python notebook
⚙️ The hidden costs of traditional workflows (and what breaks at scale)
📦 What it takes to build modular, maintainable AI apps
🧪 Why debugging LLM systems is so hard—and what better tooling looks like
🌍 What we can learn from decades of tools built for and by data practitioners
Toward the end of the episode, Hugo and Akshay walk through two live demos: Hugo shares how he’s been using Marimo to prototype an app that extracts structured data from world leader bios, and Akshay shows how Marimo handles agentic workflows with memory and tool use—built entirely in a notebook.
This episode is about tools, but it’s also about culture. If you’ve ever hit a wall with your current stack—or felt like your tools were working against you—this one’s for you.
LINKS
* marimo | a next-generation Python notebook (https://marimo.io/)
* SciPy conference, 2025 (https://www.scipy2025.scipy.org/)
* Hugo's face Marimo World Leader Face Embedding demo (https://www.youtube.com/watch?v=DO21QEcLOxM)
* Vanishing Gradients YouTube Channel (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)  
* Upcoming Events on Luma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
* Hugo's recent newsletter about upcoming events and more! (https://hugobowne.substack.com/p/ai-as-a-civilizational-technology)
* Watch the podcast here on YouTube! (https://youtube.com/live/WVxAz19tgZY?feature=share)
🎓 Want to go deeper?
Check out Hugo's course: Building LLM Applications for Data Scientists and Software Engineers.
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.
Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.
Cohort starts July 8 — Use this link for a 10% discount (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10) 
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>If we want AI systems that actually work in production, we need better infrastructure—not just better models.</p>

<p>In this episode, Hugo talks with Akshay Agrawal (Marimo, ex-Google Brain, Netflix, Stanford) about why data and AI pipelines still break down at scale, and how we can fix the fundamentals: reproducibility, composability, and reliable execution.</p>

<p>They discuss:<br>
🔁 Why reactive execution matters—and how current tools fall short<br>
🛠️ The design goals behind Marimo, a new kind of Python notebook<br>
⚙️ The hidden costs of traditional workflows (and what breaks at scale)<br>
📦 What it takes to build modular, maintainable AI apps<br>
🧪 Why debugging LLM systems is so hard—and what better tooling looks like<br>
🌍 What we can learn from decades of tools built for and by data practitioners</p>

<p>Toward the end of the episode, Hugo and Akshay walk through two live demos: Hugo shares how he’s been using Marimo to prototype an app that extracts structured data from world leader bios, and Akshay shows how Marimo handles agentic workflows with memory and tool use—built entirely in a notebook.</p>

<p>This episode is about tools, but it’s also about culture. If you’ve ever hit a wall with your current stack—or felt like your tools were working against you—this one’s for you.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://marimo.io/" rel="nofollow">marimo | a next-generation Python notebook</a></li>
<li><a href="https://www.scipy2025.scipy.org/" rel="nofollow">SciPy conference, 2025</a></li>
<li><a href="https://www.youtube.com/watch?v=DO21QEcLOxM" rel="nofollow">Hugo&#39;s face Marimo World Leader Face Embedding demo</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
<li><a href="https://youtube.com/live/WVxAz19tgZY?feature=share" rel="nofollow">Watch the podcast here on YouTube!</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out Hugo&#39;s course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts July 8 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>If we want AI systems that actually work in production, we need better infrastructure—not just better models.</p>

<p>In this episode, Hugo talks with Akshay Agrawal (Marimo, ex-Google Brain, Netflix, Stanford) about why data and AI pipelines still break down at scale, and how we can fix the fundamentals: reproducibility, composability, and reliable execution.</p>

<p>They discuss:<br>
🔁 Why reactive execution matters—and how current tools fall short<br>
🛠️ The design goals behind Marimo, a new kind of Python notebook<br>
⚙️ The hidden costs of traditional workflows (and what breaks at scale)<br>
📦 What it takes to build modular, maintainable AI apps<br>
🧪 Why debugging LLM systems is so hard—and what better tooling looks like<br>
🌍 What we can learn from decades of tools built for and by data practitioners</p>

<p>Toward the end of the episode, Hugo and Akshay walk through two live demos: Hugo shares how he’s been using Marimo to prototype an app that extracts structured data from world leader bios, and Akshay shows how Marimo handles agentic workflows with memory and tool use—built entirely in a notebook.</p>

<p>This episode is about tools, but it’s also about culture. If you’ve ever hit a wall with your current stack—or felt like your tools were working against you—this one’s for you.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://marimo.io/" rel="nofollow">marimo | a next-generation Python notebook</a></li>
<li><a href="https://www.scipy2025.scipy.org/" rel="nofollow">SciPy conference, 2025</a></li>
<li><a href="https://www.youtube.com/watch?v=DO21QEcLOxM" rel="nofollow">Hugo&#39;s face Marimo World Leader Face Embedding demo</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
<li><a href="https://youtube.com/live/WVxAz19tgZY?feature=share" rel="nofollow">Watch the podcast here on YouTube!</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out Hugo&#39;s course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts July 8 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+EYXjK_yc</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+EYXjK_yc" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 48: HOW TO BENCHMARK AGI WITH GREG KAMRADT</title>
      <link>https://vanishinggradients.fireside.fm/48</link>
      <guid isPermaLink="false">f3c73c48-530c-41aa-acd5-d6efafecd27f</guid>
      <pubDate>Fri, 23 May 2025 23:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/f3c73c48-530c-41aa-acd5-d6efafecd27f.mp3" length="126506397" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo talks with Greg Kamradt, President of the ARC Prize Foundation, about ARC-AGI: a benchmark built on Francois Chollet’s definition of intelligence as “the efficiency at which you learn new things.” Unlike most evals that focus on memorization or task completion, ARC is designed to measure generalization—and expose where today’s top models fall short.</itunes:subtitle>
      <itunes:duration>1:04:25</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/f/f3c73c48-530c-41aa-acd5-d6efafecd27f/transcript.txt" type="text/plain"/>
      <description>If we want to make progress toward AGI, we need a clear definition of intelligence—and a way to measure it.
In this episode, Hugo talks with Greg Kamradt, President of the ARC Prize Foundation, about ARC-AGI: a benchmark built on Francois Chollet’s definition of intelligence as “the efficiency at which you learn new things.” Unlike most evals that focus on memorization or task completion, ARC is designed to measure generalization—and expose where today’s top models fall short.
They discuss:
🧠 Why we still lack a shared definition of intelligence
🧪 How ARC tasks force models to learn novel skills at test time
📉 Why GPT-4-class models still underperform on ARC
🔎 The limits of traditional benchmarks like MMLU and Big-Bench
⚙️ What the OpenAI O₃ results reveal—and what they don’t
💡 Why generalization and efficiency, not raw capability, are key to AGI
Greg also shares what he’s seeing in the wild: how startups and independent researchers are using ARC as a North Star, how benchmarks shape the frontier, and why the ARC team believes we’ll know we’ve reached AGI when humans can no longer write tasks that models can’t solve.
This conversation is about evaluation—not hype. If you care about where AI is really headed, this one’s worth your time.
LINKS
* ARC Prize -- What is ARC-AGI? (https://arcprize.org/arc-agi)
* On the Measure of Intelligence by François Chollet (https://arxiv.org/abs/1911.01547)
* Greg Kamradt on Twitter (https://x.com/GregKamradt)
* Hugo's High Signal Podcast with Fei-Fei Li (https://high-signal.delphina.ai/episode/fei-fei-on-how-human-centered-ai-actually-gets-built)
* Vanishing Gradients YouTube Channel (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)  
* Upcoming Events on Luma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
* Hugo's recent newsletter about upcoming events and more! (https://hugobowne.substack.com/p/ai-as-a-civilizational-technology)
* Watch the podcast here on YouTube! (https://youtu.be/wU82fz4iRfo)
🎓 Want to go deeper?
Check out Hugo's course: Building LLM Applications for Data Scientists and Software Engineers.
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.
Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.
Cohort starts July 8 — Use this link for a 10% discount (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10) 
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs, AGI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>If we want to make progress toward AGI, we need a clear definition of intelligence—and a way to measure it.</p>

<p>In this episode, Hugo talks with Greg Kamradt, President of the ARC Prize Foundation, about ARC-AGI: a benchmark built on Francois Chollet’s definition of intelligence as “the efficiency at which you learn new things.” Unlike most evals that focus on memorization or task completion, ARC is designed to measure generalization—and expose where today’s top models fall short.</p>

<p>They discuss:<br>
🧠 Why we still lack a shared definition of intelligence<br>
🧪 How ARC tasks force models to learn novel skills at test time<br>
📉 Why GPT-4-class models still underperform on ARC<br>
🔎 The limits of traditional benchmarks like MMLU and Big-Bench<br>
⚙️ What the OpenAI O₃ results reveal—and what they don’t<br>
💡 Why generalization and efficiency, not raw capability, are key to AGI</p>

<p>Greg also shares what he’s seeing in the wild: how startups and independent researchers are using ARC as a North Star, how benchmarks shape the frontier, and why the ARC team believes we’ll know we’ve reached AGI when humans can no longer write tasks that models can’t solve.</p>

<p>This conversation is about evaluation—not hype. If you care about where AI is really headed, this one’s worth your time.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://arcprize.org/arc-agi" rel="nofollow">ARC Prize -- What is ARC-AGI?</a></li>
<li><a href="https://arxiv.org/abs/1911.01547" rel="nofollow">On the Measure of Intelligence by François Chollet</a></li>
<li><a href="https://x.com/GregKamradt" rel="nofollow">Greg Kamradt on Twitter</a></li>
<li><a href="https://high-signal.delphina.ai/episode/fei-fei-on-how-human-centered-ai-actually-gets-built" rel="nofollow">Hugo&#39;s High Signal Podcast with Fei-Fei Li</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
<li><a href="https://youtu.be/wU82fz4iRfo" rel="nofollow">Watch the podcast here on YouTube!</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out Hugo&#39;s course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts July 8 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>If we want to make progress toward AGI, we need a clear definition of intelligence—and a way to measure it.</p>

<p>In this episode, Hugo talks with Greg Kamradt, President of the ARC Prize Foundation, about ARC-AGI: a benchmark built on Francois Chollet’s definition of intelligence as “the efficiency at which you learn new things.” Unlike most evals that focus on memorization or task completion, ARC is designed to measure generalization—and expose where today’s top models fall short.</p>

<p>They discuss:<br>
🧠 Why we still lack a shared definition of intelligence<br>
🧪 How ARC tasks force models to learn novel skills at test time<br>
📉 Why GPT-4-class models still underperform on ARC<br>
🔎 The limits of traditional benchmarks like MMLU and Big-Bench<br>
⚙️ What the OpenAI O₃ results reveal—and what they don’t<br>
💡 Why generalization and efficiency, not raw capability, are key to AGI</p>

<p>Greg also shares what he’s seeing in the wild: how startups and independent researchers are using ARC as a North Star, how benchmarks shape the frontier, and why the ARC team believes we’ll know we’ve reached AGI when humans can no longer write tasks that models can’t solve.</p>

<p>This conversation is about evaluation—not hype. If you care about where AI is really headed, this one’s worth your time.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://arcprize.org/arc-agi" rel="nofollow">ARC Prize -- What is ARC-AGI?</a></li>
<li><a href="https://arxiv.org/abs/1911.01547" rel="nofollow">On the Measure of Intelligence by François Chollet</a></li>
<li><a href="https://x.com/GregKamradt" rel="nofollow">Greg Kamradt on Twitter</a></li>
<li><a href="https://high-signal.delphina.ai/episode/fei-fei-on-how-human-centered-ai-actually-gets-built" rel="nofollow">Hugo&#39;s High Signal Podcast with Fei-Fei Li</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
<li><a href="https://hugobowne.substack.com/p/ai-as-a-civilizational-technology" rel="nofollow">Hugo&#39;s recent newsletter about upcoming events and more!</a></li>
<li><a href="https://youtu.be/wU82fz4iRfo" rel="nofollow">Watch the podcast here on YouTube!</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out Hugo&#39;s course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts July 8 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+HbIFoKl-</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+HbIFoKl-" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 47: The Great Pacific Garbage Patch of Code Slop with Joe Reis</title>
      <link>https://vanishinggradients.fireside.fm/47</link>
      <guid isPermaLink="false">decc9c1a-f18a-41e9-947a-e58fa0957f1e</guid>
      <pubDate>Mon, 07 Apr 2025 10:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/decc9c1a-f18a-41e9-947a-e58fa0957f1e.mp3" length="76045085" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>What if the cost of writing code dropped to zero — but the cost of understanding it skyrocketed?

In this episode, Hugo sits down with Joe Reis to unpack how AI tooling is reshaping the software development lifecycle — from experimentation and prototyping to deployment, maintainability, and everything in between.</itunes:subtitle>
      <itunes:duration>1:19:12</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/d/decc9c1a-f18a-41e9-947a-e58fa0957f1e/transcript.txt" type="text/plain"/>
      <description>What if the cost of writing code dropped to zero — but the cost of understanding it skyrocketed?
In this episode, Hugo sits down with Joe Reis to unpack how AI tooling is reshaping the software development lifecycle — from experimentation and prototyping to deployment, maintainability, and everything in between.
Joe is the co-author of Fundamentals of Data Engineering and a longtime voice on the systems side of modern software. He’s also one of the sharpest critics of “vibe coding” — the emerging pattern of writing software by feel, with heavy reliance on LLMs and little regard for structure or quality.
We dive into:
    • Why “vibe coding” is more than a meme — and what it says about how we build today
    • How AI tools expand the surface area of software creation — for better and worse
    • What happens to technical debt, testing, and security when generation outpaces understanding
    • The changing definition of “production” in a world of ephemeral, internal, or just-good-enough tools
    • How AI is flattening the learning curve — and threatening the talent pipeline
    • Joe’s view on what real craftsmanship means in an age of disposable code
This conversation isn’t about doom, and it’s not about hype. It’s about mapping the real, messy terrain of what it means to build software today — and how to do it with care.
LINKS
* Joe's Practical Data Modeling Newsletter on Substack (https://practicaldatamodeling.substack.com/)
* Joe's Practical Data Modeling Server on Discord (https://discord.gg/HhSZVvWDBb)
* Vanishing Gradients YouTube Channel (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)  
* Upcoming Events on Luma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
🎓 Want to go deeper?
Check out my course: Building LLM Applications for Data Scientists and Software Engineers.
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.
Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.
Cohort starts July 8 — Use this link for a 10% discount (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10) 
</description>
      <itunes:keywords>AI, LLMs, data science, machine learning, data science, GenAI, vibe coding</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>What if the cost of writing code dropped to zero — but the cost of understanding it skyrocketed?</p>

<p>In this episode, Hugo sits down with Joe Reis to unpack how AI tooling is reshaping the software development lifecycle — from experimentation and prototyping to deployment, maintainability, and everything in between.</p>

<p>Joe is the co-author of Fundamentals of Data Engineering and a longtime voice on the systems side of modern software. He’s also one of the sharpest critics of “vibe coding” — the emerging pattern of writing software by feel, with heavy reliance on LLMs and little regard for structure or quality.</p>

<p>We dive into:<br>
    • Why “vibe coding” is more than a meme — and what it says about how we build today<br>
    • How AI tools expand the surface area of software creation — for better and worse<br>
    • What happens to technical debt, testing, and security when generation outpaces understanding<br>
    • The changing definition of “production” in a world of ephemeral, internal, or just-good-enough tools<br>
    • How AI is flattening the learning curve — and threatening the talent pipeline<br>
    • Joe’s view on what real craftsmanship means in an age of disposable code</p>

<p>This conversation isn’t about doom, and it’s not about hype. It’s about mapping the real, messy terrain of what it means to build software today — and how to do it with care.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://practicaldatamodeling.substack.com/" rel="nofollow">Joe&#39;s Practical Data Modeling Newsletter on Substack</a></li>
<li><a href="https://discord.gg/HhSZVvWDBb" rel="nofollow">Joe&#39;s Practical Data Modeling Server on Discord</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out my course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts July 8 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>What if the cost of writing code dropped to zero — but the cost of understanding it skyrocketed?</p>

<p>In this episode, Hugo sits down with Joe Reis to unpack how AI tooling is reshaping the software development lifecycle — from experimentation and prototyping to deployment, maintainability, and everything in between.</p>

<p>Joe is the co-author of Fundamentals of Data Engineering and a longtime voice on the systems side of modern software. He’s also one of the sharpest critics of “vibe coding” — the emerging pattern of writing software by feel, with heavy reliance on LLMs and little regard for structure or quality.</p>

<p>We dive into:<br>
    • Why “vibe coding” is more than a meme — and what it says about how we build today<br>
    • How AI tools expand the surface area of software creation — for better and worse<br>
    • What happens to technical debt, testing, and security when generation outpaces understanding<br>
    • The changing definition of “production” in a world of ephemeral, internal, or just-good-enough tools<br>
    • How AI is flattening the learning curve — and threatening the talent pipeline<br>
    • Joe’s view on what real craftsmanship means in an age of disposable code</p>

<p>This conversation isn’t about doom, and it’s not about hype. It’s about mapping the real, messy terrain of what it means to build software today — and how to do it with care.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://practicaldatamodeling.substack.com/" rel="nofollow">Joe&#39;s Practical Data Modeling Newsletter on Substack</a></li>
<li><a href="https://discord.gg/HhSZVvWDBb" rel="nofollow">Joe&#39;s Practical Data Modeling Server on Discord</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out my course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $800 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts July 8 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+WnN5dGUI</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+WnN5dGUI" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 46: Software Composition Is the New Vibe Coding</title>
      <link>https://vanishinggradients.fireside.fm/46</link>
      <guid isPermaLink="false">dcb8396f-ece2-4636-951c-8ad44d698d15</guid>
      <pubDate>Thu, 03 Apr 2025 13:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/dcb8396f-ece2-4636-951c-8ad44d698d15.mp3" length="99299288" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>What if building software felt more like composing than coding?

In this episode, Hugo and Greg explore how LLMs are reshaping the way we think about software development—from deterministic programming to a more flexible, prompt-driven, and collaborative style of building. It’s not just hype or grift—it’s a real shift in how we express intent, reason about systems, and collaborate across roles.

Hugo speaks with Greg Ceccarelli—co-founder of SpecStory, former CPO at Pluralsight, and Director of Data Science at GitHub—about the rise of software composition and how it changes the way individuals and teams create with LLMs.</itunes:subtitle>
      <itunes:duration>1:08:57</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/d/dcb8396f-ece2-4636-951c-8ad44d698d15/transcript.txt" type="text/plain"/>
      <description>What if building software felt more like composing than coding?
In this episode, Hugo and Greg explore how LLMs are reshaping the way we think about software development—from deterministic programming to a more flexible, prompt-driven, and collaborative style of building. It’s not just hype or grift—it’s a real shift in how we express intent, reason about systems, and collaborate across roles.
Hugo speaks with Greg Ceccarelli—co-founder of SpecStory, former CPO at Pluralsight, and Director of Data Science at GitHub—about the rise of software composition and how it changes the way individuals and teams create with LLMs.
We dive into:
- Why software composition is emerging as a serious alternative to traditional coding
- The real difference between vibe coding and production-minded prototyping
- How LLMs are expanding who gets to build software—and how
- What changes when you focus on intent, not just code
- What Greg is building with SpecStory to support collaborative, traceable AI-native workflows
- The challenges (and joys) of debugging and exploring with agentic tools like Cursor and Claude
We’ve removed the visual demos from the audio—but you can catch our live-coded Chrome extension and JFK document explorer on YouTube. Links below.
JFK Docs Vibe Coding Demo (YouTube) (https://youtu.be/JpXCkuV58QE)  
Chrome Extension Vibe Coding Demo (YouTube) (https://youtu.be/ESVKp37jDwc)  
Meditations on Tech (Greg’s Substack) (https://www.meditationsontech.com/)  
Simon Willison on Vibe Coding (https://simonwillison.net/2025/Mar/19/vibe-coding/)  
Johnno Whitaker: On Vibe Coding (https://johnowhitaker.dev/essays/vibe_coding.html)  
Tim O’Reilly – The End of Programming (https://www.oreilly.com/radar/the-end-of-programming-as-we-know-it/)  
Vanishing Gradients YouTube Channel (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)  
Upcoming Events on Luma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)  
Greg Ceccarelli on LinkedIn (https://www.linkedin.com/in/gregceccarelli/)  
Greg’s Hacker News Post on GOOD (https://news.ycombinator.com/item?id=43557698)  
SpecStory: GOOD – Git Companion for AI Workflows (https://github.com/specstoryai/getspecstory/blob/main/GOOD.md)
🎓 Want to go deeper?
Check out my course: Building LLM Applications for Data Scientists and Software Engineers.
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.
Includes over $2,500 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.
Cohort starts April 7 — Use this link for a 10% discount (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10)
🔍 Want to help shape the future of SpecStory?
Greg and the team are looking for design partners for their new SpecStory Teams product—built for collaborative, AI-native software development.
If you're working with LLMs in a team setting and want to influence the next wave of developer tools, you can apply here:  
👉 specstory.com/teams (https://specstory.com/teams) 
</description>
      <itunes:keywords>AI, LLMs, data science, machine learning, data science, GenAI, vibe coding</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>What if building software felt more like composing than coding?</p>

<p>In this episode, Hugo and Greg explore how LLMs are reshaping the way we think about software development—from deterministic programming to a more flexible, prompt-driven, and collaborative style of building. It’s not just hype or grift—it’s a real shift in how we express intent, reason about systems, and collaborate across roles.</p>

<p>Hugo speaks with Greg Ceccarelli—co-founder of SpecStory, former CPO at Pluralsight, and Director of Data Science at GitHub—about the rise of software composition and how it changes the way individuals and teams create with LLMs.</p>

<p>We dive into:</p>

<ul>
<li>Why software composition is emerging as a serious alternative to traditional coding</li>
<li>The real difference between vibe coding and production-minded prototyping</li>
<li>How LLMs are expanding who gets to build software—and how</li>
<li>What changes when you focus on intent, not just code</li>
<li>What Greg is building with SpecStory to support collaborative, traceable AI-native workflows</li>
<li>The challenges (and joys) of debugging and exploring with agentic tools like Cursor and Claude</li>
</ul>

<p>We’ve removed the visual demos from the audio—but you can catch our live-coded Chrome extension and JFK document explorer on YouTube. Links below.</p>

<ul>
<li><a href="https://youtu.be/JpXCkuV58QE" rel="nofollow">JFK Docs Vibe Coding Demo (YouTube)</a><br></li>
<li><a href="https://youtu.be/ESVKp37jDwc" rel="nofollow">Chrome Extension Vibe Coding Demo (YouTube)</a><br></li>
<li><a href="https://www.meditationsontech.com/" rel="nofollow">Meditations on Tech (Greg’s Substack)</a><br></li>
<li><a href="https://simonwillison.net/2025/Mar/19/vibe-coding/" rel="nofollow">Simon Willison on Vibe Coding</a><br></li>
<li><a href="https://johnowhitaker.dev/essays/vibe_coding.html" rel="nofollow">Johnno Whitaker: On Vibe Coding</a><br></li>
<li><a href="https://www.oreilly.com/radar/the-end-of-programming-as-we-know-it/" rel="nofollow">Tim O’Reilly – The End of Programming</a><br></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a><br></li>
<li><a href="https://www.linkedin.com/in/gregceccarelli/" rel="nofollow">Greg Ceccarelli on LinkedIn</a><br></li>
<li><a href="https://news.ycombinator.com/item?id=43557698" rel="nofollow">Greg’s Hacker News Post on GOOD</a><br></li>
<li><a href="https://github.com/specstoryai/getspecstory/blob/main/GOOD.md" rel="nofollow">SpecStory: GOOD – Git Companion for AI Workflows</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out my course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $2,500 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts April 7 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>

<h3>🔍 Want to help shape the future of SpecStory?</h3>

<p>Greg and the team are looking for <strong>design partners</strong> for their new SpecStory Teams product—built for collaborative, AI-native software development.</p>

<p>If you&#39;re working with LLMs in a team setting and want to influence the next wave of developer tools, you can apply here:<br><br>
👉 <a href="https://specstory.com/teams" rel="nofollow">specstory.com/teams</a></p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>What if building software felt more like composing than coding?</p>

<p>In this episode, Hugo and Greg explore how LLMs are reshaping the way we think about software development—from deterministic programming to a more flexible, prompt-driven, and collaborative style of building. It’s not just hype or grift—it’s a real shift in how we express intent, reason about systems, and collaborate across roles.</p>

<p>Hugo speaks with Greg Ceccarelli—co-founder of SpecStory, former CPO at Pluralsight, and Director of Data Science at GitHub—about the rise of software composition and how it changes the way individuals and teams create with LLMs.</p>

<p>We dive into:</p>

<ul>
<li>Why software composition is emerging as a serious alternative to traditional coding</li>
<li>The real difference between vibe coding and production-minded prototyping</li>
<li>How LLMs are expanding who gets to build software—and how</li>
<li>What changes when you focus on intent, not just code</li>
<li>What Greg is building with SpecStory to support collaborative, traceable AI-native workflows</li>
<li>The challenges (and joys) of debugging and exploring with agentic tools like Cursor and Claude</li>
</ul>

<p>We’ve removed the visual demos from the audio—but you can catch our live-coded Chrome extension and JFK document explorer on YouTube. Links below.</p>

<ul>
<li><a href="https://youtu.be/JpXCkuV58QE" rel="nofollow">JFK Docs Vibe Coding Demo (YouTube)</a><br></li>
<li><a href="https://youtu.be/ESVKp37jDwc" rel="nofollow">Chrome Extension Vibe Coding Demo (YouTube)</a><br></li>
<li><a href="https://www.meditationsontech.com/" rel="nofollow">Meditations on Tech (Greg’s Substack)</a><br></li>
<li><a href="https://simonwillison.net/2025/Mar/19/vibe-coding/" rel="nofollow">Simon Willison on Vibe Coding</a><br></li>
<li><a href="https://johnowhitaker.dev/essays/vibe_coding.html" rel="nofollow">Johnno Whitaker: On Vibe Coding</a><br></li>
<li><a href="https://www.oreilly.com/radar/the-end-of-programming-as-we-know-it/" rel="nofollow">Tim O’Reilly – The End of Programming</a><br></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients YouTube Channel</a><br></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Upcoming Events on Luma</a><br></li>
<li><a href="https://www.linkedin.com/in/gregceccarelli/" rel="nofollow">Greg Ceccarelli on LinkedIn</a><br></li>
<li><a href="https://news.ycombinator.com/item?id=43557698" rel="nofollow">Greg’s Hacker News Post on GOOD</a><br></li>
<li><a href="https://github.com/specstoryai/getspecstory/blob/main/GOOD.md" rel="nofollow">SpecStory: GOOD – Git Companion for AI Workflows</a></li>
</ul>

<p>🎓 Want to go deeper?<br>
Check out my course: <em>Building LLM Applications for Data Scientists and Software Engineers.</em><br>
Learn how to design, test, and deploy production-grade LLM systems — with observability, feedback loops, and structure built in.<br>
This isn’t about vibes or fragile agents. It’s about making LLMs reliable, testable, and actually useful.</p>

<p>Includes over $2,500 in compute credits and guest lectures from experts at DeepMind, Moderna, and more.<br>
Cohort starts April 7 — <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=LLM10" rel="nofollow">Use this link for a 10% discount</a></p>

<h3>🔍 Want to help shape the future of SpecStory?</h3>

<p>Greg and the team are looking for <strong>design partners</strong> for their new SpecStory Teams product—built for collaborative, AI-native software development.</p>

<p>If you&#39;re working with LLMs in a team setting and want to influence the next wave of developer tools, you can apply here:<br><br>
👉 <a href="https://specstory.com/teams" rel="nofollow">specstory.com/teams</a></p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+FRjX2UJC</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+FRjX2UJC" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 45: Your AI application is broken. Here’s what to do about it.</title>
      <link>https://vanishinggradients.fireside.fm/45</link>
      <guid isPermaLink="false">6bde4712-dfbe-4573-94a7-0ed8d9ad8356</guid>
      <pubDate>Fri, 21 Feb 2025 05:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/6bde4712-dfbe-4573-94a7-0ed8d9ad8356.mp3" length="74404177" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Too many teams are building AI applications without truly understanding *why* their models fail. Instead of jumping straight to LLM evaluations, dashboards, or vibe checks, how do you actually **fix** a broken AI app?  

In this episode, Hugo speaks with **Hamel Husain**, longtime ML engineer, open-source contributor, and consultant, about why debugging generative AI systems starts with *looking at your data*.  </itunes:subtitle>
      <itunes:duration>1:17:30</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Too many teams are building AI applications without truly understanding why their models fail. Instead of jumping straight to LLM evaluations, dashboards, or vibe checks, how do you actually fix a broken AI app?  
In this episode, Hugo speaks with Hamel Husain, longtime ML engineer, open-source contributor, and consultant, about why debugging generative AI systems starts with looking at your data.  
In this episode, we dive into:  
Why “look at your data” is the best debugging advice no one follows.  
How spreadsheet-based error analysis can uncover failure modes faster than complex dashboards.  
The role of synthetic data in bootstrapping evaluation.  
When to trust LLM judges—and when they’re misleading.  
Why most AI dashboards measuring truthfulness, helpfulness, and conciseness are often a waste of time.  
If you're building AI-powered applications, this episode will change how you approach debugging, iteration, and improving model performance in production.  
LINKS
The podcast livestream on YouTube (https://youtube.com/live/Vz4--82M2_0?feature=share)
Hamel's blog (https://hamel.dev/)
Hamel on twitter (https://x.com/HamelHusain)
Hugo on twitter (https://x.com/hugobowne)
Vanishing Gradients on twitter (https://x.com/vanishingdata)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Vanishing Gradients on Twitter (https://x.com/vanishingdata)
Vanishing Gradients on Lu.ma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
Building LLM Application for Data Scientists and SWEs, Hugo course on Maven (use VG25 code for 25% off) (https://maven.com/s/course/d56067f338)
Hugo is also running a free lightning lesson next week on LLM Agents: When to Use Them (and When Not To) (https://maven.com/p/ed7a72/llm-agents-when-to-use-them-and-when-not-to?utm_medium=ll_share_link&amp;utm_source=instructor) 
</description>
      <content:encoded>
        <![CDATA[<p>Too many teams are building AI applications without truly understanding <em>why</em> their models fail. Instead of jumping straight to LLM evaluations, dashboards, or vibe checks, how do you actually <strong>fix</strong> a broken AI app?  </p>

<p>In this episode, Hugo speaks with <strong>Hamel Husain</strong>, longtime ML engineer, open-source contributor, and consultant, about why debugging generative AI systems starts with <em>looking at your data</em>.  </p>

<p>In this episode, we dive into:  </p>

<ul>
<li>Why “look at your data” is the best debugging advice no one follows.<br></li>
<li>How <strong>spreadsheet-based error analysis</strong> can uncover failure modes faster than complex dashboards.<br></li>
<li>The role of <strong>synthetic data</strong> in bootstrapping evaluation.<br></li>
<li>When to trust <strong>LLM judges</strong>—and when they’re misleading.<br></li>
<li>Why most AI dashboards measuring <strong>truthfulness, helpfulness, and conciseness</strong> are often a waste of time.<br></li>
</ul>

<p>If you&#39;re building AI-powered applications, this episode will change how you approach debugging, iteration, and improving model performance in production.  </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/Vz4--82M2_0?feature=share" rel="nofollow">The podcast livestream on YouTube</a></li>
<li><a href="https://hamel.dev/" rel="nofollow">Hamel&#39;s blog</a></li>
<li><a href="https://x.com/HamelHusain" rel="nofollow">Hamel on twitter</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><p><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></p></li>
<li><p><a href="https://maven.com/s/course/d56067f338" rel="nofollow">Building LLM Application for Data Scientists and SWEs, Hugo course on Maven (use VG25 code for 25% off)</a></p></li>
<li><p><a href="https://maven.com/p/ed7a72/llm-agents-when-to-use-them-and-when-not-to?utm_medium=ll_share_link&utm_source=instructor" rel="nofollow">Hugo is also running a free lightning lesson next week on <em>LLM Agents: When to Use Them (and When Not To</em>)</a></p></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Too many teams are building AI applications without truly understanding <em>why</em> their models fail. Instead of jumping straight to LLM evaluations, dashboards, or vibe checks, how do you actually <strong>fix</strong> a broken AI app?  </p>

<p>In this episode, Hugo speaks with <strong>Hamel Husain</strong>, longtime ML engineer, open-source contributor, and consultant, about why debugging generative AI systems starts with <em>looking at your data</em>.  </p>

<p>In this episode, we dive into:  </p>

<ul>
<li>Why “look at your data” is the best debugging advice no one follows.<br></li>
<li>How <strong>spreadsheet-based error analysis</strong> can uncover failure modes faster than complex dashboards.<br></li>
<li>The role of <strong>synthetic data</strong> in bootstrapping evaluation.<br></li>
<li>When to trust <strong>LLM judges</strong>—and when they’re misleading.<br></li>
<li>Why most AI dashboards measuring <strong>truthfulness, helpfulness, and conciseness</strong> are often a waste of time.<br></li>
</ul>

<p>If you&#39;re building AI-powered applications, this episode will change how you approach debugging, iteration, and improving model performance in production.  </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/Vz4--82M2_0?feature=share" rel="nofollow">The podcast livestream on YouTube</a></li>
<li><a href="https://hamel.dev/" rel="nofollow">Hamel&#39;s blog</a></li>
<li><a href="https://x.com/HamelHusain" rel="nofollow">Hamel on twitter</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><p><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></p></li>
<li><p><a href="https://maven.com/s/course/d56067f338" rel="nofollow">Building LLM Application for Data Scientists and SWEs, Hugo course on Maven (use VG25 code for 25% off)</a></p></li>
<li><p><a href="https://maven.com/p/ed7a72/llm-agents-when-to-use-them-and-when-not-to?utm_medium=ll_share_link&utm_source=instructor" rel="nofollow">Hugo is also running a free lightning lesson next week on <em>LLM Agents: When to Use Them (and When Not To</em>)</a></p></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+-HnRr2jX</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+-HnRr2jX" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 44: The Future of AI Coding Assistants: Who’s Really in Control?</title>
      <link>https://vanishinggradients.fireside.fm/44</link>
      <guid isPermaLink="false">78988fdd-0e05-4e24-82dd-c0a406dd12a1</guid>
      <pubDate>Tue, 04 Feb 2025 13:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/78988fdd-0e05-4e24-82dd-c0a406dd12a1.mp3" length="90430405" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>AI coding assistants are reshaping how developers write, debug, and maintain code—but who’s really in control? In this episode, Hugo speaks with **Tyler Dunn**, CEO and co-founder of **Continue**, an open-source AI-powered code assistant that gives developers more customization and flexibility in their workflows.</itunes:subtitle>
      <itunes:duration>1:34:11</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/7/78988fdd-0e05-4e24-82dd-c0a406dd12a1/transcript.txt" type="text/plain"/>
      <description>AI coding assistants are reshaping how developers write, debug, and maintain code—but who’s really in control? In this episode, Hugo speaks with Tyler Dunn, CEO and co-founder of Continue, an open-source AI-powered code assistant that gives developers more customization and flexibility in their workflows.
In this episode, we dive into:
- The trade-offs between proprietary vs. open-source AI coding assistants—why open-source might be the future.
- How structured workflows, modular AI, and customization help developers maintain control over their tools.
- The evolution of AI-powered coding, from autocomplete to intelligent code suggestions and beyond.
- Why the best developer experiences come from sensible defaults with room for deeper configuration.
- The future of LLM-based software engineering, where fine-tuning models on personal and team-level data could make AI coding assistants even more effective.
With companies increasingly integrating AI into development workflows, this conversation explores the real impact of these tools—and the importance of keeping developers in the driver's seat.
LINKS
The podcast livestream on YouTube (https://youtube.com/live/8QEgVCzm46U?feature=share)
Continue's website (https://www.continue.dev/)
Continue is hiring! (https://www.continue.dev/about-us)
amplified.dev: We believe in a future where developers are amplified, not automated (https://amplified.dev/)
Beyond Prompt and Pray, Building Reliable LLM-Powered Software in an Agentic World (https://www.oreilly.com/radar/beyond-prompt-and-pray/)
LLMOps Lessons Learned: Navigating the Wild West of Production LLMs 🚀 (https://www.zenml.io/blog/llmops-lessons-learned-navigating-the-wild-west-of-production-llms)
Building effective agents by Erik Schluntz and Barry Zhang, Anthropic (https://www.anthropic.com/research/building-effective-agents)
Ty on LinkedIn (https://www.linkedin.com/in/tylerjdunn/)
Hugo on twitter (https://x.com/hugobowne)
Vanishing Gradients on twitter (https://x.com/vanishingdata)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Vanishing Gradients on Twitter (https://x.com/vanishingdata)
Vanishing Gradients on Lu.ma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk) 
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>AI coding assistants are reshaping how developers write, debug, and maintain code—but who’s really in control? In this episode, Hugo speaks with <strong>Tyler Dunn</strong>, CEO and co-founder of <strong>Continue</strong>, an open-source AI-powered code assistant that gives developers more customization and flexibility in their workflows.</p>

<p>In this episode, we dive into:</p>

<ul>
<li>The trade-offs between <strong>proprietary vs. open-source AI coding assistants</strong>—why open-source might be the future.</li>
<li>How structured workflows, modular AI, and customization help developers maintain <strong>control over their tools</strong>.</li>
<li>The evolution of AI-powered coding, from <strong>autocomplete to intelligent code suggestions</strong> and beyond.</li>
<li>Why the best developer experiences come from <strong>sensible defaults</strong> with room for deeper configuration.</li>
<li>The future of <strong>LLM-based software engineering</strong>, where fine-tuning models on personal and team-level data could make AI coding assistants even more effective.</li>
</ul>

<p>With companies increasingly integrating AI into development workflows, this conversation explores the real impact of these tools—and the importance of keeping developers in the driver&#39;s seat.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/8QEgVCzm46U?feature=share" rel="nofollow">The podcast livestream on YouTube</a></li>
<li><a href="https://www.continue.dev/" rel="nofollow">Continue&#39;s website</a></li>
<li><a href="https://www.continue.dev/about-us" rel="nofollow">Continue is hiring!</a></li>
<li><a href="https://amplified.dev/" rel="nofollow">amplified.dev: We believe in a future where developers are amplified, not automated</a></li>
<li><a href="https://www.oreilly.com/radar/beyond-prompt-and-pray/" rel="nofollow">Beyond Prompt and Pray, Building Reliable LLM-Powered Software in an Agentic World</a></li>
<li><a href="https://www.zenml.io/blog/llmops-lessons-learned-navigating-the-wild-west-of-production-llms" rel="nofollow">LLMOps Lessons Learned: Navigating the Wild West of Production LLMs 🚀</a></li>
<li><a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow">Building effective agents by Erik Schluntz and Barry Zhang, Anthropic</a></li>
<li><a href="https://www.linkedin.com/in/tylerjdunn/" rel="nofollow">Ty on LinkedIn</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>AI coding assistants are reshaping how developers write, debug, and maintain code—but who’s really in control? In this episode, Hugo speaks with <strong>Tyler Dunn</strong>, CEO and co-founder of <strong>Continue</strong>, an open-source AI-powered code assistant that gives developers more customization and flexibility in their workflows.</p>

<p>In this episode, we dive into:</p>

<ul>
<li>The trade-offs between <strong>proprietary vs. open-source AI coding assistants</strong>—why open-source might be the future.</li>
<li>How structured workflows, modular AI, and customization help developers maintain <strong>control over their tools</strong>.</li>
<li>The evolution of AI-powered coding, from <strong>autocomplete to intelligent code suggestions</strong> and beyond.</li>
<li>Why the best developer experiences come from <strong>sensible defaults</strong> with room for deeper configuration.</li>
<li>The future of <strong>LLM-based software engineering</strong>, where fine-tuning models on personal and team-level data could make AI coding assistants even more effective.</li>
</ul>

<p>With companies increasingly integrating AI into development workflows, this conversation explores the real impact of these tools—and the importance of keeping developers in the driver&#39;s seat.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/8QEgVCzm46U?feature=share" rel="nofollow">The podcast livestream on YouTube</a></li>
<li><a href="https://www.continue.dev/" rel="nofollow">Continue&#39;s website</a></li>
<li><a href="https://www.continue.dev/about-us" rel="nofollow">Continue is hiring!</a></li>
<li><a href="https://amplified.dev/" rel="nofollow">amplified.dev: We believe in a future where developers are amplified, not automated</a></li>
<li><a href="https://www.oreilly.com/radar/beyond-prompt-and-pray/" rel="nofollow">Beyond Prompt and Pray, Building Reliable LLM-Powered Software in an Agentic World</a></li>
<li><a href="https://www.zenml.io/blog/llmops-lessons-learned-navigating-the-wild-west-of-production-llms" rel="nofollow">LLMOps Lessons Learned: Navigating the Wild West of Production LLMs 🚀</a></li>
<li><a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow">Building effective agents by Erik Schluntz and Barry Zhang, Anthropic</a></li>
<li><a href="https://www.linkedin.com/in/tylerjdunn/" rel="nofollow">Ty on LinkedIn</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+8naUV1R3</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+8naUV1R3" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 43: Tales from 400+ LLM Deployments: Building Reliable AI Agents in Production</title>
      <link>https://vanishinggradients.fireside.fm/43</link>
      <guid isPermaLink="false">ff9906ad-8576-40c7-9e0f-26dff301e52c</guid>
      <pubDate>Fri, 17 Jan 2025 08:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/ff9906ad-8576-40c7-9e0f-26dff301e52c.mp3" length="58615769" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Alex Strick van Linschoten, Machine Learning Engineer at ZenML and creator of a comprehensive LLMOps database documenting over 400 deployments. Alex's extensive research into real-world LLM implementations gives him unique insight into what actually works—and what doesn't—when deploying AI agents in production.</itunes:subtitle>
      <itunes:duration>1:01:03</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/f/ff9906ad-8576-40c7-9e0f-26dff301e52c/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Alex Strick van Linschoten, Machine Learning Engineer at ZenML and creator of a comprehensive LLMOps database documenting over 400 deployments. Alex's extensive research into real-world LLM implementations gives him unique insight into what actually works—and what doesn't—when deploying AI agents in production.
In this episode, we dive into:
- The current state of AI agents in production, from successes to common failure modes
- Practical lessons learned from analyzing hundreds of real-world LLM deployments
- How companies like Anthropic, Klarna, and Dropbox are using patterns like ReAct, RAG, and microservices to build reliable systems
- The evolution of LLM capabilities, from expanding context windows to multimodal applications
- Why most companies still prefer structured workflows over fully autonomous agents
We also explore real-world case studies of production hurdles, including cascading failures, API misfires, and hallucination challenges. Alex shares concrete strategies for integrating LLMs into your pipelines while maintaining reliability and control.
Whether you're scaling agents or building LLM-powered systems, this episode offers practical insights for navigating the complex landscape of LLMOps in 2025.
LINKS
The podcast livestream on YouTube (https://youtube.com/live/-8Gr9fVVX9g?feature=share)
The LLMOps database (https://www.zenml.io/llmops-database)
All blog posts about the database (https://www.zenml.io/category/llmops)
Anthropic's Building effective agents essay (https://www.anthropic.com/research/building-effective-agents)
Alex on LinkedIn (https://www.linkedin.com/in/strickvl/)
Hugo on twitter (https://x.com/hugobowne)
Vanishing Gradients on twitter (https://x.com/vanishingdata)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Vanishing Gradients on Twitter (https://x.com/vanishingdata)
Vanishing Gradients on Lu.ma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Alex Strick van Linschoten, Machine Learning Engineer at ZenML and creator of a comprehensive LLMOps database documenting over 400 deployments. Alex&#39;s extensive research into real-world LLM implementations gives him unique insight into what actually works—and what doesn&#39;t—when deploying AI agents in production.</p>

<p>In this episode, we dive into:</p>

<ul>
<li>The current state of AI agents in production, from successes to common failure modes</li>
<li>Practical lessons learned from analyzing hundreds of real-world LLM deployments</li>
<li>How companies like Anthropic, Klarna, and Dropbox are using patterns like ReAct, RAG, and microservices to build reliable systems</li>
<li>The evolution of LLM capabilities, from expanding context windows to multimodal applications</li>
<li>Why most companies still prefer structured workflows over fully autonomous agents</li>
</ul>

<p>We also explore real-world case studies of production hurdles, including cascading failures, API misfires, and hallucination challenges. Alex shares concrete strategies for integrating LLMs into your pipelines while maintaining reliability and control.</p>

<p>Whether you&#39;re scaling agents or building LLM-powered systems, this episode offers practical insights for navigating the complex landscape of LLMOps in 2025.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/-8Gr9fVVX9g?feature=share" rel="nofollow">The podcast livestream on YouTube</a></li>
<li><a href="https://www.zenml.io/llmops-database" rel="nofollow">The LLMOps database</a></li>
<li><a href="https://www.zenml.io/category/llmops" rel="nofollow">All blog posts about the database</a></li>
<li><a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow">Anthropic&#39;s Building effective agents essay</a></li>
<li><a href="https://www.linkedin.com/in/strickvl/" rel="nofollow">Alex on LinkedIn</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Alex Strick van Linschoten, Machine Learning Engineer at ZenML and creator of a comprehensive LLMOps database documenting over 400 deployments. Alex&#39;s extensive research into real-world LLM implementations gives him unique insight into what actually works—and what doesn&#39;t—when deploying AI agents in production.</p>

<p>In this episode, we dive into:</p>

<ul>
<li>The current state of AI agents in production, from successes to common failure modes</li>
<li>Practical lessons learned from analyzing hundreds of real-world LLM deployments</li>
<li>How companies like Anthropic, Klarna, and Dropbox are using patterns like ReAct, RAG, and microservices to build reliable systems</li>
<li>The evolution of LLM capabilities, from expanding context windows to multimodal applications</li>
<li>Why most companies still prefer structured workflows over fully autonomous agents</li>
</ul>

<p>We also explore real-world case studies of production hurdles, including cascading failures, API misfires, and hallucination challenges. Alex shares concrete strategies for integrating LLMs into your pipelines while maintaining reliability and control.</p>

<p>Whether you&#39;re scaling agents or building LLM-powered systems, this episode offers practical insights for navigating the complex landscape of LLMOps in 2025.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/-8Gr9fVVX9g?feature=share" rel="nofollow">The podcast livestream on YouTube</a></li>
<li><a href="https://www.zenml.io/llmops-database" rel="nofollow">The LLMOps database</a></li>
<li><a href="https://www.zenml.io/category/llmops" rel="nofollow">All blog posts about the database</a></li>
<li><a href="https://www.anthropic.com/research/building-effective-agents" rel="nofollow">Anthropic&#39;s Building effective agents essay</a></li>
<li><a href="https://www.linkedin.com/in/strickvl/" rel="nofollow">Alex on LinkedIn</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+YiDb50ep</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+YiDb50ep" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 42: Learning, Teaching, and Building in the Age of AI</title>
      <link>https://vanishinggradients.fireside.fm/42</link>
      <guid isPermaLink="false">6af2e172-b72b-418b-baa6-369299f37b8b</guid>
      <pubDate>Sat, 04 Jan 2025 14:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/6af2e172-b72b-418b-baa6-369299f37b8b.mp3" length="76860106" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>The tables turn as Hugo sits down with Alex Andorra, host of Learning Bayesian Statistics. Hugo shares his journey from mathematics to AI, reflecting on how Bayesian inference shapes his approach to data science, teaching, and building AI-powered applications.</itunes:subtitle>
      <itunes:duration>1:20:03</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>In this episode of Vanishing Gradients, the tables turn as Hugo sits down with Alex Andorra, host of Learning Bayesian Statistics. Hugo shares his journey from mathematics to AI, reflecting on how Bayesian inference shapes his approach to data science, teaching, and building AI-powered applications.
They dive into the realities of deploying LLM applications, overcoming “proof-of-concept purgatory,” and why first principles and iteration are critical for success in AI. Whether you’re an educator, software engineer, or data scientist, this episode offers valuable insights into the intersection of AI, product development, and real-world deployment.
LINKS
The podcast on YouTube (https://www.youtube.com/watch?v=BRIYytbqtP0)
The original podcast episode (https://learnbayesstats.com/episode/122-learning-and-teaching-in-the-age-of-ai-hugo-bowne-anderson)
Alex Andorra on LinkedIn (https://www.linkedin.com/in/alex-andorra/)
Hugo on LinkedIn (https://www.linkedin.com/in/hugo-bowne-anderson-045939a5/)
Hugo on twitter (https://x.com/hugobowne)
Vanishing Gradients on twitter (https://x.com/vanishingdata)
Hugo's "Building LLM Applications for Data Scientists and Software Engineers" course (https://maven.com/s/course/d56067f338) 
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>In this episode of Vanishing Gradients, the tables turn as Hugo sits down with Alex Andorra, host of Learning Bayesian Statistics. Hugo shares his journey from mathematics to AI, reflecting on how Bayesian inference shapes his approach to data science, teaching, and building AI-powered applications.</p>

<p>They dive into the realities of deploying LLM applications, overcoming “proof-of-concept purgatory,” and why first principles and iteration are critical for success in AI. Whether you’re an educator, software engineer, or data scientist, this episode offers valuable insights into the intersection of AI, product development, and real-world deployment.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/watch?v=BRIYytbqtP0" rel="nofollow">The podcast on YouTube</a></li>
<li><a href="https://learnbayesstats.com/episode/122-learning-and-teaching-in-the-age-of-ai-hugo-bowne-anderson" rel="nofollow">The original podcast episode</a></li>
<li><a href="https://www.linkedin.com/in/alex-andorra/" rel="nofollow">Alex Andorra on LinkedIn</a></li>
<li><a href="https://www.linkedin.com/in/hugo-bowne-anderson-045939a5/" rel="nofollow">Hugo on LinkedIn</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://maven.com/s/course/d56067f338" rel="nofollow">Hugo&#39;s &quot;Building LLM Applications for Data Scientists and Software Engineers&quot; course</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>In this episode of Vanishing Gradients, the tables turn as Hugo sits down with Alex Andorra, host of Learning Bayesian Statistics. Hugo shares his journey from mathematics to AI, reflecting on how Bayesian inference shapes his approach to data science, teaching, and building AI-powered applications.</p>

<p>They dive into the realities of deploying LLM applications, overcoming “proof-of-concept purgatory,” and why first principles and iteration are critical for success in AI. Whether you’re an educator, software engineer, or data scientist, this episode offers valuable insights into the intersection of AI, product development, and real-world deployment.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/watch?v=BRIYytbqtP0" rel="nofollow">The podcast on YouTube</a></li>
<li><a href="https://learnbayesstats.com/episode/122-learning-and-teaching-in-the-age-of-ai-hugo-bowne-anderson" rel="nofollow">The original podcast episode</a></li>
<li><a href="https://www.linkedin.com/in/alex-andorra/" rel="nofollow">Alex Andorra on LinkedIn</a></li>
<li><a href="https://www.linkedin.com/in/hugo-bowne-anderson-045939a5/" rel="nofollow">Hugo on LinkedIn</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://maven.com/s/course/d56067f338" rel="nofollow">Hugo&#39;s &quot;Building LLM Applications for Data Scientists and Software Engineers&quot; course</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+MOIwQSqV</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+MOIwQSqV" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 41: Beyond Prompt Engineering: Can AI Learn to Set Its Own Goals?</title>
      <link>https://vanishinggradients.fireside.fm/41</link>
      <guid isPermaLink="false">695d8cc9-b111-4f1d-9871-82962ae023f4</guid>
      <pubDate>Tue, 31 Dec 2024 10:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/695d8cc9-b111-4f1d-9871-82962ae023f4.mp3" length="42114740" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo Bowne-Anderson hosts a panel discussion from the MLOps World and Generative AI Summit in Austin, exploring the long-term growth of AI by distinguishing real problem-solving from trend-based solutions. If you're navigating the evolving landscape of generative AI, productionizing models, or questioning the hype, this episode dives into the tough questions shaping the field.</itunes:subtitle>
      <itunes:duration>43:51</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/6/695d8cc9-b111-4f1d-9871-82962ae023f4/transcript.txt" type="text/plain"/>
      <description>Hugo Bowne-Anderson hosts a panel discussion from the MLOps World and Generative AI Summit in Austin, exploring the long-term growth of AI by distinguishing real problem-solving from trend-based solutions. If you're navigating the evolving landscape of generative AI, productionizing models, or questioning the hype, this episode dives into the tough questions shaping the field.
The panel features:  
- Ben Taylor (Jepson) (https://www.linkedin.com/in/jepsontaylor/) – CEO and Founder at VEOX Inc., with experience in AI exploration, genetic programming, and deep learning.  
- Joe Reis (https://www.linkedin.com/in/josephreis/) – Co-founder of Ternary Data and author of Fundamentals of Data Engineering.  
- Juan Sequeda (https://www.linkedin.com/in/juansequeda/) – Principal Scientist and Head of AI Lab at Data.World, known for his expertise in knowledge graphs and the semantic web.  
The discussion unpacks essential topics such as:  
- The shift from prompt engineering to goal engineering—letting AI iterate toward well-defined objectives.  
- Whether generative AI is having an electricity moment or more of a blockchain trajectory.  
- The combinatorial power of AI to explore new solutions, drawing parallels to AlphaZero redefining strategy games.  
- The POC-to-production gap and why AI projects stall.  
- Failure modes, hallucinations, and governance risks—and how to mitigate them.  
- The disconnect between executive optimism and employee workload.  
Hugo also mentions his upcoming workshop on escaping Proof-of-Concept Purgatory, which has evolved into a Maven course "Building LLM Applications for Data Scientists and Software Engineers" launching in January (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?utm_campaign=8123d0&amp;utm_medium=partner&amp;utm_source=instructor). Vanishing Gradient listeners can get 25% off the course (use the code VG25), with $1,000 in Modal compute credits included.
A huge thanks to Dave Scharbach and the Toronto Machine Learning Society for organizing the conference and to the audience for their thoughtful questions.
As we head into the new year, this conversation offers a reality check amidst the growing AI agent hype.  
LINKS
Hugo on twitter (https://x.com/hugobowne)
Hugo on LinkedIn (https://www.linkedin.com/in/hugo-bowne-anderson-045939a5/)
Vanishing Gradients on twitter (https://x.com/vanishingdata)
"Building LLM Applications for Data Scientists and Software Engineers" course (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?utm_campaign=8123d0&amp;utm_medium=partner&amp;utm_source=instructor).
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo Bowne-Anderson hosts a panel discussion from the MLOps World and Generative AI Summit in Austin, exploring the long-term growth of AI by distinguishing real problem-solving from trend-based solutions. If you&#39;re navigating the evolving landscape of generative AI, productionizing models, or questioning the hype, this episode dives into the tough questions shaping the field.</p>

<p>The panel features:  </p>

<ul>
<li><a href="https://www.linkedin.com/in/jepsontaylor/" rel="nofollow"><strong>Ben Taylor (Jepson)</strong></a> – CEO and Founder at VEOX Inc., with experience in AI exploration, genetic programming, and deep learning.<br></li>
<li><a href="https://www.linkedin.com/in/josephreis/" rel="nofollow"><strong>Joe Reis</strong></a> – Co-founder of Ternary Data and author of <em>Fundamentals of Data Engineering</em>.<br></li>
<li><a href="https://www.linkedin.com/in/juansequeda/" rel="nofollow"><strong>Juan Sequeda</strong></a> – Principal Scientist and Head of AI Lab at Data.World, known for his expertise in knowledge graphs and the semantic web.<br></li>
</ul>

<p>The discussion unpacks essential topics such as:  </p>

<ul>
<li>The shift from <strong>prompt engineering</strong> to <strong>goal engineering</strong>—letting AI iterate toward well-defined objectives.<br></li>
<li>Whether generative AI is having an <strong>electricity moment</strong> or more of a <strong>blockchain trajectory</strong>.<br></li>
<li>The <strong>combinatorial power of AI</strong> to explore new solutions, drawing parallels to AlphaZero redefining strategy games.<br></li>
<li>The <strong>POC-to-production gap</strong> and why AI projects stall.<br></li>
<li><strong>Failure modes, hallucinations, and governance risks</strong>—and how to mitigate them.<br></li>
<li>The disconnect between executive optimism and employee workload.<br></li>
</ul>

<p>Hugo also mentions his upcoming workshop on <strong>escaping Proof-of-Concept Purgatory</strong>, <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?utm_campaign=8123d0&utm_medium=partner&utm_source=instructor" rel="nofollow">which has evolved into a <strong>Maven course &quot;Building LLM Applications for Data Scientists and Software Engineers&quot; launching in January</strong></a>. Vanishing Gradient listeners can get 25% off the course (use the code VG25), with $1,000 in Modal compute credits included.</p>

<p>A huge thanks to <strong>Dave Scharbach and the Toronto Machine Learning Society</strong> for organizing the conference and to the audience for their thoughtful questions.</p>

<p>As we head into the new year, this conversation offers a reality check amidst the growing AI agent hype.  </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://www.linkedin.com/in/hugo-bowne-anderson-045939a5/" rel="nofollow">Hugo on LinkedIn</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?utm_campaign=8123d0&utm_medium=partner&utm_source=instructor" rel="nofollow">&quot;Building LLM Applications for Data Scientists and Software Engineers&quot; course</a>.</li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo Bowne-Anderson hosts a panel discussion from the MLOps World and Generative AI Summit in Austin, exploring the long-term growth of AI by distinguishing real problem-solving from trend-based solutions. If you&#39;re navigating the evolving landscape of generative AI, productionizing models, or questioning the hype, this episode dives into the tough questions shaping the field.</p>

<p>The panel features:  </p>

<ul>
<li><a href="https://www.linkedin.com/in/jepsontaylor/" rel="nofollow"><strong>Ben Taylor (Jepson)</strong></a> – CEO and Founder at VEOX Inc., with experience in AI exploration, genetic programming, and deep learning.<br></li>
<li><a href="https://www.linkedin.com/in/josephreis/" rel="nofollow"><strong>Joe Reis</strong></a> – Co-founder of Ternary Data and author of <em>Fundamentals of Data Engineering</em>.<br></li>
<li><a href="https://www.linkedin.com/in/juansequeda/" rel="nofollow"><strong>Juan Sequeda</strong></a> – Principal Scientist and Head of AI Lab at Data.World, known for his expertise in knowledge graphs and the semantic web.<br></li>
</ul>

<p>The discussion unpacks essential topics such as:  </p>

<ul>
<li>The shift from <strong>prompt engineering</strong> to <strong>goal engineering</strong>—letting AI iterate toward well-defined objectives.<br></li>
<li>Whether generative AI is having an <strong>electricity moment</strong> or more of a <strong>blockchain trajectory</strong>.<br></li>
<li>The <strong>combinatorial power of AI</strong> to explore new solutions, drawing parallels to AlphaZero redefining strategy games.<br></li>
<li>The <strong>POC-to-production gap</strong> and why AI projects stall.<br></li>
<li><strong>Failure modes, hallucinations, and governance risks</strong>—and how to mitigate them.<br></li>
<li>The disconnect between executive optimism and employee workload.<br></li>
</ul>

<p>Hugo also mentions his upcoming workshop on <strong>escaping Proof-of-Concept Purgatory</strong>, <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?utm_campaign=8123d0&utm_medium=partner&utm_source=instructor" rel="nofollow">which has evolved into a <strong>Maven course &quot;Building LLM Applications for Data Scientists and Software Engineers&quot; launching in January</strong></a>. Vanishing Gradient listeners can get 25% off the course (use the code VG25), with $1,000 in Modal compute credits included.</p>

<p>A huge thanks to <strong>Dave Scharbach and the Toronto Machine Learning Society</strong> for organizing the conference and to the audience for their thoughtful questions.</p>

<p>As we head into the new year, this conversation offers a reality check amidst the growing AI agent hype.  </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://www.linkedin.com/in/hugo-bowne-anderson-045939a5/" rel="nofollow">Hugo on LinkedIn</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
<li><a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?utm_campaign=8123d0&utm_medium=partner&utm_source=instructor" rel="nofollow">&quot;Building LLM Applications for Data Scientists and Software Engineers&quot; course</a>.</li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+9gWjlim6</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+9gWjlim6" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 40: What Every LLM Developer Needs to Know About GPUs</title>
      <link>https://vanishinggradients.fireside.fm/40</link>
      <guid isPermaLink="false">b1b66484-5fd0-4bcb-91cb-8bf7201a5ded</guid>
      <pubDate>Tue, 24 Dec 2024 15:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/b1b66484-5fd0-4bcb-91cb-8bf7201a5ded.mp3" length="99441605" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with **Charles Frye**, Developer Advocate at Modal and someone who really knows GPUs inside and out. If you’re a data scientist, machine learning engineer, AI researcher, or just someone trying to make sense of **hardware for LLMs and AI workflows**, this episode is for you.  

Charles and Hugo dive into the **practical side of GPUs**—from **running inference** on large models, to **fine-tuning** and even **training from scratch.** </itunes:subtitle>
      <itunes:duration>1:43:34</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/b/b1b66484-5fd0-4bcb-91cb-8bf7201a5ded/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Charles Frye, Developer Advocate at Modal and someone who really knows GPUs inside and out. If you’re a data scientist, machine learning engineer, AI researcher, or just someone trying to make sense of hardware for LLMs and AI workflows, this episode is for you.  
Charles and Hugo dive into the practical side of GPUs—from running inference on large models, to fine-tuning and even training from scratch. They unpack the real pain points developers face, like figuring out:  
- How much VRAM you actually need.  
- Why memory—not compute—ends up being the bottleneck.  
- How to make quick, back-of-the-envelope calculations to size up hardware for your tasks.  
- And where things like fine-tuning, quantization, and retrieval-augmented generation (RAG) fit into the mix.  
One thing Hugo really appreciate is that Charles and the Modal team recently put together the GPU Glossary—a resource that breaks down GPU internals in a way that’s actually useful for developers. We reference it a few times throughout the episode, so check it out in the show notes below.  
🔧 Charles also does a demo during the episode—some of it is visual, but we talk through the key points so you’ll still get value from the audio. If you’d like to see the demo in action, check out the livestream linked below.
This is the "Building LLM Applications for Data Scientists and Software Engineers" course that Hugo is teaching with Stefan Krawczyk (ex-StitchFix) in January (https://maven.com/s/course/d56067f338). Charles is giving a guest lecture at on hardware for LLMs, and Modal is giving all students $1K worth of compute credits (use the code VG25 for $200 off).
LINKS
The livestream on YouTube (https://www.youtube.com/live/INryb8Hjk3c?si=0cbb0-Nxem1P987d)
The GPU Glossary (https://modal.com/gpu-glossary) by the Modal team
What We’ve Learned From A Year of Building with LLMs (https://applied-llms.org/) by Charles and friends
Charles on twitter (https://x.com/charles_irl)
Hugo on twitter (https://x.com/hugobowne)
Vanishing Gradients on twitter (https://x.com/vanishingdata)
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with <strong>Charles Frye</strong>, Developer Advocate at Modal and someone who really knows GPUs inside and out. If you’re a data scientist, machine learning engineer, AI researcher, or just someone trying to make sense of <strong>hardware for LLMs and AI workflows</strong>, this episode is for you.  </p>

<p>Charles and Hugo dive into the <strong>practical side of GPUs</strong>—from <strong>running inference</strong> on large models, to <strong>fine-tuning</strong> and even <strong>training from scratch.</strong> They unpack the <strong>real pain points</strong> developers face, like figuring out:  </p>

<ul>
<li>How much VRAM you actually need.<br></li>
<li>Why memory—not compute—ends up being the bottleneck.<br></li>
<li>How to make quick, <strong>back-of-the-envelope calculations</strong> to size up hardware for your tasks.<br></li>
<li>And where things like <strong>fine-tuning, quantization, and retrieval-augmented generation (RAG)</strong> fit into the mix.<br></li>
</ul>

<p>One thing Hugo really appreciate is that Charles and the Modal team recently put together the <strong>GPU Glossary</strong>—a resource that breaks down GPU internals in a way that’s actually useful for developers. We reference it a few times throughout the episode, so check it out in the show notes below.  </p>

<p>🔧 <strong>Charles also does a demo during the episode</strong>—some of it is visual, but we talk through the key points so you’ll still get value from the audio. If you’d like to see the demo in action, check out the livestream linked below.</p>

<p><a href="https://maven.com/s/course/d56067f338" rel="nofollow">This is the &quot;Building LLM Applications for Data Scientists and Software Engineers&quot; course that Hugo is teaching with Stefan Krawczyk (ex-StitchFix) in January</a>. Charles is giving a guest lecture at on hardware for LLMs, and Modal is giving all students $1K worth of compute credits (use the code VG25 for $200 off).</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/INryb8Hjk3c?si=0cbb0-Nxem1P987d" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://modal.com/gpu-glossary" rel="nofollow">The GPU Glossary</a> by the Modal team</li>
<li><a href="https://applied-llms.org/" rel="nofollow">What We’ve Learned From A Year of Building with LLMs</a> by Charles and friends</li>
<li><a href="https://x.com/charles_irl" rel="nofollow">Charles on twitter</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with <strong>Charles Frye</strong>, Developer Advocate at Modal and someone who really knows GPUs inside and out. If you’re a data scientist, machine learning engineer, AI researcher, or just someone trying to make sense of <strong>hardware for LLMs and AI workflows</strong>, this episode is for you.  </p>

<p>Charles and Hugo dive into the <strong>practical side of GPUs</strong>—from <strong>running inference</strong> on large models, to <strong>fine-tuning</strong> and even <strong>training from scratch.</strong> They unpack the <strong>real pain points</strong> developers face, like figuring out:  </p>

<ul>
<li>How much VRAM you actually need.<br></li>
<li>Why memory—not compute—ends up being the bottleneck.<br></li>
<li>How to make quick, <strong>back-of-the-envelope calculations</strong> to size up hardware for your tasks.<br></li>
<li>And where things like <strong>fine-tuning, quantization, and retrieval-augmented generation (RAG)</strong> fit into the mix.<br></li>
</ul>

<p>One thing Hugo really appreciate is that Charles and the Modal team recently put together the <strong>GPU Glossary</strong>—a resource that breaks down GPU internals in a way that’s actually useful for developers. We reference it a few times throughout the episode, so check it out in the show notes below.  </p>

<p>🔧 <strong>Charles also does a demo during the episode</strong>—some of it is visual, but we talk through the key points so you’ll still get value from the audio. If you’d like to see the demo in action, check out the livestream linked below.</p>

<p><a href="https://maven.com/s/course/d56067f338" rel="nofollow">This is the &quot;Building LLM Applications for Data Scientists and Software Engineers&quot; course that Hugo is teaching with Stefan Krawczyk (ex-StitchFix) in January</a>. Charles is giving a guest lecture at on hardware for LLMs, and Modal is giving all students $1K worth of compute credits (use the code VG25 for $200 off).</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/INryb8Hjk3c?si=0cbb0-Nxem1P987d" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://modal.com/gpu-glossary" rel="nofollow">The GPU Glossary</a> by the Modal team</li>
<li><a href="https://applied-llms.org/" rel="nofollow">What We’ve Learned From A Year of Building with LLMs</a> by Charles and friends</li>
<li><a href="https://x.com/charles_irl" rel="nofollow">Charles on twitter</a></li>
<li><a href="https://x.com/hugobowne" rel="nofollow">Hugo on twitter</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on twitter</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+iLoLtVKm</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+iLoLtVKm" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 39: From Models to Products: Bridging Research and Practice in Generative AI at Google Labs</title>
      <link>https://vanishinggradients.fireside.fm/39</link>
      <guid isPermaLink="false">bf5453c0-4aa2-4abb-b323-20334f787512</guid>
      <pubDate>Tue, 26 Nov 2024 03:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/bf5453c0-4aa2-4abb-b323-20334f787512.mp3" length="99346310" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>From building rockets at SpaceX to advancing generative AI at Google Labs, Ravin Kumar has carved a unique path through the world of technology. In this episode, we explore how to build scalable, reliable AI systems, the skills needed to work across the AI/ML pipeline, and the real-world impact of tools like open-weight models such as Gemma. Ravin also shares insights into designing AI tools like Notebook LM with the user journey at the forefront.</itunes:subtitle>
      <itunes:duration>1:43:28</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/b/bf5453c0-4aa2-4abb-b323-20334f787512/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Ravin Kumar,*Senior Research Data Scientist at Google Labs. Ravin’s career has taken him from building rockets at SpaceX to driving data science and technology at Sweetgreen, and now to advancing generative AI research and applications at Google Labs and DeepMind. His multidisciplinary experience gives him a rare perspective on building AI systems that combine technical rigor with practical utility.
In this episode, we dive into:
    • Ravin’s fascinating career path, including the skills and mindsets needed to work effectively with AI and machine learning models at different stages of the pipeline.
    • How to build generative AI systems that are scalable, reliable, and aligned with user needs.
    • Real-world applications of generative AI, such as using open weight models such as Gemma to help a bakery streamline operations—an example of delivering tangible business value through AI.
    • The critical role of UX in AI adoption, and how Ravin approaches designing tools like Notebook LM with the user journey in mind.
We also include a live demo where Ravin uses Notebook LM to analyze my website, extract insights, and even generate a podcast-style conversation about me. While some of the demo is visual, much can be appreciated through audio, and we’ve added a link to the video in the show notes for those who want to see it in action. We’ve also included the generated segment at the end of the episode for you to enjoy.
LINKS
The livestream on YouTube (https://www.youtube.com/live/ffS6NWqoo_k)
Google Labs (https://labs.google/)
Ravin's GenAI Handbook (https://ravinkumar.com/GenAiGuidebook/book_intro.html)
Breadboard: A library for prototyping generative AI applications (https://breadboard-ai.github.io/breadboard/)
As mentioned in the episode, Hugo is teaching a four-week course, Building LLM Applications for Data Scientists and SWEs, co-led with Stefan Krawczyk (Dagworks, ex-StitchFix). The course focuses on building scalable, production-grade generative AI systems, with hands-on sessions, $1,000+ in cloud credits, live Q&amp;As, and guest lectures from industry experts.
Listeners of Vanishing Gradients can get 25% off the course using this special link (https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=VG25) or by applying the code VG25 at checkout.
</description>
      <itunes:keywords>data science, machine learning, AI, LLMs</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Ravin Kumar,*Senior Research Data Scientist at Google Labs. Ravin’s career has taken him from building rockets at SpaceX to driving data science and technology at Sweetgreen, and now to advancing generative AI research and applications at Google Labs and DeepMind. His multidisciplinary experience gives him a rare perspective on building AI systems that combine technical rigor with practical utility.</p>

<p>In this episode, we dive into:<br>
    • Ravin’s fascinating career path, including the skills and mindsets needed to work effectively with AI and machine learning models at different stages of the pipeline.<br>
    • How to build generative AI systems that are scalable, reliable, and aligned with user needs.<br>
    • Real-world applications of generative AI, such as using open weight models such as Gemma to help a bakery streamline operations—an example of delivering tangible business value through AI.<br>
    • The critical role of UX in AI adoption, and how Ravin approaches designing tools like Notebook LM with the user journey in mind.</p>

<p>We also include a live demo where Ravin uses Notebook LM to analyze my website, extract insights, and even generate a podcast-style conversation about me. While some of the demo is visual, much can be appreciated through audio, and we’ve added a link to the video in the show notes for those who want to see it in action. We’ve also included the generated segment at the end of the episode for you to enjoy.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/ffS6NWqoo_k" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://labs.google/" rel="nofollow">Google Labs</a></li>
<li><a href="https://ravinkumar.com/GenAiGuidebook/book_intro.html" rel="nofollow">Ravin&#39;s GenAI Handbook</a></li>
<li><a href="https://breadboard-ai.github.io/breadboard/" rel="nofollow">Breadboard: A library for prototyping generative AI applications</a></li>
</ul>

<p>As mentioned in the episode, Hugo is teaching a four-week course, <strong>Building LLM Applications for Data Scientists and SWEs</strong>, co-led with Stefan Krawczyk (Dagworks, ex-StitchFix). The course focuses on building scalable, production-grade generative AI systems, with hands-on sessions, $1,000+ in cloud credits, live Q&amp;As, and guest lectures from industry experts.</p>

<p>Listeners of Vanishing Gradients can get 25% off the course using <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=VG25" rel="nofollow">this special link</a> or by applying the code VG25 at checkout.</p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Ravin Kumar,*Senior Research Data Scientist at Google Labs. Ravin’s career has taken him from building rockets at SpaceX to driving data science and technology at Sweetgreen, and now to advancing generative AI research and applications at Google Labs and DeepMind. His multidisciplinary experience gives him a rare perspective on building AI systems that combine technical rigor with practical utility.</p>

<p>In this episode, we dive into:<br>
    • Ravin’s fascinating career path, including the skills and mindsets needed to work effectively with AI and machine learning models at different stages of the pipeline.<br>
    • How to build generative AI systems that are scalable, reliable, and aligned with user needs.<br>
    • Real-world applications of generative AI, such as using open weight models such as Gemma to help a bakery streamline operations—an example of delivering tangible business value through AI.<br>
    • The critical role of UX in AI adoption, and how Ravin approaches designing tools like Notebook LM with the user journey in mind.</p>

<p>We also include a live demo where Ravin uses Notebook LM to analyze my website, extract insights, and even generate a podcast-style conversation about me. While some of the demo is visual, much can be appreciated through audio, and we’ve added a link to the video in the show notes for those who want to see it in action. We’ve also included the generated segment at the end of the episode for you to enjoy.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/ffS6NWqoo_k" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://labs.google/" rel="nofollow">Google Labs</a></li>
<li><a href="https://ravinkumar.com/GenAiGuidebook/book_intro.html" rel="nofollow">Ravin&#39;s GenAI Handbook</a></li>
<li><a href="https://breadboard-ai.github.io/breadboard/" rel="nofollow">Breadboard: A library for prototyping generative AI applications</a></li>
</ul>

<p>As mentioned in the episode, Hugo is teaching a four-week course, <strong>Building LLM Applications for Data Scientists and SWEs</strong>, co-led with Stefan Krawczyk (Dagworks, ex-StitchFix). The course focuses on building scalable, production-grade generative AI systems, with hands-on sessions, $1,000+ in cloud credits, live Q&amp;As, and guest lectures from industry experts.</p>

<p>Listeners of Vanishing Gradients can get 25% off the course using <a href="https://maven.com/hugo-stefan/building-llm-apps-ds-and-swe-from-first-principles?promoCode=VG25" rel="nofollow">this special link</a> or by applying the code VG25 at checkout.</p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+RJNlkIRh</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+RJNlkIRh" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 38: The Art of Freelance AI Consulting and Products: Data, Dollars, and Deliverables</title>
      <link>https://vanishinggradients.fireside.fm/38</link>
      <guid isPermaLink="false">c1a5c8d1-777a-41b7-a123-6b06861dbc35</guid>
      <pubDate>Tue, 05 Nov 2024 10:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/c1a5c8d1-777a-41b7-a123-6b06861dbc35.mp3" length="80443270" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Jason Liu, an independent AI consultant with experience at Meta and Stitch Fix. At Stitch Fix, Jason developed impactful AI systems, like a $50 million product similarity search and the widely adopted Flight recommendation framework. Now, he helps startups and enterprises design and deploy production-level AI applications, with a focus on retrieval-augmented generation (RAG) and scalable solutions.</itunes:subtitle>
      <itunes:duration>1:23:47</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Jason Liu, an independent AI consultant with experience at Meta and Stitch Fix. At Stitch Fix, Jason developed impactful AI systems, like a $50 million product similarity search and the widely adopted Flight recommendation framework. Now, he helps startups and enterprises design and deploy production-level AI applications, with a focus on retrieval-augmented generation (RAG) and scalable solutions.
This episode is a bit of an experiment. Instead of our usual technical deep dives, we’re focusing on the world of AI consulting and freelancing. We explore Jason’s consulting playbook, covering how he structures contracts to maximize value, strategies for moving from hourly billing to securing larger deals, and the mindset shift needed to align incentives with clients. We’ll also discuss the challenges of moving from deterministic software to probabilistic AI systems and even do a live role-playing session where Jason coaches me on client engagement and pricing pitfalls.
LINKS
The livestream on YouTube (https://youtube.com/live/9CFs06UDbGI?feature=share)
Jason's Upcoming course: AI Consultant Accelerator: From Expert to High-Demand Business (https://maven.com/indie-consulting/ai-consultant-accelerator?utm_campaign=9532cc&amp;utm_medium=partner&amp;utm_source=instructor)
Hugo's upcoming course: Building LLM Applications for Data Scientists and Software Engineers (https://maven.com/s/course/d56067f338)
Jason's website (https://jxnl.co/)
Jason's indie consulting newsletter (https://indieconsulting.podia.com/)
Your AI Product Needs Evals by Hamel Husain (https://hamel.dev/blog/posts/evals/)
What We’ve Learned From A Year of Building with LLMs (https://applied-llms.org/)
Dear Future AI Consultant by Jason (https://jxnl.co/writing/#dear-future-ai-consultant)
Alex Hormozi's books (https://www.acquisition.com/books)
The Burnout Society by Byung-Chul Han (https://www.sup.org/books/theory-and-philosophy/burnout-society)
Jason on Twitter (https://x.com/jxnlco)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Vanishing Gradients' lu.ma calendar (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
Vanishing Gradients on YouTube (https://www.youtube.com/@vanishinggradients) 
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI, consulting</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Jason Liu, an independent AI consultant with experience at Meta and Stitch Fix. At Stitch Fix, Jason developed impactful AI systems, like a $50 million product similarity search and the widely adopted Flight recommendation framework. Now, he helps startups and enterprises design and deploy production-level AI applications, with a focus on retrieval-augmented generation (RAG) and scalable solutions.</p>

<p>This episode is a bit of an experiment. Instead of our usual technical deep dives, we’re focusing on the world of AI consulting and freelancing. We explore Jason’s consulting playbook, covering how he structures contracts to maximize value, strategies for moving from hourly billing to securing larger deals, and the mindset shift needed to align incentives with clients. We’ll also discuss the challenges of moving from deterministic software to probabilistic AI systems and even do a live role-playing session where Jason coaches me on client engagement and pricing pitfalls.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/9CFs06UDbGI?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://maven.com/indie-consulting/ai-consultant-accelerator?utm_campaign=9532cc&utm_medium=partner&utm_source=instructor" rel="nofollow">Jason&#39;s Upcoming course: AI Consultant Accelerator: From Expert to High-Demand Business</a></li>
<li><a href="https://maven.com/s/course/d56067f338" rel="nofollow">Hugo&#39;s upcoming course: Building LLM Applications for Data Scientists and Software Engineers</a></li>
<li><a href="https://jxnl.co/" rel="nofollow">Jason&#39;s website</a></li>
<li><a href="https://indieconsulting.podia.com/" rel="nofollow">Jason&#39;s indie consulting newsletter</a></li>
<li><a href="https://hamel.dev/blog/posts/evals/" rel="nofollow">Your AI Product Needs Evals by Hamel Husain</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://jxnl.co/writing/#dear-future-ai-consultant" rel="nofollow">Dear Future AI Consultant by Jason</a></li>
<li><a href="https://www.acquisition.com/books" rel="nofollow">Alex Hormozi&#39;s books</a></li>
<li><a href="https://www.sup.org/books/theory-and-philosophy/burnout-society" rel="nofollow">The Burnout Society by Byung-Chul Han</a></li>
<li><a href="https://x.com/jxnlco" rel="nofollow">Jason on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients&#39; lu.ma calendar</a></li>
<li><a href="https://www.youtube.com/@vanishinggradients" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Jason Liu, an independent AI consultant with experience at Meta and Stitch Fix. At Stitch Fix, Jason developed impactful AI systems, like a $50 million product similarity search and the widely adopted Flight recommendation framework. Now, he helps startups and enterprises design and deploy production-level AI applications, with a focus on retrieval-augmented generation (RAG) and scalable solutions.</p>

<p>This episode is a bit of an experiment. Instead of our usual technical deep dives, we’re focusing on the world of AI consulting and freelancing. We explore Jason’s consulting playbook, covering how he structures contracts to maximize value, strategies for moving from hourly billing to securing larger deals, and the mindset shift needed to align incentives with clients. We’ll also discuss the challenges of moving from deterministic software to probabilistic AI systems and even do a live role-playing session where Jason coaches me on client engagement and pricing pitfalls.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/9CFs06UDbGI?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://maven.com/indie-consulting/ai-consultant-accelerator?utm_campaign=9532cc&utm_medium=partner&utm_source=instructor" rel="nofollow">Jason&#39;s Upcoming course: AI Consultant Accelerator: From Expert to High-Demand Business</a></li>
<li><a href="https://maven.com/s/course/d56067f338" rel="nofollow">Hugo&#39;s upcoming course: Building LLM Applications for Data Scientists and Software Engineers</a></li>
<li><a href="https://jxnl.co/" rel="nofollow">Jason&#39;s website</a></li>
<li><a href="https://indieconsulting.podia.com/" rel="nofollow">Jason&#39;s indie consulting newsletter</a></li>
<li><a href="https://hamel.dev/blog/posts/evals/" rel="nofollow">Your AI Product Needs Evals by Hamel Husain</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://jxnl.co/writing/#dear-future-ai-consultant" rel="nofollow">Dear Future AI Consultant by Jason</a></li>
<li><a href="https://www.acquisition.com/books" rel="nofollow">Alex Hormozi&#39;s books</a></li>
<li><a href="https://www.sup.org/books/theory-and-philosophy/burnout-society" rel="nofollow">The Burnout Society by Byung-Chul Han</a></li>
<li><a href="https://x.com/jxnlco" rel="nofollow">Jason on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients&#39; lu.ma calendar</a></li>
<li><a href="https://www.youtube.com/@vanishinggradients" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+0Jkb_Pvh</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+0Jkb_Pvh" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 37: Prompt Engineering, Security in Generative AI, and the Future of AI Research Part 2</title>
      <link>https://vanishinggradients.fireside.fm/37</link>
      <guid isPermaLink="false">eadec2c4-f8f9-45b0-ae7e-5867f7201801</guid>
      <pubDate>Tue, 08 Oct 2024 17:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/eadec2c4-f8f9-45b0-ae7e-5867f7201801.mp3" length="48585166" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.</itunes:subtitle>
      <itunes:duration>50:36</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/e/eadec2c4-f8f9-45b0-ae7e-5867f7201801/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.
This is Part 2 of a special two-part episode, prompted—no pun intended—by these guys being part of a team, led by Sander, that wrote a 76-page survey analyzing prompting techniques, agents, and generative AI. The survey included contributors from OpenAI, Microsoft, the University of Maryland, Princeton, and more.
In this episode, we cover:
The Prompt Report: A comprehensive survey on prompting techniques, agents, and generative AI, including advanced evaluation methods for assessing these techniques.
Security Risks and Prompt Hacking: A detailed exploration of the security concerns surrounding prompt engineering, including Sander’s thoughts on its potential applications in cybersecurity and military contexts.
AI’s Impact Across Fields: A discussion on how generative AI is reshaping various domains, including the social sciences and security.
Multimodal AI: Updates on how large language models (LLMs) are expanding to interact with images, code, and music.
Case Study - Detecting Suicide Risk: A careful examination of how prompting techniques are being used in important areas like detecting suicide risk, showcasing the critical potential of AI in addressing sensitive, real-world challenges.
The episode concludes with a reflection on the evolving landscape of LLMs and multimodal AI, and what might be on the horizon.
If you haven’t yet, make sure to check out Part 1, where we discuss the history of NLP, prompt engineering techniques, and Sander’s development of the Learn Prompting initiative.
LINKS
The livestream on YouTube (https://youtube.com/live/FreXovgG-9A?feature=share)
The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)
Learn Prompting: Your Guide to Communicating with AI (https://learnprompting.org/)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Vanishing Gradients' lu.ma calendar (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
Vanishing Gradients on YouTube (https://www.youtube.com/@vanishinggradients)
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI, NLP</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.</p>

<p>This is Part 2 of a special two-part episode, prompted—no pun intended—by these guys being part of a team, led by Sander, that wrote a 76-page survey analyzing prompting techniques, agents, and generative AI. The survey included contributors from OpenAI, Microsoft, the University of Maryland, Princeton, and more.</p>

<p>In this episode, we cover:</p>

<ul>
<li><p><strong>The Prompt Report:</strong> A comprehensive survey on prompting techniques, agents, and generative AI, including advanced evaluation methods for assessing these techniques.</p></li>
<li><p><strong>Security Risks and Prompt Hacking:</strong> A detailed exploration of the security concerns surrounding prompt engineering, including Sander’s thoughts on its potential applications in cybersecurity and military contexts.</p></li>
<li><p><strong>AI’s Impact Across Fields:</strong> A discussion on how generative AI is reshaping various domains, including the social sciences and security.</p></li>
<li><p><strong>Multimodal AI:</strong> Updates on how large language models (LLMs) are expanding to interact with images, code, and music.</p></li>
<li><p><strong>Case Study - Detecting Suicide Risk:</strong> A careful examination of how prompting techniques are being used in important areas like detecting suicide risk, showcasing the critical potential of AI in addressing sensitive, real-world challenges.</p></li>
</ul>

<p>The episode concludes with a reflection on the evolving landscape of <strong>LLMs</strong> and multimodal AI, and what might be on the horizon.</p>

<p>If you haven’t yet, make sure to check out <strong>Part 1</strong>, where we discuss the history of NLP, prompt engineering techniques, and Sander’s development of the Learn Prompting initiative.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/FreXovgG-9A?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://arxiv.org/abs/2406.06608" rel="nofollow">The Prompt Report: A Systematic Survey of Prompting Techniques</a></li>
<li><a href="https://learnprompting.org/" rel="nofollow">Learn Prompting: Your Guide to Communicating with AI</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients&#39; lu.ma calendar</a></li>
<li><a href="https://www.youtube.com/@vanishinggradients" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.</p>

<p>This is Part 2 of a special two-part episode, prompted—no pun intended—by these guys being part of a team, led by Sander, that wrote a 76-page survey analyzing prompting techniques, agents, and generative AI. The survey included contributors from OpenAI, Microsoft, the University of Maryland, Princeton, and more.</p>

<p>In this episode, we cover:</p>

<ul>
<li><p><strong>The Prompt Report:</strong> A comprehensive survey on prompting techniques, agents, and generative AI, including advanced evaluation methods for assessing these techniques.</p></li>
<li><p><strong>Security Risks and Prompt Hacking:</strong> A detailed exploration of the security concerns surrounding prompt engineering, including Sander’s thoughts on its potential applications in cybersecurity and military contexts.</p></li>
<li><p><strong>AI’s Impact Across Fields:</strong> A discussion on how generative AI is reshaping various domains, including the social sciences and security.</p></li>
<li><p><strong>Multimodal AI:</strong> Updates on how large language models (LLMs) are expanding to interact with images, code, and music.</p></li>
<li><p><strong>Case Study - Detecting Suicide Risk:</strong> A careful examination of how prompting techniques are being used in important areas like detecting suicide risk, showcasing the critical potential of AI in addressing sensitive, real-world challenges.</p></li>
</ul>

<p>The episode concludes with a reflection on the evolving landscape of <strong>LLMs</strong> and multimodal AI, and what might be on the horizon.</p>

<p>If you haven’t yet, make sure to check out <strong>Part 1</strong>, where we discuss the history of NLP, prompt engineering techniques, and Sander’s development of the Learn Prompting initiative.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/FreXovgG-9A?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://arxiv.org/abs/2406.06608" rel="nofollow">The Prompt Report: A Systematic Survey of Prompting Techniques</a></li>
<li><a href="https://learnprompting.org/" rel="nofollow">Learn Prompting: Your Guide to Communicating with AI</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients&#39; lu.ma calendar</a></li>
<li><a href="https://www.youtube.com/@vanishinggradients" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+L7d8LdwV</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+L7d8LdwV" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 36: Prompt Engineering, Security in Generative AI, and the Future of AI Research Part 1</title>
      <link>https://vanishinggradients.fireside.fm/36</link>
      <guid isPermaLink="false">acd8aaec-1788-459d-a4e9-10feae67a19a</guid>
      <pubDate>Mon, 30 Sep 2024 18:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/acd8aaec-1788-459d-a4e9-10feae67a19a.mp3" length="61232193" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.</itunes:subtitle>
      <itunes:duration>1:03:46</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/a/acd8aaec-1788-459d-a4e9-10feae67a19a/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.
This is Part 1 of a special two-part episode, prompted—no pun intended—by these guys being part of a team, led by Sander, that wrote a 76-page survey analyzing prompting techniques, agents, and generative AI. The survey included contributors from OpenAI, Microsoft, the University of Maryland, Princeton, and more.
In this first part, 
* we’ll explore the critical role of prompt engineering, 
* &amp; diving into adversarial techniques like prompt hacking and 
* the challenges of evaluating these techniques. 
* we’ll examine the impact of few-shot learning and 
* the groundbreaking taxonomy of prompting techniques from the Prompt Report.
Along the way, 
* we’ll uncover the rich history of natural language processing (NLP) and AI, showing how modern prompting techniques evolved from early rule-based systems and statistical methods. 
* we’ll also hear how Sander’s experimentation with GPT-3 for diplomatic tasks led him to develop Learn Prompting, and 
* how Dennis highlights the accessibility of AI through prompting, which allows non-technical users to interact with AI without needing to code.
Finally, we’ll explore the future of multimodal AI, where LLMs interact with images, code, and even music creation. Make sure to tune in to Part 2, where we dive deeper into security risks, prompt hacking, and more.
LINKS
The livestream on YouTube (https://youtube.com/live/FreXovgG-9A?feature=share)
The Prompt Report: A Systematic Survey of Prompting Techniques (https://arxiv.org/abs/2406.06608)
Learn Prompting: Your Guide to Communicating with AI (https://learnprompting.org/)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Vanishing Gradients' lu.ma calendar (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk)
Vanishing Gradients on YouTube (https://www.youtube.com/@vanishinggradients)
</description>
      <itunes:keywords>AI, LLMs, damachine learning, data science, GenAI, prompt engineering</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.</p>

<p>This is Part 1 of a special two-part episode, prompted—no pun intended—by these guys being part of a team, led by Sander, that wrote a 76-page survey analyzing prompting techniques, agents, and generative AI. The survey included contributors from OpenAI, Microsoft, the University of Maryland, Princeton, and more.</p>

<p>In this first part, </p>

<ul>
<li>we’ll explore the critical role of prompt engineering, </li>
<li>&amp; diving into adversarial techniques like prompt hacking and </li>
<li>the challenges of evaluating these techniques. </li>
<li>we’ll examine the impact of few-shot learning and </li>
<li>the groundbreaking taxonomy of prompting techniques from the Prompt Report.</li>
</ul>

<p>Along the way, </p>

<ul>
<li>we’ll uncover the rich history of natural language processing (NLP) and AI, showing how modern prompting techniques evolved from early rule-based systems and statistical methods. </li>
<li>we’ll also hear how Sander’s experimentation with GPT-3 for diplomatic tasks led him to develop Learn Prompting, and </li>
<li>how Dennis highlights the accessibility of AI through prompting, which allows non-technical users to interact with AI without needing to code.</li>
</ul>

<p>Finally, we’ll explore the future of multimodal AI, where LLMs interact with images, code, and even music creation. Make sure to tune in to Part 2, where we dive deeper into security risks, prompt hacking, and more.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/FreXovgG-9A?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://arxiv.org/abs/2406.06608" rel="nofollow">The Prompt Report: A Systematic Survey of Prompting Techniques</a></li>
<li><a href="https://learnprompting.org/" rel="nofollow">Learn Prompting: Your Guide to Communicating with AI</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients&#39; lu.ma calendar</a></li>
<li><a href="https://www.youtube.com/@vanishinggradients" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with three leading figures from the world of AI research: Sander Schulhoff, a recent University of Maryland graduate and lead contributor to the Learn Prompting initiative; Philip Resnik, professor at the University of Maryland, known for his pioneering work in computational linguistics; and Dennis Peskoff, a researcher from Princeton specializing in prompt engineering and its applications in the social sciences.</p>

<p>This is Part 1 of a special two-part episode, prompted—no pun intended—by these guys being part of a team, led by Sander, that wrote a 76-page survey analyzing prompting techniques, agents, and generative AI. The survey included contributors from OpenAI, Microsoft, the University of Maryland, Princeton, and more.</p>

<p>In this first part, </p>

<ul>
<li>we’ll explore the critical role of prompt engineering, </li>
<li>&amp; diving into adversarial techniques like prompt hacking and </li>
<li>the challenges of evaluating these techniques. </li>
<li>we’ll examine the impact of few-shot learning and </li>
<li>the groundbreaking taxonomy of prompting techniques from the Prompt Report.</li>
</ul>

<p>Along the way, </p>

<ul>
<li>we’ll uncover the rich history of natural language processing (NLP) and AI, showing how modern prompting techniques evolved from early rule-based systems and statistical methods. </li>
<li>we’ll also hear how Sander’s experimentation with GPT-3 for diplomatic tasks led him to develop Learn Prompting, and </li>
<li>how Dennis highlights the accessibility of AI through prompting, which allows non-technical users to interact with AI without needing to code.</li>
</ul>

<p>Finally, we’ll explore the future of multimodal AI, where LLMs interact with images, code, and even music creation. Make sure to tune in to Part 2, where we dive deeper into security risks, prompt hacking, and more.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/FreXovgG-9A?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://arxiv.org/abs/2406.06608" rel="nofollow">The Prompt Report: A Systematic Survey of Prompting Techniques</a></li>
<li><a href="https://learnprompting.org/" rel="nofollow">Learn Prompting: Your Guide to Communicating with AI</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients&#39; lu.ma calendar</a></li>
<li><a href="https://www.youtube.com/@vanishinggradients" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+ZjJv8sdC</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+ZjJv8sdC" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 35: Open Science at NASA -- Measuring Impact and the Future of AI</title>
      <link>https://vanishinggradients.fireside.fm/35</link>
      <guid isPermaLink="false">feeeecc8-a170-48c7-ae4c-8dd64484c64c</guid>
      <pubDate>Thu, 19 Sep 2024 17:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/feeeecc8-a170-48c7-ae4c-8dd64484c64c.mp3" length="55905303" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Dr. Chelle Gentemann, Open Science Program Scientist for NASA’s Office of the Chief Science Data Officer, about NASA’s ambitious efforts to integrate AI across the research lifecycle. In this episode, we’ll dive deeper into how AI is transforming NASA’s approach to science, making data more accessible and advancing open science practices.</itunes:subtitle>
      <itunes:duration>58:13</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/f/feeeecc8-a170-48c7-ae4c-8dd64484c64c/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Dr. Chelle Gentemann, Open Science Program Scientist for NASA’s Office of the Chief Science Data Officer, about NASA’s ambitious efforts to integrate AI across the research lifecycle. In this episode, we’ll dive deeper into how AI is transforming NASA’s approach to science, making data more accessible and advancing open science practices. We explore
Measuring the Impact of Open Science: How NASA is developing new metrics to evaluate the effectiveness of open science, moving beyond traditional publication-based assessments.
The Process of Scientific Discovery: Insights into the collaborative nature of research and how breakthroughs are achieved at NASA.
** AI Applications in NASA’s Science:** From rats in space to exploring the origins of the universe, we cover how AI is being applied across NASA’s divisions to improve data accessibility and analysis.
Addressing Challenges in Open Science: The complexities of implementing open science within government agencies and research environments.
Reforming Incentive Systems: How NASA is reconsidering traditional metrics like publications and citations, and starting to recognize contributions such as software development and data sharing.
The Future of Open Science: How open science is shaping the future of research, fostering interdisciplinary collaboration, and increasing accessibility.
This conversation offers valuable insights for researchers, data scientists, and those interested in the practical applications of AI and open science. Join us as we discuss how NASA is working to make science more collaborative, reproducible, and impactful.
LINKS
The livestream on YouTube (https://youtube.com/live/VJDg3ZbkNOE?feature=share)
NASA's Open Science 101 course &lt;-- do it to learn and also to get NASA Swag! (https://openscience101.org/)
Science Cast (https://sciencecast.org/)
NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data (https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model)
Jake VanderPlas' daily conundrum tweet from 2013 (https://x.com/jakevdp/status/408678764705378304)
Replit, "an AI-powered software development &amp; deployment platform for building, sharing, and shipping software fast." (https://replit.com/) 
</description>
      <itunes:keywords>AI, LLMs, damachine learning, data science, GenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Dr. Chelle Gentemann, Open Science Program Scientist for NASA’s Office of the Chief Science Data Officer, about NASA’s ambitious efforts to integrate AI across the research lifecycle. In this episode, we’ll dive deeper into how AI is transforming NASA’s approach to science, making data more accessible and advancing open science practices. We explore</p>

<ul>
<li><strong>Measuring the Impact of Open Science:</strong> How NASA is developing new metrics to evaluate the effectiveness of open science, moving beyond traditional publication-based assessments.</li>
<li><strong>The Process of Scientific Discovery:</strong> Insights into the collaborative nature of research and how breakthroughs are achieved at NASA.</li>
<li>** AI Applications in NASA’s Science:** From rats in space to exploring the origins of the universe, we cover how AI is being applied across NASA’s divisions to improve data accessibility and analysis.</li>
<li><strong>Addressing Challenges in Open Science:</strong> The complexities of implementing open science within government agencies and research environments.</li>
<li><strong>Reforming Incentive Systems:</strong> How NASA is reconsidering traditional metrics like publications and citations, and starting to recognize contributions such as software development and data sharing.</li>
<li><strong>The Future of Open Science:</strong> How open science is shaping the future of research, fostering interdisciplinary collaboration, and increasing accessibility.</li>
</ul>

<p>This conversation offers valuable insights for researchers, data scientists, and those interested in the practical applications of AI and open science. Join us as we discuss how NASA is working to make science more collaborative, reproducible, and impactful.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/VJDg3ZbkNOE?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://openscience101.org/" rel="nofollow">NASA&#39;s Open Science 101 course &lt;-- do it to learn and also to get NASA Swag!</a></li>
<li><a href="https://sciencecast.org/" rel="nofollow">Science Cast</a></li>
<li><a href="https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model" rel="nofollow">NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data</a></li>
<li><a href="https://x.com/jakevdp/status/408678764705378304" rel="nofollow">Jake VanderPlas&#39; daily conundrum tweet from 2013</a></li>
<li><a href="https://replit.com/" rel="nofollow">Replit, &quot;an AI-powered software development &amp; deployment platform for building, sharing, and shipping software fast.&quot;</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Dr. Chelle Gentemann, Open Science Program Scientist for NASA’s Office of the Chief Science Data Officer, about NASA’s ambitious efforts to integrate AI across the research lifecycle. In this episode, we’ll dive deeper into how AI is transforming NASA’s approach to science, making data more accessible and advancing open science practices. We explore</p>

<ul>
<li><strong>Measuring the Impact of Open Science:</strong> How NASA is developing new metrics to evaluate the effectiveness of open science, moving beyond traditional publication-based assessments.</li>
<li><strong>The Process of Scientific Discovery:</strong> Insights into the collaborative nature of research and how breakthroughs are achieved at NASA.</li>
<li>** AI Applications in NASA’s Science:** From rats in space to exploring the origins of the universe, we cover how AI is being applied across NASA’s divisions to improve data accessibility and analysis.</li>
<li><strong>Addressing Challenges in Open Science:</strong> The complexities of implementing open science within government agencies and research environments.</li>
<li><strong>Reforming Incentive Systems:</strong> How NASA is reconsidering traditional metrics like publications and citations, and starting to recognize contributions such as software development and data sharing.</li>
<li><strong>The Future of Open Science:</strong> How open science is shaping the future of research, fostering interdisciplinary collaboration, and increasing accessibility.</li>
</ul>

<p>This conversation offers valuable insights for researchers, data scientists, and those interested in the practical applications of AI and open science. Join us as we discuss how NASA is working to make science more collaborative, reproducible, and impactful.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/VJDg3ZbkNOE?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://openscience101.org/" rel="nofollow">NASA&#39;s Open Science 101 course &lt;-- do it to learn and also to get NASA Swag!</a></li>
<li><a href="https://sciencecast.org/" rel="nofollow">Science Cast</a></li>
<li><a href="https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model" rel="nofollow">NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data</a></li>
<li><a href="https://x.com/jakevdp/status/408678764705378304" rel="nofollow">Jake VanderPlas&#39; daily conundrum tweet from 2013</a></li>
<li><a href="https://replit.com/" rel="nofollow">Replit, &quot;an AI-powered software development &amp; deployment platform for building, sharing, and shipping software fast.&quot;</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+QQw_WhY4</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+QQw_WhY4" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 34: The AI Revolution Will Not Be Monopolized</title>
      <link>https://vanishinggradients.fireside.fm/34</link>
      <guid isPermaLink="false">8c18d59e-9b79-4682-8e3c-ba682daf1c1c</guid>
      <pubDate>Thu, 22 Aug 2024 17:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/8c18d59e-9b79-4682-8e3c-ba682daf1c1c.mp3" length="98751972" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Ines Montani and Matthew Honnibal, the creators of spaCy and founders of Explosion AI. Collectively, they've had a huge impact on the fields of industrial natural language processing (NLP), ML, and AI through their widely-used open-source library spaCy and their innovative annotation tool Prodigy.</itunes:subtitle>
      <itunes:duration>1:42:51</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/8/8c18d59e-9b79-4682-8e3c-ba682daf1c1c/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Ines Montani and Matthew Honnibal, the creators of spaCy and founders of Explosion AI. Collectively, they've had a huge impact on the fields of industrial natural language processing (NLP), ML, and AI through their widely-used open-source library spaCy and their innovative annotation tool Prodigy. These tools have become essential for many data scientists and NLP practitioners in industry and academia alike.
In this wide-ranging discussion, we dive into:
• The evolution of applied NLP and its role in industry
• The balance between large language models and smaller, specialized models
• Human-in-the-loop distillation for creating faster, more data-private AI systems
• The challenges and opportunities in NLP, including modularity, transparency, and privacy
• The future of AI and software development
• The potential impact of AI regulation on innovation and competition
We also touch on their recent transition back to a smaller, more independent-minded company structure and the lessons learned from their journey in the AI startup world.
Ines and Matt offer invaluable insights for data scientists, machine learning practitioners, and anyone interested in the practical applications of AI. They share their thoughts on how to approach NLP projects, the importance of data quality, and the role of open-source in advancing the field.
Whether you're a seasoned NLP practitioner or just getting started with AI, this episode offers a wealth of knowledge from two of the field's most respected figures. Join us for a discussion that explores the current landscape of AI development, with insights that bridge the gap between cutting-edge research and real-world applications.
LINKS
The livestream on YouTube (https://youtube.com/live/-6o5-3cP0ik?feature=share)
How S&amp;P Global is making markets more transparent with NLP, spaCy and Prodigy (https://explosion.ai/blog/sp-global-commodities)
A practical guide to human-in-the-loop distillation (https://explosion.ai/blog/human-in-the-loop-distillation)
Laws of Tech: Commoditize Your Complement (https://gwern.net/complement)
spaCy: Industrial-Strength Natural Language Processing (https://spacy.io/)
LLMs with spaCy (https://spacy.io/usage/large-language-models)
Explosion, building developer tools for AI, Machine Learning and Natural Language Processing (https://explosion.ai/)
Back to our roots: Company update and future plans, by Matt and Ines (https://explosion.ai/blog/back-to-our-roots-company-update)
Matt's detailed blog post: back to our roots (https://honnibal.dev/blog/back-to-our-roots)
Ines on twitter (https://x.com/_inesmontani)
Matt on twitter (https://x.com/honnibal)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Check out and subcribe to our lu.ma calendar (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk) for upcoming livestreams!
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI, NLP</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Ines Montani and Matthew Honnibal, the creators of spaCy and founders of Explosion AI. Collectively, they&#39;ve had a huge impact on the fields of industrial natural language processing (NLP), ML, and AI through their widely-used open-source library spaCy and their innovative annotation tool Prodigy. These tools have become essential for many data scientists and NLP practitioners in industry and academia alike.</p>

<p>In this wide-ranging discussion, we dive into:</p>

<p>• The evolution of applied NLP and its role in industry<br>
• The balance between large language models and smaller, specialized models<br>
• Human-in-the-loop distillation for creating faster, more data-private AI systems<br>
• The challenges and opportunities in NLP, including modularity, transparency, and privacy<br>
• The future of AI and software development<br>
• The potential impact of AI regulation on innovation and competition</p>

<p>We also touch on their recent transition back to a smaller, more independent-minded company structure and the lessons learned from their journey in the AI startup world.</p>

<p>Ines and Matt offer invaluable insights for data scientists, machine learning practitioners, and anyone interested in the practical applications of AI. They share their thoughts on how to approach NLP projects, the importance of data quality, and the role of open-source in advancing the field.</p>

<p>Whether you&#39;re a seasoned NLP practitioner or just getting started with AI, this episode offers a wealth of knowledge from two of the field&#39;s most respected figures. Join us for a discussion that explores the current landscape of AI development, with insights that bridge the gap between cutting-edge research and real-world applications.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/-6o5-3cP0ik?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://explosion.ai/blog/sp-global-commodities" rel="nofollow">How S&amp;P Global is making markets more transparent with NLP, spaCy and Prodigy</a></li>
<li><a href="https://explosion.ai/blog/human-in-the-loop-distillation" rel="nofollow">A practical guide to human-in-the-loop distillation</a></li>
<li><a href="https://gwern.net/complement" rel="nofollow">Laws of Tech: Commoditize Your Complement</a></li>
<li><a href="https://spacy.io/" rel="nofollow">spaCy: Industrial-Strength Natural Language Processing</a></li>
<li><a href="https://spacy.io/usage/large-language-models" rel="nofollow">LLMs with spaCy</a></li>
<li><a href="https://explosion.ai/" rel="nofollow">Explosion, building developer tools for AI, Machine Learning and Natural Language Processing</a></li>
<li><a href="https://explosion.ai/blog/back-to-our-roots-company-update" rel="nofollow">Back to our roots: Company update and future plans, by Matt and Ines</a></li>
<li><a href="https://honnibal.dev/blog/back-to-our-roots" rel="nofollow">Matt&#39;s detailed blog post: back to our roots</a></li>
<li><a href="https://x.com/_inesmontani" rel="nofollow">Ines on twitter</a></li>
<li><a href="https://x.com/honnibal" rel="nofollow">Matt on twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p>Check out and subcribe to our <a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">lu.ma calendar</a> for upcoming livestreams!</p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Ines Montani and Matthew Honnibal, the creators of spaCy and founders of Explosion AI. Collectively, they&#39;ve had a huge impact on the fields of industrial natural language processing (NLP), ML, and AI through their widely-used open-source library spaCy and their innovative annotation tool Prodigy. These tools have become essential for many data scientists and NLP practitioners in industry and academia alike.</p>

<p>In this wide-ranging discussion, we dive into:</p>

<p>• The evolution of applied NLP and its role in industry<br>
• The balance between large language models and smaller, specialized models<br>
• Human-in-the-loop distillation for creating faster, more data-private AI systems<br>
• The challenges and opportunities in NLP, including modularity, transparency, and privacy<br>
• The future of AI and software development<br>
• The potential impact of AI regulation on innovation and competition</p>

<p>We also touch on their recent transition back to a smaller, more independent-minded company structure and the lessons learned from their journey in the AI startup world.</p>

<p>Ines and Matt offer invaluable insights for data scientists, machine learning practitioners, and anyone interested in the practical applications of AI. They share their thoughts on how to approach NLP projects, the importance of data quality, and the role of open-source in advancing the field.</p>

<p>Whether you&#39;re a seasoned NLP practitioner or just getting started with AI, this episode offers a wealth of knowledge from two of the field&#39;s most respected figures. Join us for a discussion that explores the current landscape of AI development, with insights that bridge the gap between cutting-edge research and real-world applications.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/-6o5-3cP0ik?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://explosion.ai/blog/sp-global-commodities" rel="nofollow">How S&amp;P Global is making markets more transparent with NLP, spaCy and Prodigy</a></li>
<li><a href="https://explosion.ai/blog/human-in-the-loop-distillation" rel="nofollow">A practical guide to human-in-the-loop distillation</a></li>
<li><a href="https://gwern.net/complement" rel="nofollow">Laws of Tech: Commoditize Your Complement</a></li>
<li><a href="https://spacy.io/" rel="nofollow">spaCy: Industrial-Strength Natural Language Processing</a></li>
<li><a href="https://spacy.io/usage/large-language-models" rel="nofollow">LLMs with spaCy</a></li>
<li><a href="https://explosion.ai/" rel="nofollow">Explosion, building developer tools for AI, Machine Learning and Natural Language Processing</a></li>
<li><a href="https://explosion.ai/blog/back-to-our-roots-company-update" rel="nofollow">Back to our roots: Company update and future plans, by Matt and Ines</a></li>
<li><a href="https://honnibal.dev/blog/back-to-our-roots" rel="nofollow">Matt&#39;s detailed blog post: back to our roots</a></li>
<li><a href="https://x.com/_inesmontani" rel="nofollow">Ines on twitter</a></li>
<li><a href="https://x.com/honnibal" rel="nofollow">Matt on twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p>Check out and subcribe to our <a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">lu.ma calendar</a> for upcoming livestreams!</p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+ARKDboQf</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+ARKDboQf" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 33: What We Learned Teaching LLMs to 1,000s of Data Scientists</title>
      <link>https://vanishinggradients.fireside.fm/33</link>
      <guid isPermaLink="false">9cae0a8b-259a-4b01-a0f4-e5958297542b</guid>
      <pubDate>Mon, 12 Aug 2024 18:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/9cae0a8b-259a-4b01-a0f4-e5958297542b.mp3" length="81774888" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Dan Becker and Hamel Husain, two veterans in the world of data science, machine learning, and AI education. Collectively, they’ve worked at Google, DataRobot, Airbnb, Github (where Hamel built out the pre-cursor to copilot and more). And they both currently work as independent LLM and Generative AI consultants.

Dan and Hamel recently taught a course on fine-tuning large language models that evolved into a full-fledged conference, attracting over 2,000 participants. 

In this episode, we dive deep into their experience and the unique insights it gave them into the current state and future of AI education and application.</itunes:subtitle>
      <itunes:duration>1:25:10</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/9/9cae0a8b-259a-4b01-a0f4-e5958297542b/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Dan Becker and Hamel Husain, two veterans in the world of data science, machine learning, and AI education. Collectively, they’ve worked at Google, DataRobot, Airbnb, Github (where Hamel built out the precursor to copilot and more) and they both currently work as independent LLM and Generative AI consultants.
Dan and Hamel recently taught a course on fine-tuning large language models that evolved into a full-fledged conference, attracting over 2,000 participants. This experience gave them unique insights into the current state and future of AI education and application.
In this episode, we dive into:
* The evolution of their course from fine-tuning to a comprehensive AI conference
* The unexpected challenges and insights gained from teaching LLMs to data scientists
* The current state of AI tooling and accessibility compared to a decade ago
* The role of playful experimentation in driving innovation in the field
* Thoughts on the economic impact and ROI of generative AI in various industries
* The importance of proper evaluation in machine learning projects
* Future predictions for AI education and application in the next five years
* We also touch on the challenges of using AI tools effectively, the potential for AI in physical world applications, and the need for a more nuanced understanding of AI capabilities in the workplace.
During our conversation, Dan mentions an exciting project he's been working on, which we couldn't showcase live due to technical difficulties. However, I've included a link to a video demonstration in the show notes that you won't want to miss. In this demo, Dan showcases his innovative AI-powered 3D modeling tool that allows users to create 3D printable objects simply by describing them in natural language.
LINKS
The livestream on YouTube (https://youtube.com/live/hDmnwtjktsc?feature=share)
Educational resources from Dan and Hamel's LLM course (https://parlance-labs.com/education/)
Upwork Study Finds Employee Workloads Rising Despite Increased C-Suite Investment in Artificial Intelligence (https://investors.upwork.com/news-releases/news-release-details/upwork-study-finds-employee-workloads-rising-despite-increased-c)
Episode 29: Lessons from a Year of Building with LLMs (Part 1) (https://vanishinggradients.fireside.fm/29)
Episode 30: Lessons from a Year of Building with LLMs (Part 2) (https://vanishinggradients.fireside.fm/30)
Dan's demo: Creating Physical Products with Generative AI (https://youtu.be/U5J5RUOuMkI?si=_7cYLYOU1iwweQeO)
Build Great AI, Dan's boutique consulting firm helping clients be successful with large language models (https://buildgreat.ai/)
Parlance Labs, Hamel's Practical consulting that improves your AI (https://parlance-labs.com/)
Hamel on Twitter (https://x.com/HamelHusain)
Dan on Twitter (https://x.com/dan_s_becker)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Dan Becker and Hamel Husain, two veterans in the world of data science, machine learning, and AI education. Collectively, they’ve worked at Google, DataRobot, Airbnb, Github (where Hamel built out the precursor to copilot and more) and they both currently work as independent LLM and Generative AI consultants.</p>

<p>Dan and Hamel recently taught a course on fine-tuning large language models that evolved into a full-fledged conference, attracting over 2,000 participants. This experience gave them unique insights into the current state and future of AI education and application.</p>

<p>In this episode, we dive into:</p>

<ul>
<li>The evolution of their course from fine-tuning to a comprehensive AI conference</li>
<li>The unexpected challenges and insights gained from teaching LLMs to data scientists</li>
<li>The current state of AI tooling and accessibility compared to a decade ago</li>
<li>The role of playful experimentation in driving innovation in the field</li>
<li>Thoughts on the economic impact and ROI of generative AI in various industries</li>
<li>The importance of proper evaluation in machine learning projects</li>
<li>Future predictions for AI education and application in the next five years</li>
<li>We also touch on the challenges of using AI tools effectively, the potential for AI in physical world applications, and the need for a more nuanced understanding of AI capabilities in the workplace.</li>
</ul>

<p>During our conversation, Dan mentions an exciting project he&#39;s been working on, which we couldn&#39;t showcase live due to technical difficulties. However, I&#39;ve included a link to a video demonstration in the show notes that you won&#39;t want to miss. In this demo, Dan showcases his innovative AI-powered 3D modeling tool that allows users to create 3D printable objects simply by describing them in natural language.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/hDmnwtjktsc?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://parlance-labs.com/education/" rel="nofollow">Educational resources from Dan and Hamel&#39;s LLM course</a></li>
<li><a href="https://investors.upwork.com/news-releases/news-release-details/upwork-study-finds-employee-workloads-rising-despite-increased-c" rel="nofollow">Upwork Study Finds Employee Workloads Rising Despite Increased C-Suite Investment in Artificial Intelligence</a></li>
<li><a href="https://vanishinggradients.fireside.fm/29" rel="nofollow">Episode 29: Lessons from a Year of Building with LLMs (Part 1)</a></li>
<li><a href="https://vanishinggradients.fireside.fm/30" rel="nofollow">Episode 30: Lessons from a Year of Building with LLMs (Part 2)</a></li>
<li><a href="https://youtu.be/U5J5RUOuMkI?si=_7cYLYOU1iwweQeO" rel="nofollow">Dan&#39;s demo: Creating Physical Products with Generative AI</a></li>
<li><a href="https://buildgreat.ai/" rel="nofollow">Build Great AI, Dan&#39;s boutique consulting firm helping clients be successful with large language models</a></li>
<li><a href="https://parlance-labs.com/" rel="nofollow">Parlance Labs, Hamel&#39;s Practical consulting that improves your AI</a></li>
<li><a href="https://x.com/HamelHusain" rel="nofollow">Hamel on Twitter</a></li>
<li><a href="https://x.com/dan_s_becker" rel="nofollow">Dan on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Dan Becker and Hamel Husain, two veterans in the world of data science, machine learning, and AI education. Collectively, they’ve worked at Google, DataRobot, Airbnb, Github (where Hamel built out the precursor to copilot and more) and they both currently work as independent LLM and Generative AI consultants.</p>

<p>Dan and Hamel recently taught a course on fine-tuning large language models that evolved into a full-fledged conference, attracting over 2,000 participants. This experience gave them unique insights into the current state and future of AI education and application.</p>

<p>In this episode, we dive into:</p>

<ul>
<li>The evolution of their course from fine-tuning to a comprehensive AI conference</li>
<li>The unexpected challenges and insights gained from teaching LLMs to data scientists</li>
<li>The current state of AI tooling and accessibility compared to a decade ago</li>
<li>The role of playful experimentation in driving innovation in the field</li>
<li>Thoughts on the economic impact and ROI of generative AI in various industries</li>
<li>The importance of proper evaluation in machine learning projects</li>
<li>Future predictions for AI education and application in the next five years</li>
<li>We also touch on the challenges of using AI tools effectively, the potential for AI in physical world applications, and the need for a more nuanced understanding of AI capabilities in the workplace.</li>
</ul>

<p>During our conversation, Dan mentions an exciting project he&#39;s been working on, which we couldn&#39;t showcase live due to technical difficulties. However, I&#39;ve included a link to a video demonstration in the show notes that you won&#39;t want to miss. In this demo, Dan showcases his innovative AI-powered 3D modeling tool that allows users to create 3D printable objects simply by describing them in natural language.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/hDmnwtjktsc?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://parlance-labs.com/education/" rel="nofollow">Educational resources from Dan and Hamel&#39;s LLM course</a></li>
<li><a href="https://investors.upwork.com/news-releases/news-release-details/upwork-study-finds-employee-workloads-rising-despite-increased-c" rel="nofollow">Upwork Study Finds Employee Workloads Rising Despite Increased C-Suite Investment in Artificial Intelligence</a></li>
<li><a href="https://vanishinggradients.fireside.fm/29" rel="nofollow">Episode 29: Lessons from a Year of Building with LLMs (Part 1)</a></li>
<li><a href="https://vanishinggradients.fireside.fm/30" rel="nofollow">Episode 30: Lessons from a Year of Building with LLMs (Part 2)</a></li>
<li><a href="https://youtu.be/U5J5RUOuMkI?si=_7cYLYOU1iwweQeO" rel="nofollow">Dan&#39;s demo: Creating Physical Products with Generative AI</a></li>
<li><a href="https://buildgreat.ai/" rel="nofollow">Build Great AI, Dan&#39;s boutique consulting firm helping clients be successful with large language models</a></li>
<li><a href="https://parlance-labs.com/" rel="nofollow">Parlance Labs, Hamel&#39;s Practical consulting that improves your AI</a></li>
<li><a href="https://x.com/HamelHusain" rel="nofollow">Hamel on Twitter</a></li>
<li><a href="https://x.com/dan_s_becker" rel="nofollow">Dan on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+zc4vV6kH</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+zc4vV6kH" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 32: Building Reliable and Robust ML/AI Pipelines</title>
      <link>https://vanishinggradients.fireside.fm/32</link>
      <guid isPermaLink="false">3aa4ba58-30aa-4a85-a139-e9057629171c</guid>
      <pubDate>Sat, 27 Jul 2024 13:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/3aa4ba58-30aa-4a85-a139-e9057629171c.mp3" length="72173111" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Shreya Shankar, a researcher at UC Berkeley focusing on data management systems with a human-centered approach. Shreya's work is at the cutting edge of human-computer interaction (HCI) and AI, particularly in the realm of large language models (LLMs). Her impressive background includes being the first ML engineer at Viaduct, doing research engineering at Google Brain, and software engineering at Facebook.</itunes:subtitle>
      <itunes:duration>1:15:10</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/3/3aa4ba58-30aa-4a85-a139-e9057629171c/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Shreya Shankar, a researcher at UC Berkeley focusing on data management systems with a human-centered approach. Shreya's work is at the cutting edge of human-computer interaction (HCI) and AI, particularly in the realm of large language models (LLMs). Her impressive background includes being the first ML engineer at Viaduct, doing research engineering at Google Brain, and software engineering at Facebook.
In this episode, we dive deep into the world of LLMs and the critical challenges of building reliable AI pipelines. We'll explore:
The fascinating journey from classic machine learning to the current LLM revolution
Why Shreya believes most ML problems are actually data management issues
The concept of "data flywheels" for LLM applications and how to implement them
The intriguing world of evaluating AI systems - who validates the validators?
Shreya's work on SPADE and EvalGen, innovative tools for synthesizing data quality assertions and aligning LLM evaluations with human preferences
The importance of human-in-the-loop processes in AI development
The future of low-code and no-code tools in the AI landscape
We'll also touch on the potential pitfalls of over-relying on LLMs, the concept of "Habsburg AI," and how to avoid disappearing up our own proverbial arseholes in the world of recursive AI processes.
Whether you're a seasoned AI practitioner, a curious data scientist, or someone interested in the human side of AI development, this conversation offers valuable insights into building more robust, reliable, and human-centered AI systems.
LINKS
The livestream on YouTube (https://youtube.com/live/hKV6xSJZkB0?feature=share)
Shreya's website (https://www.sh-reya.com/)
Shreya on Twitter (https://x.com/sh_reya)
Data Flywheels for LLM Applications (https://www.sh-reya.com/blog/ai-engineering-flywheel/)
SPADE: Synthesizing Data Quality Assertions for Large Language Model Pipelines (https://arxiv.org/abs/2401.03038)
What We’ve Learned From A Year of Building with LLMs (https://applied-llms.org/)
Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences (https://arxiv.org/abs/2404.12272)
Operationalizing Machine Learning: An Interview Study (https://arxiv.org/abs/2209.09125)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
In the podcast, Hugo also mentioned that this was the 5th time he and Shreya chatted publicly. which is wild!
If you want to dive deep into Shreya's work and related topics through their chats, you can check them all out here:
Outerbounds' Fireside Chat: Operationalizing ML -- Patterns and Pain Points from MLOps Practitioners (https://www.youtube.com/watch?v=7zB6ESFto_U)
The Past, Present, and Future of Generative AI (https://youtu.be/q0A9CdGWXqc?si=XmaUnQmZiXL2eagS)
LLMs, OpenAI Dev Day, and the Existential Crisis for Machine Learning Engineering (https://www.youtube.com/live/MTJHvgJtynU?si=Ncjqn5YuFBemvOJ0)
Lessons from a Year of Building with LLMs (https://youtube.com/live/c0gcsprsFig?feature=share)
Check out and subcribe to our lu.ma calendar (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk) for upcoming livestreams! 
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Shreya Shankar, a researcher at UC Berkeley focusing on data management systems with a human-centered approach. Shreya&#39;s work is at the cutting edge of human-computer interaction (HCI) and AI, particularly in the realm of large language models (LLMs). Her impressive background includes being the first ML engineer at Viaduct, doing research engineering at Google Brain, and software engineering at Facebook.</p>

<p>In this episode, we dive deep into the world of LLMs and the critical challenges of building reliable AI pipelines. We&#39;ll explore:</p>

<ul>
<li>The fascinating journey from classic machine learning to the current LLM revolution</li>
<li>Why Shreya believes most ML problems are actually data management issues</li>
<li>The concept of &quot;data flywheels&quot; for LLM applications and how to implement them</li>
<li>The intriguing world of evaluating AI systems - who validates the validators?</li>
<li>Shreya&#39;s work on SPADE and EvalGen, innovative tools for synthesizing data quality assertions and aligning LLM evaluations with human preferences</li>
<li>The importance of human-in-the-loop processes in AI development</li>
<li>The future of low-code and no-code tools in the AI landscape</li>
</ul>

<p>We&#39;ll also touch on the potential pitfalls of over-relying on LLMs, the concept of &quot;Habsburg AI,&quot; and how to avoid disappearing up our own proverbial arseholes in the world of recursive AI processes.</p>

<p>Whether you&#39;re a seasoned AI practitioner, a curious data scientist, or someone interested in the human side of AI development, this conversation offers valuable insights into building more robust, reliable, and human-centered AI systems.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/hKV6xSJZkB0?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://www.sh-reya.com/" rel="nofollow">Shreya&#39;s website</a></li>
<li><a href="https://x.com/sh_reya" rel="nofollow">Shreya on Twitter</a></li>
<li><a href="https://www.sh-reya.com/blog/ai-engineering-flywheel/" rel="nofollow">Data Flywheels for LLM Applications</a></li>
<li><a href="https://arxiv.org/abs/2401.03038" rel="nofollow">SPADE: Synthesizing Data Quality Assertions for Large Language Model Pipelines</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://arxiv.org/abs/2404.12272" rel="nofollow">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</a></li>
<li><a href="https://arxiv.org/abs/2209.09125" rel="nofollow">Operationalizing Machine Learning: An Interview Study</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p>In the podcast, Hugo also mentioned that this was the 5th time he and Shreya chatted publicly. which is wild!</p>

<p>If you want to dive deep into Shreya&#39;s work and related topics through their chats, you can check them all out here:</p>

<ol>
<li><a href="https://www.youtube.com/watch?v=7zB6ESFto_U" rel="nofollow">Outerbounds&#39; Fireside Chat: Operationalizing ML -- Patterns and Pain Points from MLOps Practitioners</a></li>
<li><a href="https://youtu.be/q0A9CdGWXqc?si=XmaUnQmZiXL2eagS" rel="nofollow">The Past, Present, and Future of Generative AI</a></li>
<li><a href="https://www.youtube.com/live/MTJHvgJtynU?si=Ncjqn5YuFBemvOJ0" rel="nofollow">LLMs, OpenAI Dev Day, and the Existential Crisis for Machine Learning Engineering</a></li>
<li><a href="https://youtube.com/live/c0gcsprsFig?feature=share" rel="nofollow">Lessons from a Year of Building with LLMs</a></li>
</ol>

<p>Check out and subcribe to our <a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">lu.ma calendar</a> for upcoming livestreams!</p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Shreya Shankar, a researcher at UC Berkeley focusing on data management systems with a human-centered approach. Shreya&#39;s work is at the cutting edge of human-computer interaction (HCI) and AI, particularly in the realm of large language models (LLMs). Her impressive background includes being the first ML engineer at Viaduct, doing research engineering at Google Brain, and software engineering at Facebook.</p>

<p>In this episode, we dive deep into the world of LLMs and the critical challenges of building reliable AI pipelines. We&#39;ll explore:</p>

<ul>
<li>The fascinating journey from classic machine learning to the current LLM revolution</li>
<li>Why Shreya believes most ML problems are actually data management issues</li>
<li>The concept of &quot;data flywheels&quot; for LLM applications and how to implement them</li>
<li>The intriguing world of evaluating AI systems - who validates the validators?</li>
<li>Shreya&#39;s work on SPADE and EvalGen, innovative tools for synthesizing data quality assertions and aligning LLM evaluations with human preferences</li>
<li>The importance of human-in-the-loop processes in AI development</li>
<li>The future of low-code and no-code tools in the AI landscape</li>
</ul>

<p>We&#39;ll also touch on the potential pitfalls of over-relying on LLMs, the concept of &quot;Habsburg AI,&quot; and how to avoid disappearing up our own proverbial arseholes in the world of recursive AI processes.</p>

<p>Whether you&#39;re a seasoned AI practitioner, a curious data scientist, or someone interested in the human side of AI development, this conversation offers valuable insights into building more robust, reliable, and human-centered AI systems.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/hKV6xSJZkB0?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://www.sh-reya.com/" rel="nofollow">Shreya&#39;s website</a></li>
<li><a href="https://x.com/sh_reya" rel="nofollow">Shreya on Twitter</a></li>
<li><a href="https://www.sh-reya.com/blog/ai-engineering-flywheel/" rel="nofollow">Data Flywheels for LLM Applications</a></li>
<li><a href="https://arxiv.org/abs/2401.03038" rel="nofollow">SPADE: Synthesizing Data Quality Assertions for Large Language Model Pipelines</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://arxiv.org/abs/2404.12272" rel="nofollow">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</a></li>
<li><a href="https://arxiv.org/abs/2209.09125" rel="nofollow">Operationalizing Machine Learning: An Interview Study</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p>In the podcast, Hugo also mentioned that this was the 5th time he and Shreya chatted publicly. which is wild!</p>

<p>If you want to dive deep into Shreya&#39;s work and related topics through their chats, you can check them all out here:</p>

<ol>
<li><a href="https://www.youtube.com/watch?v=7zB6ESFto_U" rel="nofollow">Outerbounds&#39; Fireside Chat: Operationalizing ML -- Patterns and Pain Points from MLOps Practitioners</a></li>
<li><a href="https://youtu.be/q0A9CdGWXqc?si=XmaUnQmZiXL2eagS" rel="nofollow">The Past, Present, and Future of Generative AI</a></li>
<li><a href="https://www.youtube.com/live/MTJHvgJtynU?si=Ncjqn5YuFBemvOJ0" rel="nofollow">LLMs, OpenAI Dev Day, and the Existential Crisis for Machine Learning Engineering</a></li>
<li><a href="https://youtube.com/live/c0gcsprsFig?feature=share" rel="nofollow">Lessons from a Year of Building with LLMs</a></li>
</ol>

<p>Check out and subcribe to our <a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">lu.ma calendar</a> for upcoming livestreams!</p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+KT9lqcyl</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+KT9lqcyl" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 31: Rethinking Data Science, Machine Learning, and AI</title>
      <link>https://vanishinggradients.fireside.fm/31</link>
      <guid isPermaLink="false">455d1587-7ba6-4850-920e-360d8cbe33d3</guid>
      <pubDate>Tue, 09 Jul 2024 19:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/455d1587-7ba6-4850-920e-360d8cbe33d3.mp3" length="92236825" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Vincent Warmerdam, a senior data professional and machine learning engineer at :probabl, the exclusive brand operator of scikit-learn. Vincent is known for challenging common assumptions and exploring innovative approaches in data science and machine learning.</itunes:subtitle>
      <itunes:duration>1:36:04</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/4/455d1587-7ba6-4850-920e-360d8cbe33d3/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Vincent Warmerdam, a senior data professional and machine learning engineer at :probabl, the exclusive brand operator of scikit-learn. Vincent is known for challenging common assumptions and exploring innovative approaches in data science and machine learning.
In this episode, they dive deep into rethinking established methods in data science, machine learning, and AI. We explore Vincent's principled approach to the field, including:
The critical importance of exposing yourself to real-world problems before applying ML solutions
Framing problems correctly and understanding the data generating process
The power of visualization and human intuition in data analysis
Questioning whether algorithms truly meet the actual problem at hand
The value of simple, interpretable models and when to consider more complex approaches
The importance of UI and user experience in data science tools
Strategies for preventing algorithmic failures by rethinking evaluation metrics and data quality
The potential and limitations of LLMs in the current data science landscape
The benefits of open-source collaboration and knowledge sharing in the community
Throughout the conversation, Vincent illustrates these principles with vivid, real-world examples from his extensive experience in the field. They also discuss Vincent's thoughts on the future of data science and his call to action for more knowledge sharing in the community through blogging and open dialogue.
LINKS
The livestream on YouTube (https://youtube.com/live/-CD66CI1pEo?feature=share)
Vincent's blog (https://koaning.io/)
CalmCode (https://calmcode.io/)
scikit-lego (https://koaning.github.io/scikit-lego/)
Vincent's book Data Science Fiction (WIP) (https://calmcode.io/book)
The Deon Checklist, an ethics checklist for data scientists (https://deon.drivendata.org/)
Of oaths and checklists, by DJ Patil, Hilary Mason and Mike Loukides (https://www.oreilly.com/radar/of-oaths-and-checklists/)
Vincent's Getting Started with NLP and spaCy Course course on Talk Python (https://training.talkpython.fm/courses/getting-started-with-spacy)
Vincent on twitter (https://x.com/fishnets88)
:probabl. on twitter (https://x.com/probabl_ai)
Vincent's PyData Amsterdam Keynote "Natural Intelligence is All You Need [tm]" (https://www.youtube.com/watch?v=C9p7suS-NGk)
Vincent's PyData Amsterdam 2019 talk: The profession of solving (the wrong problem)  (https://www.youtube.com/watch?v=kYMfE9u-lMo)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Check out and subcribe to our lu.ma calendar (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk) for upcoming livestreams! 
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Vincent Warmerdam, a senior data professional and machine learning engineer at :probabl, the exclusive brand operator of scikit-learn. Vincent is known for challenging common assumptions and exploring innovative approaches in data science and machine learning.</p>

<p>In this episode, they dive deep into rethinking established methods in data science, machine learning, and AI. We explore Vincent&#39;s principled approach to the field, including:</p>

<ul>
<li>The critical importance of exposing yourself to real-world problems before applying ML solutions</li>
<li>Framing problems correctly and understanding the data generating process</li>
<li>The power of visualization and human intuition in data analysis</li>
<li>Questioning whether algorithms truly meet the actual problem at hand</li>
<li>The value of simple, interpretable models and when to consider more complex approaches</li>
<li>The importance of UI and user experience in data science tools</li>
<li>Strategies for preventing algorithmic failures by rethinking evaluation metrics and data quality</li>
<li>The potential and limitations of LLMs in the current data science landscape</li>
<li>The benefits of open-source collaboration and knowledge sharing in the community</li>
</ul>

<p>Throughout the conversation, Vincent illustrates these principles with vivid, real-world examples from his extensive experience in the field. They also discuss Vincent&#39;s thoughts on the future of data science and his call to action for more knowledge sharing in the community through blogging and open dialogue.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/-CD66CI1pEo?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://koaning.io/" rel="nofollow">Vincent&#39;s blog</a></li>
<li><a href="https://calmcode.io/" rel="nofollow">CalmCode</a></li>
<li><a href="https://koaning.github.io/scikit-lego/" rel="nofollow">scikit-lego</a></li>
<li><a href="https://calmcode.io/book" rel="nofollow">Vincent&#39;s book Data Science Fiction (WIP)</a></li>
<li><a href="https://deon.drivendata.org/" rel="nofollow">The Deon Checklist, an ethics checklist for data scientists</a></li>
<li><a href="https://www.oreilly.com/radar/of-oaths-and-checklists/" rel="nofollow">Of oaths and checklists, by DJ Patil, Hilary Mason and Mike Loukides</a></li>
<li><a href="https://training.talkpython.fm/courses/getting-started-with-spacy" rel="nofollow">Vincent&#39;s Getting Started with NLP and spaCy Course course on Talk Python</a></li>
<li><a href="https://x.com/fishnets88" rel="nofollow">Vincent on twitter</a></li>
<li><a href="https://x.com/probabl_ai" rel="nofollow">:probabl. on twitter</a></li>
<li><a href="https://www.youtube.com/watch?v=C9p7suS-NGk" rel="nofollow">Vincent&#39;s PyData Amsterdam Keynote &quot;Natural Intelligence is All You Need [tm]&quot;</a></li>
<li><a href="https://www.youtube.com/watch?v=kYMfE9u-lMo" rel="nofollow">Vincent&#39;s PyData Amsterdam 2019 talk: The profession of solving (the wrong problem) </a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p>Check out and subcribe to our <a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">lu.ma calendar</a> for upcoming livestreams!</p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Vincent Warmerdam, a senior data professional and machine learning engineer at :probabl, the exclusive brand operator of scikit-learn. Vincent is known for challenging common assumptions and exploring innovative approaches in data science and machine learning.</p>

<p>In this episode, they dive deep into rethinking established methods in data science, machine learning, and AI. We explore Vincent&#39;s principled approach to the field, including:</p>

<ul>
<li>The critical importance of exposing yourself to real-world problems before applying ML solutions</li>
<li>Framing problems correctly and understanding the data generating process</li>
<li>The power of visualization and human intuition in data analysis</li>
<li>Questioning whether algorithms truly meet the actual problem at hand</li>
<li>The value of simple, interpretable models and when to consider more complex approaches</li>
<li>The importance of UI and user experience in data science tools</li>
<li>Strategies for preventing algorithmic failures by rethinking evaluation metrics and data quality</li>
<li>The potential and limitations of LLMs in the current data science landscape</li>
<li>The benefits of open-source collaboration and knowledge sharing in the community</li>
</ul>

<p>Throughout the conversation, Vincent illustrates these principles with vivid, real-world examples from his extensive experience in the field. They also discuss Vincent&#39;s thoughts on the future of data science and his call to action for more knowledge sharing in the community through blogging and open dialogue.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/-CD66CI1pEo?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://koaning.io/" rel="nofollow">Vincent&#39;s blog</a></li>
<li><a href="https://calmcode.io/" rel="nofollow">CalmCode</a></li>
<li><a href="https://koaning.github.io/scikit-lego/" rel="nofollow">scikit-lego</a></li>
<li><a href="https://calmcode.io/book" rel="nofollow">Vincent&#39;s book Data Science Fiction (WIP)</a></li>
<li><a href="https://deon.drivendata.org/" rel="nofollow">The Deon Checklist, an ethics checklist for data scientists</a></li>
<li><a href="https://www.oreilly.com/radar/of-oaths-and-checklists/" rel="nofollow">Of oaths and checklists, by DJ Patil, Hilary Mason and Mike Loukides</a></li>
<li><a href="https://training.talkpython.fm/courses/getting-started-with-spacy" rel="nofollow">Vincent&#39;s Getting Started with NLP and spaCy Course course on Talk Python</a></li>
<li><a href="https://x.com/fishnets88" rel="nofollow">Vincent on twitter</a></li>
<li><a href="https://x.com/probabl_ai" rel="nofollow">:probabl. on twitter</a></li>
<li><a href="https://www.youtube.com/watch?v=C9p7suS-NGk" rel="nofollow">Vincent&#39;s PyData Amsterdam Keynote &quot;Natural Intelligence is All You Need [tm]&quot;</a></li>
<li><a href="https://www.youtube.com/watch?v=kYMfE9u-lMo" rel="nofollow">Vincent&#39;s PyData Amsterdam 2019 talk: The profession of solving (the wrong problem) </a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p>Check out and subcribe to our <a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">lu.ma calendar</a> for upcoming livestreams!</p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+rVBUrDjC</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+rVBUrDjC" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 30: Lessons from a Year of Building with LLMs (Part 2)</title>
      <link>https://vanishinggradients.fireside.fm/30</link>
      <guid isPermaLink="false">5412d7de-a99a-48c1-a1b4-f37f9bb29254</guid>
      <pubDate>Wed, 26 Jun 2024 15:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/5412d7de-a99a-48c1-a1b4-f37f9bb29254.mp3" length="72382927" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley (Part 2).</itunes:subtitle>
      <itunes:duration>1:15:23</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/5/5412d7de-a99a-48c1-a1b4-f37f9bb29254/transcript.txt" type="text/plain"/>
      <description>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley.
These five guests, along with Jason Liu who couldn't join us, have spent the past year building real-world applications with Large Language Models (LLMs). They've distilled their experiences into a report of 42 lessons across operational, strategic, and tactical dimensions (https://applied-llms.org/), and they're here to share their insights.
We’ve split this roundtable into 2 episodes and, in this second episode, we'll explore:
An inside look at building end-to-end systems with LLMs;
The experimentation mindset: Why it's the key to successful AI products;
Building trust in AI: Strategies for getting stakeholders on board;
The art of data examination: Why looking at your data is more crucial than ever;
Evaluation strategies that separate the pros from the amateurs.
Although we're focusing on LLMs, many of these insights apply broadly to data science, machine learning, and product development, more generally.
LINKS
The livestream on YouTube (https://www.youtube.com/live/c0gcsprsFig)
The Report: What We’ve Learned From A Year of Building with LLMs (https://applied-llms.org/)
About the Guests/Authors (https://applied-llms.org/about.html) &lt;-- connect with them all on LinkedIn, follow them on Twitter, subscribe to their newsletters! (Seriously, though, the amount of collective wisdom here is 🤑
Your AI product needs evals by Hamel Husain (https://hamel.dev/blog/posts/evals/)
Prompting Fundamentals and How to Apply them Effectively by Eugene Yan (https://eugeneyan.com/writing/prompting/)
Fuck You, Show Me The Prompt by Hamel Husain (https://hamel.dev/blog/posts/prompt/)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Vanishing Gradients on Twitter (https://x.com/vanishingdata)
Vanishing Gradients on Lu.ma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk) 
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley.</p>

<p>These five guests, along with Jason Liu who couldn&#39;t join us, have spent the past year building real-world applications with Large Language Models (LLMs). They&#39;ve distilled their experiences <a href="https://applied-llms.org/" rel="nofollow">into a report of 42 lessons across operational, strategic, and tactical dimensions</a>, and they&#39;re here to share their insights.</p>

<p>We’ve split this roundtable into 2 episodes and, in this second episode, we&#39;ll explore:</p>

<ul>
<li>An inside look at building end-to-end systems with LLMs;</li>
<li>The experimentation mindset: Why it&#39;s the key to successful AI products;</li>
<li>Building trust in AI: Strategies for getting stakeholders on board;</li>
<li>The art of data examination: Why looking at your data is more crucial than ever;</li>
<li>Evaluation strategies that separate the pros from the amateurs.</li>
</ul>

<p>Although we&#39;re focusing on LLMs, many of these insights apply broadly to data science, machine learning, and product development, more generally.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/c0gcsprsFig" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">The Report: What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://applied-llms.org/about.html" rel="nofollow">About the Guests/Authors</a> &lt;-- connect with them all on LinkedIn, follow them on Twitter, subscribe to their newsletters! (Seriously, though, the amount of collective wisdom here is 🤑</li>
<li><a href="https://hamel.dev/blog/posts/evals/" rel="nofollow">Your AI product needs evals by Hamel Husain</a></li>
<li><a href="https://eugeneyan.com/writing/prompting/" rel="nofollow">Prompting Fundamentals and How to Apply them Effectively by Eugene Yan</a></li>
<li><a href="https://hamel.dev/blog/posts/prompt/" rel="nofollow">Fuck You, Show Me The Prompt by Hamel Husain</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley.</p>

<p>These five guests, along with Jason Liu who couldn&#39;t join us, have spent the past year building real-world applications with Large Language Models (LLMs). They&#39;ve distilled their experiences <a href="https://applied-llms.org/" rel="nofollow">into a report of 42 lessons across operational, strategic, and tactical dimensions</a>, and they&#39;re here to share their insights.</p>

<p>We’ve split this roundtable into 2 episodes and, in this second episode, we&#39;ll explore:</p>

<ul>
<li>An inside look at building end-to-end systems with LLMs;</li>
<li>The experimentation mindset: Why it&#39;s the key to successful AI products;</li>
<li>Building trust in AI: Strategies for getting stakeholders on board;</li>
<li>The art of data examination: Why looking at your data is more crucial than ever;</li>
<li>Evaluation strategies that separate the pros from the amateurs.</li>
</ul>

<p>Although we&#39;re focusing on LLMs, many of these insights apply broadly to data science, machine learning, and product development, more generally.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/c0gcsprsFig" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">The Report: What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://applied-llms.org/about.html" rel="nofollow">About the Guests/Authors</a> &lt;-- connect with them all on LinkedIn, follow them on Twitter, subscribe to their newsletters! (Seriously, though, the amount of collective wisdom here is 🤑</li>
<li><a href="https://hamel.dev/blog/posts/evals/" rel="nofollow">Your AI product needs evals by Hamel Husain</a></li>
<li><a href="https://eugeneyan.com/writing/prompting/" rel="nofollow">Prompting Fundamentals and How to Apply them Effectively by Eugene Yan</a></li>
<li><a href="https://hamel.dev/blog/posts/prompt/" rel="nofollow">Fuck You, Show Me The Prompt by Hamel Husain</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+4MbNmDf0</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+4MbNmDf0" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 29: Lessons from a Year of Building with LLMs (Part 1)</title>
      <link>https://vanishinggradients.fireside.fm/29</link>
      <guid isPermaLink="false">7a5a4f5a-0040-451c-82f5-fd61cf1515f4</guid>
      <pubDate>Wed, 26 Jun 2024 14:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/7a5a4f5a-0040-451c-82f5-fd61cf1515f4.mp3" length="86750692" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley (Part 1).</itunes:subtitle>
      <itunes:duration>1:30:21</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/7/7a5a4f5a-0040-451c-82f5-fd61cf1515f4/transcript.txt" type="text/plain"/>
      <description>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley.
These five guests, along with Jason Liu who couldn't join us, have spent the past year building real-world applications with Large Language Models (LLMs). They've distilled their experiences into a report of 42 lessons across operational, strategic, and tactical dimensions (https://applied-llms.org/), and they're here to share their insights.
We’ve split this roundtable into 2 episodes and, in this first episode, we'll explore:
The critical role of evaluation and monitoring in LLM applications and why they're non-negotiable, including "evals" - short for evaluations, which are automated tests for assessing LLM performance and output quality;
Why data literacy is your secret weapon in the AI landscape;
The fine-tuning dilemma: when to do it and when to skip it;
Real-world lessons from building LLM applications that textbooks won't teach you;
The evolving role of data scientists and AI engineers in the age of AI.
Although we're focusing on LLMs, many of these insights apply broadly to data science, machine learning, and product development, more generally.
LINKS
The livestream on YouTube (https://www.youtube.com/live/c0gcsprsFig)
The Report: What We’ve Learned From A Year of Building with LLMs (https://applied-llms.org/)
About the Guests/Authors (https://applied-llms.org/about.html) &lt;-- connect with them all on LinkedIn, follow them on Twitter, subscribe to their newsletters! (Seriously, though, the amount of collective wisdom here is 🤑
Your AI product needs evals by Hamel Husain (https://hamel.dev/blog/posts/evals/)
Prompting Fundamentals and How to Apply them Effectively by Eugene Yan (https://eugeneyan.com/writing/prompting/)
Fuck You, Show Me The Prompt by Hamel Husain (https://hamel.dev/blog/posts/prompt/)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Vanishing Gradients on Twitter (https://x.com/vanishingdata)
Vanishing Gradients on Lu.ma (https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk) 
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, GenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley.</p>

<p>These five guests, along with Jason Liu who couldn&#39;t join us, have spent the past year building real-world applications with Large Language Models (LLMs). They&#39;ve distilled their experiences <a href="https://applied-llms.org/" rel="nofollow">into a report of 42 lessons across operational, strategic, and tactical dimensions</a>, and they&#39;re here to share their insights.</p>

<p>We’ve split this roundtable into 2 episodes and, in this first episode, we&#39;ll explore:</p>

<ul>
<li>The critical role of evaluation and monitoring in LLM applications and why they&#39;re non-negotiable, including &quot;evals&quot; - short for evaluations, which are automated tests for assessing LLM performance and output quality;</li>
<li>Why data literacy is your secret weapon in the AI landscape;</li>
<li>The fine-tuning dilemma: when to do it and when to skip it;</li>
<li>Real-world lessons from building LLM applications that textbooks won&#39;t teach you;</li>
<li>The evolving role of data scientists and AI engineers in the age of AI.</li>
</ul>

<p>Although we&#39;re focusing on LLMs, many of these insights apply broadly to data science, machine learning, and product development, more generally.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/c0gcsprsFig" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">The Report: What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://applied-llms.org/about.html" rel="nofollow">About the Guests/Authors</a> &lt;-- connect with them all on LinkedIn, follow them on Twitter, subscribe to their newsletters! (Seriously, though, the amount of collective wisdom here is 🤑</li>
<li><a href="https://hamel.dev/blog/posts/evals/" rel="nofollow">Your AI product needs evals by Hamel Husain</a></li>
<li><a href="https://eugeneyan.com/writing/prompting/" rel="nofollow">Prompting Fundamentals and How to Apply them Effectively by Eugene Yan</a></li>
<li><a href="https://hamel.dev/blog/posts/prompt/" rel="nofollow">Fuck You, Show Me The Prompt by Hamel Husain</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks about Lessons Learned from a Year of Building with LLMs with Eugene Yan from Amazon, Bryan Bischof from Hex, Charles Frye from Modal, Hamel Husain from Parlance Labs, and Shreya Shankar from UC Berkeley.</p>

<p>These five guests, along with Jason Liu who couldn&#39;t join us, have spent the past year building real-world applications with Large Language Models (LLMs). They&#39;ve distilled their experiences <a href="https://applied-llms.org/" rel="nofollow">into a report of 42 lessons across operational, strategic, and tactical dimensions</a>, and they&#39;re here to share their insights.</p>

<p>We’ve split this roundtable into 2 episodes and, in this first episode, we&#39;ll explore:</p>

<ul>
<li>The critical role of evaluation and monitoring in LLM applications and why they&#39;re non-negotiable, including &quot;evals&quot; - short for evaluations, which are automated tests for assessing LLM performance and output quality;</li>
<li>Why data literacy is your secret weapon in the AI landscape;</li>
<li>The fine-tuning dilemma: when to do it and when to skip it;</li>
<li>Real-world lessons from building LLM applications that textbooks won&#39;t teach you;</li>
<li>The evolving role of data scientists and AI engineers in the age of AI.</li>
</ul>

<p>Although we&#39;re focusing on LLMs, many of these insights apply broadly to data science, machine learning, and product development, more generally.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/c0gcsprsFig" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://applied-llms.org/" rel="nofollow">The Report: What We’ve Learned From A Year of Building with LLMs</a></li>
<li><a href="https://applied-llms.org/about.html" rel="nofollow">About the Guests/Authors</a> &lt;-- connect with them all on LinkedIn, follow them on Twitter, subscribe to their newsletters! (Seriously, though, the amount of collective wisdom here is 🤑</li>
<li><a href="https://hamel.dev/blog/posts/evals/" rel="nofollow">Your AI product needs evals by Hamel Husain</a></li>
<li><a href="https://eugeneyan.com/writing/prompting/" rel="nofollow">Prompting Fundamentals and How to Apply them Effectively by Eugene Yan</a></li>
<li><a href="https://hamel.dev/blog/posts/prompt/" rel="nofollow">Fuck You, Show Me The Prompt by Hamel Husain</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://x.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/calendar/cal-8ImWFDQ3IEIxNWk" rel="nofollow">Vanishing Gradients on Lu.ma</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+XireG_cY</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+XireG_cY" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 28: Beyond Supervised Learning: The Rise of In-Context Learning with LLMs</title>
      <link>https://vanishinggradients.fireside.fm/28</link>
      <guid isPermaLink="false">b268a89e-4fc9-4f9f-a2a5-c7636b3fbd70</guid>
      <pubDate>Mon, 10 Jun 2024 08:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/b268a89e-4fc9-4f9f-a2a5-c7636b3fbd70.mp3" length="63014789" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Alan Nichol, co-founder and CTO of Rasa, where they build software to enable developers to create enterprise-grade conversational AI and chatbot systems across industries like telcos, healthcare, fintech, and government.</itunes:subtitle>
      <itunes:duration>1:05:38</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/b/b268a89e-4fc9-4f9f-a2a5-c7636b3fbd70/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Alan Nichol, co-founder and CTO of Rasa, where they build software to enable developers to create enterprise-grade conversational AI and chatbot systems across industries like telcos, healthcare, fintech, and government.
What's super cool is that Alan and the Rasa team have been doing this type of thing for over a decade, giving them a wealth of wisdom on how to effectively incorporate LLMs into chatbots - and how not to. For example, if you want a chatbot that takes specific and important actions like transferring money, do you want to fully entrust the conversation to one big LLM like ChatGPT, or secure what the LLMs can do inside key business logic?
In this episode, they also dive into the history of conversational AI and explore how the advent of LLMs is reshaping the field. Alan shares his perspective on how supervised learning has failed us in some ways and discusses what he sees as the most overrated and underrated aspects of LLMs.
Alan offers advice for those looking to work with LLMs and conversational AI, emphasizing the importance of not sleeping on proven techniques and looking beyond the latest hype. In a live demo, he showcases Rasa's Calm (Conversational AI with Language Models), which allows developers to define business logic declaratively and separate it from the LLM, enabling reliable execution of conversational flows.
LINKS
The livestream on YouTube (https://www.youtube.com/live/kMFBYC2pB30?si=yV5sGq1iuC47LBSi)
Alan's Rasa CALM Demo: Building Conversational AI with LLMs  (https://youtu.be/4UnxaJ-GcT0?si=6uLY3GD5DkOmWiBW)
Alan on twitter.com (https://x.com/alanmnichol)
Rasa (https://rasa.com/)
CALM, an LLM-native approach to building reliable conversational AI (https://rasa.com/docs/rasa-pro/calm/)
Task-Oriented Dialogue with In-Context Learning (https://arxiv.org/abs/2402.12234)
'We don’t know how to build conversational software yet' by Alan Nicol (https://medium.com/rasa-blog/we-don-t-know-how-to-build-conversational-software-yet-a18301db0e4b)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Upcoming Livestreams
Lessons from a Year of Building with LLMs (https://lu.ma/e8huz3s6?utm_source=vgan)
VALIDATING THE VALIDATORS with Shreya Shanker (https://lu.ma/zz3qic45?utm_source=vgan)
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Alan Nichol, co-founder and CTO of Rasa, where they build software to enable developers to create enterprise-grade conversational AI and chatbot systems across industries like telcos, healthcare, fintech, and government.</p>

<p>What&#39;s super cool is that Alan and the Rasa team have been doing this type of thing for over a decade, giving them a wealth of wisdom on how to effectively incorporate LLMs into chatbots - and how not to. For example, if you want a chatbot that takes specific and important actions like transferring money, do you want to fully entrust the conversation to one big LLM like ChatGPT, or secure what the LLMs can do inside key business logic?</p>

<p>In this episode, they also dive into the history of conversational AI and explore how the advent of LLMs is reshaping the field. Alan shares his perspective on how supervised learning has failed us in some ways and discusses what he sees as the most overrated and underrated aspects of LLMs.</p>

<p>Alan offers advice for those looking to work with LLMs and conversational AI, emphasizing the importance of not sleeping on proven techniques and looking beyond the latest hype. In a live demo, he showcases Rasa&#39;s Calm (Conversational AI with Language Models), which allows developers to define business logic declaratively and separate it from the LLM, enabling reliable execution of conversational flows.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/kMFBYC2pB30?si=yV5sGq1iuC47LBSi" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://youtu.be/4UnxaJ-GcT0?si=6uLY3GD5DkOmWiBW" rel="nofollow">Alan&#39;s Rasa CALM Demo: Building Conversational AI with LLMs </a></li>
<li><a href="https://x.com/alanmnichol" rel="nofollow">Alan on twitter.com</a></li>
<li><a href="https://rasa.com/" rel="nofollow">Rasa</a></li>
<li><a href="https://rasa.com/docs/rasa-pro/calm/" rel="nofollow">CALM, an LLM-native approach to building reliable conversational AI</a></li>
<li><a href="https://arxiv.org/abs/2402.12234" rel="nofollow">Task-Oriented Dialogue with In-Context Learning</a></li>
<li><a href="https://medium.com/rasa-blog/we-don-t-know-how-to-build-conversational-software-yet-a18301db0e4b" rel="nofollow">&#39;We don’t know how to build conversational software yet&#39; by Alan Nicol</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p><strong>Upcoming Livestreams</strong></p>

<ul>
<li><a href="https://lu.ma/e8huz3s6?utm_source=vgan" rel="nofollow">Lessons from a Year of Building with LLMs</a></li>
<li><a href="https://lu.ma/zz3qic45?utm_source=vgan" rel="nofollow">VALIDATING THE VALIDATORS with Shreya Shanker</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Alan Nichol, co-founder and CTO of Rasa, where they build software to enable developers to create enterprise-grade conversational AI and chatbot systems across industries like telcos, healthcare, fintech, and government.</p>

<p>What&#39;s super cool is that Alan and the Rasa team have been doing this type of thing for over a decade, giving them a wealth of wisdom on how to effectively incorporate LLMs into chatbots - and how not to. For example, if you want a chatbot that takes specific and important actions like transferring money, do you want to fully entrust the conversation to one big LLM like ChatGPT, or secure what the LLMs can do inside key business logic?</p>

<p>In this episode, they also dive into the history of conversational AI and explore how the advent of LLMs is reshaping the field. Alan shares his perspective on how supervised learning has failed us in some ways and discusses what he sees as the most overrated and underrated aspects of LLMs.</p>

<p>Alan offers advice for those looking to work with LLMs and conversational AI, emphasizing the importance of not sleeping on proven techniques and looking beyond the latest hype. In a live demo, he showcases Rasa&#39;s Calm (Conversational AI with Language Models), which allows developers to define business logic declaratively and separate it from the LLM, enabling reliable execution of conversational flows.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/kMFBYC2pB30?si=yV5sGq1iuC47LBSi" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://youtu.be/4UnxaJ-GcT0?si=6uLY3GD5DkOmWiBW" rel="nofollow">Alan&#39;s Rasa CALM Demo: Building Conversational AI with LLMs </a></li>
<li><a href="https://x.com/alanmnichol" rel="nofollow">Alan on twitter.com</a></li>
<li><a href="https://rasa.com/" rel="nofollow">Rasa</a></li>
<li><a href="https://rasa.com/docs/rasa-pro/calm/" rel="nofollow">CALM, an LLM-native approach to building reliable conversational AI</a></li>
<li><a href="https://arxiv.org/abs/2402.12234" rel="nofollow">Task-Oriented Dialogue with In-Context Learning</a></li>
<li><a href="https://medium.com/rasa-blog/we-don-t-know-how-to-build-conversational-software-yet-a18301db0e4b" rel="nofollow">&#39;We don’t know how to build conversational software yet&#39; by Alan Nicol</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p><strong>Upcoming Livestreams</strong></p>

<ul>
<li><a href="https://lu.ma/e8huz3s6?utm_source=vgan" rel="nofollow">Lessons from a Year of Building with LLMs</a></li>
<li><a href="https://lu.ma/zz3qic45?utm_source=vgan" rel="nofollow">VALIDATING THE VALIDATORS with Shreya Shanker</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+5IAhw6MF</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+5IAhw6MF" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 27: How to Build Terrible AI Systems</title>
      <link>https://vanishinggradients.fireside.fm/27</link>
      <guid isPermaLink="false">d42a2479-a220-4f72-bf48-946c4a393efa</guid>
      <pubDate>Fri, 31 May 2024 10:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/d42a2479-a220-4f72-bf48-946c4a393efa.mp3" length="88718026" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Jason Liu, an independent consultant who uses his expertise in recommendation systems to help fast-growing startups build out their RAG applications. He was previously at Meta and Stitch Fix is also the creator of Instructor, Flight, and an ML and data science educator.</itunes:subtitle>
      <itunes:duration>1:32:24</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/d/d42a2479-a220-4f72-bf48-946c4a393efa/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Jason Liu, an independent consultant who uses his expertise in recommendation systems to help fast-growing startups build out their RAG applications. He was previously at Meta and Stitch Fix is also the creator of Instructor, Flight, and an ML and data science educator.
They talk about how Jason approaches consulting companies across many industries, including construction and sales, in building production LLM apps, his playbook for getting ML and AI up and running to build and maintain such apps, and the future of tooling to do so.
They take an inverted thinking approach, envisaging all the failure modes that would result in building terrible AI systems, and then figure out how to avoid such pitfalls.
LINKS
The livestream on YouTube (https://youtube.com/live/USTG6sQlB6s?feature=share)
Jason's website (https://jxnl.co/)
PyDdantic is all you need, Jason's Keynote at AI Engineer Summit, 2023 (https://youtu.be/yj-wSRJwrrc?si=JIGhN0mx0i50dUR9)
How to build a terrible RAG system by Jason (https://jxnl.co/writing/2024/01/07/inverted-thinking-rag/)
To express interest in Jason's Systematically improving RAG Applications course (https://q7gjsgfstrp.typeform.com/ragcourse?typeform-source=vg)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne)
Upcoming Livestreams
Good Riddance to Supervised Learning with Alan Nichol (CTO and co-founder, Rasa) (https://lu.ma/gphzzyyn?utm_source=vgj)
Lessons from a Year of Building with LLMs (https://lu.ma/e8huz3s6?utm_source=vgj)
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Jason Liu, an independent consultant who uses his expertise in recommendation systems to help fast-growing startups build out their RAG applications. He was previously at Meta and Stitch Fix is also the creator of Instructor, Flight, and an ML and data science educator.</p>

<p>They talk about how Jason approaches consulting companies across many industries, including construction and sales, in building production LLM apps, his playbook for getting ML and AI up and running to build and maintain such apps, and the future of tooling to do so.</p>

<p>They take an inverted thinking approach, envisaging all the failure modes that would result in building terrible AI systems, and then figure out how to avoid such pitfalls.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/USTG6sQlB6s?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://jxnl.co/" rel="nofollow">Jason&#39;s website</a></li>
<li><a href="https://youtu.be/yj-wSRJwrrc?si=JIGhN0mx0i50dUR9" rel="nofollow">PyDdantic is all you need, Jason&#39;s Keynote at AI Engineer Summit, 2023</a></li>
<li><a href="https://jxnl.co/writing/2024/01/07/inverted-thinking-rag/" rel="nofollow">How to build a terrible RAG system by Jason</a></li>
<li><a href="https://q7gjsgfstrp.typeform.com/ragcourse?typeform-source=vg" rel="nofollow">To express interest in Jason&#39;s <em>Systematically improving RAG Applications</em> course</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p><strong>Upcoming Livestreams</strong></p>

<ul>
<li><a href="https://lu.ma/gphzzyyn?utm_source=vgj" rel="nofollow">Good Riddance to Supervised Learning with Alan Nichol (CTO and co-founder, Rasa)</a></li>
<li><a href="https://lu.ma/e8huz3s6?utm_source=vgj" rel="nofollow">Lessons from a Year of Building with LLMs</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Jason Liu, an independent consultant who uses his expertise in recommendation systems to help fast-growing startups build out their RAG applications. He was previously at Meta and Stitch Fix is also the creator of Instructor, Flight, and an ML and data science educator.</p>

<p>They talk about how Jason approaches consulting companies across many industries, including construction and sales, in building production LLM apps, his playbook for getting ML and AI up and running to build and maintain such apps, and the future of tooling to do so.</p>

<p>They take an inverted thinking approach, envisaging all the failure modes that would result in building terrible AI systems, and then figure out how to avoid such pitfalls.</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/USTG6sQlB6s?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://jxnl.co/" rel="nofollow">Jason&#39;s website</a></li>
<li><a href="https://youtu.be/yj-wSRJwrrc?si=JIGhN0mx0i50dUR9" rel="nofollow">PyDdantic is all you need, Jason&#39;s Keynote at AI Engineer Summit, 2023</a></li>
<li><a href="https://jxnl.co/writing/2024/01/07/inverted-thinking-rag/" rel="nofollow">How to build a terrible RAG system by Jason</a></li>
<li><a href="https://q7gjsgfstrp.typeform.com/ragcourse?typeform-source=vg" rel="nofollow">To express interest in Jason&#39;s <em>Systematically improving RAG Applications</em> course</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>

<p><strong>Upcoming Livestreams</strong></p>

<ul>
<li><a href="https://lu.ma/gphzzyyn?utm_source=vgj" rel="nofollow">Good Riddance to Supervised Learning with Alan Nichol (CTO and co-founder, Rasa)</a></li>
<li><a href="https://lu.ma/e8huz3s6?utm_source=vgj" rel="nofollow">Lessons from a Year of Building with LLMs</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+GVbKE08T</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+GVbKE08T" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 26: Developing and Training LLMs From Scratch</title>
      <link>https://vanishinggradients.fireside.fm/26</link>
      <guid isPermaLink="false">d56cd02b-11cb-4be9-a2a7-31f783ef9c1a</guid>
      <pubDate>Wed, 15 May 2024 13:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/d56cd02b-11cb-4be9-a2a7-31f783ef9c1a.mp3" length="53564523" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Sebastian Raschka, a machine learning &amp; AI researcher, programmer, and author.They’ll tell you everything you need to know about LLMs, but were too afraid to ask: from covering the entire LLM lifecycle, what type of skills you need to work with them, what type of resources and hardware, prompt engineering vs fine-tuning vs RAG, how to build an LLM from scratch, and much more.</itunes:subtitle>
      <itunes:duration>1:51:35</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Sebastian Raschka, a machine learning &amp; AI researcher, programmer, and author. As Staff Research Engineer at Lightning AI, he focuses on the intersection of AI research, software development, and large language models (LLMs).
How do you build LLMs? How can you use them, both in prototype and production settings? What are the building blocks you need to know about?
​In this episode, we’ll tell you everything you need to know about LLMs, but were too afraid to ask: from covering the entire LLM lifecycle, what type of skills you need to work with them, what type of resources and hardware, prompt engineering vs fine-tuning vs RAG, how to build an LLM from scratch, and much more.
The idea here is not that you’ll need to use an LLM you’ve built from scratch, but that we’ll learn a lot about LLMs and how to use them in the process.
Near the end we also did some live coding to fine-tune GPT-2 in order to create a spam classifier! 
LINKS
The livestream on YouTube (https://youtube.com/live/qL4JY6Y5pmA)
Sebastian's website (https://sebastianraschka.com/)
Machine Learning Q and AI: 30 Essential Questions and Answers on Machine Learning and AI by Sebastian (https://nostarch.com/machine-learning-q-and-ai)
Build a Large Language Model (From Scratch) by Sebastian (https://www.manning.com/books/build-a-large-language-model-from-scratch)
PyTorch Lightning (https://lightning.ai/docs/pytorch/stable/)
Lightning Fabric (https://lightning.ai/docs/fabric/stable/)
LitGPT (https://github.com/Lightning-AI/litgpt)
Sebastian's notebook for finetuning GPT-2 for spam classification! (https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb)
The end of fine-tuning: Jeremy Howard on the Latent Space Podcast (https://www.latent.space/p/fastai)
Our next livestream: How to Build Terrible AI Systems with Jason Liu (https://lu.ma/terrible-ai-systems?utm_source=vg)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Hugo on Twitter (https://twitter.com/hugobowne) 
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, OpenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Sebastian Raschka, a machine learning &amp; AI researcher, programmer, and author. As Staff Research Engineer at Lightning AI, he focuses on the intersection of AI research, software development, and large language models (LLMs).</p>

<p>How do you build LLMs? How can you use them, both in prototype and production settings? What are the building blocks you need to know about?</p>

<p>​In this episode, we’ll tell you everything you need to know about LLMs, but were too afraid to ask: from covering the entire LLM lifecycle, what type of skills you need to work with them, what type of resources and hardware, prompt engineering vs fine-tuning vs RAG, how to build an LLM from scratch, and much more.</p>

<p>The idea here is not that you’ll need to use an LLM you’ve built from scratch, but that we’ll learn a lot about LLMs and how to use them in the process.</p>

<p>Near the end we also did some live coding to fine-tune GPT-2 in order to create a spam classifier! </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/qL4JY6Y5pmA" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://sebastianraschka.com/" rel="nofollow">Sebastian&#39;s website</a></li>
<li><a href="https://nostarch.com/machine-learning-q-and-ai" rel="nofollow">Machine Learning Q and AI: 30 Essential Questions and Answers on Machine Learning and AI by Sebastian</a></li>
<li><a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" rel="nofollow">Build a Large Language Model (From Scratch) by Sebastian</a></li>
<li><a href="https://lightning.ai/docs/pytorch/stable/" rel="nofollow">PyTorch Lightning</a></li>
<li><a href="https://lightning.ai/docs/fabric/stable/" rel="nofollow">Lightning Fabric</a></li>
<li><a href="https://github.com/Lightning-AI/litgpt" rel="nofollow">LitGPT</a></li>
<li><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb" rel="nofollow">Sebastian&#39;s notebook for finetuning GPT-2 for spam classification!</a></li>
<li><a href="https://www.latent.space/p/fastai" rel="nofollow">The end of fine-tuning: Jeremy Howard on the Latent Space Podcast</a></li>
<li><a href="https://lu.ma/terrible-ai-systems?utm_source=vg" rel="nofollow">Our next livestream: How to Build Terrible AI Systems with Jason Liu</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Sebastian Raschka, a machine learning &amp; AI researcher, programmer, and author. As Staff Research Engineer at Lightning AI, he focuses on the intersection of AI research, software development, and large language models (LLMs).</p>

<p>How do you build LLMs? How can you use them, both in prototype and production settings? What are the building blocks you need to know about?</p>

<p>​In this episode, we’ll tell you everything you need to know about LLMs, but were too afraid to ask: from covering the entire LLM lifecycle, what type of skills you need to work with them, what type of resources and hardware, prompt engineering vs fine-tuning vs RAG, how to build an LLM from scratch, and much more.</p>

<p>The idea here is not that you’ll need to use an LLM you’ve built from scratch, but that we’ll learn a lot about LLMs and how to use them in the process.</p>

<p>Near the end we also did some live coding to fine-tune GPT-2 in order to create a spam classifier! </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/qL4JY6Y5pmA" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://sebastianraschka.com/" rel="nofollow">Sebastian&#39;s website</a></li>
<li><a href="https://nostarch.com/machine-learning-q-and-ai" rel="nofollow">Machine Learning Q and AI: 30 Essential Questions and Answers on Machine Learning and AI by Sebastian</a></li>
<li><a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" rel="nofollow">Build a Large Language Model (From Scratch) by Sebastian</a></li>
<li><a href="https://lightning.ai/docs/pytorch/stable/" rel="nofollow">PyTorch Lightning</a></li>
<li><a href="https://lightning.ai/docs/fabric/stable/" rel="nofollow">Lightning Fabric</a></li>
<li><a href="https://github.com/Lightning-AI/litgpt" rel="nofollow">LitGPT</a></li>
<li><a href="https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb" rel="nofollow">Sebastian&#39;s notebook for finetuning GPT-2 for spam classification!</a></li>
<li><a href="https://www.latent.space/p/fastai" rel="nofollow">The end of fine-tuning: Jeremy Howard on the Latent Space Podcast</a></li>
<li><a href="https://lu.ma/terrible-ai-systems?utm_source=vg" rel="nofollow">Our next livestream: How to Build Terrible AI Systems with Jason Liu</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+LbOfKHbO</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+LbOfKHbO" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 25: Fully Reproducible ML &amp; AI Workflows</title>
      <link>https://vanishinggradients.fireside.fm/25</link>
      <guid isPermaLink="false">2e66472b-34f3-4068-b6f9-4942dc757325</guid>
      <pubDate>Mon, 18 Mar 2024 23:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/2e66472b-34f3-4068-b6f9-4942dc757325.mp3" length="77423933" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Omoju Miller, a machine learning guru and founder and CEO of Fimio, where she is building 21st century dev tooling.</itunes:subtitle>
      <itunes:duration>1:20:38</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Omoju Miller, a machine learning guru and founder and CEO of Fimio, where she is building 21st century dev tooling. In the past, she was Technical Advisor to the CEO at GitHub, spent time co-leading non-profit investment in Computer Science Education for Google, and served as a volunteer advisor to the Obama administration’s White House Presidential Innovation Fellows.
We need open tools, open data, provenance, and the ability to build fully reproducible, transparent machine learning workflows. With the advent of closed-source, vendor-based APIs and compute becoming a form of gate-keeping, developer tools are at the risk of becoming commoditized and developers becoming consumers.
We’ll  talk about how ideas for escaping these burgeoning walled gardens. We’ll dive into
What fully reproducible ML workflows would look like, including git for the workflow build process,
The need for loosely coupled and composable tools that embrace a UNIX-like philosophy,
What a much more scientific toolchain would look like,
What a future open sources commons for Generative AI could look like,
What an open compute ecosystem could look like,
How to create LLMs and tooling so everyone can use them to build production-ready apps,
And much more!
LINKS
The livestream on YouTube (https://www.youtube.com/live/n81PWNsHSMk?si=pgX2hH5xADATdJMu)
Omoju on Twitter (https://twitter.com/omojumiller)
Hugo on Twitter (https://twitter.com/hugobowne)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
Lu.ma Calendar that includes details of Hugo's European Tour for Outerbounds (https://lu.ma/Outerbounds)
Blog post that includes details of Hugo's European Tour for Outerbounds (https://outerbounds.com/blog/ob-on-the-road-2024-h1/)
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, OpenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Omoju Miller, a machine learning guru and founder and CEO of Fimio, where she is building 21st century dev tooling. In the past, she was Technical Advisor to the CEO at GitHub, spent time co-leading non-profit investment in Computer Science Education for Google, and served as a volunteer advisor to the Obama administration’s White House Presidential Innovation Fellows.</p>

<p>We need open tools, open data, provenance, and the ability to build fully reproducible, transparent machine learning workflows. With the advent of closed-source, vendor-based APIs and compute becoming a form of gate-keeping, developer tools are at the risk of becoming commoditized and developers becoming consumers.</p>

<p>We’ll  talk about how ideas for escaping these burgeoning walled gardens. We’ll dive into</p>

<ul>
<li>What fully reproducible ML workflows would look like, including git for the workflow build process,</li>
<li>The need for loosely coupled and composable tools that embrace a UNIX-like philosophy,</li>
<li>What a much more scientific toolchain would look like,</li>
<li>What a future open sources commons for Generative AI could look like,</li>
<li>What an open compute ecosystem could look like,</li>
<li>How to create LLMs and tooling so everyone can use them to build production-ready apps,</li>
</ul>

<p>And much more!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/n81PWNsHSMk?si=pgX2hH5xADATdJMu" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://twitter.com/omojumiller" rel="nofollow">Omoju on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/Outerbounds" rel="nofollow">Lu.ma Calendar that includes details of Hugo&#39;s European Tour for Outerbounds</a></li>
<li><a href="https://outerbounds.com/blog/ob-on-the-road-2024-h1/" rel="nofollow">Blog post that includes details of Hugo&#39;s European Tour for Outerbounds</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Omoju Miller, a machine learning guru and founder and CEO of Fimio, where she is building 21st century dev tooling. In the past, she was Technical Advisor to the CEO at GitHub, spent time co-leading non-profit investment in Computer Science Education for Google, and served as a volunteer advisor to the Obama administration’s White House Presidential Innovation Fellows.</p>

<p>We need open tools, open data, provenance, and the ability to build fully reproducible, transparent machine learning workflows. With the advent of closed-source, vendor-based APIs and compute becoming a form of gate-keeping, developer tools are at the risk of becoming commoditized and developers becoming consumers.</p>

<p>We’ll  talk about how ideas for escaping these burgeoning walled gardens. We’ll dive into</p>

<ul>
<li>What fully reproducible ML workflows would look like, including git for the workflow build process,</li>
<li>The need for loosely coupled and composable tools that embrace a UNIX-like philosophy,</li>
<li>What a much more scientific toolchain would look like,</li>
<li>What a future open sources commons for Generative AI could look like,</li>
<li>What an open compute ecosystem could look like,</li>
<li>How to create LLMs and tooling so everyone can use them to build production-ready apps,</li>
</ul>

<p>And much more!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://www.youtube.com/live/n81PWNsHSMk?si=pgX2hH5xADATdJMu" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://twitter.com/omojumiller" rel="nofollow">Omoju on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://lu.ma/Outerbounds" rel="nofollow">Lu.ma Calendar that includes details of Hugo&#39;s European Tour for Outerbounds</a></li>
<li><a href="https://outerbounds.com/blog/ob-on-the-road-2024-h1/" rel="nofollow">Blog post that includes details of Hugo&#39;s European Tour for Outerbounds</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+upinq3em</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+upinq3em" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 24: LLM and GenAI Accessibility</title>
      <link>https://vanishinggradients.fireside.fm/24</link>
      <guid isPermaLink="false">c6ebf900-c625-493a-b4c5-27a7f31da24f</guid>
      <pubDate>Tue, 27 Feb 2024 17:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/c6ebf900-c625-493a-b4c5-27a7f31da24f.mp3" length="86459792" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Johno Whitaker, a Data Scientist/AI Researcher doing R&amp;D with answer.ai, about where we’ve come from regarding tooling and accessibility for foundation models, ML, and AI, where we are, and where we’re going.</itunes:subtitle>
      <itunes:duration>1:30:03</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Johno Whitaker, a Data Scientist/AI Researcher doing R&amp;D with answer.ai. His current focus is on generative AI, flitting between different modalities. He also likes teaching and making courses, having worked with both Hugging Face and fast.ai in these capacities.
Johno recently reminded Hugo how hard everything was 10 years ago: “Want to install TensorFlow? Good luck. Need data? Perhaps try ImageNet. But now you can use big models from Hugging Face with hi-res satellite data and do all of this in a Colab notebook. Or think ecology and vision models… or medicine and multimodal models!”
We  talk about where we’ve come from regarding tooling and accessibility for foundation models, ML, and AI, where we are, and where we’re going. We’ll delve into
What the Generative AI mindset is, in terms of using atomic building blocks, and how it evolved from both the data science and ML mindsets;
How fast.ai democratized access to deep learning, what successes they had, and what was learned;
The moving parts now required to make GenAI and ML as accessible as possible;
The importance of focusing on UX and the application in the world of generative AI and foundation models;
The skillset and toolkit needed to be an LLM and AI guru;
What they’re up to at answer.ai to democratize LLMs and foundation models.
LINKS
The livestream on YouTube (https://youtube.com/live/hxZX6fBi-W8?feature=share)
Zindi, the largest professional network for data scientists in Africa (https://zindi.africa/)
A new old kind of R&amp;D lab: Announcing Answer.AI (http://www.answer.ai/posts/2023-12-12-launch.html)
Why and how I’m shifting focus to LLMs by Johno Whitaker (https://johnowhitaker.dev/dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html)
Applying AI to Immune Cell Networks by Rachel Thomas (https://www.fast.ai/posts/2024-01-23-cytokines/)
Replicate -- a cool place to explore GenAI models, among other things (https://replicate.com/explore)
Hands-On Generative AI with Transformers and Diffusion Models (https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/)
Johno on Twitter (https://twitter.com/johnowhitaker)
Hugo on Twitter (https://twitter.com/hugobowne)
Vanishing Gradients on Twitter (https://twitter.com/vanishingdata)
SciPy 2024 CFP (https://www.scipy2024.scipy.org/#CFP)
Escaping Generative AI Walled Gardens with Omoju Miller, a Vanishing Gradients Livestream (https://lu.ma/xonnjqe4)
</description>
      <itunes:keywords>genAI, machine learning, data science</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Johno Whitaker, a Data Scientist/AI Researcher doing R&amp;D with answer.ai. His current focus is on generative AI, flitting between different modalities. He also likes teaching and making courses, having worked with both Hugging Face and fast.ai in these capacities.</p>

<p>Johno recently reminded Hugo how hard everything was 10 years ago: “Want to install TensorFlow? Good luck. Need data? Perhaps try ImageNet. But now you can use big models from Hugging Face with hi-res satellite data and do all of this in a Colab notebook. Or think ecology and vision models… or medicine and multimodal models!”</p>

<p>We  talk about where we’ve come from regarding tooling and accessibility for foundation models, ML, and AI, where we are, and where we’re going. We’ll delve into</p>

<ul>
<li>What the Generative AI mindset is, in terms of using atomic building blocks, and how it evolved from both the data science and ML mindsets;</li>
<li>How fast.ai democratized access to deep learning, what successes they had, and what was learned;</li>
<li>The moving parts now required to make GenAI and ML as accessible as possible;</li>
<li>The importance of focusing on UX and the application in the world of generative AI and foundation models;</li>
<li>The skillset and toolkit needed to be an LLM and AI guru;</li>
<li>What they’re up to at answer.ai to democratize LLMs and foundation models.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/hxZX6fBi-W8?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://zindi.africa/" rel="nofollow">Zindi, the largest professional network for data scientists in Africa</a></li>
<li><a href="http://www.answer.ai/posts/2023-12-12-launch.html" rel="nofollow">A new old kind of R&amp;D lab: Announcing Answer.AI</a></li>
<li><a href="https://johnowhitaker.dev/dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html" rel="nofollow">Why and how I’m shifting focus to LLMs by Johno Whitaker</a></li>
<li><a href="https://www.fast.ai/posts/2024-01-23-cytokines/" rel="nofollow">Applying AI to Immune Cell Networks by Rachel Thomas</a></li>
<li><a href="https://replicate.com/explore" rel="nofollow">Replicate -- a cool place to explore GenAI models, among other things</a></li>
<li><a href="https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/" rel="nofollow">Hands-On Generative AI with Transformers and Diffusion Models</a></li>
<li><a href="https://twitter.com/johnowhitaker" rel="nofollow">Johno on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://www.scipy2024.scipy.org/#CFP" rel="nofollow">SciPy 2024 CFP</a></li>
<li><a href="https://lu.ma/xonnjqe4" rel="nofollow">Escaping Generative AI Walled Gardens with Omoju Miller, a Vanishing Gradients Livestream</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Johno Whitaker, a Data Scientist/AI Researcher doing R&amp;D with answer.ai. His current focus is on generative AI, flitting between different modalities. He also likes teaching and making courses, having worked with both Hugging Face and fast.ai in these capacities.</p>

<p>Johno recently reminded Hugo how hard everything was 10 years ago: “Want to install TensorFlow? Good luck. Need data? Perhaps try ImageNet. But now you can use big models from Hugging Face with hi-res satellite data and do all of this in a Colab notebook. Or think ecology and vision models… or medicine and multimodal models!”</p>

<p>We  talk about where we’ve come from regarding tooling and accessibility for foundation models, ML, and AI, where we are, and where we’re going. We’ll delve into</p>

<ul>
<li>What the Generative AI mindset is, in terms of using atomic building blocks, and how it evolved from both the data science and ML mindsets;</li>
<li>How fast.ai democratized access to deep learning, what successes they had, and what was learned;</li>
<li>The moving parts now required to make GenAI and ML as accessible as possible;</li>
<li>The importance of focusing on UX and the application in the world of generative AI and foundation models;</li>
<li>The skillset and toolkit needed to be an LLM and AI guru;</li>
<li>What they’re up to at answer.ai to democratize LLMs and foundation models.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/hxZX6fBi-W8?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://zindi.africa/" rel="nofollow">Zindi, the largest professional network for data scientists in Africa</a></li>
<li><a href="http://www.answer.ai/posts/2023-12-12-launch.html" rel="nofollow">A new old kind of R&amp;D lab: Announcing Answer.AI</a></li>
<li><a href="https://johnowhitaker.dev/dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html" rel="nofollow">Why and how I’m shifting focus to LLMs by Johno Whitaker</a></li>
<li><a href="https://www.fast.ai/posts/2024-01-23-cytokines/" rel="nofollow">Applying AI to Immune Cell Networks by Rachel Thomas</a></li>
<li><a href="https://replicate.com/explore" rel="nofollow">Replicate -- a cool place to explore GenAI models, among other things</a></li>
<li><a href="https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/" rel="nofollow">Hands-On Generative AI with Transformers and Diffusion Models</a></li>
<li><a href="https://twitter.com/johnowhitaker" rel="nofollow">Johno on Twitter</a></li>
<li><a href="https://twitter.com/hugobowne" rel="nofollow">Hugo on Twitter</a></li>
<li><a href="https://twitter.com/vanishingdata" rel="nofollow">Vanishing Gradients on Twitter</a></li>
<li><a href="https://www.scipy2024.scipy.org/#CFP" rel="nofollow">SciPy 2024 CFP</a></li>
<li><a href="https://lu.ma/xonnjqe4" rel="nofollow">Escaping Generative AI Walled Gardens with Omoju Miller, a Vanishing Gradients Livestream</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+-XFYPu8Z</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+-XFYPu8Z" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 23: Statistical and Algorithmic Thinking in the AI Age</title>
      <link>https://vanishinggradients.fireside.fm/23</link>
      <guid isPermaLink="false">96dc5719-497e-4bdb-82e0-a336cf46ec5d</guid>
      <pubDate>Thu, 21 Dec 2023 09:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/96dc5719-497e-4bdb-82e0-a336cf46ec5d.mp3" length="77400109" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Allen Downey, curriculum designer at Brilliant, Professor Emeritus at Olin College, and author, about the key statistical and data skills we all need to navigate an increasingly data-driven and algorithmic world. The goal will be to dive deep into the statistical paradoxes and fallacies that get in the way of using data to make informed decisions. </itunes:subtitle>
      <itunes:duration>1:20:37</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Allen Downey, a curriculum designer at Brilliant, Professor Emeritus at Olin College, and the author of Think Python, Think Bayes, Think Stats, and other computer science and data science books. In 2019-20 he was a Visiting Professor at Harvard University. He previously taught at Wellesley College and Colby College and was a Visiting Scientist at Google. He is also the author of the upcoming book Probably Overthinking It!
They discuss Allen's new book and the key statistical and data skills we all need to navigate an increasingly data-driven and algorithmic world. The goal was to dive deep into the statistical paradoxes and fallacies that get in the way of using data to make informed decisions. 
For example, when it was reported in 2021 that “in the United Kingdom, 70-plus percent of the people who die now from COVID are fully vaccinated,” this was correct but the implication was entirely wrong. Their conversation jumps into many such concrete examples to get to the bottom of using data for more than “lies, damned lies, and statistics.” They cover
Information and misinformation around pandemics and the base rate fallacy;
The tools we need to comprehend the small probabilities of high-risk events such as stock market crashes, earthquakes, and more;
The many definitions of algorithmic fairness, why they can't all be met at once, and what we can do about it;
Public health, the need for robust causal inference, and variations on Berkson’s paradox, such as the low-birthweight paradox: an influential paper found that that the mortality rate for children of smokers is lower for low-birthweight babies;
Why none of us are normal in any sense of the word, both in physical and psychological measurements;
The Inspection paradox, which shows up in the criminal justice system and distorts our perception of prison sentences and the risk of repeat offenders.
LINKS
The livestream on YouTube (https://youtube.com/live/G8LulD72kzs?feature=share)
Allen Downey on Github (https://github.com/AllenDowney)
Allen's new book Probably Overthinking It! (https://greenteapress.com/wp/probably-overthinking-it/)
Allen on Twitter (https://twitter.com/AllenDowney)
Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions by Mitchell et al. (https://arxiv.org/abs/1811.07867)
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Allen Downey, a curriculum designer at Brilliant, Professor Emeritus at Olin College, and the author of Think Python, Think Bayes, Think Stats, and other computer science and data science books. In 2019-20 he was a Visiting Professor at Harvard University. He previously taught at Wellesley College and Colby College and was a Visiting Scientist at Google. He is also the author of the upcoming book Probably Overthinking It!</p>

<p>They discuss Allen&#39;s new book and the key statistical and data skills we all need to navigate an increasingly data-driven and algorithmic world. The goal was to dive deep into the statistical paradoxes and fallacies that get in the way of using data to make informed decisions. </p>

<p>For example, when it was reported in 2021 that “in the United Kingdom, 70-plus percent of the people who die now from COVID are fully vaccinated,” this was correct but the implication was entirely wrong. Their conversation jumps into many such concrete examples to get to the bottom of using data for more than “lies, damned lies, and statistics.” They cover</p>

<ul>
<li>Information and misinformation around pandemics and the base rate fallacy;</li>
<li>The tools we need to comprehend the small probabilities of high-risk events such as stock market crashes, earthquakes, and more;</li>
<li>The many definitions of algorithmic fairness, why they can&#39;t all be met at once, and what we can do about it;</li>
<li>Public health, the need for robust causal inference, and variations on Berkson’s paradox, such as the low-birthweight paradox: an influential paper found that that the mortality rate for children of smokers is lower for low-birthweight babies;</li>
<li>Why none of us are normal in any sense of the word, both in physical and psychological measurements;</li>
<li>The Inspection paradox, which shows up in the criminal justice system and distorts our perception of prison sentences and the risk of repeat offenders.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/G8LulD72kzs?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://github.com/AllenDowney" rel="nofollow">Allen Downey on Github</a></li>
<li><a href="https://greenteapress.com/wp/probably-overthinking-it/" rel="nofollow">Allen&#39;s new book Probably Overthinking It!</a></li>
<li><a href="https://twitter.com/AllenDowney" rel="nofollow">Allen on Twitter</a></li>
<li><a href="https://arxiv.org/abs/1811.07867" rel="nofollow">Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions by Mitchell et al.</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Allen Downey, a curriculum designer at Brilliant, Professor Emeritus at Olin College, and the author of Think Python, Think Bayes, Think Stats, and other computer science and data science books. In 2019-20 he was a Visiting Professor at Harvard University. He previously taught at Wellesley College and Colby College and was a Visiting Scientist at Google. He is also the author of the upcoming book Probably Overthinking It!</p>

<p>They discuss Allen&#39;s new book and the key statistical and data skills we all need to navigate an increasingly data-driven and algorithmic world. The goal was to dive deep into the statistical paradoxes and fallacies that get in the way of using data to make informed decisions. </p>

<p>For example, when it was reported in 2021 that “in the United Kingdom, 70-plus percent of the people who die now from COVID are fully vaccinated,” this was correct but the implication was entirely wrong. Their conversation jumps into many such concrete examples to get to the bottom of using data for more than “lies, damned lies, and statistics.” They cover</p>

<ul>
<li>Information and misinformation around pandemics and the base rate fallacy;</li>
<li>The tools we need to comprehend the small probabilities of high-risk events such as stock market crashes, earthquakes, and more;</li>
<li>The many definitions of algorithmic fairness, why they can&#39;t all be met at once, and what we can do about it;</li>
<li>Public health, the need for robust causal inference, and variations on Berkson’s paradox, such as the low-birthweight paradox: an influential paper found that that the mortality rate for children of smokers is lower for low-birthweight babies;</li>
<li>Why none of us are normal in any sense of the word, both in physical and psychological measurements;</li>
<li>The Inspection paradox, which shows up in the criminal justice system and distorts our perception of prison sentences and the risk of repeat offenders.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/G8LulD72kzs?feature=share" rel="nofollow">The livestream on YouTube</a></li>
<li><a href="https://github.com/AllenDowney" rel="nofollow">Allen Downey on Github</a></li>
<li><a href="https://greenteapress.com/wp/probably-overthinking-it/" rel="nofollow">Allen&#39;s new book Probably Overthinking It!</a></li>
<li><a href="https://twitter.com/AllenDowney" rel="nofollow">Allen on Twitter</a></li>
<li><a href="https://arxiv.org/abs/1811.07867" rel="nofollow">Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions by Mitchell et al.</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+YJaRAAhj</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+YJaRAAhj" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 22: LLMs, OpenAI, and the Existential Crisis for Machine Learning Engineering</title>
      <link>https://vanishinggradients.fireside.fm/22</link>
      <guid isPermaLink="false">1565738b-1090-4efe-bb2c-2a4244eff19c</guid>
      <pubDate>Tue, 28 Nov 2023 08:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/1565738b-1090-4efe-bb2c-2a4244eff19c.mp3" length="76924471" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs) join Hugo Bowne-Anderson to talk about how LLMs and OpenAI are changing the worlds of data science, machine learning, and machine learning engineering.</itunes:subtitle>
      <itunes:duration>1:20:07</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs) join Hugo Bowne-Anderson to talk about how LLMs and OpenAI are changing the worlds of data science, machine learning, and machine learning engineering.
Jeremy Howard (https://twitter.com/jeremyphoward) is co-founder of fast.ai, an ex-Chief Scientist at Kaggle, and creator of the ULMFiT approach on which all modern language models are based. Shreya Shankar (https://twitter.com/sh_reya) is at UC Berkeley, ex Google brain, Facebook, and Viaduct. Hamel Husain (https://twitter.com/HamelHusain) has his own generative AI and LLM consultancy Parlance Labs (https://parlance-labs.com/) and was previously at Outerbounds, Github, and Airbnb.
They talk about
How LLMs shift the nature of the work we do in DS and ML,
How they change the tools we use,
The ways in which they could displace the role of traditional ML (e.g. will we stop using xgboost any time soon?),
How to navigate all the new tools and techniques,
The trade-offs between open and closed models,
Reactions to the recent Open Developer Day and the increasing existential crisis for ML.
LINKS
The panel on YouTube (https://youtube.com/live/MTJHvgJtynU?feature=share)
Hugo and Jeremy's upcoming livestream on what the hell happened recently at OpenAI, among many other things (https://lu.ma/byxyzfrr?utm_source=vg)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Vanishing Gradients on twitter (https://twitter.com/VanishingData)
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science, OpenAI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs) join Hugo Bowne-Anderson to talk about how LLMs and OpenAI are changing the worlds of data science, machine learning, and machine learning engineering.</p>

<p><a href="https://twitter.com/jeremyphoward" rel="nofollow">Jeremy Howard</a> is co-founder of fast.ai, an ex-Chief Scientist at Kaggle, and creator of the ULMFiT approach on which all modern language models are based. <a href="https://twitter.com/sh_reya" rel="nofollow">Shreya Shankar</a> is at UC Berkeley, ex Google brain, Facebook, and Viaduct. <a href="https://twitter.com/HamelHusain" rel="nofollow">Hamel Husain</a> has his own generative AI and LLM consultancy <a href="https://parlance-labs.com/" rel="nofollow">Parlance Labs</a> and was previously at Outerbounds, Github, and Airbnb.</p>

<p>They talk about</p>

<ul>
<li>How LLMs shift the nature of the work we do in DS and ML,</li>
<li>How they change the tools we use,</li>
<li>The ways in which they could displace the role of traditional ML (e.g. will we stop using xgboost any time soon?),</li>
<li>How to navigate all the new tools and techniques,</li>
<li>The trade-offs between open and closed models,</li>
<li>Reactions to the recent Open Developer Day and the increasing existential crisis for ML.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/MTJHvgJtynU?feature=share" rel="nofollow">The panel on YouTube</a></li>
<li><a href="https://lu.ma/byxyzfrr?utm_source=vg" rel="nofollow">Hugo and Jeremy&#39;s upcoming livestream on what the hell happened recently at OpenAI, among many other things</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://twitter.com/VanishingData" rel="nofollow">Vanishing Gradients on twitter</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs) join Hugo Bowne-Anderson to talk about how LLMs and OpenAI are changing the worlds of data science, machine learning, and machine learning engineering.</p>

<p><a href="https://twitter.com/jeremyphoward" rel="nofollow">Jeremy Howard</a> is co-founder of fast.ai, an ex-Chief Scientist at Kaggle, and creator of the ULMFiT approach on which all modern language models are based. <a href="https://twitter.com/sh_reya" rel="nofollow">Shreya Shankar</a> is at UC Berkeley, ex Google brain, Facebook, and Viaduct. <a href="https://twitter.com/HamelHusain" rel="nofollow">Hamel Husain</a> has his own generative AI and LLM consultancy <a href="https://parlance-labs.com/" rel="nofollow">Parlance Labs</a> and was previously at Outerbounds, Github, and Airbnb.</p>

<p>They talk about</p>

<ul>
<li>How LLMs shift the nature of the work we do in DS and ML,</li>
<li>How they change the tools we use,</li>
<li>The ways in which they could displace the role of traditional ML (e.g. will we stop using xgboost any time soon?),</li>
<li>How to navigate all the new tools and techniques,</li>
<li>The trade-offs between open and closed models,</li>
<li>Reactions to the recent Open Developer Day and the increasing existential crisis for ML.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://youtube.com/live/MTJHvgJtynU?feature=share" rel="nofollow">The panel on YouTube</a></li>
<li><a href="https://lu.ma/byxyzfrr?utm_source=vg" rel="nofollow">Hugo and Jeremy&#39;s upcoming livestream on what the hell happened recently at OpenAI, among many other things</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://twitter.com/VanishingData" rel="nofollow">Vanishing Gradients on twitter</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+AMKff0eU</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+AMKff0eU" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 21: Deploying LLMs in Production: Lessons Learned</title>
      <link>https://vanishinggradients.fireside.fm/21</link>
      <guid isPermaLink="false">e329eaa4-5768-44d0-878a-a96f3f2b53f0</guid>
      <pubDate>Tue, 14 Nov 2023 16:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/e329eaa4-5768-44d0-878a-a96f3f2b53f0.mp3" length="65466947" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Hamel Husain (ex-Github, Airbnb), a machine learning engineer who loves building machine learning infrastructure and tools, about generative AI, large language models, the business value they can generate, and how to get started. </itunes:subtitle>
      <itunes:duration>1:08:11</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Hamel Husain, a machine learning engineer who loves building machine learning infrastructure and tools 👷. Hamel leads and contributes to many popular open-source machine learning projects. He also has extensive experience (20+ years) as a machine learning engineer across various industries, including large tech companies like Airbnb and GitHub.  At GitHub, he led CodeSearchNet (https://github.com/github/CodeSearchNet), a large language model for semantic search that was a precursor to CoPilot. Hamel is the founder of Parlance-Labs (https://parlance-labs.com/), a research and consultancy focused on LLMs.
They talk about generative AI, large language models, the business value they can generate, and how to get started. 
They delve into
Where Hamel is seeing the most business interest in LLMs (spoiler: the answer isn’t only tech);
Common misconceptions about LLMs;
The skills you need to work with LLMs and GenAI models;
Tools and techniques, such as fine-tuning, RAGs, LoRA, hardware, and more!
Vendor APIs vs OSS models.
LINKS
Our upcoming livestream LLMs, OpenAI Dev Day, and the Existential Crisis for Machine Learning Engineering with Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs): Sign up for free! (https://lu.ma/m81oepqe/utm_source=vghh)
Our recent livestream Data and DevOps Tools for Evaluating and Productionizing LLMs (https://youtube.com/live/B_DMMlDuJB0) with Hamel and Emil Sedgh, Lead AI engineer at Rechat -- in it, we showcase an actual industrial use case that Hamel and Emil are working on with Rechat, a real estate CRM, taking you through LLM workflows and tools.
Extended Guide: Instruction-tune Llama 2 (https://www.philschmid.de/instruction-tune-llama-2) by Philipp Schmid
The livestream recoding of this episode! (https://youtube.com/live/l7jJhL9geZQ?feature=share)
Hamel on twitter (https://twitter.com/HamelHusain)
</description>
      <itunes:keywords>AI, LLMs, machine learning, data science</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Hamel Husain, a machine learning engineer who loves building machine learning infrastructure and tools 👷. Hamel leads and contributes to many popular open-source machine learning projects. He also has extensive experience (20+ years) as a machine learning engineer across various industries, including large tech companies like Airbnb and GitHub.  At GitHub, he led <a href="https://github.com/github/CodeSearchNet" rel="nofollow">CodeSearchNet</a>, a large language model for semantic search that was a precursor to CoPilot. Hamel is the founder of <a href="https://parlance-labs.com/" rel="nofollow">Parlance-Labs</a>, a research and consultancy focused on LLMs.</p>

<p>They talk about generative AI, large language models, the business value they can generate, and how to get started. </p>

<p>They delve into</p>

<ul>
<li>Where Hamel is seeing the most business interest in LLMs (spoiler: the answer isn’t only tech);</li>
<li>Common misconceptions about LLMs;</li>
<li>The skills you need to work with LLMs and GenAI models;</li>
<li>Tools and techniques, such as fine-tuning, RAGs, LoRA, hardware, and more!</li>
<li>Vendor APIs vs OSS models.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://lu.ma/m81oepqe/utm_source=vghh" rel="nofollow">Our upcoming livestream LLMs, OpenAI Dev Day, and the Existential Crisis for Machine Learning Engineering with Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs): Sign up for free!</a></li>
<li>Our recent livestream <a href="https://youtube.com/live/B_DMMlDuJB0" rel="nofollow">Data and DevOps Tools for Evaluating and Productionizing LLMs</a> with Hamel and Emil Sedgh, Lead AI engineer at Rechat -- in it, we showcase an actual industrial use case that Hamel and Emil are working on with Rechat, a real estate CRM, taking you through LLM workflows and tools.</li>
<li><a href="https://www.philschmid.de/instruction-tune-llama-2" rel="nofollow">Extended Guide: Instruction-tune Llama 2</a> by Philipp Schmid</li>
<li><a href="https://youtube.com/live/l7jJhL9geZQ?feature=share" rel="nofollow">The livestream recoding of this episode!</a></li>
<li><a href="https://twitter.com/HamelHusain" rel="nofollow">Hamel on twitter</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Hamel Husain, a machine learning engineer who loves building machine learning infrastructure and tools 👷. Hamel leads and contributes to many popular open-source machine learning projects. He also has extensive experience (20+ years) as a machine learning engineer across various industries, including large tech companies like Airbnb and GitHub.  At GitHub, he led <a href="https://github.com/github/CodeSearchNet" rel="nofollow">CodeSearchNet</a>, a large language model for semantic search that was a precursor to CoPilot. Hamel is the founder of <a href="https://parlance-labs.com/" rel="nofollow">Parlance-Labs</a>, a research and consultancy focused on LLMs.</p>

<p>They talk about generative AI, large language models, the business value they can generate, and how to get started. </p>

<p>They delve into</p>

<ul>
<li>Where Hamel is seeing the most business interest in LLMs (spoiler: the answer isn’t only tech);</li>
<li>Common misconceptions about LLMs;</li>
<li>The skills you need to work with LLMs and GenAI models;</li>
<li>Tools and techniques, such as fine-tuning, RAGs, LoRA, hardware, and more!</li>
<li>Vendor APIs vs OSS models.</li>
</ul>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://lu.ma/m81oepqe/utm_source=vghh" rel="nofollow">Our upcoming livestream LLMs, OpenAI Dev Day, and the Existential Crisis for Machine Learning Engineering with Jeremy Howard (Fast.ai), Shreya Shankar (UC Berkeley), and Hamel Husain (Parlance Labs): Sign up for free!</a></li>
<li>Our recent livestream <a href="https://youtube.com/live/B_DMMlDuJB0" rel="nofollow">Data and DevOps Tools for Evaluating and Productionizing LLMs</a> with Hamel and Emil Sedgh, Lead AI engineer at Rechat -- in it, we showcase an actual industrial use case that Hamel and Emil are working on with Rechat, a real estate CRM, taking you through LLM workflows and tools.</li>
<li><a href="https://www.philschmid.de/instruction-tune-llama-2" rel="nofollow">Extended Guide: Instruction-tune Llama 2</a> by Philipp Schmid</li>
<li><a href="https://youtube.com/live/l7jJhL9geZQ?feature=share" rel="nofollow">The livestream recoding of this episode!</a></li>
<li><a href="https://twitter.com/HamelHusain" rel="nofollow">Hamel on twitter</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+ilwSHbnK</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+ilwSHbnK" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 20: Data Science: Past, Present, and Future</title>
      <link>https://vanishinggradients.fireside.fm/20</link>
      <guid isPermaLink="false">3c0c5565-056f-45f4-a785-ec46800bb2cd</guid>
      <pubDate>Thu, 05 Oct 2023 15:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/3c0c5565-056f-45f4-a785-ec46800bb2cd.mp3" length="83201801" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Chris Wiggins (Columbia, NYTimes) and Matthew Jones (Princeton) about their recent book How Data Happened, and the Columbia course it expands upon, data: past, present, and future.
</itunes:subtitle>
      <itunes:duration>1:26:39</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Chris Wiggins (Columbia, NYTimes) and Matthew Jones (Princeton) about their recent book How Data Happened, and the Columbia course it expands upon, data: past, present, and future.
Chris is an associate professor of applied mathematics at Columbia University and the New York Times’ chief data scientist, and Matthew is  a professor of history at Princeton University and former Guggenheim Fellow.
From facial recognition to automated decision systems that inform who gets loans and who receives bail, we all now move through a world determined by data-empowered algorithms. These technologies didn’t just appear: they are part of a history that goes back centuries, from the census enshrined in the US Constitution to the birth of eugenics in Victorian Britain to the development of Google search.
DJ Patil, former U.S. Chief Data Scientist, said of the book "This is the first comprehensive look at the history of data and how power has played a critical role in shaping the history. It’s a must read for any data scientist about how we got here and what we need to do to ensure that data works for everyone."
If you’re a data scientist, machine learning engineer, or work with data in any way, it’s increasingly important to know more about the history and future of the work that you do and understand how your work impacts society and the world.
Among other things, they'll delve into
* the history of human use of data;
* how data are used to reveal insight and support decisions;
* how data and data-powered algorithms shape, constrain, and manipulate our commercial, civic, and personal transactions and experiences; and
* how exploration and analysis of data have become part of our logic and rhetoric of communication and persuasion.
You can also sign up for our next livestreamed podcast recording here (https://www.eventbrite.com/e/data-science-past-present-and-future-tickets-695643357007?aff=kjvg)! 
LINKS
How Data Happened, the book! (https://wwnorton.com/books/how-data-happened)
data: past, present, and future, the course (https://data-ppf.github.io/)
Race After Technology, by Ruha Benjamin (https://www.ruhabenjamin.com/race-after-technology)
The problem with metrics is a big problem for AI by Rachel Thomas (https://www.ruhabenjamin.com/race-after-technology)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Chris Wiggins (Columbia, NYTimes) and Matthew Jones (Princeton) about their recent book How Data Happened, and the Columbia course it expands upon, data: past, present, and future.</p>

<p>Chris is an associate professor of applied mathematics at Columbia University and the New York Times’ chief data scientist, and Matthew is  a professor of history at Princeton University and former Guggenheim Fellow.</p>

<p>From facial recognition to automated decision systems that inform who gets loans and who receives bail, we all now move through a world determined by data-empowered algorithms. These technologies didn’t just appear: they are part of a history that goes back centuries, from the census enshrined in the US Constitution to the birth of eugenics in Victorian Britain to the development of Google search.</p>

<p>DJ Patil, former U.S. Chief Data Scientist, said of the book &quot;This is the first comprehensive look at the history of data and how power has played a critical role in shaping the history. It’s a must read for any data scientist about how we got here and what we need to do to ensure that data works for everyone.&quot;</p>

<p>If you’re a data scientist, machine learning engineer, or work with data in any way, it’s increasingly important to know more about the history and future of the work that you do and understand how your work impacts society and the world.</p>

<p>Among other things, they&#39;ll delve into</p>

<ul>
<li>the history of human use of data;</li>
<li>how data are used to reveal insight and support decisions;</li>
<li>how data and data-powered algorithms shape, constrain, and manipulate our commercial, civic, and personal transactions and experiences; and</li>
<li>how exploration and analysis of data have become part of our logic and rhetoric of communication and persuasion.</li>
</ul>

<p>You can also sign up for our next livestreamed podcast recording <a href="https://www.eventbrite.com/e/data-science-past-present-and-future-tickets-695643357007?aff=kjvg" rel="nofollow">here</a>! </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://wwnorton.com/books/how-data-happened" rel="nofollow">How Data Happened, the book!</a></li>
<li><a href="https://data-ppf.github.io/" rel="nofollow">data: past, present, and future, the course</a></li>
<li><a href="https://www.ruhabenjamin.com/race-after-technology" rel="nofollow">Race After Technology, by Ruha Benjamin</a></li>
<li><a href="https://www.ruhabenjamin.com/race-after-technology" rel="nofollow">The problem with metrics is a big problem for AI by Rachel Thomas</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Chris Wiggins (Columbia, NYTimes) and Matthew Jones (Princeton) about their recent book How Data Happened, and the Columbia course it expands upon, data: past, present, and future.</p>

<p>Chris is an associate professor of applied mathematics at Columbia University and the New York Times’ chief data scientist, and Matthew is  a professor of history at Princeton University and former Guggenheim Fellow.</p>

<p>From facial recognition to automated decision systems that inform who gets loans and who receives bail, we all now move through a world determined by data-empowered algorithms. These technologies didn’t just appear: they are part of a history that goes back centuries, from the census enshrined in the US Constitution to the birth of eugenics in Victorian Britain to the development of Google search.</p>

<p>DJ Patil, former U.S. Chief Data Scientist, said of the book &quot;This is the first comprehensive look at the history of data and how power has played a critical role in shaping the history. It’s a must read for any data scientist about how we got here and what we need to do to ensure that data works for everyone.&quot;</p>

<p>If you’re a data scientist, machine learning engineer, or work with data in any way, it’s increasingly important to know more about the history and future of the work that you do and understand how your work impacts society and the world.</p>

<p>Among other things, they&#39;ll delve into</p>

<ul>
<li>the history of human use of data;</li>
<li>how data are used to reveal insight and support decisions;</li>
<li>how data and data-powered algorithms shape, constrain, and manipulate our commercial, civic, and personal transactions and experiences; and</li>
<li>how exploration and analysis of data have become part of our logic and rhetoric of communication and persuasion.</li>
</ul>

<p>You can also sign up for our next livestreamed podcast recording <a href="https://www.eventbrite.com/e/data-science-past-present-and-future-tickets-695643357007?aff=kjvg" rel="nofollow">here</a>! </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://wwnorton.com/books/how-data-happened" rel="nofollow">How Data Happened, the book!</a></li>
<li><a href="https://data-ppf.github.io/" rel="nofollow">data: past, present, and future, the course</a></li>
<li><a href="https://www.ruhabenjamin.com/race-after-technology" rel="nofollow">Race After Technology, by Ruha Benjamin</a></li>
<li><a href="https://www.ruhabenjamin.com/race-after-technology" rel="nofollow">The problem with metrics is a big problem for AI by Rachel Thomas</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+-ke3N6gF</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+-ke3N6gF" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 19: Privacy and Security in Data Science and Machine Learning</title>
      <link>https://vanishinggradients.fireside.fm/19</link>
      <guid isPermaLink="false">87376a4e-df73-494f-88ad-09d0313b95c6</guid>
      <pubDate>Tue, 15 Aug 2023 03:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/87376a4e-df73-494f-88ad-09d0313b95c6.mp3" length="79998085" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Katharine Jarmul about privacy and security in data science and machine learning. Katharine is a Principal Data Scientist at Thoughtworks Germany focusing on privacy, ethics, and security for data science workflows.</itunes:subtitle>
      <itunes:duration>1:23:19</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Katharine Jarmul about privacy and security in data science and machine learning. Katharine is a Principal Data Scientist at Thoughtworks Germany focusing on privacy, ethics, and security for data science workflows. Previously, she has held numerous roles at large companies and startups in the US and Germany, implementing data processing and machine learning systems with a focus on reliability, testability, privacy, and security.
In this episode, Hugo and Katharine talk about
What data privacy and security are, what they aren’t and the differences between them (hopefully dispelling common misconceptions along the way!);
Why you should care about them (hint: the answers will involve regulatory, ethical, risk, and organizational concerns);
Data governance, anonymization techniques, and privacy in data pipelines;
Privacy attacks!
The state of the art in privacy-aware machine learning and data science, including federated learning;
What you need to know about the current state of regulation, including GDPR and CCPA…
And much more, all the while grounding our conversation in real-world examples from data science, machine learning, business, and life!
You can also sign up for our next livestreamed podcast recording here (https://lu.ma/4b5xalpz)! 
LINKS
Win a copy of Practical Data Privacy, Katharine's new book! (https://forms.gle/wkF92vyvjfZLM6qt8)
Katharine on twitter (https://twitter.com/kjam)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Probably Private, a newsletter for privacy and data science enthusiasts (https://probablyprivate.com/)
Probably Private on YouTube (https://www.youtube.com/@ProbablyPrivate) 
</description>
      <itunes:keywords>data science, machine learning, privacy, security, artificial intelligence</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Katharine Jarmul about privacy and security in data science and machine learning. Katharine is a Principal Data Scientist at Thoughtworks Germany focusing on privacy, ethics, and security for data science workflows. Previously, she has held numerous roles at large companies and startups in the US and Germany, implementing data processing and machine learning systems with a focus on reliability, testability, privacy, and security.</p>

<p>In this episode, Hugo and Katharine talk about</p>

<ul>
<li>What data privacy and security are, what they aren’t and the differences between them (hopefully dispelling common misconceptions along the way!);</li>
<li>Why you should care about them (hint: the answers will involve regulatory, ethical, risk, and organizational concerns);</li>
<li>Data governance, anonymization techniques, and privacy in data pipelines;</li>
<li>Privacy attacks!</li>
<li>The state of the art in privacy-aware machine learning and data science, including federated learning;</li>
<li>What you need to know about the current state of regulation, including GDPR and CCPA…</li>
</ul>

<p>And much more, all the while grounding our conversation in real-world examples from data science, machine learning, business, and life!</p>

<p>You can also sign up for our next livestreamed podcast recording <a href="https://lu.ma/4b5xalpz" rel="nofollow">here</a>! </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://forms.gle/wkF92vyvjfZLM6qt8" rel="nofollow">Win a copy of Practical Data Privacy, Katharine&#39;s new book!</a></li>
<li><a href="https://twitter.com/kjam" rel="nofollow">Katharine on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://probablyprivate.com/" rel="nofollow">Probably Private, a newsletter for privacy and data science enthusiasts</a></li>
<li><a href="https://www.youtube.com/@ProbablyPrivate" rel="nofollow">Probably Private on YouTube</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Katharine Jarmul about privacy and security in data science and machine learning. Katharine is a Principal Data Scientist at Thoughtworks Germany focusing on privacy, ethics, and security for data science workflows. Previously, she has held numerous roles at large companies and startups in the US and Germany, implementing data processing and machine learning systems with a focus on reliability, testability, privacy, and security.</p>

<p>In this episode, Hugo and Katharine talk about</p>

<ul>
<li>What data privacy and security are, what they aren’t and the differences between them (hopefully dispelling common misconceptions along the way!);</li>
<li>Why you should care about them (hint: the answers will involve regulatory, ethical, risk, and organizational concerns);</li>
<li>Data governance, anonymization techniques, and privacy in data pipelines;</li>
<li>Privacy attacks!</li>
<li>The state of the art in privacy-aware machine learning and data science, including federated learning;</li>
<li>What you need to know about the current state of regulation, including GDPR and CCPA…</li>
</ul>

<p>And much more, all the while grounding our conversation in real-world examples from data science, machine learning, business, and life!</p>

<p>You can also sign up for our next livestreamed podcast recording <a href="https://lu.ma/4b5xalpz" rel="nofollow">here</a>! </p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://forms.gle/wkF92vyvjfZLM6qt8" rel="nofollow">Win a copy of Practical Data Privacy, Katharine&#39;s new book!</a></li>
<li><a href="https://twitter.com/kjam" rel="nofollow">Katharine on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://probablyprivate.com/" rel="nofollow">Probably Private, a newsletter for privacy and data science enthusiasts</a></li>
<li><a href="https://www.youtube.com/@ProbablyPrivate" rel="nofollow">Probably Private on YouTube</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+k3gphvZu</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+k3gphvZu" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 18: Research Data Science in Biotech</title>
      <link>https://vanishinggradients.fireside.fm/18</link>
      <guid isPermaLink="false">83afeb64-21ec-4828-bf96-75a08c710391</guid>
      <pubDate>Thu, 25 May 2023 08:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/83afeb64-21ec-4828-bf96-75a08c710391.mp3" length="69807439" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Machine learning, deep learning, Bayesian inference for drug discovery, OSS, and accelerating discovery science to the speed of thought!</itunes:subtitle>
      <itunes:duration>1:12:42</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Eric Ma about Research Data Science in Biotech. Eric leads the Research team in the Data Science and Artificial Intelligence group at Moderna Therapeutics. Prior to that, he was part of a special ops data science team at the Novartis Institutes for Biomedical Research's Informatics department.
In this episode, Hugo and Eric talk about
  What tools and techniques they use for drug discovery (such as mRNA vaccines and medicines);
  The importance of machine learning, deep learning, and Bayesian inference;
  How to think more generally about such high-dimensional, multi-objective optimization problems;
  The importance of open-source software and Python;
  Institutional and cultural questions, including hiring and the trade-offs between being an individual contributor and a manager;
  How they’re approaching accelerating discovery science to the speed of thought using computation, data science, statistics, and ML.
And as always, much, much more!
LINKS
Eric's website (https://ericmjl.github.io/)
Eric on twitter (https://twitter.com/ericmjl)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Cell Biology by the Numbers by Ron Milo and Rob Phillips (http://book.bionumbers.org/)
Eric's JAX tutorials at PyCon (https://youtu.be/ztthQJQFe20) and SciPy (https://youtu.be/DmR36wtel4Y)
Eric's blog post on Hiring data scientists at Moderna! (https://ericmjl.github.io/blog/2021/8/26/hiring-data-scientists-at-moderna-2021/) 
</description>
      <itunes:keywords>machine learning, AI, data science, open source, python, biotech</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Eric Ma about Research Data Science in Biotech. Eric leads the Research team in the Data Science and Artificial Intelligence group at Moderna Therapeutics. Prior to that, he was part of a special ops data science team at the Novartis Institutes for Biomedical Research&#39;s Informatics department.</p>

<p>In this episode, Hugo and Eric talk about</p>

<ul>
<li>  What tools and techniques they use for drug discovery (such as mRNA vaccines and medicines);</li>
<li>  The importance of machine learning, deep learning, and Bayesian inference;</li>
<li>  How to think more generally about such high-dimensional, multi-objective optimization problems;</li>
<li>  The importance of open-source software and Python;</li>
<li>  Institutional and cultural questions, including hiring and the trade-offs between being an individual contributor and a manager;</li>
<li>  How they’re approaching accelerating discovery science to the speed of thought using computation, data science, statistics, and ML.</li>
</ul>

<p>And as always, much, much more!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://ericmjl.github.io/" rel="nofollow">Eric&#39;s website</a></li>
<li><a href="https://twitter.com/ericmjl" rel="nofollow">Eric on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="http://book.bionumbers.org/" rel="nofollow">Cell Biology by the Numbers by Ron Milo and Rob Phillips</a></li>
<li>Eric&#39;s JAX tutorials at <a href="https://youtu.be/ztthQJQFe20" rel="nofollow">PyCon</a> and <a href="https://youtu.be/DmR36wtel4Y" rel="nofollow">SciPy</a></li>
<li>Eric&#39;s blog post on <a href="https://ericmjl.github.io/blog/2021/8/26/hiring-data-scientists-at-moderna-2021/" rel="nofollow">Hiring data scientists at Moderna!</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Eric Ma about Research Data Science in Biotech. Eric leads the Research team in the Data Science and Artificial Intelligence group at Moderna Therapeutics. Prior to that, he was part of a special ops data science team at the Novartis Institutes for Biomedical Research&#39;s Informatics department.</p>

<p>In this episode, Hugo and Eric talk about</p>

<ul>
<li>  What tools and techniques they use for drug discovery (such as mRNA vaccines and medicines);</li>
<li>  The importance of machine learning, deep learning, and Bayesian inference;</li>
<li>  How to think more generally about such high-dimensional, multi-objective optimization problems;</li>
<li>  The importance of open-source software and Python;</li>
<li>  Institutional and cultural questions, including hiring and the trade-offs between being an individual contributor and a manager;</li>
<li>  How they’re approaching accelerating discovery science to the speed of thought using computation, data science, statistics, and ML.</li>
</ul>

<p>And as always, much, much more!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://ericmjl.github.io/" rel="nofollow">Eric&#39;s website</a></li>
<li><a href="https://twitter.com/ericmjl" rel="nofollow">Eric on twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="http://book.bionumbers.org/" rel="nofollow">Cell Biology by the Numbers by Ron Milo and Rob Phillips</a></li>
<li>Eric&#39;s JAX tutorials at <a href="https://youtu.be/ztthQJQFe20" rel="nofollow">PyCon</a> and <a href="https://youtu.be/DmR36wtel4Y" rel="nofollow">SciPy</a></li>
<li>Eric&#39;s blog post on <a href="https://ericmjl.github.io/blog/2021/8/26/hiring-data-scientists-at-moderna-2021/" rel="nofollow">Hiring data scientists at Moderna!</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+PQV8mbNn</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+PQV8mbNn" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 17: End-to-End Data Science</title>
      <link>https://vanishinggradients.fireside.fm/17</link>
      <guid isPermaLink="false">289285e2-f5aa-4900-a051-7b364f9d0bb6</guid>
      <pubDate>Fri, 17 Feb 2023 17:30:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/289285e2-f5aa-4900-a051-7b364f9d0bb6.mp3" length="73030076" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>It’s time to get real about how data science and machine learning actually deliver value! Hugo speaks with Tanya Cashorali, a data scientist and consultant that helps businesses get the most out of data, about what end-to-end data science looks like across many industries, such as retail, defense, biotech, and sports.</itunes:subtitle>
      <itunes:duration>1:16:04</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with Tanya Cashorali, a data scientist and consultant that helps businesses get the most out of data, about what end-to-end data science looks like across many industries, such as retail, defense, biotech, and sports, including
scoping out projects,
figuring out the correct questions to ask,
how projects can change,
delivering on the promise,
the importance of rapid prototyping,
what it means to put models in production, and
how to measure success.
And much more, all the while grounding their conversation in real-world examples from data science, business, and life.
In a world where most organizations think they need AI and yet 10-15% of data science actually involves model building, it’s time to get real about how data science and machine learning actually deliver value!
LINKS
Tanya on Twitter (https://twitter.com/tanyacash21)
Vanishing Gradients on YouTube (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
Saving millions with a Shiny app | Data Science Hangout with Tanya Cashorali (https://youtu.be/qdAroyFRFCg)
Our next livestream: Research Data Science in Biotech with Eric Ma (https://www.eventbrite.com/e/research-data-science-in-biotech-tickets-550400882857?aff=fs)
</description>
      <itunes:keywords>machine learning, AI, data science, open source</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Tanya Cashorali, a data scientist and consultant that helps businesses get the most out of data, about what end-to-end data science looks like across many industries, such as retail, defense, biotech, and sports, including</p>

<ul>
<li>scoping out projects,</li>
<li>figuring out the correct questions to ask,</li>
<li>how projects can change,</li>
<li>delivering on the promise,</li>
<li>the importance of rapid prototyping,</li>
<li>what it means to put models in production, and</li>
<li>how to measure success.</li>
</ul>

<p>And much more, all the while grounding their conversation in real-world examples from data science, business, and life.</p>

<p>In a world where most organizations think they need AI and yet 10-15% of data science actually involves model building, it’s time to get real about how data science and machine learning actually deliver value!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://twitter.com/tanyacash21" rel="nofollow">Tanya on Twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://youtu.be/qdAroyFRFCg" rel="nofollow">Saving millions with a Shiny app | Data Science Hangout with Tanya Cashorali</a></li>
<li><a href="https://www.eventbrite.com/e/research-data-science-in-biotech-tickets-550400882857?aff=fs" rel="nofollow">Our next livestream: Research Data Science in Biotech with Eric Ma</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Tanya Cashorali, a data scientist and consultant that helps businesses get the most out of data, about what end-to-end data science looks like across many industries, such as retail, defense, biotech, and sports, including</p>

<ul>
<li>scoping out projects,</li>
<li>figuring out the correct questions to ask,</li>
<li>how projects can change,</li>
<li>delivering on the promise,</li>
<li>the importance of rapid prototyping,</li>
<li>what it means to put models in production, and</li>
<li>how to measure success.</li>
</ul>

<p>And much more, all the while grounding their conversation in real-world examples from data science, business, and life.</p>

<p>In a world where most organizations think they need AI and yet 10-15% of data science actually involves model building, it’s time to get real about how data science and machine learning actually deliver value!</p>

<p><strong>LINKS</strong></p>

<ul>
<li><a href="https://twitter.com/tanyacash21" rel="nofollow">Tanya on Twitter</a></li>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients on YouTube</a></li>
<li><a href="https://youtu.be/qdAroyFRFCg" rel="nofollow">Saving millions with a Shiny app | Data Science Hangout with Tanya Cashorali</a></li>
<li><a href="https://www.eventbrite.com/e/research-data-science-in-biotech-tickets-550400882857?aff=fs" rel="nofollow">Our next livestream: Research Data Science in Biotech with Eric Ma</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+uaQZcym9</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+uaQZcym9" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 16: Data Science and Decision Making Under Uncertainty</title>
      <link>https://vanishinggradients.fireside.fm/16</link>
      <guid isPermaLink="false">9eb29a37-c694-45a8-bae5-38e5b3fd5849</guid>
      <pubDate>Thu, 15 Dec 2022 08:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/9eb29a37-c694-45a8-bae5-38e5b3fd5849.mp3" length="59947028" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about data science, ML, and the nitty gritty of decision making under uncertainty, including how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. </itunes:subtitle>
      <itunes:duration>1:23:15</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. 
This is part 2 of a two part conversation in which we delve into decision making under uncertainty. Feel free to check out part 1 here (https://vanishinggradients.fireside.fm/15) but this episode should also stand alone.
Why am I speaking to JD about all of this? Because not only is he a wild conversationalist with a real knack for explaining hard to grok concepts with illustrative examples and useful stories, but he has worked for many years in re-insurance, that’s right, not insurance but re-insurance – these are the people who insure the insurers so if anyone can actually tell us about risk and uncertainty in decision making, it’s him!
In part 1,  we discussed risk, uncertainty, probabilistic thinking, and simulation, all with a view towards improving decision making.
In this, part 2, we discuss the ins and outs of decision making under uncertainty, including
How data science can be more tightly coupled with the decision function in organisations;
Some common mistakes and failure modes of making decisions under uncertainty;
Heuristics for principled decision-making in data science;
The intersection of model building, storytelling, and cognitive biases to keep in mind;
As JD says, and I paraphrase, “You may think you train your models, but your models are really training you.”
Links
Vanishing Gradients' new YouTube channel! (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
JD on twitter (https://twitter.com/CMastication)
Executive Data Science, episode 5 of Vanishing Gradients, in which Jim Savage and Hugo talk through decision making and why you should always be integrating your loss function over your posterior (https://vanishinggradients.fireside.fm/5)
Fooled by Randomness by Nassim Taleb (https://en.wikipedia.org/wiki/Fooled_by_Randomness)
Superforecasting: The Art and Science of Prediction Philip E. Tetlock and Dan Gardner (https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction)
Thinking in Bets by Annie Duke (https://www.penguin.com.au/books/thinking-in-bets-9780735216372)
The Signal and the Noise: Why So Many Predictions Fail by Nate Silver (https://en.wikipedia.org/wiki/The_Signal_and_the_Noise)
Thinking, Fast and Slow by Daniel Kahneman (https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. </p>

<p>This is part 2 of a two part conversation in which we delve into decision making under uncertainty. Feel free to check out part 1 <a href="https://vanishinggradients.fireside.fm/15" rel="nofollow">here</a> but this episode should also stand alone.</p>

<p>Why am I speaking to JD about all of this? Because not only is he a wild conversationalist with a real knack for explaining hard to grok concepts with illustrative examples and useful stories, but he has worked for many years in re-insurance, that’s right, not insurance but re-insurance – these are the people who insure the insurers so if anyone can actually tell us about risk and uncertainty in decision making, it’s him!</p>

<p>In part 1,  we discussed risk, uncertainty, probabilistic thinking, and simulation, all with a view towards improving decision making.</p>

<p>In this, part 2, we discuss the ins and outs of decision making under uncertainty, including</p>

<ul>
<li>How data science can be more tightly coupled with the decision function in organisations;</li>
<li>Some common mistakes and failure modes of making decisions under uncertainty;</li>
<li>Heuristics for principled decision-making in data science;</li>
<li>The intersection of model building, storytelling, and cognitive biases to keep in mind;</li>
</ul>

<p>As JD says, and I paraphrase, “You may think you train your models, but your models are really training you.”</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients&#39; new YouTube channel!</a></li>
<li><a href="https://twitter.com/CMastication" rel="nofollow">JD on twitter</a></li>
<li><a href="https://vanishinggradients.fireside.fm/5" rel="nofollow">Executive Data Science, episode 5 of Vanishing Gradients, in which Jim Savage and Hugo talk through decision making and why you should always be integrating your loss function over your posterior</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fooled_by_Randomness" rel="nofollow">Fooled by Randomness by Nassim Taleb</a></li>
<li><a href="https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction" rel="nofollow">Superforecasting: The Art and Science of Prediction Philip E. Tetlock and Dan Gardner</a></li>
<li><a href="https://www.penguin.com.au/books/thinking-in-bets-9780735216372" rel="nofollow">Thinking in Bets by Annie Duke</a></li>
<li><a href="https://en.wikipedia.org/wiki/The_Signal_and_the_Noise" rel="nofollow">The Signal and the Noise: Why So Many Predictions Fail by Nate Silver</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" rel="nofollow">Thinking, Fast and Slow by Daniel Kahneman</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. </p>

<p>This is part 2 of a two part conversation in which we delve into decision making under uncertainty. Feel free to check out part 1 <a href="https://vanishinggradients.fireside.fm/15" rel="nofollow">here</a> but this episode should also stand alone.</p>

<p>Why am I speaking to JD about all of this? Because not only is he a wild conversationalist with a real knack for explaining hard to grok concepts with illustrative examples and useful stories, but he has worked for many years in re-insurance, that’s right, not insurance but re-insurance – these are the people who insure the insurers so if anyone can actually tell us about risk and uncertainty in decision making, it’s him!</p>

<p>In part 1,  we discussed risk, uncertainty, probabilistic thinking, and simulation, all with a view towards improving decision making.</p>

<p>In this, part 2, we discuss the ins and outs of decision making under uncertainty, including</p>

<ul>
<li>How data science can be more tightly coupled with the decision function in organisations;</li>
<li>Some common mistakes and failure modes of making decisions under uncertainty;</li>
<li>Heuristics for principled decision-making in data science;</li>
<li>The intersection of model building, storytelling, and cognitive biases to keep in mind;</li>
</ul>

<p>As JD says, and I paraphrase, “You may think you train your models, but your models are really training you.”</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients&#39; new YouTube channel!</a></li>
<li><a href="https://twitter.com/CMastication" rel="nofollow">JD on twitter</a></li>
<li><a href="https://vanishinggradients.fireside.fm/5" rel="nofollow">Executive Data Science, episode 5 of Vanishing Gradients, in which Jim Savage and Hugo talk through decision making and why you should always be integrating your loss function over your posterior</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fooled_by_Randomness" rel="nofollow">Fooled by Randomness by Nassim Taleb</a></li>
<li><a href="https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction" rel="nofollow">Superforecasting: The Art and Science of Prediction Philip E. Tetlock and Dan Gardner</a></li>
<li><a href="https://www.penguin.com.au/books/thinking-in-bets-9780735216372" rel="nofollow">Thinking in Bets by Annie Duke</a></li>
<li><a href="https://en.wikipedia.org/wiki/The_Signal_and_the_Noise" rel="nofollow">The Signal and the Noise: Why So Many Predictions Fail by Nate Silver</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" rel="nofollow">Thinking, Fast and Slow by Daniel Kahneman</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+YbiGZgDE</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+YbiGZgDE" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 15: Uncertainty, Risk, and Simulation in Data Science</title>
      <link>https://vanishinggradients.fireside.fm/15</link>
      <guid isPermaLink="false">c2e27880-6d10-4b0b-afd7-e349d219662a</guid>
      <pubDate>Thu, 08 Dec 2022 05:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/c2e27880-6d10-4b0b-afd7-e349d219662a.mp3" length="38526097" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world.</itunes:subtitle>
      <itunes:duration>53:30</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. 
This is part 1 of a two part conversation. In this, part 1, we discuss risk, uncertainty, probabilistic thinking, and simulation, all with a view towards improving decision making and we draw on examples from our personal lives, the pandemic, our jobs, the reinsurance space, and the corporate world.  In part 2, we’ll get into the nitty gritty of decision making under uncertainty.
As JD says, and I paraphrase, “You may think you train your models, but your models are really training you.”
Links
Vanishing Gradients' new YouTube channel! (https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA)
JD on twitter (https://twitter.com/CMastication)
Executive Data Science, episode 5 of Vanishing Gradients, in which Jim Savage and Hugo talk through decision making and why you should always be integrating your loss function over your posterior (https://vanishinggradients.fireside.fm/5)
Fooled by Randomness by Nassim Taleb (https://en.wikipedia.org/wiki/Fooled_by_Randomness)
Superforecasting: The Art and Science of Prediction Philip E. Tetlock and Dan Gardner (https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction)
Thinking in Bets by Annie Duke (https://www.penguin.com.au/books/thinking-in-bets-9780735216372)
The Signal and the Noise: Why So Many Predictions Fail by Nate Silver (https://en.wikipedia.org/wiki/The_Signal_and_the_Noise)
Thinking, Fast and Slow by Daniel Kahneman (https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)
 
</description>
      <itunes:keywords>oss, data science, machine learning, python, simulation, decision science</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. </p>

<p>This is part 1 of a two part conversation. In this, part 1, we discuss risk, uncertainty, probabilistic thinking, and simulation, all with a view towards improving decision making and we draw on examples from our personal lives, the pandemic, our jobs, the reinsurance space, and the corporate world.  In part 2, we’ll get into the nitty gritty of decision making under uncertainty.</p>

<p>As JD says, and I paraphrase, “You may think you train your models, but your models are really training you.”</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients&#39; new YouTube channel!</a></li>
<li><a href="https://twitter.com/CMastication" rel="nofollow">JD on twitter</a></li>
<li><a href="https://vanishinggradients.fireside.fm/5" rel="nofollow">Executive Data Science, episode 5 of Vanishing Gradients, in which Jim Savage and Hugo talk through decision making and why you should always be integrating your loss function over your posterior</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fooled_by_Randomness" rel="nofollow">Fooled by Randomness by Nassim Taleb</a></li>
<li><a href="https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction" rel="nofollow">Superforecasting: The Art and Science of Prediction Philip E. Tetlock and Dan Gardner</a></li>
<li><a href="https://www.penguin.com.au/books/thinking-in-bets-9780735216372" rel="nofollow">Thinking in Bets by Annie Duke</a></li>
<li><a href="https://en.wikipedia.org/wiki/The_Signal_and_the_Noise" rel="nofollow">The Signal and the Noise: Why So Many Predictions Fail by Nate Silver</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" rel="nofollow">Thinking, Fast and Slow by Daniel Kahneman</a></li>
<li></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with JD Long, agricultural economist, quant, and stochastic modeler, about decision making under uncertainty and how we can use our knowledge of risk, uncertainty, probabilistic thinking, causal inference, and more to help us use data science and machine learning to make better decisions in an uncertain world. </p>

<p>This is part 1 of a two part conversation. In this, part 1, we discuss risk, uncertainty, probabilistic thinking, and simulation, all with a view towards improving decision making and we draw on examples from our personal lives, the pandemic, our jobs, the reinsurance space, and the corporate world.  In part 2, we’ll get into the nitty gritty of decision making under uncertainty.</p>

<p>As JD says, and I paraphrase, “You may think you train your models, but your models are really training you.”</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.youtube.com/channel/UC_NafIo-Ku2loOLrzm45ABA" rel="nofollow">Vanishing Gradients&#39; new YouTube channel!</a></li>
<li><a href="https://twitter.com/CMastication" rel="nofollow">JD on twitter</a></li>
<li><a href="https://vanishinggradients.fireside.fm/5" rel="nofollow">Executive Data Science, episode 5 of Vanishing Gradients, in which Jim Savage and Hugo talk through decision making and why you should always be integrating your loss function over your posterior</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fooled_by_Randomness" rel="nofollow">Fooled by Randomness by Nassim Taleb</a></li>
<li><a href="https://en.wikipedia.org/wiki/Superforecasting:_The_Art_and_Science_of_Prediction" rel="nofollow">Superforecasting: The Art and Science of Prediction Philip E. Tetlock and Dan Gardner</a></li>
<li><a href="https://www.penguin.com.au/books/thinking-in-bets-9780735216372" rel="nofollow">Thinking in Bets by Annie Duke</a></li>
<li><a href="https://en.wikipedia.org/wiki/The_Signal_and_the_Noise" rel="nofollow">The Signal and the Noise: Why So Many Predictions Fail by Nate Silver</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" rel="nofollow">Thinking, Fast and Slow by Daniel Kahneman</a></li>
<li></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+LZSvcs5v</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+LZSvcs5v" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 14: Decision Science, MLOps, and Machine Learning Everywhere</title>
      <link>https://vanishinggradients.fireside.fm/14</link>
      <guid isPermaLink="false">c02c6e9f-2a38-4f03-a8f5-4b19ed8966c3</guid>
      <pubDate>Mon, 21 Nov 2022 10:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/c02c6e9f-2a38-4f03-a8f5-4b19ed8966c3.mp3" length="66269255" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo reads 3 audio essays about decision science, MLOps, and what happens when machine learning models are everywhere</itunes:subtitle>
      <itunes:duration>1:09:01</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo Bowne-Anderson, host of Vanishing Gradients, reads 3 audio essays about decision science, MLOps, and what happens when machine learning models are everywhere.
Links
Our upcoming Vanishing Gradients live recording of Data Science and Decision Making Under Uncertainty with Hugo and JD Long! (https://www.eventbrite.com/e/data-science-and-decision-making-under-uncertainty-tickets-467379864757?aff=vg)
Decision-Making in a Time of Crisis (https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/) by Hugo Bowne-Anderson
MLOps and DevOps: Why Data Makes It Different (https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/) by Ville Tuulos and Hugo Bowne-Anderson
The above essay syndicated on VentureBeat (https://venturebeat.com/business/mlops-vs-devops-why-data-makes-it-different/)
When models are everywhere (https://www.oreilly.com/radar/when-models-are-everywhere/) by Hugo Bowne-Anderson and Mike Loukides 
</description>
      <content:encoded>
        <![CDATA[<p>Hugo Bowne-Anderson, host of Vanishing Gradients, reads 3 audio essays about decision science, MLOps, and what happens when machine learning models are everywhere.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.eventbrite.com/e/data-science-and-decision-making-under-uncertainty-tickets-467379864757?aff=vg" rel="nofollow">Our upcoming Vanishing Gradients live recording of Data Science and Decision Making Under Uncertainty with Hugo and JD Long!</a></li>
<li><a href="https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/" rel="nofollow">Decision-Making in a Time of Crisis</a> by Hugo Bowne-Anderson</li>
<li><a href="https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/" rel="nofollow">MLOps and DevOps: Why Data Makes It Different</a> by Ville Tuulos and Hugo Bowne-Anderson</li>
<li><a href="https://venturebeat.com/business/mlops-vs-devops-why-data-makes-it-different/" rel="nofollow">The above essay syndicated on VentureBeat</a></li>
<li><a href="https://www.oreilly.com/radar/when-models-are-everywhere/" rel="nofollow">When models are everywhere</a> by Hugo Bowne-Anderson and Mike Loukides</li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo Bowne-Anderson, host of Vanishing Gradients, reads 3 audio essays about decision science, MLOps, and what happens when machine learning models are everywhere.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.eventbrite.com/e/data-science-and-decision-making-under-uncertainty-tickets-467379864757?aff=vg" rel="nofollow">Our upcoming Vanishing Gradients live recording of Data Science and Decision Making Under Uncertainty with Hugo and JD Long!</a></li>
<li><a href="https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/" rel="nofollow">Decision-Making in a Time of Crisis</a> by Hugo Bowne-Anderson</li>
<li><a href="https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/" rel="nofollow">MLOps and DevOps: Why Data Makes It Different</a> by Ville Tuulos and Hugo Bowne-Anderson</li>
<li><a href="https://venturebeat.com/business/mlops-vs-devops-why-data-makes-it-different/" rel="nofollow">The above essay syndicated on VentureBeat</a></li>
<li><a href="https://www.oreilly.com/radar/when-models-are-everywhere/" rel="nofollow">When models are everywhere</a> by Hugo Bowne-Anderson and Mike Loukides</li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+DonZH9yF</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+DonZH9yF" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 13: The Data Science Skills Gap, Economics, and Public Health</title>
      <link>https://vanishinggradients.fireside.fm/13</link>
      <guid isPermaLink="false">0d9dafd4-c27b-4e49-9431-58c70de4d82d</guid>
      <pubDate>Wed, 12 Oct 2022 09:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/0d9dafd4-c27b-4e49-9431-58c70de4d82d.mp3" length="59542966" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Norma Padron, CEO of EmpiricaLab, about data science education and continuous learning for people working in healthcare, broadly construed, along with how we can think about the democratization of data science skills more generally.</itunes:subtitle>
      <itunes:duration>1:22:41</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speak with Norma Padron about data science education and continuous learning for people working in healthcare, broadly construed, along with how we can think about the democratization of data science skills more generally.
Norma is CEO of EmpiricaLab, where her team‘s mission is to bridge work and training and empower healthcare teams to focus on what they care about the most: patient care. In a word, EmpiricaLab is a platform focused on peer learning and last-mile training for healthcare teams.
As you’ll discover, Norma’s background is fascinating: with a Ph.D. in health policy and management from Yale University, a master's degree in economics from Duke University (among other things), and then working with multiple early stage digital health companies to accelerate their growth and scale, this is a wide ranging conversation about how and where learning actually occurs, particularly with respect to data science; we talk about how the worlds of economics and econometrics, including causal inference, can be used to make data science and more robust and less fragile field, and why these disciplines are essential to both public and health policy. It was really invigorating to talk about the data skills gaps that exists in organizations and how Norma’s team at Empiricalab is thinking about solving it in the health space using a 3 tiered solution of content creation, a social layer, and an information discovery platform. 
All of this in service of a key question we’re facing in this field: how do you get the right data skills, tools, and workflows, in the hands of the people who need them, when the space is evolving so quickly?
Links
Norma's website (https://www.normapadron.com/)
EmpiricaLab (https://www.empiricalab.com/)
Norma on twitter (https://twitter.com/NormaPadron__)
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speak with Norma Padron about data science education and continuous learning for people working in healthcare, broadly construed, along with how we can think about the democratization of data science skills more generally.</p>

<p>Norma is CEO of EmpiricaLab, where her team‘s mission is to bridge work and training and empower healthcare teams to focus on what they care about the most: patient care. In a word, EmpiricaLab is a platform focused on peer learning and last-mile training for healthcare teams.</p>

<p>As you’ll discover, Norma’s background is fascinating: with a Ph.D. in health policy and management from Yale University, a master&#39;s degree in economics from Duke University (among other things), and then working with multiple early stage digital health companies to accelerate their growth and scale, this is a wide ranging conversation about how and where learning actually occurs, particularly with respect to data science; we talk about how the worlds of economics and econometrics, including causal inference, can be used to make data science and more robust and less fragile field, and why these disciplines are essential to both public and health policy. It was really invigorating to talk about the data skills gaps that exists in organizations and how Norma’s team at Empiricalab is thinking about solving it in the health space using a 3 tiered solution of content creation, a social layer, and an information discovery platform. </p>

<p>All of this in service of a key question we’re facing in this field: how do you get the right data skills, tools, and workflows, in the hands of the people who need them, when the space is evolving so quickly?</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.normapadron.com/" rel="nofollow">Norma&#39;s website</a></li>
<li><a href="https://www.empiricalab.com/" rel="nofollow">EmpiricaLab</a></li>
<li><a href="https://twitter.com/NormaPadron__" rel="nofollow">Norma on twitter</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speak with Norma Padron about data science education and continuous learning for people working in healthcare, broadly construed, along with how we can think about the democratization of data science skills more generally.</p>

<p>Norma is CEO of EmpiricaLab, where her team‘s mission is to bridge work and training and empower healthcare teams to focus on what they care about the most: patient care. In a word, EmpiricaLab is a platform focused on peer learning and last-mile training for healthcare teams.</p>

<p>As you’ll discover, Norma’s background is fascinating: with a Ph.D. in health policy and management from Yale University, a master&#39;s degree in economics from Duke University (among other things), and then working with multiple early stage digital health companies to accelerate their growth and scale, this is a wide ranging conversation about how and where learning actually occurs, particularly with respect to data science; we talk about how the worlds of economics and econometrics, including causal inference, can be used to make data science and more robust and less fragile field, and why these disciplines are essential to both public and health policy. It was really invigorating to talk about the data skills gaps that exists in organizations and how Norma’s team at Empiricalab is thinking about solving it in the health space using a 3 tiered solution of content creation, a social layer, and an information discovery platform. </p>

<p>All of this in service of a key question we’re facing in this field: how do you get the right data skills, tools, and workflows, in the hands of the people who need them, when the space is evolving so quickly?</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.normapadron.com/" rel="nofollow">Norma&#39;s website</a></li>
<li><a href="https://www.empiricalab.com/" rel="nofollow">EmpiricaLab</a></li>
<li><a href="https://twitter.com/NormaPadron__" rel="nofollow">Norma on twitter</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+HbkK4BqW</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+HbkK4BqW" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 12: Data Science for Social Media: Twitter and Reddit</title>
      <link>https://vanishinggradients.fireside.fm/12</link>
      <guid isPermaLink="false">edfe9061-d42f-4c7d-b0af-e769252ae94e</guid>
      <pubDate>Fri, 30 Sep 2022 10:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/edfe9061-d42f-4c7d-b0af-e769252ae94e.mp3" length="89041208" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Katie Bauer about her time working in data science at both Twitter and Reddit. At the time of recording, Katie was a data science manager at Twitter and prior to that, a founding member of the data team at Reddit. </itunes:subtitle>
      <itunes:duration>1:32:45</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <description>Hugo speakswith Katie Bauer (https://twitter.com/imightbemary) about her time working in data science at both Twitter and Reddit. At the time of recording, Katie was a data science manager at Twitter and prior to that, a founding member of the data team at Reddit. She’s now Head of Data Science at Gloss Genius so congrats on the new job, Katie!
In this conversation, we dive into what type of challenges social media companies face that data science is equipped to solve: in doing so, we traverse 
the difference and similarities in companies such as Twitter and Reddit, 
the major differences in being an early member of a data team and joining an established data function at a larger organization, 
the supreme importance of robust measurement and telemetry in data science, along with 
the mixed incentives for career data scientists, such as building flashy new things instead of maintaining existing infrastructure.
I’ve always found conversations with Katie to be a treasure trove of insights into data science and machine learning practice, along with key learnings about data science management. 
In a word, Katie helps me to understand our space better. In this conversation, she told me that one important function data science can serve in any organization is creating a shared context for lots of different people in the org. We dive deep into what this actually means, how it can play out, traversing the world of dashboards, metric stores, feature stores, machine learning products, the need for top-down support, and much, much more.
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speakswith <a href="https://twitter.com/imightbemary" rel="nofollow">Katie Bauer</a> about her time working in data science at both Twitter and Reddit. At the time of recording, Katie was a data science manager at Twitter and prior to that, a founding member of the data team at Reddit. She’s now Head of Data Science at Gloss Genius so congrats on the new job, Katie!</p>

<p>In this conversation, we dive into what type of challenges social media companies face that data science is equipped to solve: in doing so, we traverse </p>

<ul>
<li>the difference and similarities in companies such as Twitter and Reddit, </li>
<li>the major differences in being an early member of a data team and joining an established data function at a larger organization, </li>
<li>the supreme importance of robust measurement and telemetry in data science, along with </li>
<li>the mixed incentives for career data scientists, such as building flashy new things instead of maintaining existing infrastructure.</li>
</ul>

<p>I’ve always found conversations with Katie to be a treasure trove of insights into data science and machine learning practice, along with key learnings about data science management. </p>

<p>In a word, Katie helps me to understand our space better. In this conversation, she told me that one important function data science can serve in any organization is creating a shared context for lots of different people in the org. We dive deep into what this actually means, how it can play out, traversing the world of dashboards, metric stores, feature stores, machine learning products, the need for top-down support, and much, much more.</p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speakswith <a href="https://twitter.com/imightbemary" rel="nofollow">Katie Bauer</a> about her time working in data science at both Twitter and Reddit. At the time of recording, Katie was a data science manager at Twitter and prior to that, a founding member of the data team at Reddit. She’s now Head of Data Science at Gloss Genius so congrats on the new job, Katie!</p>

<p>In this conversation, we dive into what type of challenges social media companies face that data science is equipped to solve: in doing so, we traverse </p>

<ul>
<li>the difference and similarities in companies such as Twitter and Reddit, </li>
<li>the major differences in being an early member of a data team and joining an established data function at a larger organization, </li>
<li>the supreme importance of robust measurement and telemetry in data science, along with </li>
<li>the mixed incentives for career data scientists, such as building flashy new things instead of maintaining existing infrastructure.</li>
</ul>

<p>I’ve always found conversations with Katie to be a treasure trove of insights into data science and machine learning practice, along with key learnings about data science management. </p>

<p>In a word, Katie helps me to understand our space better. In this conversation, she told me that one important function data science can serve in any organization is creating a shared context for lots of different people in the org. We dive deep into what this actually means, how it can play out, traversing the world of dashboards, metric stores, feature stores, machine learning products, the need for top-down support, and much, much more.</p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+Sy7GP97x</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+Sy7GP97x" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 11: Data Science: The Great Stagnation</title>
      <link>https://vanishinggradients.fireside.fm/11</link>
      <guid isPermaLink="false">697e817a-b886-4057-9dc1-4c9868c0b064</guid>
      <pubDate>Fri, 16 Sep 2022 12:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/697e817a-b886-4057-9dc1-4c9868c0b064.mp3" length="101417351" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Mark Saroufim, an Applied AI Engineer at Meta who works on PyTorch where his team’s main focus is making it as easy as possible for people to deploy PyTorch in production outside Meta. </itunes:subtitle>
      <itunes:duration>1:45:38</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/6/697e817a-b886-4057-9dc1-4c9868c0b064/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Mark Saroufim, an Applied AI Engineer at Meta who works on PyTorch where his team’s main focus is making it as easy as possible for people to deploy PyTorch in production outside Meta. 
Mark first came on our radar with an essay he wrote called Machine Learning: the Great Stagnation (https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation), which was concerned with the stagnation in machine learning in  academic research and in which he stated
Machine learning researchers can now engage in risk-free, high-income, high-prestige work. They are today’s Medieval Catholic priests.
This is just the tip of the icebergs of Mark’s critical and often sociological eye and one of the reasons I was excited to speak with him.
In this conversation, we talk about the importance of open source software in modern data science and machine learning and how Mark thinks about making it as easy to use as possible. We also talk about risk assessments in considering whether to adopt open source or not, the supreme importance of good documentation, and what we can learn from the world of video game development when thinking about open source.
We then dive into the rise of the machine learning cult leader persona, in the context of examples such as Hugging Face and the community they’ve built. We discuss the role of marketing in open source tooling, along with for profit data science and ML tooling, how it can impact you as an end user, and how much of data science can be considered differing forms of live action role playing and simulation.
We also talk about developer marketing and content for data professionals and how we see some of the largest names in ML researchers being those that have gigantic Twitter followers, such as Andrei Karpathy. This is part of a broader trend in society about the skills that are required to capture significant mind share these days.
If that’s not enough, we jump into how machine learning ideally allows businesses to build sustainable and defensible moats, by which we mean the ability to maintain competitive advantages over competitors to retain market share.
In between this interview and its release, PyTorch joined the Linux Foundation, which is something we’ll need to get Mark back to discuss sometime.
Links
The Myth of Objective Tech Screens (https://marksaroufim.substack.com/p/the-myth-of-objective-tech-screens)
Machine Learning: The Great Stagnation (https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation)
Fear the Boom and Bust: Keynes vs. Hayek - The Original Economics Rap Battle! (https://www.youtube.com/watch?v=d0nERTFo-Sk)
History and the Security of Property (https://archive.ph/dRXEK#selection-21.0-21.36) by Nick Szabo
Mark on YouTube (https://www.youtube.com/marksaroufim)
Mark's Substack (https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation)
Mark's Discord (https://discord.com/invite/drmuTjWZrm)
</description>
      <itunes:keywords>machine learning, AI, data science, open source</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Mark Saroufim, an Applied AI Engineer at Meta who works on PyTorch where his team’s main focus is making it as easy as possible for people to deploy PyTorch in production outside Meta. </p>

<p>Mark first came on our radar with an essay he wrote called <a href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation" rel="nofollow">Machine Learning: the Great Stagnation</a>, which was concerned with the stagnation in machine learning in  academic research and in which he stated</p>

<blockquote>
<p>Machine learning researchers can now engage in risk-free, high-income, high-prestige work. They are today’s Medieval Catholic priests.</p>
</blockquote>

<p>This is just the tip of the icebergs of Mark’s critical and often sociological eye and one of the reasons I was excited to speak with him.</p>

<p>In this conversation, we talk about the importance of open source software in modern data science and machine learning and how Mark thinks about making it as easy to use as possible. We also talk about risk assessments in considering whether to adopt open source or not, the supreme importance of good documentation, and what we can learn from the world of video game development when thinking about open source.</p>

<p>We then dive into the rise of the machine learning cult leader persona, in the context of examples such as Hugging Face and the community they’ve built. We discuss the role of marketing in open source tooling, along with for profit data science and ML tooling, how it can impact you as an end user, and how much of data science can be considered differing forms of live action role playing and simulation.</p>

<p>We also talk about developer marketing and content for data professionals and how we see some of the largest names in ML researchers being those that have gigantic Twitter followers, such as Andrei Karpathy. This is part of a broader trend in society about the skills that are required to capture significant mind share these days.</p>

<p>If that’s not enough, we jump into how machine learning ideally allows businesses to build sustainable and defensible moats, by which we mean the ability to maintain competitive advantages over competitors to retain market share.</p>

<p>In between this interview and its release, PyTorch joined the Linux Foundation, which is something we’ll need to get Mark back to discuss sometime.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://marksaroufim.substack.com/p/the-myth-of-objective-tech-screens" rel="nofollow">The Myth of Objective Tech Screens</a></li>
<li><a href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation" rel="nofollow">Machine Learning: The Great Stagnation</a></li>
<li><a href="https://www.youtube.com/watch?v=d0nERTFo-Sk" rel="nofollow">Fear the Boom and Bust: Keynes vs. Hayek - The Original Economics Rap Battle!</a></li>
<li><a href="https://archive.ph/dRXEK#selection-21.0-21.36" rel="nofollow">History and the Security of Property</a> by Nick Szabo</li>
<li><a href="https://www.youtube.com/marksaroufim" rel="nofollow">Mark on YouTube</a></li>
<li><a href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation" rel="nofollow">Mark&#39;s Substack</a></li>
<li><a href="https://discord.com/invite/drmuTjWZrm" rel="nofollow">Mark&#39;s Discord</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Mark Saroufim, an Applied AI Engineer at Meta who works on PyTorch where his team’s main focus is making it as easy as possible for people to deploy PyTorch in production outside Meta. </p>

<p>Mark first came on our radar with an essay he wrote called <a href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation" rel="nofollow">Machine Learning: the Great Stagnation</a>, which was concerned with the stagnation in machine learning in  academic research and in which he stated</p>

<blockquote>
<p>Machine learning researchers can now engage in risk-free, high-income, high-prestige work. They are today’s Medieval Catholic priests.</p>
</blockquote>

<p>This is just the tip of the icebergs of Mark’s critical and often sociological eye and one of the reasons I was excited to speak with him.</p>

<p>In this conversation, we talk about the importance of open source software in modern data science and machine learning and how Mark thinks about making it as easy to use as possible. We also talk about risk assessments in considering whether to adopt open source or not, the supreme importance of good documentation, and what we can learn from the world of video game development when thinking about open source.</p>

<p>We then dive into the rise of the machine learning cult leader persona, in the context of examples such as Hugging Face and the community they’ve built. We discuss the role of marketing in open source tooling, along with for profit data science and ML tooling, how it can impact you as an end user, and how much of data science can be considered differing forms of live action role playing and simulation.</p>

<p>We also talk about developer marketing and content for data professionals and how we see some of the largest names in ML researchers being those that have gigantic Twitter followers, such as Andrei Karpathy. This is part of a broader trend in society about the skills that are required to capture significant mind share these days.</p>

<p>If that’s not enough, we jump into how machine learning ideally allows businesses to build sustainable and defensible moats, by which we mean the ability to maintain competitive advantages over competitors to retain market share.</p>

<p>In between this interview and its release, PyTorch joined the Linux Foundation, which is something we’ll need to get Mark back to discuss sometime.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://marksaroufim.substack.com/p/the-myth-of-objective-tech-screens" rel="nofollow">The Myth of Objective Tech Screens</a></li>
<li><a href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation" rel="nofollow">Machine Learning: The Great Stagnation</a></li>
<li><a href="https://www.youtube.com/watch?v=d0nERTFo-Sk" rel="nofollow">Fear the Boom and Bust: Keynes vs. Hayek - The Original Economics Rap Battle!</a></li>
<li><a href="https://archive.ph/dRXEK#selection-21.0-21.36" rel="nofollow">History and the Security of Property</a> by Nick Szabo</li>
<li><a href="https://www.youtube.com/marksaroufim" rel="nofollow">Mark on YouTube</a></li>
<li><a href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation" rel="nofollow">Mark&#39;s Substack</a></li>
<li><a href="https://discord.com/invite/drmuTjWZrm" rel="nofollow">Mark&#39;s Discord</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+bQjYuP3N</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+bQjYuP3N" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 10: Investing in Machine Learning</title>
      <link>https://vanishinggradients.fireside.fm/10</link>
      <guid isPermaLink="false">4552d501-5bc5-43c9-9246-5dbd221ebd06</guid>
      <pubDate>Fri, 19 Aug 2022 01:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/4552d501-5bc5-43c9-9246-5dbd221ebd06.mp3" length="83101043" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Sarah Catanzaro, General Partner at Amplify Partners, about investing in data science and machine learning tooling and where we see progress happening in the space.</itunes:subtitle>
      <itunes:duration>1:26:33</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/4/4552d501-5bc5-43c9-9246-5dbd221ebd06/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Sarah Catanzaro, General Partner at Amplify Partners, about investing in data science and machine learning tooling and where we see progress happening in the space.
Sarah invests in the tools that we both wish we had earlier in our careers: tools that enable data scientists and machine learners to collect, store, manage, analyze, and model data more effectively. As you’ll discover, Sarah identifies as a scientist first and an investor second and still believes that her mission is to enable companies to become data-driven and to generate ROI through machine and statistical learning. In her words, she’s still that cuckoo kid who’s ranting and raving about how data and AI will shift every tide.
In this conversation, we talk about what scientific inquiry actually is and the elements of playfulness and seriousness it necessarily involves, and how it can be used to generate business value. We talk about Sarah’s unorthodox path from a data scientist working in defense to her time at Palantir and how that led her to build out a data team and function for a venture capital firm and then to becoming a VC in the data tooling space.
We then really dive into the data science and machine learning tooling space to figure out why it’s so fragmented: we look to the data analytics stack and software engineering communities to find historical tethers that may be useful. We discuss the moving parts that led to the establishment of a standard, a system of record, and clearly defined roles in analytics and what we can learn from that for machine learning!
We also dive into the development of tools, workflows, and division of labour as partial exercises in pattern recognition and how this can be at odds with the variance we see in the machine learning landscape, more generally!
Two take-aways are that we need best practices and we need  more standardization.
We also discussed that, with all our focus and conversations on tools, what conversation we’re missing and Sarah was adamant that we need to be focusing on questions, not solutions, and even questioning what ML is useful for and what it isn’t, diving into a bunch of thoughtful and nuanced examples.
I’m also grateful that Sarah let me take her down a slightly dangerous and self-critical path where we riffed on both our roles in potentially contributing to the tragedy of commons we’re all experiencing in the data tooling landscape, me working in tool building, developer relations, and in marketing, and Sarah in venture capital. 
</description>
      <itunes:keywords>oss, data science, machine learning, python</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Sarah Catanzaro, General Partner at Amplify Partners, about investing in data science and machine learning tooling and where we see progress happening in the space.</p>

<p>Sarah invests in the tools that we both wish we had earlier in our careers: tools that enable data scientists and machine learners to collect, store, manage, analyze, and model data more effectively. As you’ll discover, Sarah identifies as a scientist first and an investor second and still believes that her mission is to enable companies to become data-driven and to generate ROI through machine and statistical learning. In her words, she’s still that cuckoo kid who’s ranting and raving about how data and AI will shift every tide.</p>

<p>In this conversation, we talk about what scientific inquiry actually is and the elements of playfulness and seriousness it necessarily involves, and how it can be used to generate business value. We talk about Sarah’s unorthodox path from a data scientist working in defense to her time at Palantir and how that led her to build out a data team and function for a venture capital firm and then to becoming a VC in the data tooling space.</p>

<p>We then really dive into the data science and machine learning tooling space to figure out why it’s so fragmented: we look to the data analytics stack and software engineering communities to find historical tethers that may be useful. We discuss the moving parts that led to the establishment of a standard, a system of record, and clearly defined roles in analytics and what we can learn from that for machine learning!</p>

<p>We also dive into the development of tools, workflows, and division of labour as partial exercises in pattern recognition and how this can be at odds with the variance we see in the machine learning landscape, more generally!</p>

<p>Two take-aways are that we need best practices and we need  more standardization.</p>

<p>We also discussed that, with all our focus and conversations on tools, what conversation we’re missing and Sarah was adamant that we need to be focusing on questions, not solutions, and even questioning what ML is useful for and what it isn’t, diving into a bunch of thoughtful and nuanced examples.</p>

<p>I’m also grateful that Sarah let me take her down a slightly dangerous and self-critical path where we riffed on both our roles in potentially contributing to the tragedy of commons we’re all experiencing in the data tooling landscape, me working in tool building, developer relations, and in marketing, and Sarah in venture capital. </p>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Sarah Catanzaro, General Partner at Amplify Partners, about investing in data science and machine learning tooling and where we see progress happening in the space.</p>

<p>Sarah invests in the tools that we both wish we had earlier in our careers: tools that enable data scientists and machine learners to collect, store, manage, analyze, and model data more effectively. As you’ll discover, Sarah identifies as a scientist first and an investor second and still believes that her mission is to enable companies to become data-driven and to generate ROI through machine and statistical learning. In her words, she’s still that cuckoo kid who’s ranting and raving about how data and AI will shift every tide.</p>

<p>In this conversation, we talk about what scientific inquiry actually is and the elements of playfulness and seriousness it necessarily involves, and how it can be used to generate business value. We talk about Sarah’s unorthodox path from a data scientist working in defense to her time at Palantir and how that led her to build out a data team and function for a venture capital firm and then to becoming a VC in the data tooling space.</p>

<p>We then really dive into the data science and machine learning tooling space to figure out why it’s so fragmented: we look to the data analytics stack and software engineering communities to find historical tethers that may be useful. We discuss the moving parts that led to the establishment of a standard, a system of record, and clearly defined roles in analytics and what we can learn from that for machine learning!</p>

<p>We also dive into the development of tools, workflows, and division of labour as partial exercises in pattern recognition and how this can be at odds with the variance we see in the machine learning landscape, more generally!</p>

<p>Two take-aways are that we need best practices and we need  more standardization.</p>

<p>We also discussed that, with all our focus and conversations on tools, what conversation we’re missing and Sarah was adamant that we need to be focusing on questions, not solutions, and even questioning what ML is useful for and what it isn’t, diving into a bunch of thoughtful and nuanced examples.</p>

<p>I’m also grateful that Sarah let me take her down a slightly dangerous and self-critical path where we riffed on both our roles in potentially contributing to the tragedy of commons we’re all experiencing in the data tooling landscape, me working in tool building, developer relations, and in marketing, and Sarah in venture capital. </p>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+RCjFZeiv</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+RCjFZeiv" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>9: AutoML, Literate Programming, and Data Tooling Cargo Cults</title>
      <link>https://vanishinggradients.fireside.fm/9</link>
      <guid isPermaLink="false">86c9a94f-4c33-40a8-aa83-50a9e125484b</guid>
      <pubDate>Tue, 19 Jul 2022 23:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/86c9a94f-4c33-40a8-aa83-50a9e125484b.mp3" length="97642250" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Hamel Husain, Head of Data Science at Outerbounds,  with extensive experience in data science consulting, at DataRobot, Airbnb, and Github.</itunes:subtitle>
      <itunes:duration>1:41:42</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/8/86c9a94f-4c33-40a8-aa83-50a9e125484b/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Hamel Husain, Head of Data Science at Outerbounds,  with extensive experience in data science consulting, at DataRobot, Airbnb, and Github.
In this conversation, they talk about Hamel's early days in data science, consulting for a wide array of companies, such as Crocs, restaurants, and casinos in Las Vegas, diving into what data science even looked like in 2005 and how you could think about delivering business value using data and analytics back then.
They talk about his trajectory in moving to data science and machine learning in Silicon Valley, what his expectations were, and what he actually found there.
They then take a dive into AutoML, discussing what should be automated in Machine learning and what shouldn’t. They talk about software engineering best practices and what aspects it would be useful for data scientists to know about.
They also got to talk about the importance of literate programming, notebooks, and documentation in data science and ML. All this and more!
Links
Hamel on twitter (https://twitter.com/HamelHusain)
The Outerbounds documentation project repo (https://github.com/outerbounds/docs)
Practical Advice for R in Production (https://www.rstudio.com/blog/practical-advice-for-r-in-production-answering-your-questions/)
nbdev: Create delightful python projects using Jupyter Notebooks (https://nbdev.fast.ai/) 
</description>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Hamel Husain, Head of Data Science at Outerbounds,  with extensive experience in data science consulting, at DataRobot, Airbnb, and Github.</p>

<p>In this conversation, they talk about Hamel&#39;s early days in data science, consulting for a wide array of companies, such as Crocs, restaurants, and casinos in Las Vegas, diving into what data science even looked like in 2005 and how you could think about delivering business value using data and analytics back then.</p>

<p>They talk about his trajectory in moving to data science and machine learning in Silicon Valley, what his expectations were, and what he actually found there.</p>

<p>They then take a dive into AutoML, discussing what should be automated in Machine learning and what shouldn’t. They talk about software engineering best practices and what aspects it would be useful for data scientists to know about.</p>

<p>They also got to talk about the importance of literate programming, notebooks, and documentation in data science and ML. All this and more!</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://twitter.com/HamelHusain" rel="nofollow">Hamel on twitter</a></li>
<li><a href="https://github.com/outerbounds/docs" rel="nofollow">The Outerbounds documentation project repo</a></li>
<li><a href="https://www.rstudio.com/blog/practical-advice-for-r-in-production-answering-your-questions/" rel="nofollow">Practical Advice for R in Production</a></li>
<li><a href="https://nbdev.fast.ai/" rel="nofollow">nbdev: Create delightful python projects using Jupyter Notebooks</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Hamel Husain, Head of Data Science at Outerbounds,  with extensive experience in data science consulting, at DataRobot, Airbnb, and Github.</p>

<p>In this conversation, they talk about Hamel&#39;s early days in data science, consulting for a wide array of companies, such as Crocs, restaurants, and casinos in Las Vegas, diving into what data science even looked like in 2005 and how you could think about delivering business value using data and analytics back then.</p>

<p>They talk about his trajectory in moving to data science and machine learning in Silicon Valley, what his expectations were, and what he actually found there.</p>

<p>They then take a dive into AutoML, discussing what should be automated in Machine learning and what shouldn’t. They talk about software engineering best practices and what aspects it would be useful for data scientists to know about.</p>

<p>They also got to talk about the importance of literate programming, notebooks, and documentation in data science and ML. All this and more!</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://twitter.com/HamelHusain" rel="nofollow">Hamel on twitter</a></li>
<li><a href="https://github.com/outerbounds/docs" rel="nofollow">The Outerbounds documentation project repo</a></li>
<li><a href="https://www.rstudio.com/blog/practical-advice-for-r-in-production-answering-your-questions/" rel="nofollow">Practical Advice for R in Production</a></li>
<li><a href="https://nbdev.fast.ai/" rel="nofollow">nbdev: Create delightful python projects using Jupyter Notebooks</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+OgKMlZzV</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+OgKMlZzV" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 8: The Open Source Cybernetic Revolution</title>
      <link>https://vanishinggradients.fireside.fm/8</link>
      <guid isPermaLink="false">fe4aec2a-6f67-4259-ae88-6baefd6f008e</guid>
      <pubDate>Mon, 16 May 2022 15:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/fe4aec2a-6f67-4259-ae88-6baefd6f008e.mp3" length="63326903" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Peter Wang, CEO of Anaconda,  about what the value proposition of data science actually is, data not as the new oil, but rather data as toxic, nuclear sludge, the fact that data isn’t real (and what we really have are frozen models), and the future promise of data science, Gifting economies with finite game economics thrust onto them.

They also dive into an experimental conversation around open source software development as a model for the development of human civilization, in the context of developing systems that prize local generativity over global extractive principles. If that’s a mouthful, which it was, or an earful, which it may have been, all will be revealed in the conversation.
</itunes:subtitle>
      <itunes:duration>1:05:57</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/f/fe4aec2a-6f67-4259-ae88-6baefd6f008e/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Peter Wang, CEO of Anaconda,  about what the value proposition of data science actually is, data not as the new oil, but rather data as toxic, nuclear sludge, the fact that data isn’t real (and what we really have are frozen models), and the future promise of data science.
They also dive into an experimental conversation around open source software development as a model for the development of human civilization, in the context of developing systems that prize local generativity over global extractive principles. If that’s a mouthful, which it was, or an earful, which it may have been, all will be revealed in the conversation.
LInks
Peter on twitter (https://twitter.com/pwang)
Anaconda Nucleus (https://anaconda.cloud/)
Jordan Hall on the Jim Rutt Show (https://www.jimruttshow.com/jordan-greenhall-hall/): Game B
Meditations On Moloch (https://slatestarcodex.com/2014/07/30/meditations-on-moloch) -- On multipolar traps
Here Comes Everybody: The Power of Organizing Without Organizations (https://en.wikipedia.org/wiki/Here_Comes_Everybody_(book)) by Clay Shirky
Finite and Infinite Games (https://en.wikipedia.org/wiki/Finite_and_Infinite_Games) by James Carse
Governing the Commons: The Evolution of Institutions for Collective Action (https://www.cambridge.org/core/books/governing-the-commons/7AB7AE11BADA84409C34815CC288CD79) by Elinor Olstrom
Elinor Ostrom's 8 Principles for Managing A Commmons (https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons)
Haunted by Data (https://idlewords.com/talks/haunted_by_data.htm), a beautiful and mesmerising talk by Pinboard.in founder Maciej Ceglowski 
</description>
      <itunes:keywords>oss, data science, machine learning, python</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Peter Wang, CEO of Anaconda,  about what the value proposition of data science actually is, data not as the new oil, but rather data as toxic, nuclear sludge, the fact that data isn’t real (and what we really have are frozen models), and the future promise of data science.</p>

<p>They also dive into an experimental conversation around open source software development as a model for the development of human civilization, in the context of developing systems that prize local generativity over global extractive principles. If that’s a mouthful, which it was, or an earful, which it may have been, all will be revealed in the conversation.</p>

<p><strong>LInks</strong></p>

<ul>
<li><a href="https://twitter.com/pwang" rel="nofollow">Peter on twitter</a></li>
<li><a href="https://anaconda.cloud/" rel="nofollow">Anaconda Nucleus</a></li>
<li><a href="https://www.jimruttshow.com/jordan-greenhall-hall/" rel="nofollow">Jordan Hall on the Jim Rutt Show</a>: Game B</li>
<li><a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch" rel="nofollow">Meditations On Moloch</a> -- On multipolar traps</li>
<li><a href="https://en.wikipedia.org/wiki/Here_Comes_Everybody_(book)" rel="nofollow">Here Comes Everybody: The Power of Organizing Without Organizations</a> by Clay Shirky</li>
<li><a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games" rel="nofollow">Finite and Infinite Games</a> by James Carse</li>
<li><a href="https://www.cambridge.org/core/books/governing-the-commons/7AB7AE11BADA84409C34815CC288CD79" rel="nofollow">Governing the Commons: The Evolution of Institutions for Collective Action</a> by Elinor Olstrom</li>
<li><a href="https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons" rel="nofollow">Elinor Ostrom&#39;s 8 Principles for Managing A Commmons</a></li>
<li><a href="https://idlewords.com/talks/haunted_by_data.htm" rel="nofollow">Haunted by Data</a>, a beautiful and mesmerising talk by Pinboard.in founder Maciej Ceglowski</li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Peter Wang, CEO of Anaconda,  about what the value proposition of data science actually is, data not as the new oil, but rather data as toxic, nuclear sludge, the fact that data isn’t real (and what we really have are frozen models), and the future promise of data science.</p>

<p>They also dive into an experimental conversation around open source software development as a model for the development of human civilization, in the context of developing systems that prize local generativity over global extractive principles. If that’s a mouthful, which it was, or an earful, which it may have been, all will be revealed in the conversation.</p>

<p><strong>LInks</strong></p>

<ul>
<li><a href="https://twitter.com/pwang" rel="nofollow">Peter on twitter</a></li>
<li><a href="https://anaconda.cloud/" rel="nofollow">Anaconda Nucleus</a></li>
<li><a href="https://www.jimruttshow.com/jordan-greenhall-hall/" rel="nofollow">Jordan Hall on the Jim Rutt Show</a>: Game B</li>
<li><a href="https://slatestarcodex.com/2014/07/30/meditations-on-moloch" rel="nofollow">Meditations On Moloch</a> -- On multipolar traps</li>
<li><a href="https://en.wikipedia.org/wiki/Here_Comes_Everybody_(book)" rel="nofollow">Here Comes Everybody: The Power of Organizing Without Organizations</a> by Clay Shirky</li>
<li><a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games" rel="nofollow">Finite and Infinite Games</a> by James Carse</li>
<li><a href="https://www.cambridge.org/core/books/governing-the-commons/7AB7AE11BADA84409C34815CC288CD79" rel="nofollow">Governing the Commons: The Evolution of Institutions for Collective Action</a> by Elinor Olstrom</li>
<li><a href="https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons" rel="nofollow">Elinor Ostrom&#39;s 8 Principles for Managing A Commmons</a></li>
<li><a href="https://idlewords.com/talks/haunted_by_data.htm" rel="nofollow">Haunted by Data</a>, a beautiful and mesmerising talk by Pinboard.in founder Maciej Ceglowski</li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+P8WunAls</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+P8WunAls" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 7: The Evolution of Python for Data Science</title>
      <link>https://vanishinggradients.fireside.fm/7</link>
      <guid isPermaLink="false">da4fab18-c5fa-460d-9ddf-0c8f1e60f3f8</guid>
      <pubDate>Mon, 02 May 2022 06:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/da4fab18-c5fa-460d-9ddf-0c8f1e60f3f8.mp3" length="60022178" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Peter Wang, CEO of Anaconda, about how Python became so big in data science, machine learning, and AI. They jump into many of the technical and sociological beginnings of Python being used for data science, a history of PyData, the conda distribution, and NUMFOCUS.
</itunes:subtitle>
      <itunes:duration>1:02:31</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/d/da4fab18-c5fa-460d-9ddf-0c8f1e60f3f8/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Peter Wang, CEO of Anaconda, about how Python became so big in data science, machine learning, and AI. They jump into many of the technical and sociological beginnings of Python being used for data science, a history of PyData, the conda distribution, and NUMFOCUS.
They also talk about the emergence of online collaborative environments, particularly with respect to open source, and attempt to figure out the movings parts of PyData and why it has had the impact it has, including the fact that many core developers were not computer scientists or software engineers, but rather scientists and researchers building tools that they needed on an as-needed basis
They also discuss the challenges in getting adoption for Python and the things that the PyData stack solves, those that it doesn’t and what progress is being made there.
People who have listened to Hugo podcast for some time may have recognized that he's interested in the sociology of the data science space and he really considered speaking with Peter a fascinating opportunity to delve into how the Pythonic data science space evolved, particularly with respect to tooling, not only because Peter had a front row seat for much of it, but that he was one of several key actors at various different points. On top of this, Hugo wanted to allow Peter’s inner sociologist room to breathe and evolve in this conversation. 
What happens then is slightly experimental – Peter is a deep, broad, and occasionally hallucinatory thinker and Hugo wanted to explore new spaces with him so we hope you enjoy the experiments they play as they begin to discuss open-source software in the broader context of finite and infinite games and how OSS is a paradigm of humanity’s ability to create generative, nourishing and anti-rivlarous systems where, by anti-rivalrous, we mean things that become more valuable for everyone the more people use them! But we need to be mindful of finite-game dynamics (for example, those driven by corporate incentives) co-opting and parasitizing the generative systems that we build.
These are all considerations they delve far deeper into in Part 2 of this interview, which will be the next episode of VG, where we also dive into the relationship  between OSS, tools, and venture capital, amonh many others things.
LInks
Peter on twitter (https://twitter.com/pwang)
Anaconda Nucleus (https://anaconda.cloud/)
Calling out SciPy on diversity (even though it hurts) (https://ilovesymposia.com/2015/04/03/calling-out-scipy-on-diversity/) by Juan Nunez-Iglesias
Here Comes Everybody: The Power of Organizing Without Organizations (https://en.wikipedia.org/wiki/Here_Comes_Everybody_(book)) by Clay Shirky
Finite and Infinite Games (https://en.wikipedia.org/wiki/Finite_and_Infinite_Games) by James Carse
Governing the Commons: The Evolution of Institutions for Collective Action (https://www.cambridge.org/core/books/governing-the-commons/7AB7AE11BADA84409C34815CC288CD79) by Elinor Olstrom
Elinor Ostrom's 8 Principles for Managing A Commmons (https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons) 
</description>
      <itunes:keywords>oss, data science, machine learning, python</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Peter Wang, CEO of Anaconda, about how Python became so big in data science, machine learning, and AI. They jump into many of the technical and sociological beginnings of Python being used for data science, a history of PyData, the conda distribution, and NUMFOCUS.</p>

<p>They also talk about the emergence of online collaborative environments, particularly with respect to open source, and attempt to figure out the movings parts of PyData and why it has had the impact it has, including the fact that many core developers were not computer scientists or software engineers, but rather scientists and researchers building tools that they needed on an as-needed basis</p>

<p>They also discuss the challenges in getting adoption for Python and the things that the PyData stack solves, those that it doesn’t and what progress is being made there.</p>

<p>People who have listened to Hugo podcast for some time may have recognized that he&#39;s interested in the sociology of the data science space and he really considered speaking with Peter a fascinating opportunity to delve into how the Pythonic data science space evolved, particularly with respect to tooling, not only because Peter had a front row seat for much of it, but that he was one of several key actors at various different points. On top of this, Hugo wanted to allow Peter’s inner sociologist room to breathe and evolve in this conversation. </p>

<p>What happens then is slightly experimental – Peter is a deep, broad, and occasionally hallucinatory thinker and Hugo wanted to explore new spaces with him so we hope you enjoy the experiments they play as they begin to discuss open-source software in the broader context of finite and infinite games and how OSS is a paradigm of humanity’s ability to create generative, nourishing and anti-rivlarous systems where, by anti-rivalrous, we mean things that become more valuable for everyone the more people use them! But we need to be mindful of finite-game dynamics (for example, those driven by corporate incentives) co-opting and parasitizing the generative systems that we build.</p>

<p>These are all considerations they delve far deeper into in Part 2 of this interview, which will be the next episode of VG, where we also dive into the relationship  between OSS, tools, and venture capital, amonh many others things.</p>

<p><strong>LInks</strong></p>

<ul>
<li><a href="https://twitter.com/pwang" rel="nofollow">Peter on twitter</a></li>
<li><a href="https://anaconda.cloud/" rel="nofollow">Anaconda Nucleus</a></li>
<li><a href="https://ilovesymposia.com/2015/04/03/calling-out-scipy-on-diversity/" rel="nofollow">Calling out SciPy on diversity (even though it hurts)</a> by Juan Nunez-Iglesias</li>
<li><a href="https://en.wikipedia.org/wiki/Here_Comes_Everybody_(book)" rel="nofollow">Here Comes Everybody: The Power of Organizing Without Organizations</a> by Clay Shirky</li>
<li><a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games" rel="nofollow">Finite and Infinite Games</a> by James Carse</li>
<li><a href="https://www.cambridge.org/core/books/governing-the-commons/7AB7AE11BADA84409C34815CC288CD79" rel="nofollow">Governing the Commons: The Evolution of Institutions for Collective Action</a> by Elinor Olstrom</li>
<li><a href="https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons" rel="nofollow">Elinor Ostrom&#39;s 8 Principles for Managing A Commmons</a></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Peter Wang, CEO of Anaconda, about how Python became so big in data science, machine learning, and AI. They jump into many of the technical and sociological beginnings of Python being used for data science, a history of PyData, the conda distribution, and NUMFOCUS.</p>

<p>They also talk about the emergence of online collaborative environments, particularly with respect to open source, and attempt to figure out the movings parts of PyData and why it has had the impact it has, including the fact that many core developers were not computer scientists or software engineers, but rather scientists and researchers building tools that they needed on an as-needed basis</p>

<p>They also discuss the challenges in getting adoption for Python and the things that the PyData stack solves, those that it doesn’t and what progress is being made there.</p>

<p>People who have listened to Hugo podcast for some time may have recognized that he&#39;s interested in the sociology of the data science space and he really considered speaking with Peter a fascinating opportunity to delve into how the Pythonic data science space evolved, particularly with respect to tooling, not only because Peter had a front row seat for much of it, but that he was one of several key actors at various different points. On top of this, Hugo wanted to allow Peter’s inner sociologist room to breathe and evolve in this conversation. </p>

<p>What happens then is slightly experimental – Peter is a deep, broad, and occasionally hallucinatory thinker and Hugo wanted to explore new spaces with him so we hope you enjoy the experiments they play as they begin to discuss open-source software in the broader context of finite and infinite games and how OSS is a paradigm of humanity’s ability to create generative, nourishing and anti-rivlarous systems where, by anti-rivalrous, we mean things that become more valuable for everyone the more people use them! But we need to be mindful of finite-game dynamics (for example, those driven by corporate incentives) co-opting and parasitizing the generative systems that we build.</p>

<p>These are all considerations they delve far deeper into in Part 2 of this interview, which will be the next episode of VG, where we also dive into the relationship  between OSS, tools, and venture capital, amonh many others things.</p>

<p><strong>LInks</strong></p>

<ul>
<li><a href="https://twitter.com/pwang" rel="nofollow">Peter on twitter</a></li>
<li><a href="https://anaconda.cloud/" rel="nofollow">Anaconda Nucleus</a></li>
<li><a href="https://ilovesymposia.com/2015/04/03/calling-out-scipy-on-diversity/" rel="nofollow">Calling out SciPy on diversity (even though it hurts)</a> by Juan Nunez-Iglesias</li>
<li><a href="https://en.wikipedia.org/wiki/Here_Comes_Everybody_(book)" rel="nofollow">Here Comes Everybody: The Power of Organizing Without Organizations</a> by Clay Shirky</li>
<li><a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games" rel="nofollow">Finite and Infinite Games</a> by James Carse</li>
<li><a href="https://www.cambridge.org/core/books/governing-the-commons/7AB7AE11BADA84409C34815CC288CD79" rel="nofollow">Governing the Commons: The Evolution of Institutions for Collective Action</a> by Elinor Olstrom</li>
<li><a href="https://www.onthecommons.org/magazine/elinor-ostroms-8-principles-managing-commmons" rel="nofollow">Elinor Ostrom&#39;s 8 Principles for Managing A Commmons</a></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+02JRshyH</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+02JRshyH" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 6: Bullshit Jobs in Data Science (and what to do about them)</title>
      <link>https://vanishinggradients.fireside.fm/6</link>
      <guid isPermaLink="false">811a664b-7b02-45b1-8cd7-84155bf4e39d</guid>
      <pubDate>Tue, 05 Apr 2022 07:00:00 +1000</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/811a664b-7b02-45b1-8cd7-84155bf4e39d.mp3" length="83542646" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Jacqueline Nolis, Chief Product Officer at Saturn Cloud (formerly Head of Data Science), about all types of failure modes in data science, ML, and AI, and they delve into bullshit jobs in data science (yes, that’s a technical term, as you’ll find out) –they discuss the elements that are bullshit, the elements that aren’t, and how to increase the ratio of the latter to the former.
</itunes:subtitle>
      <itunes:duration>1:27:01</itunes:duration>
      <itunes:explicit>yes</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/8/811a664b-7b02-45b1-8cd7-84155bf4e39d/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Jacqueline Nolis, Chief Product Officer at Saturn Cloud (formerly Head of Data Science), about all types of failure modes in data science, ML, and AI, and they delve into bullshit jobs in data science (yes, that’s a technical term, as you’ll find out) –they discuss the elements that are bullshit, the elements that aren’t, and how to increase the ratio of the latter to the former.
They also talk about her journey in moving from mainly working in prescriptive analytics building reports in PDFs and power points to deploying machine learning products in production. They delve into her motion from doing data science to designing products for data scientists and how to think about choosing career paths. Jacqueline has been an individual contributor, a team lead, and a principal data scientist so has  a lot of valuable experience here. They talk about her experience of transitioning gender while working in data science and they work hard to find a bright vision for the future of this industry!
Links
Jacqueline on twitter (https://twitter.com/skyetetra)
Building a Career in Data Science (https://jnolis.com/book/) by Jacqueline and Emily Robinson
Saturn Cloud (https://saturncloud.io/)
Why are we so surprised? (http://allendowney.blogspot.com/2016/11/why-are-we-so-surprised.html), a post by Allen Downey on communicating and thinking through uncertainty
Data Mishaps Night! (https://datamishapsnight.com/)
The Trump administration’s “cubic model” of coronavirus deaths, explained (https://www.vox.com/2020/5/8/21250641/kevin-hassett-cubic-model-smoothing) by Matthew Yglesias
Working Class Deep Learner (https://marksaroufim.substack.com/p/working-class-deep-learner?s=r) by Mark Saroufim 
</description>
      <itunes:keywords>machine learning, AI, data science</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Jacqueline Nolis, Chief Product Officer at Saturn Cloud (formerly Head of Data Science), about all types of failure modes in data science, ML, and AI, and they delve into bullshit jobs in data science (yes, that’s a technical term, as you’ll find out) –they discuss the elements that are bullshit, the elements that aren’t, and how to increase the ratio of the latter to the former.</p>

<p>They also talk about her journey in moving from mainly working in prescriptive analytics building reports in PDFs and power points to deploying machine learning products in production. They delve into her motion from doing data science to designing products for data scientists and how to think about choosing career paths. Jacqueline has been an individual contributor, a team lead, and a principal data scientist so has  a lot of valuable experience here. They talk about her experience of transitioning gender while working in data science and they work hard to find a bright vision for the future of this industry!</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://twitter.com/skyetetra" rel="nofollow">Jacqueline on twitter</a></li>
<li><a href="https://jnolis.com/book/" rel="nofollow">Building a Career in Data Science</a> by Jacqueline and Emily Robinson</li>
<li><a href="https://saturncloud.io/" rel="nofollow">Saturn Cloud</a></li>
<li><a href="http://allendowney.blogspot.com/2016/11/why-are-we-so-surprised.html" rel="nofollow">Why are we so surprised?</a>, a post by Allen Downey on communicating and thinking through uncertainty</li>
<li><a href="https://datamishapsnight.com/" rel="nofollow">Data Mishaps Night!</a></li>
<li><a href="https://www.vox.com/2020/5/8/21250641/kevin-hassett-cubic-model-smoothing" rel="nofollow">The Trump administration’s “cubic model” of coronavirus deaths, explained</a> by Matthew Yglesias</li>
<li><a href="https://marksaroufim.substack.com/p/working-class-deep-learner?s=r" rel="nofollow">Working Class Deep Learner</a> by Mark Saroufim</li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Jacqueline Nolis, Chief Product Officer at Saturn Cloud (formerly Head of Data Science), about all types of failure modes in data science, ML, and AI, and they delve into bullshit jobs in data science (yes, that’s a technical term, as you’ll find out) –they discuss the elements that are bullshit, the elements that aren’t, and how to increase the ratio of the latter to the former.</p>

<p>They also talk about her journey in moving from mainly working in prescriptive analytics building reports in PDFs and power points to deploying machine learning products in production. They delve into her motion from doing data science to designing products for data scientists and how to think about choosing career paths. Jacqueline has been an individual contributor, a team lead, and a principal data scientist so has  a lot of valuable experience here. They talk about her experience of transitioning gender while working in data science and they work hard to find a bright vision for the future of this industry!</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://twitter.com/skyetetra" rel="nofollow">Jacqueline on twitter</a></li>
<li><a href="https://jnolis.com/book/" rel="nofollow">Building a Career in Data Science</a> by Jacqueline and Emily Robinson</li>
<li><a href="https://saturncloud.io/" rel="nofollow">Saturn Cloud</a></li>
<li><a href="http://allendowney.blogspot.com/2016/11/why-are-we-so-surprised.html" rel="nofollow">Why are we so surprised?</a>, a post by Allen Downey on communicating and thinking through uncertainty</li>
<li><a href="https://datamishapsnight.com/" rel="nofollow">Data Mishaps Night!</a></li>
<li><a href="https://www.vox.com/2020/5/8/21250641/kevin-hassett-cubic-model-smoothing" rel="nofollow">The Trump administration’s “cubic model” of coronavirus deaths, explained</a> by Matthew Yglesias</li>
<li><a href="https://marksaroufim.substack.com/p/working-class-deep-learner?s=r" rel="nofollow">Working Class Deep Learner</a> by Mark Saroufim</li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+eGS1Ro9g</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+eGS1Ro9g" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 5: Executive Data Science</title>
      <link>https://vanishinggradients.fireside.fm/5</link>
      <guid isPermaLink="false">9078010f-454b-4bcf-bafc-f54f44e04868</guid>
      <pubDate>Wed, 23 Mar 2022 16:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/9078010f-454b-4bcf-bafc-f54f44e04868.mp3" length="103917601" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Jim Savage, the Director of Data Science at Schmidt Futures, about the need for data science in executive training and decision, what data scientists can learn from economists, the perils of "data for good", and why you should always be integrating your loss function over your posterior.</itunes:subtitle>
      <itunes:duration>1:48:14</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/9/9078010f-454b-4bcf-bafc-f54f44e04868/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Jim Savage, the Director of Data Science at Schmidt Futures, about the need for data science in executive training and decision, what data scientists can learn from economists, the perils of "data for good", and why you should always be integrating your loss function over your posterior.
Jim and Hugo talk about what data science is and isn’t capable of, what can actually deliver value, and what people really enjoy doing: the intersection in this Venn diagram is where we need to focus energy and it may not be quite what you think it is!
They then dive into Jim's thoughts on what he dubs Executive Data Science. You may be aware of the slicing of the data science and machine learning spaces into descriptive analytics, predictive analytics, and prescriptive analytics but, being the thought surgeon that he is, Jim proposes a different slicing into 
(1) tool building OR data science as a product, 
(2) tools to automate and augment parts of us, and 
(3) what Jim calls Executive Data Science.
Jim and Hugo also talk about decision theory, the woeful state of causal inference techniques in contemporary data science, and what techniques it would behoove us all to import from econometrics and economics, more generally. If that’s not enough, they talk about the importance of thinking through the data generating process and things that can go wrong if you don’t. In terms of allowing your data work to inform your decision making, thery also discuss Jim’s maxim “ALWAYS BE INTEGRATING YOUR LOSS FUNCTION OVER YOUR POSTERIOR”
Last but definitively not least, as Jim has worked in the data for good space for much of his career, they talk about what this actually means, with particular reference to fast.ai founder &amp; QUT professor of practice Rachel Thomas’  blog post called “Doing Data Science for Social Good, Responsibly” (https://www.fast.ai/2021/11/23/data-for-good/). Rachel’s post takes as its starting point the following words of Sarah Hooker, a researcher at Google Brain:
"Data for good" is an imprecise term that says little about who we serve, the tools used, or the goals. Being more precise can help us be more accountable &amp; have a greater positive impact.
And Jim and I discuss his work in the light of these foundational considerations.
Links
Jim on twitter (https://twitter.com/abiylfoyp/)
What Is Causal Inference?An Introduction for Data Scientists (https://www.oreilly.com/radar/what-is-causal-inference/) by Hugo Bowne-Anderson and Mike Loukides
 Jim's must-watch Data Council talk on Productizing Structural Models (https://www.datacouncil.ai/talks/productizing-structural-models)
 [Mastering Metrics}(https://www.masteringmetrics.com/) by Angrist and Pischke
 Mostly Harmless Econometrics: An Empiricist's Companion (https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics) by Angrist and Pischke
 The Book of Why (https://en.wikipedia.org/wiki/The_Book_of_Why) by Judea Pearl
Decision-Making in a Time of Crisis (https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/) by Hugo Bowne-Anderson
Doing Data Science for Social Good, Responsibly (https://www.fast.ai/2021/11/23/data-for-good/) by Rachel Thomas
</description>
      <itunes:keywords>data science, executive, machine learning, economics, AI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Jim Savage, the Director of Data Science at Schmidt Futures, about the need for data science in executive training and decision, what data scientists can learn from economists, the perils of &quot;data for good&quot;, and why you should always be integrating your loss function over your posterior.</p>

<p>Jim and Hugo talk about what data science is and isn’t capable of, what can actually deliver value, and what people really enjoy doing: the intersection in this Venn diagram is where we need to focus energy and it may not be quite what you think it is!</p>

<p>They then dive into Jim&#39;s thoughts on what he dubs Executive Data Science. You may be aware of the slicing of the data science and machine learning spaces into descriptive analytics, predictive analytics, and prescriptive analytics but, being the thought surgeon that he is, Jim proposes a different slicing into </p>

<p>(1) tool building OR data science as a product, </p>

<p>(2) tools to automate and augment parts of us, and </p>

<p>(3) what Jim calls Executive Data Science.</p>

<p>Jim and Hugo also talk about decision theory, the woeful state of causal inference techniques in contemporary data science, and what techniques it would behoove us all to import from econometrics and economics, more generally. If that’s not enough, they talk about the importance of thinking through the data generating process and things that can go wrong if you don’t. In terms of allowing your data work to inform your decision making, thery also discuss Jim’s maxim “ALWAYS BE INTEGRATING YOUR LOSS FUNCTION OVER YOUR POSTERIOR”</p>

<p>Last but definitively not least, as Jim has worked in the data for good space for much of his career, they talk about what this actually means, with particular reference to fast.ai founder &amp; QUT professor of practice Rachel Thomas’  blog post called <a href="https://www.fast.ai/2021/11/23/data-for-good/" rel="nofollow">“Doing Data Science for Social Good, Responsibly”</a>. Rachel’s post takes as its starting point the following words of Sarah Hooker, a researcher at Google Brain:</p>

<blockquote>
<p>&quot;Data for good&quot; is an imprecise term that says little about who we serve, the tools used, or the goals. Being more precise can help us be more accountable &amp; have a greater positive impact.</p>
</blockquote>

<p>And Jim and I discuss his work in the light of these foundational considerations.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://twitter.com/abiylfoyp/" rel="nofollow">Jim on twitter</a></li>
<li><a href="https://www.oreilly.com/radar/what-is-causal-inference/" rel="nofollow">What Is Causal Inference?An Introduction for Data Scientists</a> by Hugo Bowne-Anderson and Mike Loukides</li>
<li> Jim&#39;s must-watch Data Council talk on <a href="https://www.datacouncil.ai/talks/productizing-structural-models" rel="nofollow">Productizing Structural Models</a></li>
<li> [Mastering Metrics}(<a href="https://www.masteringmetrics.com/" rel="nofollow">https://www.masteringmetrics.com/</a>) by Angrist and Pischke</li>
<li> <a href="https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics" rel="nofollow">Mostly Harmless Econometrics: An Empiricist&#39;s Companion</a> by Angrist and Pischke</li>
<li> <a href="https://en.wikipedia.org/wiki/The_Book_of_Why" rel="nofollow">The Book of Why</a> by Judea Pearl</li>
<li><a href="https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/" rel="nofollow">Decision-Making in a Time of Crisis</a> by Hugo Bowne-Anderson</li>
<li><a href="https://www.fast.ai/2021/11/23/data-for-good/" rel="nofollow">Doing Data Science for Social Good, Responsibly</a> by Rachel Thomas</li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Jim Savage, the Director of Data Science at Schmidt Futures, about the need for data science in executive training and decision, what data scientists can learn from economists, the perils of &quot;data for good&quot;, and why you should always be integrating your loss function over your posterior.</p>

<p>Jim and Hugo talk about what data science is and isn’t capable of, what can actually deliver value, and what people really enjoy doing: the intersection in this Venn diagram is where we need to focus energy and it may not be quite what you think it is!</p>

<p>They then dive into Jim&#39;s thoughts on what he dubs Executive Data Science. You may be aware of the slicing of the data science and machine learning spaces into descriptive analytics, predictive analytics, and prescriptive analytics but, being the thought surgeon that he is, Jim proposes a different slicing into </p>

<p>(1) tool building OR data science as a product, </p>

<p>(2) tools to automate and augment parts of us, and </p>

<p>(3) what Jim calls Executive Data Science.</p>

<p>Jim and Hugo also talk about decision theory, the woeful state of causal inference techniques in contemporary data science, and what techniques it would behoove us all to import from econometrics and economics, more generally. If that’s not enough, they talk about the importance of thinking through the data generating process and things that can go wrong if you don’t. In terms of allowing your data work to inform your decision making, thery also discuss Jim’s maxim “ALWAYS BE INTEGRATING YOUR LOSS FUNCTION OVER YOUR POSTERIOR”</p>

<p>Last but definitively not least, as Jim has worked in the data for good space for much of his career, they talk about what this actually means, with particular reference to fast.ai founder &amp; QUT professor of practice Rachel Thomas’  blog post called <a href="https://www.fast.ai/2021/11/23/data-for-good/" rel="nofollow">“Doing Data Science for Social Good, Responsibly”</a>. Rachel’s post takes as its starting point the following words of Sarah Hooker, a researcher at Google Brain:</p>

<blockquote>
<p>&quot;Data for good&quot; is an imprecise term that says little about who we serve, the tools used, or the goals. Being more precise can help us be more accountable &amp; have a greater positive impact.</p>
</blockquote>

<p>And Jim and I discuss his work in the light of these foundational considerations.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://twitter.com/abiylfoyp/" rel="nofollow">Jim on twitter</a></li>
<li><a href="https://www.oreilly.com/radar/what-is-causal-inference/" rel="nofollow">What Is Causal Inference?An Introduction for Data Scientists</a> by Hugo Bowne-Anderson and Mike Loukides</li>
<li> Jim&#39;s must-watch Data Council talk on <a href="https://www.datacouncil.ai/talks/productizing-structural-models" rel="nofollow">Productizing Structural Models</a></li>
<li> [Mastering Metrics}(<a href="https://www.masteringmetrics.com/" rel="nofollow">https://www.masteringmetrics.com/</a>) by Angrist and Pischke</li>
<li> <a href="https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics" rel="nofollow">Mostly Harmless Econometrics: An Empiricist&#39;s Companion</a> by Angrist and Pischke</li>
<li> <a href="https://en.wikipedia.org/wiki/The_Book_of_Why" rel="nofollow">The Book of Why</a> by Judea Pearl</li>
<li><a href="https://www.oreilly.com/radar/decision-making-in-a-time-of-crisis/" rel="nofollow">Decision-Making in a Time of Crisis</a> by Hugo Bowne-Anderson</li>
<li><a href="https://www.fast.ai/2021/11/23/data-for-good/" rel="nofollow">Doing Data Science for Social Good, Responsibly</a> by Rachel Thomas</li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+1CLcvuRV</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+1CLcvuRV" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 4: Machine Learning at T-Mobile</title>
      <link>https://vanishinggradients.fireside.fm/4</link>
      <guid isPermaLink="false">32f4444c-6c16-4411-ab8a-2adbf23b65c8</guid>
      <pubDate>Thu, 10 Mar 2022 10:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/32f4444c-6c16-4411-ab8a-2adbf23b65c8.mp3" length="100002470" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Heather Nolis, Principal Machine Learning engineer at T-mobile, about what data science, machine learning, and AI look like at T-mobile, along with Heather’s path from a software development intern there to principal ML engineer running a team of 15.
</itunes:subtitle>
      <itunes:duration>1:44:10</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/3/32f4444c-6c16-4411-ab8a-2adbf23b65c8/transcript.txt" type="text/plain"/>
      <description>Hugo speaks with Heather Nolis, Principal Machine Learning engineer at T-mobile, about what data science, machine learning, and AI look like at T-mobile, along with Heather’s path from a software development intern there to principal ML engineer running a team of 15.
They talk about: how to build a DS culture from scratch and what executive-level support looks like, as well as how to demonstrate machine learning value early on from a shark tank style pitch night to the initial investment through to the POC and building out the function; all the great work they do with R and the Tidyverse in production; what it’s like to be a lesbian in tech, and about what it was like to discover she was autistic and how that impacted her work; how to measure and demonstrate success and ROI for the org; some massive data science fails!; how to deal with execs wanting you to use the latest GPT-X – in a fragmented tooling landscape; how to use the simplest technology to deliver the most value.
Finally, the team just hired their first FT ethicist and they speak about how ethics can be embedded in a team and across an institution.
Links
Put R in prod (https://putrinprod.com/):  Tools and guides to put R models into production
Enterprise Web Services with Neural Networks Using R and TensorFlow (https://medium.com/tmobile-tech/enterprise-web-services-with-neural-networks-using-r-and-tensorflow-a09c1b100c11)
Heather on twitter (https://twitter.com/heatherklus) 
T-Mobile is hiring! (https://www.t-mobile.com/careers)
Hugo's upcoming fireside chat and AMA with Hilary Parker about how to actually produce sustainable business value using machine learning and product management for ML! (https://www.eventbrite.com/e/select-ml-project-where-value-is-not-null-tickets-284000161127?aff=hba)  
</description>
      <itunes:keywords>deep learning, data science, machine learning, AI, telecommunications</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Hugo speaks with Heather Nolis, Principal Machine Learning engineer at T-mobile, about what data science, machine learning, and AI look like at T-mobile, along with Heather’s path from a software development intern there to principal ML engineer running a team of 15.</p>

<p>They talk about: how to build a DS culture from scratch and what executive-level support looks like, as well as how to demonstrate machine learning value early on from a shark tank style pitch night to the initial investment through to the POC and building out the function; all the great work they do with R and the Tidyverse in production; what it’s like to be a lesbian in tech, and about what it was like to discover she was autistic and how that impacted her work; how to measure and demonstrate success and ROI for the org; some massive data science fails!; how to deal with execs wanting you to use the latest GPT-X – in a fragmented tooling landscape; how to use the simplest technology to deliver the most value.</p>

<p>Finally, the team just hired their first FT ethicist and they speak about how ethics can be embedded in a team and across an institution.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://putrinprod.com/" rel="nofollow">Put R in prod</a>:  Tools and guides to put R models into production</li>
<li><a href="https://medium.com/tmobile-tech/enterprise-web-services-with-neural-networks-using-r-and-tensorflow-a09c1b100c11" rel="nofollow">Enterprise Web Services with Neural Networks Using R and TensorFlow</a></li>
<li><a href="https://twitter.com/heatherklus" rel="nofollow">Heather on twitter</a> </li>
<li><p><a href="https://www.t-mobile.com/careers" rel="nofollow">T-Mobile is hiring!</a></p></li>
<li><p><a href="https://www.eventbrite.com/e/select-ml-project-where-value-is-not-null-tickets-284000161127?aff=hba" rel="nofollow">Hugo&#39;s upcoming fireside chat and AMA with Hilary Parker about <strong>how to actually produce sustainable business value</strong> using machine learning and product management for ML!</a> </p></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Hugo speaks with Heather Nolis, Principal Machine Learning engineer at T-mobile, about what data science, machine learning, and AI look like at T-mobile, along with Heather’s path from a software development intern there to principal ML engineer running a team of 15.</p>

<p>They talk about: how to build a DS culture from scratch and what executive-level support looks like, as well as how to demonstrate machine learning value early on from a shark tank style pitch night to the initial investment through to the POC and building out the function; all the great work they do with R and the Tidyverse in production; what it’s like to be a lesbian in tech, and about what it was like to discover she was autistic and how that impacted her work; how to measure and demonstrate success and ROI for the org; some massive data science fails!; how to deal with execs wanting you to use the latest GPT-X – in a fragmented tooling landscape; how to use the simplest technology to deliver the most value.</p>

<p>Finally, the team just hired their first FT ethicist and they speak about how ethics can be embedded in a team and across an institution.</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://putrinprod.com/" rel="nofollow">Put R in prod</a>:  Tools and guides to put R models into production</li>
<li><a href="https://medium.com/tmobile-tech/enterprise-web-services-with-neural-networks-using-r-and-tensorflow-a09c1b100c11" rel="nofollow">Enterprise Web Services with Neural Networks Using R and TensorFlow</a></li>
<li><a href="https://twitter.com/heatherklus" rel="nofollow">Heather on twitter</a> </li>
<li><p><a href="https://www.t-mobile.com/careers" rel="nofollow">T-Mobile is hiring!</a></p></li>
<li><p><a href="https://www.eventbrite.com/e/select-ml-project-where-value-is-not-null-tickets-284000161127?aff=hba" rel="nofollow">Hugo&#39;s upcoming fireside chat and AMA with Hilary Parker about <strong>how to actually produce sustainable business value</strong> using machine learning and product management for ML!</a> </p></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+jB6TWOJz</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+jB6TWOJz" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 3: Language Tech For All</title>
      <link>https://vanishinggradients.fireside.fm/3</link>
      <guid isPermaLink="false">8f08dc5e-bb75-4fec-9db9-3808cd980ba9</guid>
      <pubDate>Tue, 01 Mar 2022 13:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/8f08dc5e-bb75-4fec-9db9-3808cd980ba9.mp3" length="88851890" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo speaks with Rachael Tatman about the democratization of natural language processing, conversational AI, and chatbots, including, among other things, the data scientist’s responsibility to end-users and stakeholders.</itunes:subtitle>
      <itunes:duration>1:32:33</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/8/8f08dc5e-bb75-4fec-9db9-3808cd980ba9/transcript.txt" type="text/plain"/>
      <description>Rachael Tatman is a senior developer advocate for Rasa, where she’s helping developers build and deploy ML chatbots using their open source framework.
Rachael has a PhD in Linguistics from the University of Washington where her research was on computational sociolinguistics, or how our social identity affects the way we use language in computational contexts. Previously she was a data scientist at Kaggle and she’s still a Kaggle Grandmaster.
In this conversation, Rachael and I talk about the history of NLP and conversational AI//chatbots and we dive into the fascinating tension between rule-based techniques and  ML and deep learning – we also talk about how to incorporate machine and human intelligence together  by thinking through questions such as “should a response to a human ever be automated?” Spoiler alert: the answer is a resounding NO WAY! 
In this journey, something that becomes apparent is that many of the trends, concepts, questions, and answers, although framed for NLP and chatbots, are applicable to much of data science, more generally.
We also discuss the data scientist’s responsibility to end-users and stakeholders using, among other things, the lens of considering those whose data you’re working with to be data donors.
We then consider what globalized language technology looks like and can look like, what we can learn from the history of science here, particularly given that so much training data and models are in English when it accounts for so little of language spoken globally. 
Links
Rachael's website (https://www.rctatman.com/)
Rasa (https://rasa.com/)
Speech and Language Processing (https://web.stanford.edu/~jurafsky/slp3/)
by  Dan Jurafsky and James H. Martin 
Masakhane (https://twitter.com/MasakhaneNLP), putting African languages on the #NLP map since 2019
The Distributed AI Research Institute (https://www.dair-institute.org/), a space for independent, community-rooted AI research, free from Big Tech’s pervasive influence
The Algorithmic Justice League (https://www.ajl.org/), unmasking AI harms and biases
Black in AI (https://blackinai.github.io/#/), increasing the presence and inclusion of Black people in the field of AI by creating space for sharing ideas, fostering collaborations, mentorship and advocacy
Hugo's blog post on his new job and why it's exciting for him to double down on helping scientists do better science (https://outerbounds.com/blog/hba-excited-to-join-metaflow-and-outerbounds/) 
</description>
      <itunes:keywords>machine learning, natural language processing, data science, deep learning, chatbots</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Rachael Tatman is a senior developer advocate for Rasa, where she’s helping developers build and deploy ML chatbots using their open source framework.</p>

<p>Rachael has a PhD in Linguistics from the University of Washington where her research was on computational sociolinguistics, or how our social identity affects the way we use language in computational contexts. Previously she was a data scientist at Kaggle and she’s still a Kaggle Grandmaster.</p>

<p>In this conversation, Rachael and I talk about the history of NLP and conversational AI//chatbots and we dive into the fascinating tension between rule-based techniques and  ML and deep learning – we also talk about how to incorporate machine and human intelligence together  by thinking through questions such as “should a response to a human ever be automated?” Spoiler alert: the answer is a resounding NO WAY! </p>

<p>In this journey, something that becomes apparent is that many of the trends, concepts, questions, and answers, although framed for NLP and chatbots, are applicable to much of data science, more generally.</p>

<p>We also discuss the data scientist’s responsibility to end-users and stakeholders using, among other things, the lens of considering those whose data you’re working with to be data donors.</p>

<p>We then consider what globalized language technology looks like and can look like, what we can learn from the history of science here, particularly given that so much training data and models are in English when it accounts for so little of language spoken globally. </p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.rctatman.com/" rel="nofollow">Rachael&#39;s website</a></li>
<li><a href="https://rasa.com/" rel="nofollow">Rasa</a></li>
<li><a href="https://web.stanford.edu/%7Ejurafsky/slp3/" rel="nofollow">Speech and Language Processing</a>
by  Dan Jurafsky and James H. Martin 

<ul>
<li><a href="https://twitter.com/MasakhaneNLP" rel="nofollow">Masakhane</a>, putting African languages on the #NLP map since 2019</li>
<li><a href="https://www.dair-institute.org/" rel="nofollow">The Distributed AI Research Institute</a>, a space for independent, community-rooted AI research, free from Big Tech’s pervasive influence</li>
<li><a href="https://www.ajl.org/" rel="nofollow">The Algorithmic Justice League</a>, unmasking AI harms and biases</li>
<li><a href="https://blackinai.github.io/#/" rel="nofollow">Black in AI</a>, increasing the presence and inclusion of Black people in the field of AI by creating space for sharing ideas, fostering collaborations, mentorship and advocacy</li>
<li><a href="https://outerbounds.com/blog/hba-excited-to-join-metaflow-and-outerbounds/" rel="nofollow">Hugo&#39;s blog post on his new job and why it&#39;s exciting for him to double down on helping scientists do better science</a></li>
</ul></li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Rachael Tatman is a senior developer advocate for Rasa, where she’s helping developers build and deploy ML chatbots using their open source framework.</p>

<p>Rachael has a PhD in Linguistics from the University of Washington where her research was on computational sociolinguistics, or how our social identity affects the way we use language in computational contexts. Previously she was a data scientist at Kaggle and she’s still a Kaggle Grandmaster.</p>

<p>In this conversation, Rachael and I talk about the history of NLP and conversational AI//chatbots and we dive into the fascinating tension between rule-based techniques and  ML and deep learning – we also talk about how to incorporate machine and human intelligence together  by thinking through questions such as “should a response to a human ever be automated?” Spoiler alert: the answer is a resounding NO WAY! </p>

<p>In this journey, something that becomes apparent is that many of the trends, concepts, questions, and answers, although framed for NLP and chatbots, are applicable to much of data science, more generally.</p>

<p>We also discuss the data scientist’s responsibility to end-users and stakeholders using, among other things, the lens of considering those whose data you’re working with to be data donors.</p>

<p>We then consider what globalized language technology looks like and can look like, what we can learn from the history of science here, particularly given that so much training data and models are in English when it accounts for so little of language spoken globally. </p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.rctatman.com/" rel="nofollow">Rachael&#39;s website</a></li>
<li><a href="https://rasa.com/" rel="nofollow">Rasa</a></li>
<li><a href="https://web.stanford.edu/%7Ejurafsky/slp3/" rel="nofollow">Speech and Language Processing</a>
by  Dan Jurafsky and James H. Martin 

<ul>
<li><a href="https://twitter.com/MasakhaneNLP" rel="nofollow">Masakhane</a>, putting African languages on the #NLP map since 2019</li>
<li><a href="https://www.dair-institute.org/" rel="nofollow">The Distributed AI Research Institute</a>, a space for independent, community-rooted AI research, free from Big Tech’s pervasive influence</li>
<li><a href="https://www.ajl.org/" rel="nofollow">The Algorithmic Justice League</a>, unmasking AI harms and biases</li>
<li><a href="https://blackinai.github.io/#/" rel="nofollow">Black in AI</a>, increasing the presence and inclusion of Black people in the field of AI by creating space for sharing ideas, fostering collaborations, mentorship and advocacy</li>
<li><a href="https://outerbounds.com/blog/hba-excited-to-join-metaflow-and-outerbounds/" rel="nofollow">Hugo&#39;s blog post on his new job and why it&#39;s exciting for him to double down on helping scientists do better science</a></li>
</ul></li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+FDFTM_SW</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+FDFTM_SW" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
    <item>
      <title>Episode 2: Making Data Science Uncool Again</title>
      <link>https://vanishinggradients.fireside.fm/2</link>
      <guid isPermaLink="false">65695b45-10a7-4785-adca-f1aeaa5818bc</guid>
      <pubDate>Mon, 21 Feb 2022 10:00:00 +1100</pubDate>
      <author>Hugo Bowne-Anderson</author>
      <enclosure url="https://aphid.fireside.fm/d/1437767933/140c3904-8258-4c39-a698-a112b7077bd7/65695b45-10a7-4785-adca-f1aeaa5818bc.mp3" length="101524103" type="audio/mpeg"/>
      <itunes:episodeType>full</itunes:episodeType>
      <itunes:season>1</itunes:season>
      <itunes:author>Hugo Bowne-Anderson</itunes:author>
      <itunes:subtitle>Hugo talks with Jeremy Howard about the past, present, and future of data science, machine learning, and AI, with a focus on the democratization of deep learning.</itunes:subtitle>
      <itunes:duration>1:45:45</itunes:duration>
      <itunes:explicit>no</itunes:explicit>
      <itunes:image href="https://media24.fireside.fm/file/fireside-images-2024/podcasts/images/1/140c3904-8258-4c39-a698-a112b7077bd7/cover.jpg?v=1"/>
      <podcast:transcript url="https://assets.fireside.fm/file/fireside-images-2024/podcasts/transcripts/1/140c3904-8258-4c39-a698-a112b7077bd7/episodes/6/65695b45-10a7-4785-adca-f1aeaa5818bc/transcript.txt" type="text/plain"/>
      <description>Jeremy Howard is a data scientist, researcher, developer, educator, and entrepreneur. Jeremy is a founding researcher at fast.ai, a research institute dedicated to making deep learning more accessible. He is also a Distinguished Research Scientist at the University of San Francisco, the chair of WAMRI, and is Chief Scientist at platform.ai.
In this conversation, we’ll be talking about the history of data science, machine learning, and AI, where we’ve come from and where we’re going, how new techniques can be applied to real-world problems, whether it be deep learning to medicine or porting techniques from computer vision to NLP. We’ll also talk about what’s present and what’s missing in the ML skills revolution, what software engineering skills data scientists need to learn, how to cope in a space of such fragmented tooling, and paths for emerging out of the shadow of FAANG. If that’s not enough, we’ll jump into how spreading DS skills around the globe involves serious investments in education, building software, communities, and research, along with diving into the social challenges that the information age and the AI revolution (so to speak) bring with it.
But to get to all of this, you’ll need to listen to a few minutes of us chatting about chocolate biscuits in Australia!
Links
* &lt;a href="https://www.fast.ai/" target="_blank"&gt;fast.ai · making neural nets uncool again&lt;/a&gt;
* nbdev: create delightful python projects using Jupyter Notebooks (https://github.com/fastai/nbdev)
* The fastai book, published as Jupyter Notebooks (https://github.com/fastai/fastbook)
* Deep Learning for Coders with fastai and PyTorch (https://www.oreilly.com/library/view/deep-learning-for/9781492045519/)
* The wonderful and terrifying implications of computers that can learn (https://www.youtube.com/watch?v=t4kyRyKyOpo) -- Jeremy' awesome TED talk!
* Manna (https://marshallbrain.com/manna)  by Marshall Brain
* Ghost Work (https://ghostwork.info/) by Mary L. Gray and Siddharth Suri
* Uberland (https://www.ucpress.edu/book/9780520324800/uberland) by Alex Rosenblat
</description>
      <itunes:keywords>deep learning, data science, machine learning, AI</itunes:keywords>
      <content:encoded>
        <![CDATA[<p>Jeremy Howard is a data scientist, researcher, developer, educator, and entrepreneur. Jeremy is a founding researcher at fast.ai, a research institute dedicated to making deep learning more accessible. He is also a Distinguished Research Scientist at the University of San Francisco, the chair of WAMRI, and is Chief Scientist at platform.ai.</p>

<p>In this conversation, we’ll be talking about the history of data science, machine learning, and AI, where we’ve come from and where we’re going, how new techniques can be applied to real-world problems, whether it be deep learning to medicine or porting techniques from computer vision to NLP. We’ll also talk about what’s present and what’s missing in the ML skills revolution, what software engineering skills data scientists need to learn, how to cope in a space of such fragmented tooling, and paths for emerging out of the shadow of FAANG. If that’s not enough, we’ll jump into how spreading DS skills around the globe involves serious investments in education, building software, communities, and research, along with diving into the social challenges that the information age and the AI revolution (so to speak) bring with it.</p>

<p>But to get to all of this, you’ll need to listen to a few minutes of us chatting about chocolate biscuits in Australia!</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.fast.ai/" target="_blank">fast.ai · making neural nets uncool again</a></li>
<li><a href="https://github.com/fastai/nbdev" rel="nofollow">nbdev: create delightful python projects using Jupyter Notebooks</a></li>
<li><a href="https://github.com/fastai/fastbook" rel="nofollow">The fastai book, published as Jupyter Notebooks</a></li>
<li><a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/" rel="nofollow">Deep Learning for Coders with fastai and PyTorch</a></li>
<li><a href="https://www.youtube.com/watch?v=t4kyRyKyOpo" rel="nofollow">The wonderful and terrifying implications of computers that can learn</a> -- Jeremy&#39; awesome TED talk!</li>
<li><a href="https://marshallbrain.com/manna" rel="nofollow">Manna</a>  by Marshall Brain</li>
<li><a href="https://ghostwork.info/" rel="nofollow">Ghost Work</a> by Mary L. Gray and Siddharth Suri</li>
<li><a href="https://www.ucpress.edu/book/9780520324800/uberland" rel="nofollow">Uberland</a> by Alex Rosenblat</li>
</ul>]]>
      </content:encoded>
      <itunes:summary>
        <![CDATA[<p>Jeremy Howard is a data scientist, researcher, developer, educator, and entrepreneur. Jeremy is a founding researcher at fast.ai, a research institute dedicated to making deep learning more accessible. He is also a Distinguished Research Scientist at the University of San Francisco, the chair of WAMRI, and is Chief Scientist at platform.ai.</p>

<p>In this conversation, we’ll be talking about the history of data science, machine learning, and AI, where we’ve come from and where we’re going, how new techniques can be applied to real-world problems, whether it be deep learning to medicine or porting techniques from computer vision to NLP. We’ll also talk about what’s present and what’s missing in the ML skills revolution, what software engineering skills data scientists need to learn, how to cope in a space of such fragmented tooling, and paths for emerging out of the shadow of FAANG. If that’s not enough, we’ll jump into how spreading DS skills around the globe involves serious investments in education, building software, communities, and research, along with diving into the social challenges that the information age and the AI revolution (so to speak) bring with it.</p>

<p>But to get to all of this, you’ll need to listen to a few minutes of us chatting about chocolate biscuits in Australia!</p>

<p><strong>Links</strong></p>

<ul>
<li><a href="https://www.fast.ai/" target="_blank">fast.ai · making neural nets uncool again</a></li>
<li><a href="https://github.com/fastai/nbdev" rel="nofollow">nbdev: create delightful python projects using Jupyter Notebooks</a></li>
<li><a href="https://github.com/fastai/fastbook" rel="nofollow">The fastai book, published as Jupyter Notebooks</a></li>
<li><a href="https://www.oreilly.com/library/view/deep-learning-for/9781492045519/" rel="nofollow">Deep Learning for Coders with fastai and PyTorch</a></li>
<li><a href="https://www.youtube.com/watch?v=t4kyRyKyOpo" rel="nofollow">The wonderful and terrifying implications of computers that can learn</a> -- Jeremy&#39; awesome TED talk!</li>
<li><a href="https://marshallbrain.com/manna" rel="nofollow">Manna</a>  by Marshall Brain</li>
<li><a href="https://ghostwork.info/" rel="nofollow">Ghost Work</a> by Mary L. Gray and Siddharth Suri</li>
<li><a href="https://www.ucpress.edu/book/9780520324800/uberland" rel="nofollow">Uberland</a> by Alex Rosenblat</li>
</ul>]]>
      </itunes:summary>
      <fireside:playerURL>https://fireside.fm/player/v2/iFNgGGYw+rIFACPbj</fireside:playerURL>
      <fireside:playerEmbedCode>
        <![CDATA[<iframe src="https://fireside.fm/player/v2/iFNgGGYw+rIFACPbj" width="740" height="200" frameborder="0" scrolling="no">]]>
      </fireside:playerEmbedCode>
      <podcast:person email="" href="http://hugobowne.github.io/" role="host">Hugo Bowne-Anderson</podcast:person>
    </item>
  </channel>
</rss>
