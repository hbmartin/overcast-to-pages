<?xml version="1.0" encoding="UTF-8" ?>
<?xml-stylesheet href="https://feeds.buzzsprout.com/styles.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:podcast="https://podcastindex.org/namespace/1.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:psc="http://podlove.org/simple-chapters" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
  <atom:link href="https://feeds.buzzsprout.com/1947123.rss" rel="self" type="application/rss+xml" />
  <atom:link href="https://pubsubhubbub.appspot.com/" rel="hub" xmlns="http://www.w3.org/2005/Atom" />
  <title>The AutoML Podcast</title>

  <lastBuildDate>Fri, 07 Mar 2025 11:36:02 -0500</lastBuildDate>
  <link>http://www.automlpodcast.com</link>
  <language>en-us</language>
  <copyright>© 2025 The AutoML Podcast</copyright>
  <podcast:locked>yes</podcast:locked>
    <podcast:guid>fcaf0ee2-802f-53fd-be8c-6c43c18cfc2c</podcast:guid>
  <itunes:author>AutoML Media</itunes:author>
  <itunes:type>episodic</itunes:type>
  <itunes:explicit>false</itunes:explicit>
  <description><![CDATA[A show about the science and engineering behind AutoML.]]></description>
  <generator>Buzzsprout (https://www.buzzsprout.com)</generator>
  <itunes:owner>
    <itunes:name>AutoML Media</itunes:name>
  </itunes:owner>
  <image>
     <url>https://storage.buzzsprout.com/vkvhh9j0i9oxsaln4530of0dz20y?.jpg</url>
     <title>The AutoML Podcast</title>
     <link>http://www.automlpodcast.com</link>
  </image>
  <itunes:image href="https://storage.buzzsprout.com/vkvhh9j0i9oxsaln4530of0dz20y?.jpg" />
  <itunes:category text="Technology" />
  <item>
    <itunes:title>Nyckel - Building an AutoML Startup</itunes:title>
    <title>Nyckel - Building an AutoML Startup</title>
    <itunes:summary><![CDATA[Oscar Beijbom is talking about what it's like to run an AutoML startup: Nyckel. Beyond that, we chat about the differences between academia and industry, what truly matters in application and more. Check out Nyckel at: https://www.nyckel.com/ ]]></itunes:summary>
    <description><![CDATA[<p>Oscar Beijbom is talking about what it&apos;s like to run an AutoML startup: Nyckel. Beyond that, we chat about the differences between academia and industry, what truly matters in application and more.</p><p>Check out Nyckel at: https://www.nyckel.com/</p>]]></description>
    <content:encoded><![CDATA[<p>Oscar Beijbom is talking about what it&apos;s like to run an AutoML startup: Nyckel. Beyond that, we chat about the differences between academia and industry, what truly matters in application and more.</p><p>Check out Nyckel at: https://www.nyckel.com/</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/16752060-nyckel-building-an-automl-startup.mp3" length="58344673" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-16752060</guid>
    <pubDate>Fri, 07 Mar 2025 11:00:00 -0500</pubDate>
    <podcast:transcript url="https://www.buzzsprout.com/1947123/16752060/transcript" type="text/html" />
    <itunes:duration>4859</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Neural Architecture Search: Insights from 1000 Papers</itunes:title>
    <title>Neural Architecture Search: Insights from 1000 Papers</title>
    <itunes:summary><![CDATA[Colin White, head of research at Abacus AI, takes us on a tour of Neural Architecture Search: its origins, important paradigms and the future of NAS in the age of LLMs. If you're looking for a broad overview of NAS, this is the podcast for you! ]]></itunes:summary>
    <description><![CDATA[<p>Colin White, head of research at Abacus AI, takes us on a tour of Neural Architecture Search: its origins, important paradigms and the future of NAS in the age of LLMs. If you&apos;re looking for a broad overview of NAS, this is the podcast for you!</p>]]></description>
    <content:encoded><![CDATA[<p>Colin White, head of research at Abacus AI, takes us on a tour of Neural Architecture Search: its origins, important paradigms and the future of NAS in the age of LLMs. If you&apos;re looking for a broad overview of NAS, this is the podcast for you!</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/16214329-neural-architecture-search-insights-from-1000-papers.mp3" length="54565522" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-16214329</guid>
    <pubDate>Tue, 03 Dec 2024 09:00:00 -0500</pubDate>
    <podcast:transcript url="https://www.buzzsprout.com/1947123/16214329/transcript" type="text/html" />
    <itunes:duration>4544</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How</itunes:title>
    <title>Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How</title>
    <itunes:summary><![CDATA[There are so many great foundation models in many different domains - but how do you choose one for your specific problem? And how can you best finetune it? Sebastian Pineda has an answer: Quicktune can help select the best model and tune it for specific use cases. Listen to find out when this will be a Huggingface feature and if hyperparameter optimization is even important in finetuning models (spoiler: very much so)! ]]></itunes:summary>
    <description><![CDATA[<p>There are so many great foundation models in many different domains - but how do you choose one for your specific problem? And how can you best finetune it? Sebastian Pineda has an answer: Quicktune can help select the best model and tune it for specific use cases. Listen to find out when this will be a Huggingface feature and if hyperparameter optimization is even important in finetuning models (spoiler: very much so)!</p>]]></description>
    <content:encoded><![CDATA[<p>There are so many great foundation models in many different domains - but how do you choose one for your specific problem? And how can you best finetune it? Sebastian Pineda has an answer: Quicktune can help select the best model and tune it for specific use cases. Listen to find out when this will be a Huggingface feature and if hyperparameter optimization is even important in finetuning models (spoiler: very much so)!</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/15551866-quick-tune-quickly-learning-which-pretrained-model-to-finetune-and-how.mp3" length="38235057" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-15551866</guid>
    <pubDate>Thu, 08 Aug 2024 11:00:00 -0400</pubDate>
    <itunes:duration>3184</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Discovering Temporally-Aware Reinforcement Learning Algorithms</itunes:title>
    <title>Discovering Temporally-Aware Reinforcement Learning Algorithms</title>
    <itunes:summary><![CDATA[Designing algorithms by hand is hard, so Chris Lu and Matthew Jackson talk about how to meta-learn them for reinforcement learning.  Many of the concepts in this episode are interesting to meta-learning approaches as a whole, though: "how expressive can we be and still perform well?", "how can we get the necessary data to generalize?" and "how do we make the resulting algorithm easy to apply in practice?" are problems that come up for any learning-based approach to AutoML and some of the...]]></itunes:summary>
    <description><![CDATA[<p>Designing algorithms by hand is hard, so Chris Lu and Matthew Jackson talk about how to meta-learn them for reinforcement learning.  Many of the concepts in this episode are interesting to meta-learning approaches as a whole, though: &quot;how expressive can we be and still perform well?&quot;, &quot;how can we get the necessary data to generalize?&quot; and &quot;how do we make the resulting algorithm easy to apply in practice?&quot; are problems that come up for any learning-based approach to AutoML and some of the topics we dive into.</p>]]></description>
    <content:encoded><![CDATA[<p>Designing algorithms by hand is hard, so Chris Lu and Matthew Jackson talk about how to meta-learn them for reinforcement learning.  Many of the concepts in this episode are interesting to meta-learning approaches as a whole, though: &quot;how expressive can we be and still perform well?&quot;, &quot;how can we get the necessary data to generalize?&quot; and &quot;how do we make the resulting algorithm easy to apply in practice?&quot; are problems that come up for any learning-based approach to AutoML and some of the topics we dive into.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/15303231-discovering-temporally-aware-reinforcement-learning-algorithms.mp3" length="36933514" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-15303231</guid>
    <pubDate>Mon, 24 Jun 2024 12:00:00 -0400</pubDate>
    <itunes:duration>3075</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>X Hacking: The Threat of Misguided AutoML</itunes:title>
    <title>X Hacking: The Threat of Misguided AutoML</title>
    <itunes:summary><![CDATA[AutoML can be a tool for good, but there are pitfalls along the way. Rahul Sharma and David Selby tell us about how AutoML systems can be used to give us false impressions about explainability metrics of ML systems - maliciously, but also on accident. While this episode isn't talking about a new exciting AutoML method, it can tell us a lot about what can go wrong in applying AutoML and what we should think about when we build tools for ML novices to use. ]]></itunes:summary>
    <description><![CDATA[<p>AutoML can be a tool for good, but there are pitfalls along the way. Rahul Sharma and David Selby tell us about how AutoML systems can be used to give us false impressions about explainability metrics of ML systems - maliciously, but also on accident. While this episode isn&apos;t talking about a new exciting AutoML method, it can tell us a lot about what can go wrong in applying AutoML and what we should think about when we build tools for ML novices to use.</p>]]></description>
    <content:encoded><![CDATA[<p>AutoML can be a tool for good, but there are pitfalls along the way. Rahul Sharma and David Selby tell us about how AutoML systems can be used to give us false impressions about explainability metrics of ML systems - maliciously, but also on accident. While this episode isn&apos;t talking about a new exciting AutoML method, it can tell us a lot about what can go wrong in applying AutoML and what we should think about when we build tools for ML novices to use.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/15144192-x-hacking-the-threat-of-misguided-automl.mp3" length="39482576" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-15144192</guid>
    <pubDate>Mon, 27 May 2024 00:00:00 -0400</pubDate>
    <itunes:duration>3288</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Introduction To New Co-Host, Theresa Eimer</itunes:title>
    <title>Introduction To New Co-Host, Theresa Eimer</title>
    <itunes:summary><![CDATA[In today's episode, we're introducing the very special Theresa Eimer to the show.   Theresa will be taking over the hosting of many of the future episodes. Theresa has already recorded multiple episodes and we are stoked to air those shortly.  We also spend a few moments explaining my relative absence in the last few months (since the war in the middle east erupted) and what I'm up to now.  Theresa, we are all so excited to be doing this together!  To learn more about Theresa, Follow her on T...]]></itunes:summary>
    <description><![CDATA[<p>In today&apos;s episode, we&apos;re introducing the very special Theresa Eimer to the show. <br/><br/>Theresa will be taking over the hosting of many of the future episodes. Theresa has already recorded multiple episodes and we are stoked to air those shortly.<br/><br/>We also spend a few moments explaining my relative absence in the last few months (since the war in the middle east erupted) and what I&apos;m up to now.<br/><br/>Theresa, we are all so excited to be doing this together!<br/><br/>To learn more about Theresa,<br/>Follow her on Twitter here: <a href='https://twitter.com/The_Eimer'>https://twitter.com/The_Eimer</a><br/>Connect with her on LinkedIn here: <a href='https://www.linkedin.com/in/theresa-eimer-a724b5b0/'>https://www.linkedin.com/in/theresa-eimer-a724b5b0/</a><br/><br/>As you&apos;ll hear in the episode, she&apos;s also one of the co-organizers of COSEAL, which you can learn more about here: <a href='https://www.coseal.net/'>https://www.coseal.net/</a></p>]]></description>
    <content:encoded><![CDATA[<p>In today&apos;s episode, we&apos;re introducing the very special Theresa Eimer to the show. <br/><br/>Theresa will be taking over the hosting of many of the future episodes. Theresa has already recorded multiple episodes and we are stoked to air those shortly.<br/><br/>We also spend a few moments explaining my relative absence in the last few months (since the war in the middle east erupted) and what I&apos;m up to now.<br/><br/>Theresa, we are all so excited to be doing this together!<br/><br/>To learn more about Theresa,<br/>Follow her on Twitter here: <a href='https://twitter.com/The_Eimer'>https://twitter.com/The_Eimer</a><br/>Connect with her on LinkedIn here: <a href='https://www.linkedin.com/in/theresa-eimer-a724b5b0/'>https://www.linkedin.com/in/theresa-eimer-a724b5b0/</a><br/><br/>As you&apos;ll hear in the episode, she&apos;s also one of the co-organizers of COSEAL, which you can learn more about here: <a href='https://www.coseal.net/'>https://www.coseal.net/</a></p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/15019846-introduction-to-new-co-host-theresa-eimer.mp3" length="10073157" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-15019846</guid>
    <pubDate>Sun, 26 May 2024 23:00:00 -0400</pubDate>
    <itunes:duration>837</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>AutoGluon: The Story</itunes:title>
    <title>AutoGluon: The Story</title>
    <itunes:summary><![CDATA[Today we're talking with Nick Erickson from AutoGluon.  We discuss AutoGluon's fascinating origin story, its unique point of view, the science and engineering behind some of its unique contributions, Amazon's Machine Learning University, AutoGluon's multi-layer stack ensembler in all its detail, their feature preprocessing pipeline, their feature type inference, their adaptive approach to early stopping, controlling for inference speeds, the different multi-modal architectures, the ML culture...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re talking with Nick Erickson from AutoGluon.<br/><br/>We discuss AutoGluon&apos;s fascinating origin story, its unique point of view, the science and engineering behind some of its unique contributions, Amazon&apos;s Machine Learning University, AutoGluon&apos;s multi-layer stack ensembler in all its detail, their feature preprocessing pipeline, their feature type inference, their adaptive approach to early stopping, controlling for inference speeds, the different multi-modal architectures, the ML culture at Amazon, the unique challenges of time series, the role of competitions, the decision to reject hyperparameter optimization, benchmarking in AutoML, what the research community can do to help industry along, AutoGluon&apos;s relationship with pre-trained tabular models like Tab-PFN, whether the rise of LLMs is likely to affect AutoGluon, what&apos;s stopping more people from adopting AutoML solutions, AutoGluon Cloud, the dream and reality of an auto-benchmarking tool, how to contribute to their project, and many, many other topics.<br/><br/>This was one of my favorite episodes. Nick, thank you for joining!<br/><br/>You can follow Nick on Twitter here: <a href='https://twitter.com/innixma'>@innixma</a>.<br/>And you can follow AutoGluon on GitHub here: <a href='https://github.com/autogluon'>https://github.com/autogluon</a>.<br/><br/>Some more resources on AutoGluon:</p><ul><li>The original AutoGluon Paper: &quot;AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data&quot;: <a href='https://arxiv.org/abs/2003.06505'>https://arxiv.org/abs/2003.06505</a></li><li>AutoML Fall School 2022 AutoGluon presentation, a good way to understand the philosophy behind AutoGluon: <a href='https://www.youtube.com/watch?v=VAAITEds-28'>https://www.youtube.com/watch?v=VAAITEds-28</a></li><li>AutoGluon multi-modal paper: <a href='https://dl.acm.org/doi/abs/10.1145/3534678.3542616'>https://dl.acm.org/doi/abs/10.1145/3534678.3542616</a></li></ul>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re talking with Nick Erickson from AutoGluon.<br/><br/>We discuss AutoGluon&apos;s fascinating origin story, its unique point of view, the science and engineering behind some of its unique contributions, Amazon&apos;s Machine Learning University, AutoGluon&apos;s multi-layer stack ensembler in all its detail, their feature preprocessing pipeline, their feature type inference, their adaptive approach to early stopping, controlling for inference speeds, the different multi-modal architectures, the ML culture at Amazon, the unique challenges of time series, the role of competitions, the decision to reject hyperparameter optimization, benchmarking in AutoML, what the research community can do to help industry along, AutoGluon&apos;s relationship with pre-trained tabular models like Tab-PFN, whether the rise of LLMs is likely to affect AutoGluon, what&apos;s stopping more people from adopting AutoML solutions, AutoGluon Cloud, the dream and reality of an auto-benchmarking tool, how to contribute to their project, and many, many other topics.<br/><br/>This was one of my favorite episodes. Nick, thank you for joining!<br/><br/>You can follow Nick on Twitter here: <a href='https://twitter.com/innixma'>@innixma</a>.<br/>And you can follow AutoGluon on GitHub here: <a href='https://github.com/autogluon'>https://github.com/autogluon</a>.<br/><br/>Some more resources on AutoGluon:</p><ul><li>The original AutoGluon Paper: &quot;AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data&quot;: <a href='https://arxiv.org/abs/2003.06505'>https://arxiv.org/abs/2003.06505</a></li><li>AutoML Fall School 2022 AutoGluon presentation, a good way to understand the philosophy behind AutoGluon: <a href='https://www.youtube.com/watch?v=VAAITEds-28'>https://www.youtube.com/watch?v=VAAITEds-28</a></li><li>AutoGluon multi-modal paper: <a href='https://dl.acm.org/doi/abs/10.1145/3534678.3542616'>https://dl.acm.org/doi/abs/10.1145/3534678.3542616</a></li></ul>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/13530251-autogluon-the-story.mp3" length="139210910" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-13530251</guid>
    <pubDate>Tue, 05 Sep 2023 09:00:00 -0400</pubDate>
    <itunes:duration>11598</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How to Integrate Logic and Argumentation into Human-Centric AutoML</itunes:title>
    <title>How to Integrate Logic and Argumentation into Human-Centric AutoML</title>
    <itunes:summary><![CDATA[Today we're talking with Joseph Giovanelli about his work on integrating logic and argumentation into AutoML systems.  Joseph is a PhD student at the University of Bologna. He was more recently in Hannover working on ethics and fairness with Marius’ team. The paper he published presents his framework, HAMLET, which stands for Human-centric AutoML via Logic and Argumentation. It allows a user to iteratively specify constraints in a formal manner and, once defined, those constraints become logi...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re talking with Joseph Giovanelli about his work on integrating logic and argumentation into AutoML systems.<br/><br/>Joseph is a PhD student at the University of Bologna. He was more recently in Hannover working on ethics and fairness with Marius’ team.</p><p>The paper he published presents his framework, HAMLET, which stands for Human-centric AutoML via Logic and Argumentation. It allows a user to iteratively specify constraints in a formal manner and, once defined, those constraints become logical premises. Those premises, when combined together, can produce conflicts with one another, thereby reducing the search space and providing deeper intuition back to the user.<br/><br/>To learn more about HAMLET, see the paper here: <a href='https://ceur-ws.org/Vol-3135/dataplat_short2.pdf'>https://ceur-ws.org/Vol-3135/dataplat_short2.pdf</a> and the repo here: <a href='https://github.com/QueueInc/HAMLET'>https://github.com/QueueInc/HAMLET</a><br/><br/>To follow Joseph on LinkedIn, see his profile here: <a href='https://www.linkedin.com/in/joseph-giovanelli/'>https://www.linkedin.com/in/joseph-giovanelli/</a><br/><br/></p>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re talking with Joseph Giovanelli about his work on integrating logic and argumentation into AutoML systems.<br/><br/>Joseph is a PhD student at the University of Bologna. He was more recently in Hannover working on ethics and fairness with Marius’ team.</p><p>The paper he published presents his framework, HAMLET, which stands for Human-centric AutoML via Logic and Argumentation. It allows a user to iteratively specify constraints in a formal manner and, once defined, those constraints become logical premises. Those premises, when combined together, can produce conflicts with one another, thereby reducing the search space and providing deeper intuition back to the user.<br/><br/>To learn more about HAMLET, see the paper here: <a href='https://ceur-ws.org/Vol-3135/dataplat_short2.pdf'>https://ceur-ws.org/Vol-3135/dataplat_short2.pdf</a> and the repo here: <a href='https://github.com/QueueInc/HAMLET'>https://github.com/QueueInc/HAMLET</a><br/><br/>To follow Joseph on LinkedIn, see his profile here: <a href='https://www.linkedin.com/in/joseph-giovanelli/'>https://www.linkedin.com/in/joseph-giovanelli/</a><br/><br/></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/13107433-how-to-integrate-logic-and-argumentation-into-human-centric-automl.mp3" length="31100251" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-13107433</guid>
    <pubDate>Mon, 26 Jun 2023 04:00:00 -0400</pubDate>
    <itunes:duration>2589</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How to Design an AutoML System using Error Decomposition</itunes:title>
    <title>How to Design an AutoML System using Error Decomposition</title>
    <itunes:summary><![CDATA[Today we're talking with Caitlin Owen, a post-doc at the University of Otago about her work on error decomposition.  She recently published a paper titled "Towards Explainable AutoML Using Error Decomposition" about how a more granular view of the components of error can lead the construction of better AutoML systems.   Read her paper here: https://link.springer.com/chapter/10.1007/978-3-031-22695-3_13 Follow her on Twitter here: @CaitAshfordOwen Connect with her on LinkedIn here: https://www...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re talking with Caitlin Owen, a post-doc at the University of Otago about her work on error decomposition.<br/><br/>She recently published a paper titled &quot;Towards Explainable AutoML Using Error Decomposition&quot; about how a more granular view of the components of error can lead the construction of better AutoML systems. <br/><br/>Read her paper here: <a href='https://link.springer.com/chapter/10.1007/978-3-031-22695-3_13'>https://link.springer.com/chapter/10.1007/978-3-031-22695-3_13</a><br/>Follow her on Twitter here: <a href='https://twitter.com/CaitAshfordOwen'>@CaitAshfordOwen</a><br/>Connect with her on LinkedIn here: <a href='https://www.linkedin.com/in/caitlin-owen-5b9b08193/'>https://www.linkedin.com/in/caitlin-owen-5b9b08193/</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re talking with Caitlin Owen, a post-doc at the University of Otago about her work on error decomposition.<br/><br/>She recently published a paper titled &quot;Towards Explainable AutoML Using Error Decomposition&quot; about how a more granular view of the components of error can lead the construction of better AutoML systems. <br/><br/>Read her paper here: <a href='https://link.springer.com/chapter/10.1007/978-3-031-22695-3_13'>https://link.springer.com/chapter/10.1007/978-3-031-22695-3_13</a><br/>Follow her on Twitter here: <a href='https://twitter.com/CaitAshfordOwen'>@CaitAshfordOwen</a><br/>Connect with her on LinkedIn here: <a href='https://www.linkedin.com/in/caitlin-owen-5b9b08193/'>https://www.linkedin.com/in/caitlin-owen-5b9b08193/</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12972915-how-to-design-an-automl-system-using-error-decomposition.mp3" length="20901191" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12972915</guid>
    <pubDate>Sat, 03 Jun 2023 21:00:00 -0400</pubDate>
    <itunes:duration>1739</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>The Semantic Layer and AutoML</itunes:title>
    <title>The Semantic Layer and AutoML</title>
    <itunes:summary><![CDATA[Today we're talking with Gaurav Rao, the EVP &amp; GM of Machine Learning and AI at AtScale, a company centered around the semantic layer.  For some time now, I've been feeling that there is a deep connection between a formal articulation of business context and the realization of the dream of AutoML, so I searched for people in the space who can help shine light on this direction.  Gaurav is one of the few who can speak about this. As you'll hear, he's extremely pedagogic and he's walking us...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re talking with Gaurav Rao, the EVP &amp; GM of Machine Learning and AI at AtScale, a company centered around the semantic layer.<br/><br/>For some time now, I&apos;ve been feeling that there is a deep connection between a formal articulation of business context and the realization of the dream of AutoML, so I searched for people in the space who can help shine light on this direction.<br/><br/>Gaurav is one of the few who can speak about this. As you&apos;ll hear, he&apos;s extremely pedagogic and he&apos;s walking us through the origins of the concept, how it addresses some of the challenges that businesses face when trying to operationalize their ML, what it takes to build a universal semantic layer, how downstream ML applications are affected by the presence or absence of a semantic layer, and how the space of AutoML factors into this.<br/><br/>Connect his Gaurav and learn more about the semantic layer through his LinkedIn: <a href='https://www.linkedin.com/in/gauravraotechenthusiast/'>https://www.linkedin.com/in/gauravraotechenthusiast/</a><br/><br/></p>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re talking with Gaurav Rao, the EVP &amp; GM of Machine Learning and AI at AtScale, a company centered around the semantic layer.<br/><br/>For some time now, I&apos;ve been feeling that there is a deep connection between a formal articulation of business context and the realization of the dream of AutoML, so I searched for people in the space who can help shine light on this direction.<br/><br/>Gaurav is one of the few who can speak about this. As you&apos;ll hear, he&apos;s extremely pedagogic and he&apos;s walking us through the origins of the concept, how it addresses some of the challenges that businesses face when trying to operationalize their ML, what it takes to build a universal semantic layer, how downstream ML applications are affected by the presence or absence of a semantic layer, and how the space of AutoML factors into this.<br/><br/>Connect his Gaurav and learn more about the semantic layer through his LinkedIn: <a href='https://www.linkedin.com/in/gauravraotechenthusiast/'>https://www.linkedin.com/in/gauravraotechenthusiast/</a><br/><br/></p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12847413-the-semantic-layer-and-automl.mp3" length="41511618" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12847413</guid>
    <pubDate>Tue, 16 May 2023 10:00:00 -0400</pubDate>
    <itunes:duration>3457</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Foundation Models: The term and its origins</itunes:title>
    <title>Foundation Models: The term and its origins</title>
    <itunes:summary><![CDATA[Today Ankush Garg is speaking with Rishi Bommasani, PhD student at Stanford and one of the originator of the term Foundation Models.  They’re talking about the origins of the term Foundation Model, which he and his group advanced, in the paper "On the Opportunities and Risks of Foundation Models". They’ll talk about self-supervision, issues of scale, the motivation behind the terminology, the origins of the Research for Foundation Models Institute at Stanford, outcome homogenization, emergenc...]]></itunes:summary>
    <description><![CDATA[<p>Today Ankush Garg is speaking with Rishi Bommasani, PhD student at Stanford and one of the originator of the term Foundation Models.<br/><br/>They’re talking about the origins of the term Foundation Model, which he and his group advanced, in the paper &quot;On the Opportunities and Risks of Foundation Models&quot;.</p><p>They’ll talk about self-supervision, issues of scale, the motivation behind the terminology, the origins of the Research for Foundation Models Institute at Stanford, outcome homogenization, emergence and phase transitions, and some of the social consequences to look out for.</p><p>Thank you both for this conversation. As the world is coming to terms with GPT-4, this will be increasingly relevant.</p><p>Paper: <a href='https://arxiv.org/abs/2108.07258'>On the Opportunities and Risks of Foundation Models.</a><br/>Rishi&apos;s twitter: <a href='https://twitter.com/RishiBommasani'>@RishiBommasani</a><br/><a href='https://crfm.stanford.edu/'>Center for Research on Foundation Models</a> (CRFM)</p>]]></description>
    <content:encoded><![CDATA[<p>Today Ankush Garg is speaking with Rishi Bommasani, PhD student at Stanford and one of the originator of the term Foundation Models.<br/><br/>They’re talking about the origins of the term Foundation Model, which he and his group advanced, in the paper &quot;On the Opportunities and Risks of Foundation Models&quot;.</p><p>They’ll talk about self-supervision, issues of scale, the motivation behind the terminology, the origins of the Research for Foundation Models Institute at Stanford, outcome homogenization, emergence and phase transitions, and some of the social consequences to look out for.</p><p>Thank you both for this conversation. As the world is coming to terms with GPT-4, this will be increasingly relevant.</p><p>Paper: <a href='https://arxiv.org/abs/2108.07258'>On the Opportunities and Risks of Foundation Models.</a><br/>Rishi&apos;s twitter: <a href='https://twitter.com/RishiBommasani'>@RishiBommasani</a><br/><a href='https://crfm.stanford.edu/'>Center for Research on Foundation Models</a> (CRFM)</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12748084-foundation-models-the-term-and-its-origins.mp3" length="50651604" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12748084</guid>
    <pubDate>Sat, 29 Apr 2023 17:00:00 -0400</pubDate>
    <itunes:duration>4218</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>The Business and Engineering of AutoML Products with Raymond Peck</itunes:title>
    <title>The Business and Engineering of AutoML Products with Raymond Peck</title>
    <itunes:summary><![CDATA[Today we're talking with Raymond Peck, a senior engineer and director in the AutoML space. He spent time at H2O, dotData, Alteryx and many other places.  This is a fascinating conversation about the business, engineering, and science of machine learning automation in production. Learning about his experience is crucial for understanding the biography of the space.  We discuss the early motivations behind AutoML, the initial value propositions that propelled the first movers in the market, the...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re talking with Raymond Peck, a senior engineer and director in the AutoML space. He spent time at H2O, dotData, Alteryx and many other places.<br/><br/>This is a fascinating conversation about the business, engineering, and science of machine learning automation in production. Learning about his experience is crucial for understanding the biography of the space.<br/><br/>We discuss the early motivations behind AutoML, the initial value propositions that propelled the first movers in the market, the market dynamics that operated in the early days, the evolution of the relevant engineering and science, how customers evaluate AutoML tools, the role of feature engineering and relational tables, the crucial role that explainability plays in AutoML, and many more topics.</p><p>Raymond is a prolific writer on LinkedIn. You should follow him here: <a href='https://www.linkedin.com/in/raymondpeck/'>https://www.linkedin.com/in/raymondpeck/</a>.</p>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re talking with Raymond Peck, a senior engineer and director in the AutoML space. He spent time at H2O, dotData, Alteryx and many other places.<br/><br/>This is a fascinating conversation about the business, engineering, and science of machine learning automation in production. Learning about his experience is crucial for understanding the biography of the space.<br/><br/>We discuss the early motivations behind AutoML, the initial value propositions that propelled the first movers in the market, the market dynamics that operated in the early days, the evolution of the relevant engineering and science, how customers evaluate AutoML tools, the role of feature engineering and relational tables, the crucial role that explainability plays in AutoML, and many more topics.</p><p>Raymond is a prolific writer on LinkedIn. You should follow him here: <a href='https://www.linkedin.com/in/raymondpeck/'>https://www.linkedin.com/in/raymondpeck/</a>.</p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12602017-the-business-and-engineering-of-automl-products-with-raymond-peck.mp3" length="87763566" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12602017</guid>
    <pubDate>Thu, 06 Apr 2023 13:00:00 -0400</pubDate>
    <itunes:duration>7311</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>TabPFN: A Revolution in AutoML?</itunes:title>
    <title>TabPFN: A Revolution in AutoML?</title>
    <itunes:summary><![CDATA[Today we’re talking to Noah Hollmann and Samuel Muller about their paper on TabPFN - which is an incredible spin on AutoML based on Bayesian inference and transformers.  [Quick note on audio quality]: Some of the tracks have not recorded perfectly but I felt that the content there was too important not to release. Sorry for any ear-strain! In the episode, we spend some time discussing posterior predictive probabilities before discussing how exactly they’ve pre-fitted their network, how they g...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re talking to Noah Hollmann and Samuel Muller about their paper on TabPFN - which is an incredible spin on AutoML based on Bayesian inference and transformers.<br/><br/>[Quick note on audio quality]: Some of the tracks have not recorded perfectly but I felt that the content there was too important not to release. Sorry for any ear-strain!</p><p>In the episode, we spend some time discussing posterior predictive probabilities before discussing how exactly they’ve pre-fitted their network, how they got their training data, what the network looks like, and how the system is performing.</p><p><br/>To give you a taste of it, on datasets up to 1,000 training instances and 100 features, it takes less than a second to train and predict a classifier!<br/><br/>Read their paper here: <a href='https://arxiv.org/pdf/2207.01848.pdf'>https://arxiv.org/pdf/2207.01848.pdf</a><br/><br/>Follow Samuel on Twitter, here: <a href='https://twitter.com/SamuelMullr'>https://twitter.com/SamuelMullr</a><br/><br/>Follow Noah on Twitter, here: <a href='https://twitter.com/noahholl'>https://twitter.com/noahholl</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re talking to Noah Hollmann and Samuel Muller about their paper on TabPFN - which is an incredible spin on AutoML based on Bayesian inference and transformers.<br/><br/>[Quick note on audio quality]: Some of the tracks have not recorded perfectly but I felt that the content there was too important not to release. Sorry for any ear-strain!</p><p>In the episode, we spend some time discussing posterior predictive probabilities before discussing how exactly they’ve pre-fitted their network, how they got their training data, what the network looks like, and how the system is performing.</p><p><br/>To give you a taste of it, on datasets up to 1,000 training instances and 100 features, it takes less than a second to train and predict a classifier!<br/><br/>Read their paper here: <a href='https://arxiv.org/pdf/2207.01848.pdf'>https://arxiv.org/pdf/2207.01848.pdf</a><br/><br/>Follow Samuel on Twitter, here: <a href='https://twitter.com/SamuelMullr'>https://twitter.com/SamuelMullr</a><br/><br/>Follow Noah on Twitter, here: <a href='https://twitter.com/noahholl'>https://twitter.com/noahholl</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12363305-tabpfn-a-revolution-in-automl.mp3" length="55035450" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12363305</guid>
    <pubDate>Thu, 02 Mar 2023 16:00:00 -0500</pubDate>
    <itunes:duration>4584</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How financial institutions manage model risk</itunes:title>
    <title>How financial institutions manage model risk</title>
    <itunes:summary><![CDATA[Today we’re talking to Sean Sexton, the Director of Modeling and Analytics Consulting at KPMG, about the role of models in financial institutions and how the risks associated with them is managed.  This turned out to be an incredibly deep and interesting topic, and we really only scratched the surface of it.  Sean has a unique ability to summarize developments in an entire space. If you're interested to learn more about modeling in financial institutions and about the history of how we got he...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re talking to Sean Sexton, the Director of Modeling and Analytics Consulting at KPMG, about the role of models in financial institutions and how the risks associated with them is managed.<br/><br/>This turned out to be an incredibly deep and interesting topic, and we really only scratched the surface of it.<br/><br/>Sean has a unique ability to summarize developments in an entire space. If you&apos;re interested to learn more about modeling in financial institutions and about the history of how we got here, you should definitely study his dissertation, on managing model risk, here: <a href='https://macsphere.mcmaster.ca/handle/11375/28049'>https://macsphere.mcmaster.ca/handle/11375/28049</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re talking to Sean Sexton, the Director of Modeling and Analytics Consulting at KPMG, about the role of models in financial institutions and how the risks associated with them is managed.<br/><br/>This turned out to be an incredibly deep and interesting topic, and we really only scratched the surface of it.<br/><br/>Sean has a unique ability to summarize developments in an entire space. If you&apos;re interested to learn more about modeling in financial institutions and about the history of how we got here, you should definitely study his dissertation, on managing model risk, here: <a href='https://macsphere.mcmaster.ca/handle/11375/28049'>https://macsphere.mcmaster.ca/handle/11375/28049</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12200637-how-financial-institutions-manage-model-risk.mp3" length="52021468" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12200637</guid>
    <pubDate>Tue, 07 Feb 2023 03:00:00 -0500</pubDate>
    <itunes:duration>4333</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How to solve dynamical systems by fusing data and mechanism</itunes:title>
    <title>How to solve dynamical systems by fusing data and mechanism</title>
    <itunes:summary><![CDATA[Today we’re talking to Matt Levine. Matt is a PhD student in computing and mathematical sciences at Caltech, and he focuses on improving the prediction and inference of physical systems by blending together both mechanistic modeling and  machine learning.  This episode is one of my favorites: we go pretty deep into dynamical systems, and into Matt's new framework for solving them by blending traditional, mechanistic, approaches with machine learning. This is a fascinating use of machine ...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re talking to Matt Levine. Matt is a PhD student in computing and mathematical sciences at Caltech, and he focuses on improving the prediction and inference of physical systems by blending together both mechanistic modeling and  machine learning.<br/><br/>This episode is one of my favorites: we go pretty deep into dynamical systems, and into Matt&apos;s new framework for solving them by blending traditional, mechanistic, approaches with machine learning. This is a fascinating use of machine learning, and hopefully gets us one step closer to the automation of science, in general.<br/><br/></p><p><a href='https://arxiv.org/abs/2107.06658'>A Framework for Machine Learning of Model Error in Dynamical Systems</a> - https://arxiv.org/abs/2107.06658<br/><br/><b>Related works</b><br/><a href='https://epubs.siam.org/doi/abs/10.1137/21M1434477'>Autodifferentiable Ensemble Kalman Filters</a> - https://epubs.siam.org/doi/abs/10.1137/21M1434477<br/><br/><a href='https://arxiv.org/abs/2001.04385'>Universal Differential Equations for Scientific Machine Learning</a> - https://arxiv.org/abs/2001.04385<br/><br/><a href='https://ieeexplore.ieee.org/document/366006'>Continuous-time nonlinear signal processing: a neural network based approach for gray box identification</a> - https://ieeexplore.ieee.org/document/366006<br/><br/><a href='https://www.sciencedirect.com/science/article/abs/pii/S0098135496003365'>A generalised approach to process state estimation using hybrid artificial neural network/mechanistic models</a> - https://www.sciencedirect.com/science/article/abs/pii/S0098135496003365<br/><br/></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re talking to Matt Levine. Matt is a PhD student in computing and mathematical sciences at Caltech, and he focuses on improving the prediction and inference of physical systems by blending together both mechanistic modeling and  machine learning.<br/><br/>This episode is one of my favorites: we go pretty deep into dynamical systems, and into Matt&apos;s new framework for solving them by blending traditional, mechanistic, approaches with machine learning. This is a fascinating use of machine learning, and hopefully gets us one step closer to the automation of science, in general.<br/><br/></p><p><a href='https://arxiv.org/abs/2107.06658'>A Framework for Machine Learning of Model Error in Dynamical Systems</a> - https://arxiv.org/abs/2107.06658<br/><br/><b>Related works</b><br/><a href='https://epubs.siam.org/doi/abs/10.1137/21M1434477'>Autodifferentiable Ensemble Kalman Filters</a> - https://epubs.siam.org/doi/abs/10.1137/21M1434477<br/><br/><a href='https://arxiv.org/abs/2001.04385'>Universal Differential Equations for Scientific Machine Learning</a> - https://arxiv.org/abs/2001.04385<br/><br/><a href='https://ieeexplore.ieee.org/document/366006'>Continuous-time nonlinear signal processing: a neural network based approach for gray box identification</a> - https://ieeexplore.ieee.org/document/366006<br/><br/><a href='https://www.sciencedirect.com/science/article/abs/pii/S0098135496003365'>A generalised approach to process state estimation using hybrid artificial neural network/mechanistic models</a> - https://www.sciencedirect.com/science/article/abs/pii/S0098135496003365<br/><br/></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/12024520-how-to-solve-dynamical-systems-by-fusing-data-and-mechanism.mp3" length="50148518" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-12024520</guid>
    <pubDate>Thu, 12 Jan 2023 07:00:00 -0500</pubDate>
    <itunes:duration>4176</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>DASH: How to Search Over Convolutions</itunes:title>
    <title>DASH: How to Search Over Convolutions</title>
    <itunes:summary><![CDATA[Today we’re chatting with Junhong Shen, a PhD student at Carnegie Mellon.  Junhong and her team are working on the generalizability of NAS algorithms across a diverse set of tasks.  Today we'll be talking about DASH, a NAS algorithm that takes diversity of tasks at its center. In order to implement DASH, Junhong and her team implemented three clever ideas that she'll share with us.  Efficient Architecture Search for Diverse Tasks - https://arxiv.org/pdf/2204.07554.pdf  Tackling Diverse Tasks ...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re chatting with Junhong Shen, a PhD student at Carnegie Mellon.<br/><br/>Junhong and her team are working on the generalizability of NAS algorithms across a diverse set of tasks.<br/><br/>Today we&apos;ll be talking about DASH, a NAS algorithm that takes diversity of tasks at its center. In order to implement DASH, Junhong and her team implemented three clever ideas that she&apos;ll share with us.<br/><br/>Efficient Architecture Search for Diverse Tasks - <a href='https://arxiv.org/pdf/2204.07554.pdf'>https://arxiv.org/pdf/2204.07554.pdf</a><br/><br/>Tackling Diverse Tasks with Neural Architecture Search - <a href='https://blog.ml.cmu.edu/2022/10/14/tackling-diverse-tasks-with-neural-architecture-search/'>https://blog.ml.cmu.edu/2022/10/14/tackling-diverse-tasks-with-neural-architecture-search/</a></p><p>Does AutoML work for diverse tasks? - <a href='https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks/#:~:text=The'>https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks</a><br/><br/>Follow Junhong <a href='https://twitter.com/JunhongShen1'>@JunhongShen1</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re chatting with Junhong Shen, a PhD student at Carnegie Mellon.<br/><br/>Junhong and her team are working on the generalizability of NAS algorithms across a diverse set of tasks.<br/><br/>Today we&apos;ll be talking about DASH, a NAS algorithm that takes diversity of tasks at its center. In order to implement DASH, Junhong and her team implemented three clever ideas that she&apos;ll share with us.<br/><br/>Efficient Architecture Search for Diverse Tasks - <a href='https://arxiv.org/pdf/2204.07554.pdf'>https://arxiv.org/pdf/2204.07554.pdf</a><br/><br/>Tackling Diverse Tasks with Neural Architecture Search - <a href='https://blog.ml.cmu.edu/2022/10/14/tackling-diverse-tasks-with-neural-architecture-search/'>https://blog.ml.cmu.edu/2022/10/14/tackling-diverse-tasks-with-neural-architecture-search/</a></p><p>Does AutoML work for diverse tasks? - <a href='https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks/#:~:text=The'>https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks</a><br/><br/>Follow Junhong <a href='https://twitter.com/JunhongShen1'>@JunhongShen1</a></p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11901849-dash-how-to-search-over-convolutions.mp3" length="56549169" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11901849</guid>
    <pubDate>Mon, 19 Dec 2022 21:00:00 -0500</pubDate>
    <itunes:duration>4710</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Human-Centered AutoML: The New Paradigm</itunes:title>
    <title>Human-Centered AutoML: The New Paradigm</title>
    <itunes:summary><![CDATA[Today we're speaking with Marius Lindauer and it is certainly one of my favorite episodes! As you’ll hear, Marius is full of ideas for where AutoML systems can and should go. These ideas are crystallized in a blog-post, published here: https://www.automl.org/rethinking-automl-advancing-from-a-machine-centered-to-human-centered-paradigm/ If you’re searching for research directions, this conversation left me with dozens of ideas. Marius and his team are doing phenomenal work to make AutoML syst...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re speaking with Marius Lindauer and it is certainly one of my favorite episodes!</p><p>As you’ll hear, Marius is full of ideas for where AutoML systems can and should go. These ideas are crystallized in a blog-post, published here: <a href='https://www.automl.org/rethinking-automl-advancing-from-a-machine-centered-to-human-centered-paradigm/'>https://www.automl.org/rethinking-automl-advancing-from-a-machine-centered-to-human-centered-paradigm/</a></p><p>If you’re searching for research directions, this conversation left me with dozens of ideas. Marius and his team are doing phenomenal work to make AutoML systems more trustworthy and more human-centric.<br/><br/>We will be reviewing content from the following papers:</p><ul><li>Bayesian Optimization with a Prior for the Optimum - <a href='https://arxiv.org/abs/2006.14608'>https://arxiv.org/abs/2006.14608</a></li><li>Explaining Hyperparameter Optimization via Partial Dependence Plots - <a href='https://arxiv.org/abs/2111.04820'>https://arxiv.org/abs/2111.04820</a></li><li>Enhancing Explainability of Hyperparameter Optimization via Bayesian Algorithm Execution - <a href='https://arxiv.org/abs/2206.05447'>https://arxiv.org/abs/2206.05447</a></li><li>πBO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization - <a href='https://arxiv.org/abs/2204.11051'>https://arxiv.org/abs/2204.11051</a></li></ul><p><br/>Follow Marius on Twitter here - <a href='https://twitter.com/LindauerMarius'>https://twitter.com/LindauerMarius</a>, and AutoML.org on Twitter here - <a href='https://twitter.com/AutoML_org'>https://twitter.com/AutoML_org</a>.<br/><br/>To help Marius complete the survey he mentioned, please visit the link here - <a href='https://www.soscisurvey.de/hpo-method-validation/'>https://www.soscisurvey.de/hpo-method-validation/</a>.<br/><br/>To learn more about AutoML, visit AutoML.org, here - <a href='https://www.automl.org/'>https://www.automl.org/</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re speaking with Marius Lindauer and it is certainly one of my favorite episodes!</p><p>As you’ll hear, Marius is full of ideas for where AutoML systems can and should go. These ideas are crystallized in a blog-post, published here: <a href='https://www.automl.org/rethinking-automl-advancing-from-a-machine-centered-to-human-centered-paradigm/'>https://www.automl.org/rethinking-automl-advancing-from-a-machine-centered-to-human-centered-paradigm/</a></p><p>If you’re searching for research directions, this conversation left me with dozens of ideas. Marius and his team are doing phenomenal work to make AutoML systems more trustworthy and more human-centric.<br/><br/>We will be reviewing content from the following papers:</p><ul><li>Bayesian Optimization with a Prior for the Optimum - <a href='https://arxiv.org/abs/2006.14608'>https://arxiv.org/abs/2006.14608</a></li><li>Explaining Hyperparameter Optimization via Partial Dependence Plots - <a href='https://arxiv.org/abs/2111.04820'>https://arxiv.org/abs/2111.04820</a></li><li>Enhancing Explainability of Hyperparameter Optimization via Bayesian Algorithm Execution - <a href='https://arxiv.org/abs/2206.05447'>https://arxiv.org/abs/2206.05447</a></li><li>πBO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization - <a href='https://arxiv.org/abs/2204.11051'>https://arxiv.org/abs/2204.11051</a></li></ul><p><br/>Follow Marius on Twitter here - <a href='https://twitter.com/LindauerMarius'>https://twitter.com/LindauerMarius</a>, and AutoML.org on Twitter here - <a href='https://twitter.com/AutoML_org'>https://twitter.com/AutoML_org</a>.<br/><br/>To help Marius complete the survey he mentioned, please visit the link here - <a href='https://www.soscisurvey.de/hpo-method-validation/'>https://www.soscisurvey.de/hpo-method-validation/</a>.<br/><br/>To learn more about AutoML, visit AutoML.org, here - <a href='https://www.automl.org/'>https://www.automl.org/</a></p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11804713-human-centered-automl-the-new-paradigm.mp3" length="50937444" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11804713</guid>
    <pubDate>Fri, 02 Dec 2022 20:00:00 -0500</pubDate>
    <itunes:duration>4242</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>BERT-Sort: How to use language models to semantically order categorical values</itunes:title>
    <title>BERT-Sort: How to use language models to semantically order categorical values</title>
    <itunes:summary><![CDATA[Today Ankush Garg is talking to Mehdi Bahrami about his recent project: BERT-Sort.  BERT-Sort is an example of how large language models can add useful context to tabular datasets, and to AutoML systems.  Mehdi is a Member of Research Staff at Fujitsu and, as he describes, he began using AutoML systems for his research, yet he came across some crucial limitations of existing solutions. The modifications he made highlight a promising future for the relationship between language models and Auto...]]></itunes:summary>
    <description><![CDATA[<p>Today Ankush Garg is talking to Mehdi Bahrami about his recent project: BERT-Sort.<br/><br/>BERT-Sort is an example of how large language models can add useful context to tabular datasets, and to AutoML systems.<br/><br/>Mehdi is a Member of Research Staff at Fujitsu and, as he describes, he began using AutoML systems for his research, yet he came across some crucial limitations of existing solutions. The modifications he made highlight a promising future for the relationship between language models and AutoML. This is a direction we&apos;re going to continue to explore on the show.<br/><br/>References:<br/>BERT-Sort: A Zero-shot MLM Semantic Encoder on Ordinal Features for AutoML - <a href='https://proceedings.mlr.press/v188/bahrami22a.html'>https://proceedings.mlr.press/v188/bahrami22a.html</a><br/><br/>PyTorrent: A Python Library Corpus for Large-scale Language Models: <a href='https://arxiv.org/abs/2110.01710'>https://arxiv.org/abs/2110.01710</a><br/><br/>AugmentedCode: Examining the Effects of Natural Language Resources in Code Retrieval Models: <a href='https://arxiv.org/abs/2110.08512'>https://arxiv.org/abs/2110.08512</a><br/><br/></p>]]></description>
    <content:encoded><![CDATA[<p>Today Ankush Garg is talking to Mehdi Bahrami about his recent project: BERT-Sort.<br/><br/>BERT-Sort is an example of how large language models can add useful context to tabular datasets, and to AutoML systems.<br/><br/>Mehdi is a Member of Research Staff at Fujitsu and, as he describes, he began using AutoML systems for his research, yet he came across some crucial limitations of existing solutions. The modifications he made highlight a promising future for the relationship between language models and AutoML. This is a direction we&apos;re going to continue to explore on the show.<br/><br/>References:<br/>BERT-Sort: A Zero-shot MLM Semantic Encoder on Ordinal Features for AutoML - <a href='https://proceedings.mlr.press/v188/bahrami22a.html'>https://proceedings.mlr.press/v188/bahrami22a.html</a><br/><br/>PyTorrent: A Python Library Corpus for Large-scale Language Models: <a href='https://arxiv.org/abs/2110.01710'>https://arxiv.org/abs/2110.01710</a><br/><br/>AugmentedCode: Examining the Effects of Natural Language Resources in Code Retrieval Models: <a href='https://arxiv.org/abs/2110.08512'>https://arxiv.org/abs/2110.08512</a><br/><br/></p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11758051-bert-sort-how-to-use-language-models-to-semantically-order-categorical-values.mp3" length="29277100" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11758051</guid>
    <pubDate>Thu, 24 Nov 2022 15:00:00 -0500</pubDate>
    <itunes:duration>2437</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>SAT: The Peculiar Origins of AutoML</itunes:title>
    <title>SAT: The Peculiar Origins of AutoML</title>
    <itunes:summary><![CDATA[In today's episode, we’re talking to Lars Kothoff about the fascinating origin story of AutoML (as he sees it), and how it emerged from the SAT community. While talking to many of you, it became clear that this origin story is one that a lot of people have some vague sense about, but not a very concrete knowledge of so hopefully this episode can help to flesh out the narrative with greater clarity.  We'll be discussing his survey paper from 2016, "Algorithm selection for combinatorial search ...]]></itunes:summary>
    <description><![CDATA[<p>In today&apos;s episode, we’re talking to Lars Kothoff about the fascinating origin story of AutoML (as he sees it), and how it emerged from the SAT community.</p><p>While talking to many of you, it became clear that this origin story is one that a lot of people have some vague sense about, but not a very concrete knowledge of so hopefully this episode can help to flesh out the narrative with greater clarity.<br/><br/>We&apos;ll be discussing his survey paper from 2016, &quot;Algorithm selection for combinatorial search problems: A survey&quot;, to be found here: <br/><br/><a href='https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=P37OkUUAAAAJ&amp;citation_for_view=P37OkUUAAAAJ:W7OEmFMy1HYC'>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=P37OkUUAAAAJ&amp;citation_for_view=P37OkUUAAAAJ:W7OEmFMy1HYC<br/></a><br/>As well as the original SAT-zilla paper, to be found here:<br/><a href='https://www.aaai.org/Papers/JAIR/Vol32/JAIR-3214.pdf'>https://www.aaai.org/Papers/JAIR/Vol32/JAIR-3214.pdf</a><br/><br/>Follow Lars&apos; work on Twitter, <a href='https://twitter.com/larskotthoff'>@larskotthoff</a>.</p>]]></description>
    <content:encoded><![CDATA[<p>In today&apos;s episode, we’re talking to Lars Kothoff about the fascinating origin story of AutoML (as he sees it), and how it emerged from the SAT community.</p><p>While talking to many of you, it became clear that this origin story is one that a lot of people have some vague sense about, but not a very concrete knowledge of so hopefully this episode can help to flesh out the narrative with greater clarity.<br/><br/>We&apos;ll be discussing his survey paper from 2016, &quot;Algorithm selection for combinatorial search problems: A survey&quot;, to be found here: <br/><br/><a href='https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=P37OkUUAAAAJ&amp;citation_for_view=P37OkUUAAAAJ:W7OEmFMy1HYC'>https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=P37OkUUAAAAJ&amp;citation_for_view=P37OkUUAAAAJ:W7OEmFMy1HYC<br/></a><br/>As well as the original SAT-zilla paper, to be found here:<br/><a href='https://www.aaai.org/Papers/JAIR/Vol32/JAIR-3214.pdf'>https://www.aaai.org/Papers/JAIR/Vol32/JAIR-3214.pdf</a><br/><br/>Follow Lars&apos; work on Twitter, <a href='https://twitter.com/larskotthoff'>@larskotthoff</a>.</p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11678454-sat-the-peculiar-origins-of-automl.mp3" length="50211652" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11678454</guid>
    <pubDate>Fri, 11 Nov 2022 18:00:00 -0500</pubDate>
    <itunes:duration>4182</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How to use evolutionary strategies for online AutoML</itunes:title>
    <title>How to use evolutionary strategies for online AutoML</title>
    <itunes:summary><![CDATA[Today we’re talking to Cedric Kulbach about online learning, the challenges of doing it properly, why it is so promising, how it’s connected to evolutionary strategies, and recent advances in the field that can help to unlock these promises.  We then discuss the close connection between online learning and AutoML systems, and we explore a recent framework that he recently published, called EvoAutoML. Cedric is a PhD student at Karlsruhe Institute of Technology, in Germany, where was defending...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re talking to Cedric Kulbach about online learning, the challenges of doing it properly, why it is so promising, how it’s connected to evolutionary strategies, and recent advances in the field that can help to unlock these promises.<br/><br/>We then discuss the close connection between online learning and AutoML systems, and we explore a recent framework that he recently published, called EvoAutoML.</p><p>Cedric is a PhD student at Karlsruhe Institute of Technology, in Germany, where was defending his thesis 2 days after we spoke!<br/><br/>You can find his paper here: https://hal.inria.fr/hal-03667231/document</p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re talking to Cedric Kulbach about online learning, the challenges of doing it properly, why it is so promising, how it’s connected to evolutionary strategies, and recent advances in the field that can help to unlock these promises.<br/><br/>We then discuss the close connection between online learning and AutoML systems, and we explore a recent framework that he recently published, called EvoAutoML.</p><p>Cedric is a PhD student at Karlsruhe Institute of Technology, in Germany, where was defending his thesis 2 days after we spoke!<br/><br/>You can find his paper here: https://hal.inria.fr/hal-03667231/document</p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11509982-how-to-use-evolutionary-strategies-for-online-automl.mp3" length="44138530" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11509982</guid>
    <pubDate>Mon, 17 Oct 2022 09:00:00 -0400</pubDate>
    <itunes:duration>3676</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>A Narration of The Bitter Lesson</itunes:title>
    <title>A Narration of The Bitter Lesson</title>
    <itunes:summary><![CDATA[This short episode is a narration of Richard Sutton's The Bitter Lesson.  Richard Sutton is a distinguished research scientist at DeepMind and a professor of computing science at the University of Alberta. He is considered one of the founders of modern computational reinforcement learning, having several significant contributions to the field, including temporal difference learning and policy gradient methods.  If you haven't yet read his piece, it is definitely worth reading. This episode is...]]></itunes:summary>
    <description><![CDATA[<p>This short episode is a narration of Richard Sutton&apos;s The Bitter Lesson.<br/><br/>Richard Sutton is a distinguished research scientist at DeepMind and a professor of computing science at the University of Alberta. He is considered one of the founders of modern computational reinforcement learning, having several significant contributions to the field, including temporal difference learning and policy gradient methods.<br/><br/>If you haven&apos;t yet read his piece, it is definitely worth reading. This episode is a narration of it.<br/><br/>I would like to invite more discussion about these thoughts, so if you know of anybody who would like to discuss them, please send me to me at hello@automlpodcast.com.<br/><br/>You can read the article here: <a href='http://www.incompleteideas.net/IncIdeas/BitterLesson.html'>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a></p>]]></description>
    <content:encoded><![CDATA[<p>This short episode is a narration of Richard Sutton&apos;s The Bitter Lesson.<br/><br/>Richard Sutton is a distinguished research scientist at DeepMind and a professor of computing science at the University of Alberta. He is considered one of the founders of modern computational reinforcement learning, having several significant contributions to the field, including temporal difference learning and policy gradient methods.<br/><br/>If you haven&apos;t yet read his piece, it is definitely worth reading. This episode is a narration of it.<br/><br/>I would like to invite more discussion about these thoughts, so if you know of anybody who would like to discuss them, please send me to me at hello@automlpodcast.com.<br/><br/>You can read the article here: <a href='http://www.incompleteideas.net/IncIdeas/BitterLesson.html'>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11341976-a-narration-of-the-bitter-lesson.mp3" length="5144189" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11341976</guid>
    <pubDate>Mon, 26 Sep 2022 09:00:00 -0400</pubDate>
    <itunes:duration>426</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Examining Tabular Deep Learning</itunes:title>
    <title>Examining Tabular Deep Learning</title>
    <itunes:summary><![CDATA[More drama in the contest between traditional machine learning models and deep learning models when it comes to tabular data. We have on the show Vadim Borisov, a research fellow at the University of Tubingen as well as Kathrin Sessler, a PhD student from the same university. This episode will be led by Ankush Garg, exploring Vadim and Kathrin's recent paper “Deep Neural Networks and Tabular Data: A Survey”. Paper: https://arxiv.org/pdf/2110.01889.pdf SAINT paper: https://arxiv.org/abs/2106.0...]]></itunes:summary>
    <description><![CDATA[<p>More drama in the contest between traditional machine learning models and deep learning models when it comes to tabular data.</p><p>We have on the show Vadim Borisov, a research fellow at the University of Tubingen as well as Kathrin Sessler, a PhD student from the same university.</p><p>This episode will be led by Ankush Garg, exploring Vadim and Kathrin&apos;s recent paper “Deep Neural Networks and Tabular Data: A Survey”.</p><p>Paper: <a href='https://arxiv.org/pdf/2110.01889.pdf'>https://arxiv.org/pdf/2110.01889.pdf</a><br/>SAINT paper: <a href='https://arxiv.org/abs/2106.01342'>https://arxiv.org/abs/2106.01342</a><br/>Deep learning architectures and the hippocampus paper: <a href='https://arxiv.org/abs/2112.04035'>https://arxiv.org/abs/2112.04035</a><br/><br/>We will also do a brief form of house keeping about the show and some of the upcoming episodes.<br/><br/>As always, if you have ideas, questions, feedback, or would like to be interviewed on the show: please reach out at hello@automlpodcast.com.</p>]]></description>
    <content:encoded><![CDATA[<p>More drama in the contest between traditional machine learning models and deep learning models when it comes to tabular data.</p><p>We have on the show Vadim Borisov, a research fellow at the University of Tubingen as well as Kathrin Sessler, a PhD student from the same university.</p><p>This episode will be led by Ankush Garg, exploring Vadim and Kathrin&apos;s recent paper “Deep Neural Networks and Tabular Data: A Survey”.</p><p>Paper: <a href='https://arxiv.org/pdf/2110.01889.pdf'>https://arxiv.org/pdf/2110.01889.pdf</a><br/>SAINT paper: <a href='https://arxiv.org/abs/2106.01342'>https://arxiv.org/abs/2106.01342</a><br/>Deep learning architectures and the hippocampus paper: <a href='https://arxiv.org/abs/2112.04035'>https://arxiv.org/abs/2112.04035</a><br/><br/>We will also do a brief form of house keeping about the show and some of the upcoming episodes.<br/><br/>As always, if you have ideas, questions, feedback, or would like to be interviewed on the show: please reach out at hello@automlpodcast.com.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11341629-examining-tabular-deep-learning.mp3" length="30277820" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11341629</guid>
    <pubDate>Mon, 19 Sep 2022 10:00:00 -0400</pubDate>
    <itunes:duration>2521</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>The Paths to AGI</itunes:title>
    <title>The Paths to AGI</title>
    <itunes:summary><![CDATA[Today we’re speaking with Jeff Clune about a new path towards general artificial intelligence, that he calls AI Generating Algorithms (AI-GAs). Jeff is a Professor of Computer Science at the University of British Columbia. He was previously a Senior Research Scientist at Uber AI, and more recently a Research Team Leader at OpenAI. He's currently a Faculty Member at the Vector Institute. We’re going to be talking about whether it is even possible that humans never create AGI, the enormous pote...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re speaking with Jeff Clune about a new path towards general artificial intelligence, that he calls AI Generating Algorithms (AI-GAs).</p><p>Jeff is a Professor of Computer Science at the University of British Columbia. He was previously a Senior Research Scientist at Uber AI, and more recently a Research Team Leader at OpenAI. He&apos;s currently a Faculty Member at the Vector Institute.</p><p>We’re going to be talking about whether it is even possible that humans never create AGI, the enormous potential upside of AGI, the risks of AGI (including existential risks), Jeff’s fascinating transition into the field of AI, whether academia remains receptive to similar academic transitions, how long it might take to get to AGI and whether we have the right playbook for it, the prevailing status quo track (he mentions a couple): The first being a large language model approach, that he calls “standing on the shoulders of giant human datasets”, some of the limitations of this approach, the main track, that he calls “the manual path to AGI”, his new contribution, the AI Generating algorithms, which is essentially how we get out of the way and let the AI do much more, whether we already have an existence proof for how this can work, the three pillars of the AI Generating approach, AI Generating environments and capability oriented evaluation, the production of interesting or useful behaviors, the hidden Turing award to the person who would figure out AI Generating environment, learning pathologies, the circuitous paths of innovation, the seeming miracle of Darwinian evolution and the challenges of replicating it, the increasingly complex environments Darwinian evolution produces, the moral dimension to toying around with increasingly intelligent agents, AI and vegetarianism, and the extent to which we should be dismissive of the subjective claims of increasingly intelligent models.<br/><br/>His paper can be found here: <a href='https://arxiv.org/abs/1905.10985'>https://arxiv.org/abs/1905.10985</a><br/>Follow him on Twitter here: <a href='https://twitter.com/jeffclune'>https://twitter.com/jeffclune</a><br/>Learn more on his website here: <a href='http://jeffclune.com/'>http://jeffclune.com/</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re speaking with Jeff Clune about a new path towards general artificial intelligence, that he calls AI Generating Algorithms (AI-GAs).</p><p>Jeff is a Professor of Computer Science at the University of British Columbia. He was previously a Senior Research Scientist at Uber AI, and more recently a Research Team Leader at OpenAI. He&apos;s currently a Faculty Member at the Vector Institute.</p><p>We’re going to be talking about whether it is even possible that humans never create AGI, the enormous potential upside of AGI, the risks of AGI (including existential risks), Jeff’s fascinating transition into the field of AI, whether academia remains receptive to similar academic transitions, how long it might take to get to AGI and whether we have the right playbook for it, the prevailing status quo track (he mentions a couple): The first being a large language model approach, that he calls “standing on the shoulders of giant human datasets”, some of the limitations of this approach, the main track, that he calls “the manual path to AGI”, his new contribution, the AI Generating algorithms, which is essentially how we get out of the way and let the AI do much more, whether we already have an existence proof for how this can work, the three pillars of the AI Generating approach, AI Generating environments and capability oriented evaluation, the production of interesting or useful behaviors, the hidden Turing award to the person who would figure out AI Generating environment, learning pathologies, the circuitous paths of innovation, the seeming miracle of Darwinian evolution and the challenges of replicating it, the increasingly complex environments Darwinian evolution produces, the moral dimension to toying around with increasingly intelligent agents, AI and vegetarianism, and the extent to which we should be dismissive of the subjective claims of increasingly intelligent models.<br/><br/>His paper can be found here: <a href='https://arxiv.org/abs/1905.10985'>https://arxiv.org/abs/1905.10985</a><br/>Follow him on Twitter here: <a href='https://twitter.com/jeffclune'>https://twitter.com/jeffclune</a><br/>Learn more on his website here: <a href='http://jeffclune.com/'>http://jeffclune.com/</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11265970-the-paths-to-agi.mp3" length="51955766" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11265970</guid>
    <pubDate>Mon, 05 Sep 2022 19:00:00 -0400</pubDate>
    <itunes:duration>4327</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How to evaluate a metalearning system</itunes:title>
    <title>How to evaluate a metalearning system</title>
    <itunes:summary><![CDATA[Today I'm speaking with Jan N. van Rijn about metalearning.  Jan is an assistant professor at Leiden University, where he also did his PhD. He is one of the founders of the OpenML Foundation, he previously did a post-doc at Freiburg in the Frank Hutter lab, and he is one of the authors of the metalearning book, which we'll be discussing.  We’ll be primarily examining the contents of their chapter titled “Evaluating Recommendations of Metalearning/AutoML Systems”.  We'll be covering such topic...]]></itunes:summary>
    <description><![CDATA[<p>Today I&apos;m speaking with Jan N. van Rijn about metalearning.<br/><br/>Jan is an assistant professor at Leiden University, where he also did his PhD. He is one of the founders of the OpenML Foundation, he previously did a post-doc at Freiburg in the Frank Hutter lab, and he is one of the authors of the metalearning book, which we&apos;ll be discussing.<br/><br/>We’ll be primarily examining the contents of their chapter titled “Evaluating Recommendations of Metalearning/AutoML Systems”.<br/><br/>We&apos;ll be covering such topics as OpenML and metalearning, how the space of AutoML has changed in the last few years,  benchmarks in the space of AutoML, how benchmarks measure progress of a field, some of the challenges benchmarks present, the need to build better tooling around benchmarks, the rise of NAS within the AutoML umbrella, the scope of AutoML (algorithm selection, hyperparameter optimization, the CASH problem, pipeline optimization, etc.),  how meta learning helps traverse through the various scopes of AutoML problem types, metalearning in the context of hyperparameter optimizations, the importance of properly designing meta-datasets, approaches to the inputs and outputs of meta-models and their advantages and disadvantages, surrogate models, how AutoML systems interact with meta-models, how to think about metalearning across dataset difficulties, diagnosing meta-models and meta-datasets, how to compare different metadata systems, loss-time curves, meta-features and the various approaches to creating them (including dataset 2 vec), contemporary meta-learning in a deep-learning context, and other topics.</p><p>Link to book - <a href='https://link.springer.com/book/10.1007/978-3-030-67024-5'>https://link.springer.com/book/10.1007/978-3-030-67024-5</a><br/>Link to OpenML - <a href='https://www.openml.org'>https://www.openml.org/</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today I&apos;m speaking with Jan N. van Rijn about metalearning.<br/><br/>Jan is an assistant professor at Leiden University, where he also did his PhD. He is one of the founders of the OpenML Foundation, he previously did a post-doc at Freiburg in the Frank Hutter lab, and he is one of the authors of the metalearning book, which we&apos;ll be discussing.<br/><br/>We’ll be primarily examining the contents of their chapter titled “Evaluating Recommendations of Metalearning/AutoML Systems”.<br/><br/>We&apos;ll be covering such topics as OpenML and metalearning, how the space of AutoML has changed in the last few years,  benchmarks in the space of AutoML, how benchmarks measure progress of a field, some of the challenges benchmarks present, the need to build better tooling around benchmarks, the rise of NAS within the AutoML umbrella, the scope of AutoML (algorithm selection, hyperparameter optimization, the CASH problem, pipeline optimization, etc.),  how meta learning helps traverse through the various scopes of AutoML problem types, metalearning in the context of hyperparameter optimizations, the importance of properly designing meta-datasets, approaches to the inputs and outputs of meta-models and their advantages and disadvantages, surrogate models, how AutoML systems interact with meta-models, how to think about metalearning across dataset difficulties, diagnosing meta-models and meta-datasets, how to compare different metadata systems, loss-time curves, meta-features and the various approaches to creating them (including dataset 2 vec), contemporary meta-learning in a deep-learning context, and other topics.</p><p>Link to book - <a href='https://link.springer.com/book/10.1007/978-3-030-67024-5'>https://link.springer.com/book/10.1007/978-3-030-67024-5</a><br/>Link to OpenML - <a href='https://www.openml.org'>https://www.openml.org/</a></p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11219791-how-to-evaluate-a-metalearning-system.mp3" length="49993766" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11219791</guid>
    <pubDate>Mon, 29 Aug 2022 00:00:00 -0400</pubDate>
    <itunes:duration>4164</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Active Dendrites: Brain-inspired multi-task learning</itunes:title>
    <title>Active Dendrites: Brain-inspired multi-task learning</title>
    <itunes:summary><![CDATA[Today we’re speaking with three researchers: Karan Grewal, Abhi Iyer and Akash Velu, about multi-task learning and how their new brain-inspired approach can help tackle it.  We’ll be discussing what a task is, what exactly we mean by multi-task systems, distances between tasks, the difference between continual learning and multi-task learning, catastrophic forgetting, catastrophic interference and their causes, various approaches out there like context-dependent gating and synaptic intelligen...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re speaking with three researchers: Karan Grewal, Abhi Iyer and Akash Velu, about multi-task learning and how their new brain-inspired approach can help tackle it.<br/><br/>We’ll be discussing what a task is, what exactly we mean by multi-task systems, distances between tasks, the difference between continual learning and multi-task learning, catastrophic forgetting, catastrophic interference and their causes, various approaches out there like context-dependent gating and synaptic intelligence, the role of scale, and sparsity, we’ll cover some basics of the brain like dendrites, proximal and distal dendrites, apical and basal dendrites, how active dendrites can help us solve the challenges of gradient interference in multi-task learning, how they attach dendrites to each of their neurons in their deep learning models, their various approaches to representing context vectors, the challenges of brain-inspired approaches to machine learning, and some speculation about the future of this line of research.<br/><br/>Some references:<br/>Their paper can be found here: <a href='https://arxiv.org/abs/2201.00042'>https://arxiv.org/abs/2201.00042</a><br/><br/>All three of them had a great appearance on the Yannic Kilcher YouTube channel here: <a href='https://youtu.be/smxwT82o40Y'>https://youtu.be/smxwT82o40Y</a><br/><br/>They mentioned Hierarchical Temporal Memory as the foundation stone for a lot of their research: <a href='https://numenta.com/blog/2019/10/24/machine-learning-guide-to-htm'>https://numenta.com/blog/2019/10/24/machine-learning-guide-to-htm</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re speaking with three researchers: Karan Grewal, Abhi Iyer and Akash Velu, about multi-task learning and how their new brain-inspired approach can help tackle it.<br/><br/>We’ll be discussing what a task is, what exactly we mean by multi-task systems, distances between tasks, the difference between continual learning and multi-task learning, catastrophic forgetting, catastrophic interference and their causes, various approaches out there like context-dependent gating and synaptic intelligence, the role of scale, and sparsity, we’ll cover some basics of the brain like dendrites, proximal and distal dendrites, apical and basal dendrites, how active dendrites can help us solve the challenges of gradient interference in multi-task learning, how they attach dendrites to each of their neurons in their deep learning models, their various approaches to representing context vectors, the challenges of brain-inspired approaches to machine learning, and some speculation about the future of this line of research.<br/><br/>Some references:<br/>Their paper can be found here: <a href='https://arxiv.org/abs/2201.00042'>https://arxiv.org/abs/2201.00042</a><br/><br/>All three of them had a great appearance on the Yannic Kilcher YouTube channel here: <a href='https://youtu.be/smxwT82o40Y'>https://youtu.be/smxwT82o40Y</a><br/><br/>They mentioned Hierarchical Temporal Memory as the foundation stone for a lot of their research: <a href='https://numenta.com/blog/2019/10/24/machine-learning-guide-to-htm'>https://numenta.com/blog/2019/10/24/machine-learning-guide-to-htm</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11189007-active-dendrites-brain-inspired-multi-task-learning.mp3" length="49228654" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11189007</guid>
    <pubDate>Tue, 23 Aug 2022 11:00:00 -0400</pubDate>
    <itunes:duration>4100</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Smart NAS via Co-Regulated Shaping Reinforcement</itunes:title>
    <title>Smart NAS via Co-Regulated Shaping Reinforcement</title>
    <itunes:summary><![CDATA[Today I’m speaking with Mayukh Das about using neural architecture search for resource-constrained devices and about a new multi-objective reinforcement-learning based framework that he recently published called AUTOCOMET.   We’ll be covering such topics as how NAS research is done both at Samsung and at Microsoft,  the relationship between NAS and product teams, devices and the various types of constraints they expose, how to featurize hardware contexts, layer-wise latency calculations,...]]></itunes:summary>
    <description><![CDATA[<p>Today I’m speaking with Mayukh Das about using neural architecture search for resource-constrained devices and about a new multi-objective reinforcement-learning based framework that he recently published called AUTOCOMET.<br/><br/></p><p>We’ll be covering such topics as how NAS research is done both at Samsung and at Microsoft,  the relationship between NAS and product teams, devices and the various types of constraints they expose, how to featurize hardware contexts, layer-wise latency calculations, surrogate models and the kinds of hardware-aware data they require, the current limitations of NAS, reinforcement learning and NAS, multi-objective optimization in the context of reinforcement learning, reward sparsity, reward shaping and shaping functions, primary and secondary rewards, their concept of co-regulated shaping, Q functions and the effects of potentials, AUTOCOMET, the future of NAS and other topics.<br/><br/>Thank you for tuning in!<br/><br/>To learn more about AUTOCOMET, find the paper here: https://arxiv.org/pdf/2203.15408.pdf</p>]]></description>
    <content:encoded><![CDATA[<p>Today I’m speaking with Mayukh Das about using neural architecture search for resource-constrained devices and about a new multi-objective reinforcement-learning based framework that he recently published called AUTOCOMET.<br/><br/></p><p>We’ll be covering such topics as how NAS research is done both at Samsung and at Microsoft,  the relationship between NAS and product teams, devices and the various types of constraints they expose, how to featurize hardware contexts, layer-wise latency calculations, surrogate models and the kinds of hardware-aware data they require, the current limitations of NAS, reinforcement learning and NAS, multi-objective optimization in the context of reinforcement learning, reward sparsity, reward shaping and shaping functions, primary and secondary rewards, their concept of co-regulated shaping, Q functions and the effects of potentials, AUTOCOMET, the future of NAS and other topics.<br/><br/>Thank you for tuning in!<br/><br/>To learn more about AUTOCOMET, find the paper here: https://arxiv.org/pdf/2203.15408.pdf</p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11097461-smart-nas-via-co-regulated-shaping-reinforcement.mp3" length="45880756" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11097461</guid>
    <pubDate>Sun, 07 Aug 2022 23:00:00 -0400</pubDate>
    <itunes:duration>3821</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>The measures of intelligence</itunes:title>
    <title>The measures of intelligence</title>
    <itunes:summary><![CDATA[Today we’re speaking with José Hernández-Orallo. José is a Professor at the Polytechnic University of València in Spain and a Senior Research Fellow at the Leverhulme Centre for the Future of Intelligence, at Cambridge. We'll be covering an enormous amount of ground surrounding intelligence and its evaluation.   We’ll touch on topics such as operating conditions in ML, agent characteristic curves,, the challenge with average performance scores, task-oriented evaluation, capability-oriented ev...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re speaking with José Hernández-Orallo. José is a Professor at the Polytechnic University of València in Spain and a Senior Research Fellow at the Leverhulme Centre for the Future of Intelligence, at Cambridge.</p><p>We&apos;ll be covering an enormous amount of ground surrounding intelligence and its evaluation. <br/><br/>We’ll touch on topics such as operating conditions in ML, agent characteristic curves,, the challenge with average performance scores, task-oriented evaluation, capability-oriented evaluation - in humans and other animals, animal-AI Olympics, meta-data annotation, performance robustness as a function of latent attributes, the limitations of aggregating behavioral performance into a handful of metrics, whether intelligence is meaningless or all-explanatory, the concept of generality in intelligence, AGI and the problem of distributions of tasks, calibration of task difficulties, whether capability metrics are incommensurable, complexity and compressibility, compression-based algorithmic information theory, the limits of intelligence, the intelligence of hybrid systems, whether we’re ready to talk about consciousness given our limited understanding of intelligence, whether rights should be tethered to pain or to cognition, animal rights, and what the AutoML community can do to help with this line of work.<br/><br/>If you&apos;re interested in this line of work, here is a recent paper by José and his team: <a href='/e5c8bf9182f74816bd6c3753f52d343d'>https://ryanburnell.com/wp-content/uploads/Burnell-et-al-2022-Not-a-Number.pdf</a><br/><br/>His book, The Measure of All Minds, can be found here: <a href='https://www.amazon.com/Measure-All-Minds-Evaluating-Intelligence/dp/1107153018'>https://www.amazon.com/Measure-All-Minds-Evaluating-Intelligence/dp/1107153018</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re speaking with José Hernández-Orallo. José is a Professor at the Polytechnic University of València in Spain and a Senior Research Fellow at the Leverhulme Centre for the Future of Intelligence, at Cambridge.</p><p>We&apos;ll be covering an enormous amount of ground surrounding intelligence and its evaluation. <br/><br/>We’ll touch on topics such as operating conditions in ML, agent characteristic curves,, the challenge with average performance scores, task-oriented evaluation, capability-oriented evaluation - in humans and other animals, animal-AI Olympics, meta-data annotation, performance robustness as a function of latent attributes, the limitations of aggregating behavioral performance into a handful of metrics, whether intelligence is meaningless or all-explanatory, the concept of generality in intelligence, AGI and the problem of distributions of tasks, calibration of task difficulties, whether capability metrics are incommensurable, complexity and compressibility, compression-based algorithmic information theory, the limits of intelligence, the intelligence of hybrid systems, whether we’re ready to talk about consciousness given our limited understanding of intelligence, whether rights should be tethered to pain or to cognition, animal rights, and what the AutoML community can do to help with this line of work.<br/><br/>If you&apos;re interested in this line of work, here is a recent paper by José and his team: <a href='/e5c8bf9182f74816bd6c3753f52d343d'>https://ryanburnell.com/wp-content/uploads/Burnell-et-al-2022-Not-a-Number.pdf</a><br/><br/>His book, The Measure of All Minds, can be found here: <a href='https://www.amazon.com/Measure-All-Minds-Evaluating-Intelligence/dp/1107153018'>https://www.amazon.com/Measure-All-Minds-Evaluating-Intelligence/dp/1107153018</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11022528-the-measures-of-intelligence.mp3" length="42658905" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11022528</guid>
    <pubDate>Mon, 25 Jul 2022 07:00:00 -0400</pubDate>
    <itunes:duration>4263</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Upgrading human evaluators with assessor models</itunes:title>
    <title>Upgrading human evaluators with assessor models</title>
    <itunes:summary><![CDATA[Today I’m talking with Wout Schellaert about assessor models. Wout is a PhD student at the Polytechnic University of Valencia.. We’ll be covering a lot of different topics, such as the distributional hypothesis in machine learning, evaluation criteria, the reductive nature of current evaluation methods, task systems, the desiderata of assessor models, how to build assessor models, when to use them, what happens when our models become increasingly complex, how assessor models can help with AI ...]]></itunes:summary>
    <description><![CDATA[<p>Today I’m talking with Wout Schellaert about assessor models. Wout is a PhD student at the Polytechnic University of Valencia..</p><p>We’ll be covering a lot of different topics, such as the distributional hypothesis in machine learning, evaluation criteria, the reductive nature of current evaluation methods, task systems, the desiderata of assessor models, how to build assessor models, when to use them, what happens when our models become increasingly complex, how assessor models can help with AI explainability, whether they can help against adversarial scenarios, the challenges of scoring, bias in human evaluators, how assessor models relate to AI alignment, and other topics.</p><p>If you&apos;d like to learn more about assessor models, visit the following pages.<br/><br/>José Hernández-Orallo (Wout&apos;s advisor): <a href='https://scholar.google.com/citations?user=n9AWbcAAAAAJ&amp;hl=en'>https://scholar.google.com/citations?user=n9AWbcAAAAAJ&amp;hl=en</a></p><p>Training on the Test Set: Mapping the System-Problem Space in AI paper: <a href='https://www.aaai.org/AAAI22Papers/SMT-00432-Hernandez-OralloJ.pdf'>https://www.aaai.org/AAAI22Papers/SMT-00432-Hernandez-OralloJ.pdf</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today I’m talking with Wout Schellaert about assessor models. Wout is a PhD student at the Polytechnic University of Valencia..</p><p>We’ll be covering a lot of different topics, such as the distributional hypothesis in machine learning, evaluation criteria, the reductive nature of current evaluation methods, task systems, the desiderata of assessor models, how to build assessor models, when to use them, what happens when our models become increasingly complex, how assessor models can help with AI explainability, whether they can help against adversarial scenarios, the challenges of scoring, bias in human evaluators, how assessor models relate to AI alignment, and other topics.</p><p>If you&apos;d like to learn more about assessor models, visit the following pages.<br/><br/>José Hernández-Orallo (Wout&apos;s advisor): <a href='https://scholar.google.com/citations?user=n9AWbcAAAAAJ&amp;hl=en'>https://scholar.google.com/citations?user=n9AWbcAAAAAJ&amp;hl=en</a></p><p>Training on the Test Set: Mapping the System-Problem Space in AI paper: <a href='https://www.aaai.org/AAAI22Papers/SMT-00432-Hernandez-OralloJ.pdf'>https://www.aaai.org/AAAI22Papers/SMT-00432-Hernandez-OralloJ.pdf</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11019820-upgrading-human-evaluators-with-assessor-models.mp3" length="42697823" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11019820</guid>
    <pubDate>Sun, 24 Jul 2022 17:00:00 -0400</pubDate>
    <itunes:duration>3556</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How to explain using analogies</itunes:title>
    <title>How to explain using analogies</title>
    <itunes:summary><![CDATA[Today we’re talking to Karthi Ramamurthy about a novel approach to similarity learning explainability. Karthi is a research staff member in IBM Research at the Watson Research Center. He studies the relationship between humans, machines, data and the societal implications of machine learning. He was involved in the initial development of the open source AI Fairness 360 toolkit, where he’s still an active contributor. His papers won various best paper awards like the 2015 IEEE International Co...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re talking to Karthi Ramamurthy about a novel approach to similarity learning explainability.</p><p>Karthi is a research staff member in IBM Research at the Watson Research Center.</p><p>He studies the relationship between humans, machines, data and the societal implications of machine learning.</p><p>He was involved in the initial development of the open source AI Fairness 360 toolkit, where he’s still an active contributor.</p><p>His papers won various best paper awards like the 2015 IEEE International Conference on Data Science and Advanced Analytics.</p><p>He is an associate editor of Digital Signal Processing and a member of the IEEE and he holds a PhD in electrical engineering from Arizona State University.</p><p>We will be discussing a recent paper that he published about explainability methods for similarity learners. So we’ll go into detail about what similarity learning, or metric learning, is.<br/><br/>You can find the paper here: https://arxiv.org/pdf/2202.01153v1.pdf<br/><br/>To learn more about metric learning, please see this survey paper: https://people.bu.edu/bkulis/pubs/ftml_metric_learning.pdf.</p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re talking to Karthi Ramamurthy about a novel approach to similarity learning explainability.</p><p>Karthi is a research staff member in IBM Research at the Watson Research Center.</p><p>He studies the relationship between humans, machines, data and the societal implications of machine learning.</p><p>He was involved in the initial development of the open source AI Fairness 360 toolkit, where he’s still an active contributor.</p><p>His papers won various best paper awards like the 2015 IEEE International Conference on Data Science and Advanced Analytics.</p><p>He is an associate editor of Digital Signal Processing and a member of the IEEE and he holds a PhD in electrical engineering from Arizona State University.</p><p>We will be discussing a recent paper that he published about explainability methods for similarity learners. So we’ll go into detail about what similarity learning, or metric learning, is.<br/><br/>You can find the paper here: https://arxiv.org/pdf/2202.01153v1.pdf<br/><br/>To learn more about metric learning, please see this survey paper: https://people.bu.edu/bkulis/pubs/ftml_metric_learning.pdf.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11013972-how-to-explain-using-analogies.mp3" length="42444192" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11013972</guid>
    <pubDate>Sun, 24 Jul 2022 14:00:00 -0400</pubDate>
    <itunes:duration>3534</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How is NAS going to evolve?</itunes:title>
    <title>How is NAS going to evolve?</title>
    <itunes:summary><![CDATA[Today I’m speaking with Vasco Lopes, about the state of Neural Architecture Search, NAS, and about a new method that he published that takes a very creative look at how to do NAS. We’ll be discussing the motivation behind NAS, the current state of its deployment, the biggest use-cases today, the three components that make up NAS, the drawbacks to the current NAS paradigm, search spaces and how to design them, search strategies and how to choose them, graph representations of neural architectu...]]></itunes:summary>
    <description><![CDATA[<p>Today I’m speaking with Vasco Lopes, about the state of Neural Architecture Search, NAS, and about a new method that he published that takes a very creative look at how to do NAS.</p><p>We’ll be discussing the motivation behind NAS, the current state of its deployment, the biggest use-cases today, the three components that make up NAS, the drawbacks to the current NAS paradigm, search spaces and how to design them, search strategies and how to choose them, graph representations of neural architectures, evaluation strategies and zero-cost approximations, bias in search space design, risks, the future of hand-designed architectures, and other topics.</p><p>Vasco is a PhD student at NOVA School of Science and Technology in Portugal and a co-founder of a Computer Vision startup called DeepNeuronic.</p><p>Find his paper, Towards Less Constrained Macro-Neural Architecture Search, here: <a href='https://arxiv.org/pdf/2203.05508.pdf'>https://arxiv.org/pdf/2203.05508.pdf</a></p>]]></description>
    <content:encoded><![CDATA[<p>Today I’m speaking with Vasco Lopes, about the state of Neural Architecture Search, NAS, and about a new method that he published that takes a very creative look at how to do NAS.</p><p>We’ll be discussing the motivation behind NAS, the current state of its deployment, the biggest use-cases today, the three components that make up NAS, the drawbacks to the current NAS paradigm, search spaces and how to design them, search strategies and how to choose them, graph representations of neural architectures, evaluation strategies and zero-cost approximations, bias in search space design, risks, the future of hand-designed architectures, and other topics.</p><p>Vasco is a PhD student at NOVA School of Science and Technology in Portugal and a co-founder of a Computer Vision startup called DeepNeuronic.</p><p>Find his paper, Towards Less Constrained Macro-Neural Architecture Search, here: <a href='https://arxiv.org/pdf/2203.05508.pdf'>https://arxiv.org/pdf/2203.05508.pdf</a></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11018654-how-is-nas-going-to-evolve.mp3" length="36787631" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11018654</guid>
    <pubDate>Sun, 24 Jul 2022 13:00:00 -0400</pubDate>
    <itunes:duration>3063</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>How deep learning can be used for tabular datasets</itunes:title>
    <title>How deep learning can be used for tabular datasets</title>
    <itunes:summary><![CDATA[Today I’m speaking with Yury Gorishniy about the state of the competition between Deep Learning and Gradient Boosted Decision Trees when it comes to tabular datasets, and about a recent paper he published that seems to take a stab at improving the state of deep learning on tabular datasets. We discuss whether or not there exists a gap between deep learning and gradient boosted decision trees, what the future of a gap might look like, and the extent to which the embedding of numerical features...]]></itunes:summary>
    <description><![CDATA[<p>Today I’m speaking with Yury Gorishniy about the state of the competition between Deep Learning and Gradient Boosted Decision Trees when it comes to tabular datasets, and about a recent paper he published that seems to take a stab at improving the state of deep learning on tabular datasets.</p><p>We discuss whether or not there exists a gap between deep learning and gradient boosted decision trees, what the future of a gap might look like, and the extent to which the embedding of numerical features can give deep learning architectures a necessary boost in performance.<br/><br/>Two of his recent papers are useful in this discussion:</p><ul><li>On Embeddings for Numerical Features in Tabular Deep Learning - <a href='https://arxiv.org/abs/2203.05556'>https://arxiv.org/abs/2203.05556</a></li><li>Revisiting Deep Learning Models for Tabular Data - <a href='https://arxiv.org/abs/2106.11959'>https://arxiv.org/abs/2106.11959</a></li></ul><p><br/>You can find Yury in the following places:</p><ul><li>GitHub - https://github.com/Yura52</li><li>Twitter - https://twitter.com/YuraFiftyTwo</li></ul><p>Enjoy!</p>]]></description>
    <content:encoded><![CDATA[<p>Today I’m speaking with Yury Gorishniy about the state of the competition between Deep Learning and Gradient Boosted Decision Trees when it comes to tabular datasets, and about a recent paper he published that seems to take a stab at improving the state of deep learning on tabular datasets.</p><p>We discuss whether or not there exists a gap between deep learning and gradient boosted decision trees, what the future of a gap might look like, and the extent to which the embedding of numerical features can give deep learning architectures a necessary boost in performance.<br/><br/>Two of his recent papers are useful in this discussion:</p><ul><li>On Embeddings for Numerical Features in Tabular Deep Learning - <a href='https://arxiv.org/abs/2203.05556'>https://arxiv.org/abs/2203.05556</a></li><li>Revisiting Deep Learning Models for Tabular Data - <a href='https://arxiv.org/abs/2106.11959'>https://arxiv.org/abs/2106.11959</a></li></ul><p><br/>You can find Yury in the following places:</p><ul><li>GitHub - https://github.com/Yura52</li><li>Twitter - https://twitter.com/YuraFiftyTwo</li></ul><p>Enjoy!</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11015546-how-deep-learning-can-be-used-for-tabular-datasets.mp3" length="61681848" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11015546</guid>
    <pubDate>Sat, 23 Jul 2022 15:00:00 -0400</pubDate>
    <itunes:duration>5138</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>When is missing data not a problem?</itunes:title>
    <title>When is missing data not a problem?</title>
    <itunes:summary><![CDATA[Today we’ll be speaking with Julian Morimoto about missing data, its impact on the reliability of statistical inference, and two theorems that he recently discovered using concepts from real analysis about what guarantees we can expect, at the limit of arbitrarily large data sets. Julian has a background in math, and studied law at Harvard Law School and he speaks about the unique challenges of adopting machine learning in the legal world due to the various mechanisms of missingness in confid...]]></itunes:summary>
    <description><![CDATA[<p>Today we’ll be speaking with Julian Morimoto about missing data, its impact on the reliability of statistical inference, and two theorems that he recently discovered using concepts from real analysis about what guarantees we can expect, at the limit of arbitrarily large data sets.</p><p>Julian has a background in math, and studied law at Harvard Law School and he speaks about the unique challenges of adopting machine learning in the legal world due to the various mechanisms of missingness in confidential documents.<br/><br/>If you&apos;re interested in learning more about missing data and statistical inference, here are several resources to help you get started:</p><ul><li>GOV2001 - Harvard: <a href='https://projects.iq.harvard.edu/gov2001/home'>https://projects.iq.harvard.edu/gov2001/home</a></li><li>One hour course by Dr. Gary Keng on missing data on YouTube: <a href='https://www.youtube.com/watch?v=qlPs8Ioa56Y'>https://www.youtube.com/watch?v=qlPs8Ioa56Y</a></li><li>Julian’s paper: <a href='https://arxiv.org/abs/2112.09275v4'>https://arxiv.org/abs/2112.09275v4</a></li><li>Statistical Analysis with Missing Data book: <a href='https://www.amazon.com/Statistical-Analysis-Missing-Roderick-Little/dp/0471183865'>https://www.amazon.com/Statistical-Analysis-Missing-Roderick-Little/dp/0471183865</a>?</li></ul>]]></description>
    <content:encoded><![CDATA[<p>Today we’ll be speaking with Julian Morimoto about missing data, its impact on the reliability of statistical inference, and two theorems that he recently discovered using concepts from real analysis about what guarantees we can expect, at the limit of arbitrarily large data sets.</p><p>Julian has a background in math, and studied law at Harvard Law School and he speaks about the unique challenges of adopting machine learning in the legal world due to the various mechanisms of missingness in confidential documents.<br/><br/>If you&apos;re interested in learning more about missing data and statistical inference, here are several resources to help you get started:</p><ul><li>GOV2001 - Harvard: <a href='https://projects.iq.harvard.edu/gov2001/home'>https://projects.iq.harvard.edu/gov2001/home</a></li><li>One hour course by Dr. Gary Keng on missing data on YouTube: <a href='https://www.youtube.com/watch?v=qlPs8Ioa56Y'>https://www.youtube.com/watch?v=qlPs8Ioa56Y</a></li><li>Julian’s paper: <a href='https://arxiv.org/abs/2112.09275v4'>https://arxiv.org/abs/2112.09275v4</a></li><li>Statistical Analysis with Missing Data book: <a href='https://www.amazon.com/Statistical-Analysis-Missing-Roderick-Little/dp/0471183865'>https://www.amazon.com/Statistical-Analysis-Missing-Roderick-Little/dp/0471183865</a>?</li></ul>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11015050-when-is-missing-data-not-a-problem.mp3" length="39676581" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11015050</guid>
    <pubDate>Sat, 23 Jul 2022 11:00:00 -0400</pubDate>
    <itunes:duration>3304</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Why this show</itunes:title>
    <title>Why this show</title>
    <itunes:summary><![CDATA[In this episode, Adam introduces the show, the motivations for it, and why and how you should participate. ]]></itunes:summary>
    <description><![CDATA[<p>In this episode, Adam introduces the show, the motivations for it, and why and how you should participate.</p>]]></description>
    <content:encoded><![CDATA[<p>In this episode, Adam introduces the show, the motivations for it, and why and how you should participate.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11013924-why-this-show.mp3" length="2936984" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11013924</guid>
    <pubDate>Sat, 23 Jul 2022 01:00:00 -0400</pubDate>
    <itunes:duration>242</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Are your experiments reproducible?</itunes:title>
    <title>Are your experiments reproducible?</title>
    <itunes:summary><![CDATA[Today we're speaking with Luigi Quaranta about the state of reproducibility in machine learning.  Luigi published a taxonomy of support for reproducibility by various tools in the space and together we’re exploring the need for reproducibility, challenges and limitations, how to evaluate opportunities for improving your current systems, and what the future might hold.  A few papers would be relevant here.  The first is A Taxonomy of Tools for Reproducible Machine Learning Experiments: http://...]]></itunes:summary>
    <description><![CDATA[<p>Today we&apos;re speaking with Luigi Quaranta about the state of reproducibility in machine learning.<br/><br/>Luigi published a taxonomy of support for reproducibility by various tools in the space and together we’re exploring the need for reproducibility, challenges and limitations, how to evaluate opportunities for improving your current systems, and what the future might hold.<br/><br/>A few papers would be relevant here.<br/><br/>The first is A Taxonomy of Tools for Reproducible Machine<br/>Learning Experiments: http://ceur-ws.org/Vol-3078/paper-81.pdf<br/><br/>and the second is a study of how to make Jupyter notebook code of high quality, Eliciting Best Practices for Collaboration with Computational Notebooks: https://dl.acm.org/doi/abs/10.1145/3512934.<br/><br/><br/></p>]]></description>
    <content:encoded><![CDATA[<p>Today we&apos;re speaking with Luigi Quaranta about the state of reproducibility in machine learning.<br/><br/>Luigi published a taxonomy of support for reproducibility by various tools in the space and together we’re exploring the need for reproducibility, challenges and limitations, how to evaluate opportunities for improving your current systems, and what the future might hold.<br/><br/>A few papers would be relevant here.<br/><br/>The first is A Taxonomy of Tools for Reproducible Machine<br/>Learning Experiments: http://ceur-ws.org/Vol-3078/paper-81.pdf<br/><br/>and the second is a study of how to make Jupyter notebook code of high quality, Eliciting Best Practices for Collaboration with Computational Notebooks: https://dl.acm.org/doi/abs/10.1145/3512934.<br/><br/><br/></p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/11013831-are-your-experiments-reproducible.mp3" length="37784478" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-11013831</guid>
    <pubDate>Sat, 23 Jul 2022 00:00:00 -0400</pubDate>
    <itunes:duration>3146</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Manipulating Your Reputation</itunes:title>
    <title>Manipulating Your Reputation</title>
    <itunes:summary><![CDATA[In this episode, Adam speaks with Doctor Torsten Ensslin about simulating reputation networks and their manipulation using Information Theory.  Torsten is an Astrophysicist and cosmologist at the Max Plank institute, where he’s held many titles and positions. His current scientific work investigates theoretical cosmology and information field theory.   As he discusses in this episode, Torsten co-created a simulation of how reputation propagates through a network of agents and how those a...]]></itunes:summary>
    <description><![CDATA[<p>In this episode, Adam speaks with Doctor Torsten Ensslin about simulating reputation networks and their manipulation using Information Theory.<br/><br/>Torsten is an Astrophysicist and cosmologist at the Max Plank institute, where he’s held many titles and positions. His current scientific work investigates theoretical cosmology and information field theory. <br/><br/>As he discusses in this episode, Torsten co-created a simulation of how reputation propagates through a network of agents and how those agents can manipulate their communication strategies, through lies, targeted deceit, propaganda, and many other tactics, in an effort to achieve domination over their networks.</p><p>One of the theses of Torsten’s work is that in order to understand the impact of our AI on our existing systems, it’s important to have a deeper understanding of those systems to begin with. His simulation is a first step in that direction: an attempt to model the interaction of various agents according to information theoretic principles and study the consequent emergent phenomena.<br/><br/>There are two versions of the paper.<br/><br/>The first is a shorter summary of the work, light on the math, and can be found here: <a href='https://psyarxiv.com/wqcmb/'>https://psyarxiv.com/wqcmb/</a>. <br/><br/>The longer version of the paper can be found here: <a href='https://arxiv.org/pdf/2106.05414.pdf'>https://arxiv.org/pdf/2106.05414.pdf</a>.<br/><br/>Enjoy!</p>]]></description>
    <content:encoded><![CDATA[<p>In this episode, Adam speaks with Doctor Torsten Ensslin about simulating reputation networks and their manipulation using Information Theory.<br/><br/>Torsten is an Astrophysicist and cosmologist at the Max Plank institute, where he’s held many titles and positions. His current scientific work investigates theoretical cosmology and information field theory. <br/><br/>As he discusses in this episode, Torsten co-created a simulation of how reputation propagates through a network of agents and how those agents can manipulate their communication strategies, through lies, targeted deceit, propaganda, and many other tactics, in an effort to achieve domination over their networks.</p><p>One of the theses of Torsten’s work is that in order to understand the impact of our AI on our existing systems, it’s important to have a deeper understanding of those systems to begin with. His simulation is a first step in that direction: an attempt to model the interaction of various agents according to information theoretic principles and study the consequent emergent phenomena.<br/><br/>There are two versions of the paper.<br/><br/>The first is a shorter summary of the work, light on the math, and can be found here: <a href='https://psyarxiv.com/wqcmb/'>https://psyarxiv.com/wqcmb/</a>. <br/><br/>The longer version of the paper can be found here: <a href='https://arxiv.org/pdf/2106.05414.pdf'>https://arxiv.org/pdf/2106.05414.pdf</a>.<br/><br/>Enjoy!</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10661436-manipulating-your-reputation.mp3" length="41288740" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10661436</guid>
    <pubDate>Mon, 30 May 2022 00:00:00 -0400</pubDate>
    <itunes:duration>3438</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Multi-Objective AutoML</itunes:title>
    <title>Multi-Objective AutoML</title>
    <itunes:summary><![CDATA[In this episode, Adam discusses Multi-objective optimization with Laurent Parmentier. Laurent works at OVHCloud, most recently as a data scientist but previously in various software engineering roles. He published his thesis on AutoML at OVHCloud, and had previously released a paper titled TPOT-SH: A Faster Optimization Algorithm to Solve the AutoML Problem on Large Datasets. The conversation centers around two classic papers in AutoML: Multi-Objective Automatic Machine Learning with Autoxgbo...]]></itunes:summary>
    <description><![CDATA[<p>In this episode, Adam discusses Multi-objective optimization with Laurent Parmentier.</p><p>Laurent works at OVHCloud, most recently as a data scientist but previously in various software engineering roles. He published his thesis on AutoML at OVHCloud, and had previously released a paper titled TPOT-SH: A Faster Optimization Algorithm to Solve the AutoML Problem on Large Datasets.</p><p>The conversation centers around two classic papers in AutoML:</p><ul><li>Multi-Objective Automatic Machine Learning with AutoxgboostMC by Pfisterer et al: https://arxiv.org/abs/1908.10796<br/><br/></li><li>An ADMM Based Framework for AutoML Pipeline Configuration by Sijia Liu, Parikshit Yam et al: https://arxiv.org/abs/1905.00424</li></ul><p>Enjoy!</p>]]></description>
    <content:encoded><![CDATA[<p>In this episode, Adam discusses Multi-objective optimization with Laurent Parmentier.</p><p>Laurent works at OVHCloud, most recently as a data scientist but previously in various software engineering roles. He published his thesis on AutoML at OVHCloud, and had previously released a paper titled TPOT-SH: A Faster Optimization Algorithm to Solve the AutoML Problem on Large Datasets.</p><p>The conversation centers around two classic papers in AutoML:</p><ul><li>Multi-Objective Automatic Machine Learning with AutoxgboostMC by Pfisterer et al: https://arxiv.org/abs/1908.10796<br/><br/></li><li>An ADMM Based Framework for AutoML Pipeline Configuration by Sijia Liu, Parikshit Yam et al: https://arxiv.org/abs/1905.00424</li></ul><p>Enjoy!</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10618259-multi-objective-automl.mp3" length="34771699" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10618259</guid>
    <pubDate>Mon, 23 May 2022 00:00:00 -0400</pubDate>
    <itunes:duration>2895</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>ML Interpretability with Jessica Schrouff</itunes:title>
    <title>ML Interpretability with Jessica Schrouff</title>
    <itunes:summary><![CDATA[This episode launches us into the deep waters of ML interpretability with Jessica Schrouff. Jessica is a Senior Research Scientist at Google Research working on machine learning for healthcare. Before joining Google in 2019, she was a postdoctoral fellow at University College London (UK) and Stanford University (USA), developing machine learning techniques for neuroscience discovery and clinical predictions. Throughout her career, her interests have lied not only in the technical advancement ...]]></itunes:summary>
    <description><![CDATA[<p>This episode launches us into the deep waters of ML interpretability with Jessica Schrouff.</p><p>Jessica is a Senior Research Scientist at Google Research working on machine learning for healthcare. Before joining Google in 2019, she was a postdoctoral fellow at University College London (UK) and Stanford University (USA), developing machine learning techniques for neuroscience discovery and clinical predictions.</p><p>Throughout her career, her interests have lied not only in the technical advancement of machine learning methods, but also in critical aspects of their deployment such as their credibility, fairness, robustness or interpretability.</p><p>As you’ll hear, we are orbiting Jessica’s latest paper titled “Best of both worlds: local and global explanations with human-understandable concepts”. You will find lots more reference material in the show notes on the website.<br/><br/>Jessica, thank you for joining us!</p>]]></description>
    <content:encoded><![CDATA[<p>This episode launches us into the deep waters of ML interpretability with Jessica Schrouff.</p><p>Jessica is a Senior Research Scientist at Google Research working on machine learning for healthcare. Before joining Google in 2019, she was a postdoctoral fellow at University College London (UK) and Stanford University (USA), developing machine learning techniques for neuroscience discovery and clinical predictions.</p><p>Throughout her career, her interests have lied not only in the technical advancement of machine learning methods, but also in critical aspects of their deployment such as their credibility, fairness, robustness or interpretability.</p><p>As you’ll hear, we are orbiting Jessica’s latest paper titled “Best of both worlds: local and global explanations with human-understandable concepts”. You will find lots more reference material in the show notes on the website.<br/><br/>Jessica, thank you for joining us!</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10513864-ml-interpretability-with-jessica-schrouff.mp3" length="37145005" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10513864</guid>
    <pubDate>Mon, 16 May 2022 00:00:00 -0400</pubDate>
    <itunes:duration>3712</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Continual Learning with Iman Mirzadeh</itunes:title>
    <title>Continual Learning with Iman Mirzadeh</title>
    <itunes:summary><![CDATA[This is a conversation between data scientist Ankush Garg, from Telepath, and fourth-year Ph.D. student Iman Mirzadeh and they’ll be talking about Continual Learning and about Iman’s paper titled “Architecture Matters in Continual Learning”. Iman is interested in Artificial General Intelligence and so he’s researching systems that can over time develop increasingly more complex skills and a richer body of knowledge. Iman received his Bachelor’s degree in Computer Engineering from the Universi...]]></itunes:summary>
    <description><![CDATA[<p>This is a conversation between data scientist Ankush Garg, from Telepath, and fourth-year Ph.D. student Iman Mirzadeh and they’ll be talking about Continual Learning and about Iman’s paper titled “Architecture Matters in Continual Learning”.</p><p>Iman is interested in Artificial General Intelligence and so he’s researching systems that can over time develop increasingly more complex skills and a richer body of knowledge.</p><p>Iman received his Bachelor’s degree in Computer Engineering from the University of Tehran and his Master’s degree from Washington State University and that’s where he’s currently a research assistant.</p>]]></description>
    <content:encoded><![CDATA[<p>This is a conversation between data scientist Ankush Garg, from Telepath, and fourth-year Ph.D. student Iman Mirzadeh and they’ll be talking about Continual Learning and about Iman’s paper titled “Architecture Matters in Continual Learning”.</p><p>Iman is interested in Artificial General Intelligence and so he’s researching systems that can over time develop increasingly more complex skills and a richer body of knowledge.</p><p>Iman received his Bachelor’s degree in Computer Engineering from the University of Tehran and his Master’s degree from Washington State University and that’s where he’s currently a research assistant.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10513852-continual-learning-with-iman-mirzadeh.mp3" length="24264235" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10513852</guid>
    <pubDate>Mon, 02 May 2022 00:00:00 -0400</pubDate>
    <itunes:duration>2019</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>MLOps: Research and Vision</itunes:title>
    <title>MLOps: Research and Vision</title>
    <itunes:summary><![CDATA[Today we’re talking about MLOps - with our guide Georgios Symeonidis and we’ll be orienting around a recent paper he published titled “MLOps - Definitions, Tools and Challenges”. Georgios studied electrical and computer engineering at Democritus University of Thrace at Xanthi, in Greece. He specialized in information and electronics. He’s also worked as a research engineer at Athena Research and Innovation Center - studying ML systems in production. For his PhD, he’s researching Reliable Mach...]]></itunes:summary>
    <description><![CDATA[<p>Today we’re talking about MLOps - with our guide Georgios Symeonidis and we’ll be orienting around a recent paper he published titled “MLOps - Definitions, Tools and Challenges”.</p><p>Georgios studied electrical and computer engineering at Democritus University of Thrace at Xanthi, in Greece. He specialized in information and electronics.</p><p>He’s also worked as a research engineer at Athena Research and Innovation Center - studying ML systems in production. For his PhD, he’s researching Reliable Machine Learning pipelines for mature MLOps systems at International Hellenic University in Greece.</p>]]></description>
    <content:encoded><![CDATA[<p>Today we’re talking about MLOps - with our guide Georgios Symeonidis and we’ll be orienting around a recent paper he published titled “MLOps - Definitions, Tools and Challenges”.</p><p>Georgios studied electrical and computer engineering at Democritus University of Thrace at Xanthi, in Greece. He specialized in information and electronics.</p><p>He’s also worked as a research engineer at Athena Research and Innovation Center - studying ML systems in production. For his PhD, he’s researching Reliable Machine Learning pipelines for mature MLOps systems at International Hellenic University in Greece.</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10513802-mlops-research-and-vision.mp3" length="30265585" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10513802</guid>
    <pubDate>Wed, 27 Apr 2022 08:00:00 -0400</pubDate>
    <itunes:duration>2520</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Curriculum Learning in AutoML</itunes:title>
    <title>Curriculum Learning in AutoML</title>
    <itunes:summary><![CDATA[This episode covers the relationship between Curriculum Learning and AutoML with Lucas Nildaimon dos Santos Silva.  Lucas is a data scientist at americanas s.a., in Brazil, and is currently pursuing a Ph.D. in computer science from the Federal University of São Carlos where he also did his master’s. He’s researching machine learning and NLP and has recently published a paper on Curriculum Learning in AutoML systems, found below.  https://ieeexplore.ieee.org/document/9680164 ]]></itunes:summary>
    <description><![CDATA[<p>This episode covers the relationship between Curriculum Learning and AutoML with Lucas Nildaimon dos Santos Silva.<br/><br/>Lucas is a data scientist at americanas s.a., in Brazil, and is currently pursuing a Ph.D. in computer science from the Federal University of São Carlos where he also did his master’s. He’s researching machine learning and NLP and has recently published a paper on Curriculum Learning in AutoML systems, found below.<br/><br/>https://ieeexplore.ieee.org/document/9680164</p>]]></description>
    <content:encoded><![CDATA[<p>This episode covers the relationship between Curriculum Learning and AutoML with Lucas Nildaimon dos Santos Silva.<br/><br/>Lucas is a data scientist at americanas s.a., in Brazil, and is currently pursuing a Ph.D. in computer science from the Federal University of São Carlos where he also did his master’s. He’s researching machine learning and NLP and has recently published a paper on Curriculum Learning in AutoML systems, found below.<br/><br/>https://ieeexplore.ieee.org/document/9680164</p>]]></content:encoded>
    <itunes:author>AutoML Media</itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10208404-curriculum-learning-in-automl.mp3" length="20832043" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10208404</guid>
    <pubDate>Mon, 07 Mar 2022 22:00:00 -0500</pubDate>
    <itunes:duration>1733</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:season>1</itunes:season>
    <itunes:episode>15</itunes:episode>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
  <item>
    <itunes:title>Statistical Physics and Inference Problems</itunes:title>
    <title>Statistical Physics and Inference Problems</title>
    <itunes:summary><![CDATA[In this episode, we explore the relationship between Machine Learning and Statistical Mechanics with the guidance of Alia Abbara. This conversation centers around her PhD dissertation and we cover such topics as the relationship between statistics and physics,  the long legacy of physics on machine learning, and the role of physical intuition in the future of machine learning.  Find her original paper here: https://tel.archives-ouvertes.fr/tel-03497407/document ]]></itunes:summary>
    <description><![CDATA[<p>In this episode, we explore the relationship between Machine Learning and Statistical Mechanics with the guidance of Alia Abbara. This conversation centers around her PhD dissertation and we cover such topics as the relationship between statistics and physics,  the long legacy of physics on machine learning, and the role of physical intuition in the future of machine learning.<br/><br/>Find her original paper here: https://tel.archives-ouvertes.fr/tel-03497407/document</p>]]></description>
    <content:encoded><![CDATA[<p>In this episode, we explore the relationship between Machine Learning and Statistical Mechanics with the guidance of Alia Abbara. This conversation centers around her PhD dissertation and we cover such topics as the relationship between statistics and physics,  the long legacy of physics on machine learning, and the role of physical intuition in the future of machine learning.<br/><br/>Find her original paper here: https://tel.archives-ouvertes.fr/tel-03497407/document</p>]]></content:encoded>
    <itunes:author></itunes:author>
    <enclosure url="https://www.buzzsprout.com/1947123/episodes/10148666-statistical-physics-and-inference-problems.mp3" length="25060734" type="audio/mpeg" />
    <guid isPermaLink="false">Buzzsprout-10148666</guid>
    <pubDate>Sat, 26 Feb 2022 18:00:00 -0500</pubDate>
    <itunes:duration>2086</itunes:duration>
    <itunes:keywords></itunes:keywords>
    <itunes:episodeType>full</itunes:episodeType>
    <itunes:explicit>false</itunes:explicit>
  </item>
</channel>
</rss>
