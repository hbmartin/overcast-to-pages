<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:anchor="https://anchor.fm/xmlns" xmlns:podcast="https://podcastindex.org/namespace/1.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:psc="http://podlove.org/simple-chapters">
	<channel>
		<title><![CDATA[Weaviate Podcast]]></title>
		<description><![CDATA[Join Connor Shorten as he interviews machine learning experts and explores Weaviate use cases from users and customers.

]]></description>
		<link>https://weaviate.io/</link>
		<generator>Anchor Podcasts</generator>
		<lastBuildDate>Wed, 25 Jun 2025 14:33:02 GMT</lastBuildDate>
		<atom:link href="https://anchor.fm/s/cffc3468/podcast/rss" rel="self" type="application/rss+xml"/>
		<author><![CDATA[Weaviate]]></author>
		<copyright><![CDATA[Weaviate]]></copyright>
		<language><![CDATA[en]]></language>
		<atom:link rel="hub" href="https://pubsubhubbub.appspot.com/"/>
		<itunes:author>Weaviate</itunes:author>
		<itunes:summary>Join Connor Shorten as he interviews machine learning experts and explores Weaviate use cases from users and customers.

</itunes:summary>
		<itunes:type>episodic</itunes:type>
		<itunes:owner>
			<itunes:name>Weaviate</itunes:name>
			<itunes:email>connor@weaviate.io</itunes:email>
		</itunes:owner>
		<itunes:explicit>false</itunes:explicit>
		<itunes:category text="Technology"/>
		<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
		<item>
			<title><![CDATA[RAG Benchmarks with Nandan Thakur - Weaviate Podcast #124!]]></title>
			<description><![CDATA[<p>Nandan Thakur is a Ph.D. student at the University of Waterloo! Nandan has worked on many of the most impactful works in Retrieval-Augmented Generation (RAG) and Information Retrieval. His work ranges from benchmarks such as BEIR, MIRACLE, TREC, and FreshStack, to improving the training of embedding models and re-rankings, and more!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/RAG-Benchmarks-with-Nandan-Thakur---Weaviate-Podcast-124-e34n9i6</link>
			<guid isPermaLink="false">61cba157-302f-4944-9f96-85f9ddff04a5</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 25 Jun 2025 13:05:37 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/104621062/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-5-25%2F26da8af9-1b15-05ed-a8d1-0412d9d8e7cb.mp3" length="126644175" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Nandan Thakur is a Ph.D. student at the University of Waterloo! Nandan has worked on many of the most impactful works in Retrieval-Augmented Generation (RAG) and Information Retrieval. His work ranges from benchmarks such as BEIR, MIRACLE, TREC, and FreshStack, to improving the training of embedding models and re-rankings, and more!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:04:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[MUVERA with Rajesh Jayaram and Roberto Esposito - Weaviate Podcast #123!]]></title>
			<description><![CDATA[<p>Multi-vector retrieval offers richer, more nuanced search, but often comes with a significant cost in storage and computational overhead. How can we harness the power of multi-vector representations without breaking the bank? Rajesh Jayaram, the first author of the groundbreaking MUVERA algorithm from Google, and Roberto Esposito from Weaviate, who spearheaded its implementation, reveal how MUVERA tackles this critical challenge.</p><p>Dive deep into MUVERA, a novel compression technique specifically designed for multi-vector retrieval. Rajesh and Roberto explain how it leverages contextualized token embeddings and innovative fixed dimensional encodings to dramatically reduce storage requirements while maintaining high retrieval accuracy. Discover the intricacies of quantization within MUVERA, the interpretability benefits of this approach, and how LSH clustering can play a role in topic modeling with these compressed representations.</p><p>This conversation explores the core mechanics of efficient multi-vector retrieval, the challenges of benchmarking these advanced systems, and the evolving landscape of vector database schemas designed to handle such complex data. Rajesh and Roberto also share their insights on the future directions in artificial intelligence where efficient, high-dimensional data representation is paramount.</p><p>Whether you&#39;re an AI researcher grappling with the scalability of vector search, an engineer building advanced retrieval systems, or fascinated by the cutting edge of information retrieval and AI frameworks, this episode delivers unparalleled insights directly from the source. You&#39;ll gain a fundamental understanding of MUVERA, practical considerations for its application in making multi-vector retrieval feasible, and a clear view of future directions in AI.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/MUVERA-with-Rajesh-Jayaram-and-Roberto-Esposito---Weaviate-Podcast-123-e33fnpi</link>
			<guid isPermaLink="false">3c00bcb6-5bf5-4fde-90cc-4becc05b53fd</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 28 May 2025 13:48:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/103324914/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-28%2Fbb2ebe32-4baa-fc59-5b80-0091a6c444b6.mp3" length="141826446" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Multi-vector retrieval offers richer, more nuanced search, but often comes with a significant cost in storage and computational overhead. How can we harness the power of multi-vector representations without breaking the bank? Rajesh Jayaram, the first author of the groundbreaking MUVERA algorithm from Google, and Roberto Esposito from Weaviate, who spearheaded its implementation, reveal how MUVERA tackles this critical challenge.&lt;/p&gt;&lt;p&gt;Dive deep into MUVERA, a novel compression technique specifically designed for multi-vector retrieval. Rajesh and Roberto explain how it leverages contextualized token embeddings and innovative fixed dimensional encodings to dramatically reduce storage requirements while maintaining high retrieval accuracy. Discover the intricacies of quantization within MUVERA, the interpretability benefits of this approach, and how LSH clustering can play a role in topic modeling with these compressed representations.&lt;/p&gt;&lt;p&gt;This conversation explores the core mechanics of efficient multi-vector retrieval, the challenges of benchmarking these advanced systems, and the evolving landscape of vector database schemas designed to handle such complex data. Rajesh and Roberto also share their insights on the future directions in artificial intelligence where efficient, high-dimensional data representation is paramount.&lt;/p&gt;&lt;p&gt;Whether you&amp;#39;re an AI researcher grappling with the scalability of vector search, an engineer building advanced retrieval systems, or fascinated by the cutting edge of information retrieval and AI frameworks, this episode delivers unparalleled insights directly from the source. You&amp;#39;ll gain a fundamental understanding of MUVERA, practical considerations for its application in making multi-vector retrieval feasible, and a clear view of future directions in AI.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:13:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Patronus AI with Anand Kannappan - Weaviate Podcast #122!]]></title>
			<description><![CDATA[<p>AI agents are getting more complex and harder to debug. How do you know what&#39;s happening when your agent makes 20+ function calls? What if you have a Multi-Agent System orchestrating several Agents? Anand Kannappan, co-founder of Patronus AI, reveals how their groundbreaking tool Percival transforms agent debugging and evaluation. Percival can instantly analyze complex agent traces, it pinpoints failures across 60 different modes, and it automatically suggests prompt fixes to improve performance. Anand unpacks several of these common failure modes. This includes the critical challenges of &quot;context explosion&quot; where agents process millions of tokens. He also explains domain adaptation for specific use cases, and the complex challenge of multi-agent orchestration. The paradigm of AI Evals is shifting from static evaluation to dynamic oversight! Also learn how Percival&#39;s memory architecture leverages both episodic and semantic knowledge with Weaviate!This conversation explores powerful concepts like process vs. outcome rewards and LLM-as-judge approaches. Anand shares his vision for &quot;agentic supervision&quot; where equally capable AI systems provide oversight for complex agent workflows. Whether you&#39;re building AI agents, evaluating LLM systems, or interested in how debugging autonomous systems will evolve, this episode delivers concrete techniques. You&#39;ll gain philosophical insights on evaluation and a roadmap for how evaluation must transform to keep pace with increasingly autonomous AI systems.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Patronus-AI-with-Anand-Kannappan---Weaviate-Podcast-122-e32srl0</link>
			<guid isPermaLink="false">dfd5a4b4-46a8-4ed3-a21c-e42443fc2a33</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 15 May 2025 14:00:19 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/102706272/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-15%2F8e0ecc09-3370-7cfc-a77a-8c6b2681e527.mp3" length="119429526" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;AI agents are getting more complex and harder to debug. How do you know what&amp;#39;s happening when your agent makes 20+ function calls? What if you have a Multi-Agent System orchestrating several Agents? Anand Kannappan, co-founder of Patronus AI, reveals how their groundbreaking tool Percival transforms agent debugging and evaluation. Percival can instantly analyze complex agent traces, it pinpoints failures across 60 different modes, and it automatically suggests prompt fixes to improve performance. Anand unpacks several of these common failure modes. This includes the critical challenges of &amp;quot;context explosion&amp;quot; where agents process millions of tokens. He also explains domain adaptation for specific use cases, and the complex challenge of multi-agent orchestration. The paradigm of AI Evals is shifting from static evaluation to dynamic oversight! Also learn how Percival&amp;#39;s memory architecture leverages both episodic and semantic knowledge with Weaviate!This conversation explores powerful concepts like process vs. outcome rewards and LLM-as-judge approaches. Anand shares his vision for &amp;quot;agentic supervision&amp;quot; where equally capable AI systems provide oversight for complex agent workflows. Whether you&amp;#39;re building AI agents, evaluating LLM systems, or interested in how debugging autonomous systems will evolve, this episode delivers concrete techniques. You&amp;#39;ll gain philosophical insights on evaluation and a roadmap for how evaluation must transform to keep pace with increasingly autonomous AI systems.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:01:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Haize Labs with Leonard Tang - Weaviate Podcast #121!]]></title>
			<description><![CDATA[<p>How do you ensure your AI systems actually do what you expect them to do? Leonard Tang takes us deep into the revolutionary world of AI evaluation with concrete techniques you can apply today. Learn how Haize Labs is transforming AI testing through &quot;scaling judge-time compute&quot; - stacking weaker models to effectively evaluate stronger ones. Leonard unpacks the game-changing Verdict library that outperforms frontier models by 10-20% while dramatically reducing costs. Discover practical insights on creating contrastive evaluation sets that extract maximum signal from human feedback, implementing debate-based judging systems, and building custom reward models that align with enterprise needs. The conversation reveals powerful nuggets like using randomized agent debates to achieve consensus and lightweight guardrail models that run alongside inference. Whether you&#39;re developing AI applications or simply fascinated by how we&#39;ll ensure increasingly powerful AI systems perform as expected, this episode delivers immediate value with techniques you can implement right away, philosophical perspectives on AI safety, and a glimpse into the future of evaluation that will fundamentally shape how AI evolves.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Haize-Labs-with-Leonard-Tang---Weaviate-Podcast-121-e32mts3</link>
			<guid isPermaLink="false">55d07111-2348-48da-a1a2-f6a38cf2c3f6</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 12 May 2025 14:56:52 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/102511939/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-11%2F9301155a-7b15-111f-e54c-5c1d7c0d9caf.mp3" length="105959363" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;How do you ensure your AI systems actually do what you expect them to do? Leonard Tang takes us deep into the revolutionary world of AI evaluation with concrete techniques you can apply today. Learn how Haize Labs is transforming AI testing through &amp;quot;scaling judge-time compute&amp;quot; - stacking weaker models to effectively evaluate stronger ones. Leonard unpacks the game-changing Verdict library that outperforms frontier models by 10-20% while dramatically reducing costs. Discover practical insights on creating contrastive evaluation sets that extract maximum signal from human feedback, implementing debate-based judging systems, and building custom reward models that align with enterprise needs. The conversation reveals powerful nuggets like using randomized agent debates to achieve consensus and lightweight guardrail models that run alongside inference. Whether you&amp;#39;re developing AI applications or simply fascinated by how we&amp;#39;ll ensure increasingly powerful AI systems perform as expected, this episode delivers immediate value with techniques you can implement right away, philosophical perspectives on AI safety, and a glimpse into the future of evaluation that will fundamentally shape how AI evolves.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:54:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Box AI with Ben Kus and Bob van Luijt]]></title>
			<description><![CDATA[<p>Ben walks us through Box&#39;s three-layer infrastructure puzzle: First, the mind-boggling base infrastructure (think millions of interactions per second and trillions of files). Second, their unique multi-tenant security challenge - unlike most SaaS platforms, Box users share content across company boundaries, making traditional tenant isolation impossible. And third, ensuring AI respects all these complex permissions while still delivering value. The podcast then dives further into how vector embeddings can balloon file sizes - a few hundred bytes of text can require 4-6KB of vector data storage! We also dig into why RAG remains essential despite growing context windows, and how Box is developing AI agents that transform painful enterprise processes like RFP responses.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Box-AI-with-Ben-Kus-and-Bob-van-Luijt-e32gu1n</link>
			<guid isPermaLink="false">454029c3-d570-4757-a86b-060249bb4a79</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 07 May 2025 14:00:01 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/102315511/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-7%2F05827969-da2c-f44d-2786-26c8119f4446.mp3" length="109004864" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Ben walks us through Box&amp;#39;s three-layer infrastructure puzzle: First, the mind-boggling base infrastructure (think millions of interactions per second and trillions of files). Second, their unique multi-tenant security challenge - unlike most SaaS platforms, Box users share content across company boundaries, making traditional tenant isolation impossible. And third, ensuring AI respects all these complex permissions while still delivering value. The podcast then dives further into how vector embeddings can balloon file sizes - a few hundred bytes of text can require 4-6KB of vector data storage! We also dig into why RAG remains essential despite growing context windows, and how Box is developing AI agents that transform painful enterprise processes like RFP responses.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:32</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Structured Outputs with Will Kurt and Cameron Pfiffer - Weaviate Podcast #119!]]></title>
			<description><![CDATA[<p>Hey everyone! Thanks so much for watching another episode of the Weaviate Podcast! Dive into the fascinating world of structured outputs with Will Kurt and Cameron Pfeiffer, the brilliant minds behind Outlines, the revolutionary open-source library from .txt.ai that&#39;s changing how we interact with LLMs. In this episode, we explore how constrained decoding enables predictable, reliable outputs from language models—unlocking everything from perfect JSON generation to guided reasoning processes.Will and Cameron share their journey to founding .txt.ai, explain the technical magic behind Outlines (hint: it involves finite state machines!), and debunk misconceptions around structured generation performance. You&#39;ll discover practical applications like knowledge graph construction, metadata extraction, and report generation that simply weren&#39;t possible before this technology.Whether you&#39;re building AI systems or curious about where the field is heading, you&#39;ll gain valuable insights on how structured outputs integrate with inference engines like vLLM, why multi-task inference outperforms single-task approaches, and how this technology enables scalable agent systems that could transform software architecture forever. Join us for this mind-expanding conversation about one of AI&#39;s most important but under appreciated innovations—and discover why the future might belong to systems that combine freedom with structure.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Structured-Outputs-with-Will-Kurt-and-Cameron-Pfiffer---Weaviate-Podcast-119-e31apoq</link>
			<guid isPermaLink="false">79e196de-90c4-49d4-8fa5-bc44691e71fa</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 09 Apr 2025 15:47:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/101065946/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-3-9%2Fd7dc7aba-2bfd-43db-8ec5-c406386a45e8.mp3" length="137563456" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thanks so much for watching another episode of the Weaviate Podcast! Dive into the fascinating world of structured outputs with Will Kurt and Cameron Pfeiffer, the brilliant minds behind Outlines, the revolutionary open-source library from .txt.ai that&amp;#39;s changing how we interact with LLMs. In this episode, we explore how constrained decoding enables predictable, reliable outputs from language models—unlocking everything from perfect JSON generation to guided reasoning processes.Will and Cameron share their journey to founding .txt.ai, explain the technical magic behind Outlines (hint: it involves finite state machines!), and debunk misconceptions around structured generation performance. You&amp;#39;ll discover practical applications like knowledge graph construction, metadata extraction, and report generation that simply weren&amp;#39;t possible before this technology.Whether you&amp;#39;re building AI systems or curious about where the field is heading, you&amp;#39;ll gain valuable insights on how structured outputs integrate with inference engines like vLLM, why multi-task inference outperforms single-task approaches, and how this technology enables scalable agent systems that could transform software architecture forever. Join us for this mind-expanding conversation about one of AI&amp;#39;s most important but under appreciated innovations—and discover why the future might belong to systems that combine freedom with structure.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:10:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Synthetic Data with David Berenstein and Ben Burtenshaw - Weaviate Podcast #118!]]></title>
			<description><![CDATA[<p>Synthetic Data: The Building Bocks of AI&#39;s Future! Hey everyone! I am SUPER EXCITED to publish the 118th episode of the Weaviate Podcast featuring David Berenstein and Ben Burtenshaw from HuggingFace! This podcast explores the intricacies of synthetic data generation, detailing methodologies such as data augmentation, distillation, and instruction refinement. The conversation delves into persona-driven synthetic data, highlighting applications like Persona Hub, and discusses algorithms to enhance diversity, complexity, and quality of generated data. Additionally, they cover integration with Hugging Face’s ecosystem, including Argilla for annotation, AutoTrain for fine-tuning, and advanced data exploration tools like the Data Studio and SQL console. The podcast also touches upon the potential for synthetic image data generation and the exciting future of AI education and accessibility.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Synthetic-Data-with-David-Berenstein-and-Ben-Burtenshaw---Weaviate-Podcast-118-e30l10v</link>
			<guid isPermaLink="false">8e2aa6e2-4f9d-4125-bc9b-6dd4509ffcdf</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 25 Mar 2025 13:59:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/100352479/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-25%2F70166db4-a1e4-ead9-9e57-3fcae3adda3f.mp3" length="120983236" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Synthetic Data: The Building Bocks of AI&amp;#39;s Future! Hey everyone! I am SUPER EXCITED to publish the 118th episode of the Weaviate Podcast featuring David Berenstein and Ben Burtenshaw from HuggingFace! This podcast explores the intricacies of synthetic data generation, detailing methodologies such as data augmentation, distillation, and instruction refinement. The conversation delves into persona-driven synthetic data, highlighting applications like Persona Hub, and discusses algorithms to enhance diversity, complexity, and quality of generated data. Additionally, they cover integration with Hugging Face’s ecosystem, including Argilla for annotation, AutoTrain for fine-tuning, and advanced data exploration tools like the Data Studio and SQL console. The podcast also touches upon the potential for synthetic image data generation and the exciting future of AI education and accessibility.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:01</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Letta AI with Sarah Wooders - Weaviate Podcast #117!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 117th episode of the Weaviate podcast! In this episode, we dive deep into the cutting edge of AI agent development with Sarah Wooders, co-founder and CTO of Letta AI. Emerging from Berkeley&#39;s Sky Computing Lab, Sarah and her team have pioneered a revolutionary approach to stateful agents - AI systems that genuinely remember both you and themselves across extended conversations. The conversation explores how the groundbreaking MemGPT project evolved into Letta&#39;s comprehensive Agent Development Environment (ADE), which empowers developers to build truly persistent AI experiences. Sarah shares powerful insights on context management, memory prioritization, and the critical role of databases in agent architecture. Whether you&#39;re building AI systems or simply curious about where conversational AI is heading, this episode illuminates how the future of agents depends not just on their reasoning capabilities, but on their ability to maintain coherent identity and memory over time.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Letta-AI-with-Sarah-Wooders---Weaviate-Podcast-117-e2vl587</link>
			<guid isPermaLink="false">ce27d1e5-a244-48bc-a6c5-5fc4097a82c8</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 03 Mar 2025 17:00:47 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/99308231/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-3%2Fc804cb38-756a-0dfa-cf38-2cb021fde695.mp3" length="112349019" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 117th episode of the Weaviate podcast! In this episode, we dive deep into the cutting edge of AI agent development with Sarah Wooders, co-founder and CTO of Letta AI. Emerging from Berkeley&amp;#39;s Sky Computing Lab, Sarah and her team have pioneered a revolutionary approach to stateful agents - AI systems that genuinely remember both you and themselves across extended conversations. The conversation explores how the groundbreaking MemGPT project evolved into Letta&amp;#39;s comprehensive Agent Development Environment (ADE), which empowers developers to build truly persistent AI experiences. Sarah shares powerful insights on context management, memory prioritization, and the critical role of databases in agent architecture. Whether you&amp;#39;re building AI systems or simply curious about where conversational AI is heading, this episode illuminates how the future of agents depends not just on their reasoning capabilities, but on their ability to maintain coherent identity and memory over time.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:34</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Agent Experience with Matt Biilmann, Sebastian Witalec, and Charles Pierse - Weaviate Podcast #116!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching another episode of the Weaviate Podcast! I am SUPER excited to welcome Matt Biilmann, Co-Founder and CEO of Netlify, as well as Sebastian Witalec and Charles Pierse from Weaviate to discuss Agent Experience! You have probably heard about how you can connect LLMs to external software tools. This supercharges the capabilities of AI systems and what they can do. So what does that mean for you as a software developer?This podcast explores different ideas around designing software user experiences for Agents as well as Humans. How do we write documentation for Agents differently than Humans? How do we design REST or gRPC APIs, or programming languages clients, for Agents differently than Humans? llms.txt, JSON tool definitions, agents to agent, breaking changes, … there were so many interesting topics explored in this podcast! I really hope you find it useful! As always more than happy to discuss these ideas further with you!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Agent-Experience-with-Matt-Biilmann--Sebastian-Witalec--and-Charles-Pierse---Weaviate-Podcast-116-e2vfbqq</link>
			<guid isPermaLink="false">7b338fda-b7b1-4b90-b9d4-0f0d3b7a67cf</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 27 Feb 2025 14:58:24 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/99118362/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-27%2Fb96f14cc-1835-d637-e194-85ef85f946d0.mp3" length="102139472" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching another episode of the Weaviate Podcast! I am SUPER excited to welcome Matt Biilmann, Co-Founder and CEO of Netlify, as well as Sebastian Witalec and Charles Pierse from Weaviate to discuss Agent Experience! You have probably heard about how you can connect LLMs to external software tools. This supercharges the capabilities of AI systems and what they can do. So what does that mean for you as a software developer?This podcast explores different ideas around designing software user experiences for Agents as well as Humans. How do we write documentation for Agents differently than Humans? How do we design REST or gRPC APIs, or programming languages clients, for Agents differently than Humans? llms.txt, JSON tool definitions, agents to agent, breaking changes, … there were so many interesting topics explored in this podcast! I really hope you find it useful! As always more than happy to discuss these ideas further with you!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:52:09</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Optimizing Retrieval Agents with Shirley Wu - Weaviate Podcast #115!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 115th episode of the Weaviate Podcast featuring Shirley Wu from Stanford University!</p><p>We explore the innovative Avatar Optimizer—a novel framework that leverages contrastive reasoning to refine LLM agent prompts for optimal tool usage. Shirley explains how this self-improving system evolves through iterative feedback by contrasting positive and negative examples, enabling agents to handle complex tasks more effectively.</p><p>We also dive into the STaRK Benchmark, a comprehensive testbed designed to evaluate retrieval systems on semi-structured knowledge bases. The discussion highlights the challenges of unifying textual and relational retrieval, exploring concepts such as multi-vector embeddings, relational graphs, and dynamic data modeling. Learn how these approaches help overcome information loss, enhance precision, and enable scalable, context-aware retrieval in diverse domains—from product recommendations to precision medicine.</p><p>Whether you’re interested in advanced prompt optimization, multi-agent system design, or the future of human-centered language models, this episode offers a wealth of insights and a forward-looking perspective on integrating sophisticated AI techniques into real-world applications.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Optimizing-Retrieval-Agents-with-Shirley-Wu---Weaviate-Podcast-115-e2v3d59</link>
			<guid isPermaLink="false">9b770b4c-771f-4148-8d61-470117ed87bc</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 19 Feb 2025 15:00:04 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/98726505/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-19%2Fcf660243-f314-2b65-3d30-f32aa114a7ff.mp3" length="117896413" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 115th episode of the Weaviate Podcast featuring Shirley Wu from Stanford University!&lt;/p&gt;&lt;p&gt;We explore the innovative Avatar Optimizer—a novel framework that leverages contrastive reasoning to refine LLM agent prompts for optimal tool usage. Shirley explains how this self-improving system evolves through iterative feedback by contrasting positive and negative examples, enabling agents to handle complex tasks more effectively.&lt;/p&gt;&lt;p&gt;We also dive into the STaRK Benchmark, a comprehensive testbed designed to evaluate retrieval systems on semi-structured knowledge bases. The discussion highlights the challenges of unifying textual and relational retrieval, exploring concepts such as multi-vector embeddings, relational graphs, and dynamic data modeling. Learn how these approaches help overcome information loss, enhance precision, and enable scalable, context-aware retrieval in diverse domains—from product recommendations to precision medicine.&lt;/p&gt;&lt;p&gt;Whether you’re interested in advanced prompt optimization, multi-agent system design, or the future of human-centered language models, this episode offers a wealth of insights and a forward-looking perspective on integrating sophisticated AI techniques into real-world applications.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:20</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Contextual AI with Amanpreet Singh - Weaviate Podcast #114!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 114th episode of the Weaviate Podcast featuring Amanpreet Singh, Co-Founder and CTO of Contextual AI! Contextual AI is at the forefront of production-grade RAG agents! I learned so much from this conversation! We began by discussing the vision of RAG 2.0, jointly optimizing generative and retrieval models! This then lead us to discuss Agentic RAG and how the RAG 2.0 roadmap is evolving with emerging perspectives on tool use. Amanpreet continues to further motivate the importance of continual learning of the model and the prompt / few-shot examples -- discussing the limits of prompt engineering. Personally I have to admit I think I have been a bit too bullish on only tuning instructions / examples, Amanpreet made an excellent case for updating the weights of the models as well -- citing issues such as parametric knowledge conflicts, and later on discussing how Mechanistic Interpretability is used to audit models and their updates in enterprise settings. We then discussed Contextual AI&#39;s LMUnit for evaluating these systems. This then lead us into my favorite part of the podcast, a deep dive into RL algorithms for LLMs. I highly recommend checking out the links below to learn more about Contextual&#39;s innovations on APO and KTO! We then discuss the importance of domain specific data, Mechanistic Interpretability, return to another question on RAG 2.0, and conclude with Amanpreet&#39;s most exciting future directions for AI! I hope you enjoy the podcast!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Contextual-AI-with-Amanpreet-Singh---Weaviate-Podcast-114-e2up9pb</link>
			<guid isPermaLink="false">817e9552-3fc8-429f-9fe3-54f5b4e92734</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 12 Feb 2025 15:00:59 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/98395371/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-12%2Fd59f87fc-c656-9d69-1026-83d964deebdd.mp3" length="112728118" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 114th episode of the Weaviate Podcast featuring Amanpreet Singh, Co-Founder and CTO of Contextual AI! Contextual AI is at the forefront of production-grade RAG agents! I learned so much from this conversation! We began by discussing the vision of RAG 2.0, jointly optimizing generative and retrieval models! This then lead us to discuss Agentic RAG and how the RAG 2.0 roadmap is evolving with emerging perspectives on tool use. Amanpreet continues to further motivate the importance of continual learning of the model and the prompt / few-shot examples -- discussing the limits of prompt engineering. Personally I have to admit I think I have been a bit too bullish on only tuning instructions / examples, Amanpreet made an excellent case for updating the weights of the models as well -- citing issues such as parametric knowledge conflicts, and later on discussing how Mechanistic Interpretability is used to audit models and their updates in enterprise settings. We then discussed Contextual AI&amp;#39;s LMUnit for evaluating these systems. This then lead us into my favorite part of the podcast, a deep dive into RL algorithms for LLMs. I highly recommend checking out the links below to learn more about Contextual&amp;#39;s innovations on APO and KTO! We then discuss the importance of domain specific data, Mechanistic Interpretability, return to another question on RAG 2.0, and conclude with Amanpreet&amp;#39;s most exciting future directions for AI! I hope you enjoy the podcast!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:56</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Cartesia AI with Karan Goel - Weaviate Podcast #113!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 113th episode of the Weaviate Podcast with Karan Goel from Cartesia AI! Cartesia AI is leading the AI world in text-to-speech models! As exciting as these new applications in speech generation are, Cartesia is also building around an incredibly exciting new neural network architecture that cuts across all of AI -- State Space Models. State Space Models (SSMs) present a new approach to modeling long sequences circumventing the quadratic attention bottlenecks of transformers. In the podcast, we discuss Karan&#39;s perspectives around end-to-end modeling, long context and Multimodal processing, building and deploying a new kind of model, and more! I hope you find the podcast interesting and useful! As always more than happy to discuss these ideas further with you! Thanks for listening!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Cartesia-AI-with-Karan-Goel---Weaviate-Podcast-113-e2u3jpq</link>
			<guid isPermaLink="false">2a6f0778-9a46-49f4-973b-72153b901741</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 28 Jan 2025 15:55:45 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/97684730/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-28%2F2df75eba-1ddc-4fec-b1a9-c02ee2855be2.mp3" length="104723230" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 113th episode of the Weaviate Podcast with Karan Goel from Cartesia AI! Cartesia AI is leading the AI world in text-to-speech models! As exciting as these new applications in speech generation are, Cartesia is also building around an incredibly exciting new neural network architecture that cuts across all of AI -- State Space Models. State Space Models (SSMs) present a new approach to modeling long sequences circumventing the quadratic attention bottlenecks of transformers. In the podcast, we discuss Karan&amp;#39;s perspectives around end-to-end modeling, long context and Multimodal processing, building and deploying a new kind of model, and more! I hope you find the podcast interesting and useful! As always more than happy to discuss these ideas further with you! Thanks for listening!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Google Vertex AI RAG Engine with Lewis Liu and Bob van Luijt - Weaviate Podcast #112!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 112th episode of the Weaviate Podcast! This is another super exciting one, diving into the release of the Vertex AI RAG Engine, its integration with Weaviate and thoughts on the future of connecting AI systems with knowledge sources! The podcast begins by reflecting on Bob's experience speaking at Google in 2016 on Knowledge Graphs! This transitions into discussing the evolution of knowledge representation perspectives and things like the semantic web, ontologies, search indexes, and data warehouses. This then leads to discussing how much knowledge is encoded in the prompts themselves and the resurrection of rule-based systems with LLMs! The podcast transitions back to topics around the modern consensus in RAG pipeline engineering. Lewis suggests that parsing in data ingestion is the biggest bottleneck and low hanging fruit to fix. Bob presents the re-indexing problem and how it is additionally complicated with embedding models! Discussing the state of knowledge representation systems inspired me to ask Bob further about his vision with Generative Feedback Loops and controlling databases with LLMs, How open ended will this be? We then discuss the role that Agentic Architectures and Compound AI Systems are having on the state of AI. What is the right way to connect prompts with other prompts, external tools, and agents? The podcast then concludes by discussing a really interesting emerging pattern in the deployment of RAG systems. Whereas the first generation of RAG systems typically were user facing, such as customer support chatbots, the next generation is more API-based. The launch of the Vertex AI RAG Engine quickly shows you how to use RAG Engine as a tool for a Gemini Agent!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Google-Vertex-AI-RAG-Engine-with-Lewis-Liu-and-Bob-van-Luijt---Weaviate-Podcast-112-e2thhse</link>
			<guid isPermaLink="false">04a8fb4a-4f2c-4576-9859-fa3b32ab03f8</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 15 Jan 2025 15:57:26 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/97092942/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-15%2F7716d973-43d3-dc3b-decc-2c7d48bd4902.mp3" length="114179902" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 112th episode of the Weaviate Podcast! This is another super exciting one, diving into the release of the Vertex AI RAG Engine, its integration with Weaviate and thoughts on the future of connecting AI systems with knowledge sources! The podcast begins by reflecting on Bob&apos;s experience speaking at Google in 2016 on Knowledge Graphs! This transitions into discussing the evolution of knowledge representation perspectives and things like the semantic web, ontologies, search indexes, and data warehouses. This then leads to discussing how much knowledge is encoded in the prompts themselves and the resurrection of rule-based systems with LLMs! The podcast transitions back to topics around the modern consensus in RAG pipeline engineering. Lewis suggests that parsing in data ingestion is the biggest bottleneck and low hanging fruit to fix. Bob presents the re-indexing problem and how it is additionally complicated with embedding models! Discussing the state of knowledge representation systems inspired me to ask Bob further about his vision with Generative Feedback Loops and controlling databases with LLMs, How open ended will this be? We then discuss the role that Agentic Architectures and Compound AI Systems are having on the state of AI. What is the right way to connect prompts with other prompts, external tools, and agents? The podcast then concludes by discussing a really interesting emerging pattern in the deployment of RAG systems. Whereas the first generation of RAG systems typically were user facing, such as customer support chatbots, the next generation is more API-based. The launch of the Vertex AI RAG Engine quickly shows you how to use RAG Engine as a tool for a Gemini Agent!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:58:16</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Morningstar Intelligence Engine with Aravind Kesiraju - Weaviate Podcast #111!]]></title>
			<description><![CDATA[<p>Hey everyone! I am SUPER EXCITED to publish the 111th Weaviate Podcast with Aravind Kesiraju from Morningstar! Aravind is a Principal Software Engineer who has lead the development behind the Morningstar Intelligence Engine! There are so many interesting aspects to this, and if you are building Agentic systems that would benefit from a high-quality financial retrieval API, you should check this out right now! The podcast dives into all sorts of ingredients that went into building this system: from custom RAG data pipelines with content management system integrations and embedding task queues, to exploring new chunking strategies, tool marketplaces, ReAct Agents, Text-to-SQL, and all sorts of other things!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Morningstar-Intelligence-Engine-with-Aravind-Kesiraju---Weaviate-Podcast-111-e2t7lq7</link>
			<guid isPermaLink="false">5bc16f52-f2e9-4737-868f-f94a198c6490</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 08 Jan 2025 15:01:53 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/96769287/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-8%2F925f9201-8a93-403c-1702-a5f2e0848327.mp3" length="104237409" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! I am SUPER EXCITED to publish the 111th Weaviate Podcast with Aravind Kesiraju from Morningstar! Aravind is a Principal Software Engineer who has lead the development behind the Morningstar Intelligence Engine! There are so many interesting aspects to this, and if you are building Agentic systems that would benefit from a high-quality financial retrieval API, you should check this out right now! The podcast dives into all sorts of ingredients that went into building this system: from custom RAG data pipelines with content management system integrations and embedding task queues, to exploring new chunking strategies, tool marketplaces, ReAct Agents, Text-to-SQL, and all sorts of other things!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:25</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Arctic Embed with Luke Merrick, Puxuan Yu, and Charles Pierse - Weaviate Podcast #110!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 110th episode of the Weaviate Podcast! Today we are diving into Snowflake’s Arctic Embedding model series and their newly released Arctic Embed 2.0 open-source model, additionally supporting multilingual text embeddings. The podcast covers the origin of Arctic Embed, Pre-training embedding models, Matryoshka Representation Learning (MRL), Fine-tuning embedding models, Synthetic Query Generation, Hard Negative Mining, and Single-Vector Embeddings Models in the cohort of Multi-Vector ColBERT, SPLADE, and Re-rankers.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Arctic-Embed-with-Luke-Merrick--Puxuan-Yu--and-Charles-Pierse---Weaviate-Podcast-110-e2sg168</link>
			<guid isPermaLink="false">1c50fbbf-1a91-4d76-9dbd-acb665380552</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 18 Dec 2024 15:02:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/95994504/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-11-18%2Fb6409cb7-eacb-5b9f-e98e-c78b326fd21b.mp3" length="183792621" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 110th episode of the Weaviate Podcast! Today we are diving into Snowflake’s Arctic Embedding model series and their newly released Arctic Embed 2.0 open-source model, additionally supporting multilingual text embeddings. The podcast covers the origin of Arctic Embed, Pre-training embedding models, Matryoshka Representation Learning (MRL), Fine-tuning embedding models, Synthetic Query Generation, Hard Negative Mining, and Single-Vector Embeddings Models in the cohort of Multi-Vector ColBERT, SPLADE, and Re-rankers.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:33:39</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Agentic RAG with Erika Cardenas - Weaviate Podcast #109!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 109th episode of the Weaviate Podcast with Erika Cardenas! Erika, in collaboration with Leonie Monigatti, have recently published "What is Agentic RAG". This blog post that was even covered in VentureBeat with additional quotes from Weaviate Co-Founder and CEO Bob van Luijt! This podcast continues the discussion on all things Agentic RAG, covering the basics of Agents, how Agentic RAG changes the game compared to Vanilla RAG systems, Multi-Agent Systems and CrewAI / OpenAI Swarm, Letta, DSPy, and many more! The podcast also anchors by discussing Agentic Generative Feedback Loops and how we are using Agents to improve the quality and expand the capabilities of Generative Feedback Loops!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Agentic-RAG-with-Erika-Cardenas---Weaviate-Podcast-109-e2qu48a</link>
			<guid isPermaLink="false">d909b56a-fa69-4b51-aed3-83888542e741</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 13 Nov 2024 15:10:48 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/94359242/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-13%2F8535860a-a97b-23e6-6d61-59fead2bf50b.mp3" length="66584585" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 109th episode of the Weaviate Podcast with Erika Cardenas! Erika, in collaboration with Leonie Monigatti, have recently published &quot;What is Agentic RAG&quot;. This blog post that was even covered in VentureBeat with additional quotes from Weaviate Co-Founder and CEO Bob van Luijt! This podcast continues the discussion on all things Agentic RAG, covering the basics of Agents, how Agentic RAG changes the game compared to Vanilla RAG systems, Multi-Agent Systems and CrewAI / OpenAI Swarm, Letta, DSPy, and many more! The podcast also anchors by discussing Agentic Generative Feedback Loops and how we are using Agents to improve the quality and expand the capabilities of Generative Feedback Loops!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:34:08</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Let Me Speak Freely? with Zhi Rui Tam - Weaviate Podcast #108!]]></title>
			<description><![CDATA[<p>JSON mode has been one of the biggest enablers for working with Large Language Models! JSON mode is even expanding into Multimodal Foundation models! But how exactly is JSON mode achieved?</p>
<p>There are generally 3 paths to JSON mode: (1) constrained generation (such as Outlines), (2) begging the model for a JSON response in the prompt, and (3) A two stage process of generate-then-format.</p>
<p>I am BEYOND EXCITED to publish the 108th Weaviate Podcast with Zhi Rui Tam, the lead author of Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models!</p>
<p>As the title of the paper suggests, although constrained generation is awesome because of its reliability, we may be sacrificing the performance of the LLM by producing our JSON with this method.</p>
<p>The podcast dives into how these experiments identify this and all sorts of details about the potential and implementation details of Structured Outputs. I particularly love the conversation topic of incredible Complex Structured Outputs, such as generating 10 values in a single inference.</p>
<p>I hope you enjoy the podcast! As always please reach out if you would like to discuss any of these ideas further!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Let-Me-Speak-Freely--with-Zhi-Rui-Tam---Weaviate-Podcast-108-e2qma7v</link>
			<guid isPermaLink="false">273244bd-7b07-4d4d-8de4-1a57590b5ef6</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 07 Nov 2024 15:56:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/94103231/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-7%2Fbf838c37-eb79-43d2-bd1a-03b86d232f71.mp3" length="78122803" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;JSON mode has been one of the biggest enablers for working with Large Language Models! JSON mode is even expanding into Multimodal Foundation models! But how exactly is JSON mode achieved?&lt;/p&gt;
&lt;p&gt;There are generally 3 paths to JSON mode: (1) constrained generation (such as Outlines), (2) begging the model for a JSON response in the prompt, and (3) A two stage process of generate-then-format.&lt;/p&gt;
&lt;p&gt;I am BEYOND EXCITED to publish the 108th Weaviate Podcast with Zhi Rui Tam, the lead author of Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models!&lt;/p&gt;
&lt;p&gt;As the title of the paper suggests, although constrained generation is awesome because of its reliability, we may be sacrificing the performance of the LLM by producing our JSON with this method.&lt;/p&gt;
&lt;p&gt;The podcast dives into how these experiments identify this and all sorts of details about the potential and implementation details of Structured Outputs. I particularly love the conversation topic of incredible Complex Structured Outputs, such as generating 10 values in a single inference.&lt;/p&gt;
&lt;p&gt;I hope you enjoy the podcast! As always please reach out if you would like to discuss any of these ideas further!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:40:04</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[SWE-bench with John Yang and Carlos E. Jimenez - Weaviate Podcast #107!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 107th episode of the Weaviate Podcast! This one dives into SWE-bench, SWE-agent, and most recently SWE-bench Multimodal with John Yang from Stanford University and Carlos E. Jimenez from Princeton University! One of the most impactful applications of AI we have seen so far is in programming and software engineering! John, Carlos, and team are at the cutting-edge of developing and benchmarking these systems! I learned so much from the conversation and I really hope you find it interesting and useful as well!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/SWE-bench-with-John-Yang-and-Carlos-E--Jimenez---Weaviate-Podcast-107-e2qbc4c</link>
			<guid isPermaLink="false">bd41003b-b131-4f8c-97fc-88b8fd3c4fdb</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 30 Oct 2024 15:03:46 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/93744716/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-30%2F5fe65c19-84e3-5f6b-070d-4b28b10cefad.mp3" length="114051105" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 107th episode of the Weaviate Podcast! This one dives into SWE-bench, SWE-agent, and most recently SWE-bench Multimodal with John Yang from Stanford University and Carlos E. Jimenez from Princeton University! One of the most impactful applications of AI we have seen so far is in programming and software engineering! John, Carlos, and team are at the cutting-edge of developing and benchmarking these systems! I learned so much from the conversation and I really hope you find it interesting and useful as well!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:58:23</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[AI in Education with Rose E. Wang - Weaviate Podcast #106!]]></title>
			<description><![CDATA[<p>Hey everyone! I am SUPER excited to publish the 106th episode of the Weaviate Podcast featuring Rose E. Wang!! Rose is a Ph.D. student at Stanford University where she has lead incredible research at the cutting-edge of AI applications in Education. The podcast heavily discusses her recent work on Tutor CoPilot! Tutor CoPilot is one of the world's largest randomized control trials on the impact AI is having on education, testing 900 students and 1800 tutors in grades K-12. I think this is such an inspiring study and it is interesting to see the data coming in quantifying the impact AI is having on education. I was amazed by the depth of how Rose things about education and learning strategies and how well she integrates cutting-edge topics in AI! I hope you find the podcast interesting and useful!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/AI-in-Education-with-Rose-E--Wang---Weaviate-Podcast-106-e2q0e88</link>
			<guid isPermaLink="false">c8b52a97-b336-47f9-8b9a-9a9c02c74694</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 22 Oct 2024 14:34:37 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/93386440/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-22%2F1bcb6b71-b7b4-4a23-0383-b19faf5cb1f2.mp3" length="100225845" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! I am SUPER excited to publish the 106th episode of the Weaviate Podcast featuring Rose E. Wang!! Rose is a Ph.D. student at Stanford University where she has lead incredible research at the cutting-edge of AI applications in Education. The podcast heavily discusses her recent work on Tutor CoPilot! Tutor CoPilot is one of the world&apos;s largest randomized control trials on the impact AI is having on education, testing 900 students and 1800 tutors in grades K-12. I think this is such an inspiring study and it is interesting to see the data coming in quantifying the impact AI is having on education. I was amazed by the depth of how Rose things about education and learning strategies and how well she integrates cutting-edge topics in AI! I hope you find the podcast interesting and useful!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Compound AI Systems with Philip Kiely - Weaviate Podcast #105!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for tuning into the 105th episode of the Weaviate Podcast! This one features Philip Kiely diving into all sorts of apsects related to Compound AI Systems! We are now seeing far better results with AI models by breaking up tasks into multiple stages and inferences. Philip explains the work they are doing at Baseten to optimize and scale deployments of these emerging systems and all sorts of aspects about them from Structured Generation to their distinction with Agents! I hope you find it useful!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Compound-AI-Systems-with-Philip-Kiely---Weaviate-Podcast-105-e2pq14a</link>
			<guid isPermaLink="false">78850f2c-793c-4cb2-ab08-ff3cd94707f4</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 17 Oct 2024 14:36:31 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/93176394/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-17%2F6b641570-8379-8e73-123d-bb4aad8124a5.mp3" length="110799507" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for tuning into the 105th episode of the Weaviate Podcast! This one features Philip Kiely diving into all sorts of apsects related to Compound AI Systems! We are now seeing far better results with AI models by breaking up tasks into multiple stages and inferences. Philip explains the work they are doing at Baseten to optimize and scale deployments of these emerging systems and all sorts of aspects about them from Structured Generation to their distinction with Agents! I hope you find it useful!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:56:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[AI Agents That Matter with Sayash Kapoor and Benedikt Stroebl - Weaviate Podcast #104!]]></title>
			<description><![CDATA[<p>AI Researchers have overfit to maximizing state-of-the-art accuracy at the expense of the cost to run these AI systems! We need to account for cost during optimization. Even if a chatbot can produce an amazing answer, it isn&#39;t that valuable if it costs, say $5 per response!

I am beyond excited to present the 104th Weaviate Podcast with Sayash Kapoor and Benedikt Stroebl from Princeton Language and Intelligence! Sayash and Benedikt are co-first authors of &quot;AI Agents That Matter&quot;! This is one of my favorite papers I&#39;ve studied recently which introduces Pareto Optimal optimization to DSPy and really tames the chaos of Agent benchmarking!

This was such a fun conversation! I am beyond grateful to have met them both and to feature their research on the Weaviate Podcast! I hope you find it interesting and useful!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/AI-Agents-That-Matter-with-Sayash-Kapoor-and-Benedikt-Stroebl---Weaviate-Podcast-104-e2oiavb</link>
			<guid isPermaLink="false">ca182522-a94b-4349-8a97-12dfae9d44f1</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 18 Sep 2024 14:31:12 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/91875755/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-18%2Fded7ed59-cac2-83cd-928a-6e13b5a5687c.mp3" length="119184231" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;AI Researchers have overfit to maximizing state-of-the-art accuracy at the expense of the cost to run these AI systems! We need to account for cost during optimization. Even if a chatbot can produce an amazing answer, it isn&amp;#39;t that valuable if it costs, say $5 per response!

I am beyond excited to present the 104th Weaviate Podcast with Sayash Kapoor and Benedikt Stroebl from Princeton Language and Intelligence! Sayash and Benedikt are co-first authors of &amp;quot;AI Agents That Matter&amp;quot;! This is one of my favorite papers I&amp;#39;ve studied recently which introduces Pareto Optimal optimization to DSPy and really tames the chaos of Agent benchmarking!

This was such a fun conversation! I am beyond grateful to have met them both and to feature their research on the Weaviate Podcast! I hope you find it interesting and useful!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:43</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[MIPRO and DSPy with Krista Opsahl-Ong! - Weaviate Podcast #103]]></title>
			<description><![CDATA[<p>I am beyond excited to publish our interview with Krista Opsahl-Ong from Stanford University! Krista is the lead author of MIPRO, short for Multi-prompt Instruction Proposal Optimizer, and one of the leading developers and scientists behind DSPy!</p>
<p>This was such a fun discussion beginning with the motivation of Automated Prompt Engineering, Multi-Layer Language Programs (also commonly referred to as Compound AI Systems), and their intersection. We then dove into the details of how MIPRO achieves this and miscellaneous topics in AI from Structured Outputs to Agents, DSPy for Code Generation, and more!</p>
<p>I really hope you enjoy the podcast! As always, more than happy to answer any questions or discuss any ideas about the content in the podcast!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/MIPRO-and-DSPy-with-Krista-Opsahl-Ong----Weaviate-Podcast-103-e2nna6t</link>
			<guid isPermaLink="false">1c5f80ed-c940-400f-bc72-fb1cc8f37a50</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 28 Aug 2024 13:07:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/90990237/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-28%2F38d14a6d-0d46-2243-7555-f6d5b89473a1.mp3" length="118204900" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;I am beyond excited to publish our interview with Krista Opsahl-Ong from Stanford University! Krista is the lead author of MIPRO, short for Multi-prompt Instruction Proposal Optimizer, and one of the leading developers and scientists behind DSPy!&lt;/p&gt;
&lt;p&gt;This was such a fun discussion beginning with the motivation of Automated Prompt Engineering, Multi-Layer Language Programs (also commonly referred to as Compound AI Systems), and their intersection. We then dove into the details of how MIPRO achieves this and miscellaneous topics in AI from Structured Outputs to Agents, DSPy for Code Generation, and more!&lt;/p&gt;
&lt;p&gt;I really hope you enjoy the podcast! As always, more than happy to answer any questions or discuss any ideas about the content in the podcast!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[AI-Native Development with Guy Podjarny and Bob van Luijt - Weaviate Podcast #102!]]></title>
			<description><![CDATA[<p>AI is completely transforming how we build software! But how exactly? What does it mean for a software application to be AI-Native versus AI-Enabled? How many other aspects of software development and creativity are impacted by AI?</p>
<p><br></p>
<p>I am super excited to publish our 102nd Weaviate Podcast with Guy Podjarny and Bob van Luijt on AI-Native Development!</p>
<p><br></p>
<p>Guy Podjarny is a co-founder of Snyk, a remarkably successful Cybersecurity company. He is now back on the founder journey, diving into AI-Native Development with Tessl!</p>
<p><br></p>
<p>Guy and Bob both have so much expertise in how software is developed and shipped to the world. There are so many interesting nuggets in this from defining AI-Native to Stateful AI, AI-assisted coding, subjectivity in software specification, personalized content, and much more!</p>
<p><br></p>
<p>I hope you enjoy the podcast, this was a really fun and interesting one!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/AI-Native-Development-with-Guy-Podjarny-and-Bob-van-Luijt---Weaviate-Podcast-102-e2n67dq</link>
			<guid isPermaLink="false">ab75302c-581b-40c6-881b-3f110423b501</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 14 Aug 2024 13:52:31 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/90430330/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-14%2Facf74450-184b-50c7-196a-094c7f8e0032.mp3" length="103412929" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;AI is completely transforming how we build software! But how exactly? What does it mean for a software application to be AI-Native versus AI-Enabled? How many other aspects of software development and creativity are impacted by AI?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I am super excited to publish our 102nd Weaviate Podcast with Guy Podjarny and Bob van Luijt on AI-Native Development!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Guy Podjarny is a co-founder of Snyk, a remarkably successful Cybersecurity company. He is now back on the founder journey, diving into AI-Native Development with Tessl!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Guy and Bob both have so much expertise in how software is developed and shipped to the world. There are so many interesting nuggets in this from defining AI-Native to Stateful AI, AI-assisted coding, subjectivity in software specification, personalized content, and much more!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I hope you enjoy the podcast, this was a really fun and interesting one!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:52:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Scaling Pandas with Devin Petersohn - Weaviate Podcast #101!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 101st episode of the Weaviate Podcast with Devin Petersohn! Devin is the creator of Modin, one of the world&#39;s most advanced systems for scaling Pandas! Devin then went onto co-found Ponder, which was acquired by Snowflake in early 2023. This was one of my favorite podcasts of all time, I learned so much about the internals of Data Systems and I hope you do as well!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Scaling-Pandas-with-Devin-Petersohn---Weaviate-Podcast-101-e2m4pk8</link>
			<guid isPermaLink="false">68e10f6d-6a2f-4dc2-b1bb-d963da791b8f</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 17 Jul 2024 14:36:52 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/89334856/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-17%2Ff1d50681-f18b-d8d0-0018-5f860d738959.mp3" length="92599993" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 101st episode of the Weaviate Podcast with Devin Petersohn! Devin is the creator of Modin, one of the world&amp;#39;s most advanced systems for scaling Pandas! Devin then went onto co-found Ponder, which was acquired by Snowflake in early 2023. This was one of my favorite podcasts of all time, I learned so much about the internals of Data Systems and I hope you do as well!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:47:49</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Generative UIs with Lucas Negritto and Bob van Luijt!]]></title>
			<description><![CDATA[<p>What is an AI-native application? This has been one of the questions we are most interested in answering at Weaviate! This podcast explores this question with Weaviate Co-founder Bob van Luijt and Lucas Negritto. Formerly at OpenAI, Lucas is now building Odapt, a remarkable example of such an application where we no longer use front-end code, rather rendering the UI entirely within the generative model!! There are many interesting topics covered such as of course, firstly how this works and how you build these systems, as well as native multimodality, subjective feedback, and more! I hope you find the podcast interesting and useful!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Generative-UIs-with-Lucas-Negritto-and-Bob-van-Luijt-e2ll9ot</link>
			<guid isPermaLink="false">9b829ba1-4441-4020-8a59-3d870f004d8b</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 04 Jul 2024 14:04:21 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/88827101/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-4%2F71f436cb-d386-5c53-c76d-a249ef606073.mp3" length="85444588" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;What is an AI-native application? This has been one of the questions we are most interested in answering at Weaviate! This podcast explores this question with Weaviate Co-founder Bob van Luijt and Lucas Negritto. Formerly at OpenAI, Lucas is now building Odapt, a remarkable example of such an application where we no longer use front-end code, rather rendering the UI entirely within the generative model!! There are many interesting topics covered such as of course, firstly how this works and how you build these systems, as well as native multimodality, subjective feedback, and more! I hope you find the podcast interesting and useful!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:43:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ACORN with Liana Patel and Abdel Rodriguez - Weaviate Podcast #99!]]></title>
			<description><![CDATA[<p>Liana Patel is a Ph.D. student at Stanford University who is the lead author of ACORN, a breakthrough in Approximate Nearest Neighbor Search with Filters! Also joining the podcast is Abdel Rodriguez, a Vector Index Researcher and Engineer at Weaviate. This podcast dives into all sorts of details behind ACORN. Starting with how Liana developed her interest in Approximate Nearest Neighbor Search algorithms and then transitioning into how ACORN differs from previous approaches, the Two-Hop Neighborhood Heuristic, Predicate Subgraphs, Experimental Details, and many more topics! Major thank you to Liana and Abdel for joining the podcast, this was such a fun conversation packed with insights about Proximity Graph algorithms for Vector Search with Filtering!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/ACORN-with-Liana-Patel-and-Abdel-Rodriguez---Weaviate-Podcast-99-e2l9nvk</link>
			<guid isPermaLink="false">10d99521-979d-47af-842c-60efbda7fc5a</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 25 Jun 2024 14:01:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/88448436/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-25%2F6c775aed-62b2-20e6-5788-b0d84898287a.mp3" length="105822578" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Liana Patel is a Ph.D. student at Stanford University who is the lead author of ACORN, a breakthrough in Approximate Nearest Neighbor Search with Filters! Also joining the podcast is Abdel Rodriguez, a Vector Index Researcher and Engineer at Weaviate. This podcast dives into all sorts of details behind ACORN. Starting with how Liana developed her interest in Approximate Nearest Neighbor Search algorithms and then transitioning into how ACORN differs from previous approaches, the Two-Hop Neighborhood Heuristic, Predicate Subgraphs, Experimental Details, and many more topics! Major thank you to Liana and Abdel for joining the podcast, this was such a fun conversation packed with insights about Proximity Graph algorithms for Vector Search with Filtering!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:55</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Window Search Tree with Josh Engels - Weaviate Podcast #98!]]></title>
			<description><![CDATA[<p>Josh Engels is a Ph.D. student at MIT who has published several works advancing the state of the art in Vector Search. Josh has recently developed the Window Search Tree, a new algorithm particularly targeted for improving Filtered Vector Search. Even more particularly than that, the WST algorithm targets Filtered Search with continuous-valued filters such as &quot;price&quot; or &quot;date&quot;, also known as range filters. This is a huge application for Vector Databases and it was incredible getting to pick Josh&#39;s brain on how this works and the state of Approximate Nearest Neighbor Search!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Window-Search-Tree-with-Josh-Engels---Weaviate-Podcast-98-e2l2m2h</link>
			<guid isPermaLink="false">7640e70a-f0c6-441f-8e0f-4e510db89e25</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 19 Jun 2024 13:59:36 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/88217105/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-19%2F34361c19-2539-9d5e-cce5-690003e3f491.mp3" length="115415463" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Josh Engels is a Ph.D. student at MIT who has published several works advancing the state of the art in Vector Search. Josh has recently developed the Window Search Tree, a new algorithm particularly targeted for improving Filtered Vector Search. Even more particularly than that, the WST algorithm targets Filtered Search with continuous-valued filters such as &amp;quot;price&amp;quot; or &amp;quot;date&amp;quot;, also known as range filters. This is a huge application for Vector Databases and it was incredible getting to pick Josh&amp;#39;s brain on how this works and the state of Approximate Nearest Neighbor Search!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:58:56</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Future of Search with Nils Reimers and Erika Cardenas - Weaviate Podcast #97!]]></title>
			<description><![CDATA[<p>Hey everyone! I am SUPER excited to publish our 97th Weaviate Podcast on the state of AI-powered Search technology featuring Nils Reimers and Erika Cardenas! Erika and I have been super excited about Cohere&#39;s latest works to advance RAG and Search and it was amazing getting to pick Nils&#39; brain about all these topics!</p>
<p><br></p>
<p>We began with the development of Compass! Nils explains the current problem with embeddings as a soup!! For example, imagine embedding this video description, the first part is about the launch of a podcast, whereas this part is about an embedding algorithm -- how do we form representations of multi-aspect chunks of text?</p>
<p><br></p>
<p>We dove into all the details of this from the distinction of multi-aspect embeddings with LLM or &quot;smart&quot; chunkers, ColBERT, &quot;Embed Small, Retrieve Big&quot;, and many other topics as well from Cross Encoder Re-rankers to Data Cleaning with Generative Feedback Loops, RAG Evaluation, Vector Quantization, and more!</p>
<p><br></p>
<p>I really hope you enjoy the podcast! It was such an educational experience for Erika and I and we really hope you enjoy it as well!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/The-Future-of-Search-with-Nils-Reimers-and-Erika-Cardenas---Weaviate-Podcast-97-e2koom7</link>
			<guid isPermaLink="false">3b183ff1-f442-4711-85f0-f0d81f377ca4</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 11 Jun 2024 13:58:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/87892103/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-11%2Fea6344b5-f048-3d68-697e-37d32d7225f3.mp3" length="116570972" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! I am SUPER excited to publish our 97th Weaviate Podcast on the state of AI-powered Search technology featuring Nils Reimers and Erika Cardenas! Erika and I have been super excited about Cohere&amp;#39;s latest works to advance RAG and Search and it was amazing getting to pick Nils&amp;#39; brain about all these topics!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We began with the development of Compass! Nils explains the current problem with embeddings as a soup!! For example, imagine embedding this video description, the first part is about the launch of a podcast, whereas this part is about an embedding algorithm -- how do we form representations of multi-aspect chunks of text?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We dove into all the details of this from the distinction of multi-aspect embeddings with LLM or &amp;quot;smart&amp;quot; chunkers, ColBERT, &amp;quot;Embed Small, Retrieve Big&amp;quot;, and many other topics as well from Cross Encoder Re-rankers to Data Cleaning with Generative Feedback Loops, RAG Evaluation, Vector Quantization, and more!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I really hope you enjoy the podcast! It was such an educational experience for Erika and I and we really hope you enjoy it as well!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:59:41</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Deep Learning with Letitia Parcalabescu - Weaviate Podcast #96!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 96th episode of the Weaviate podcast featuring Letitia Parcalabescu! While completing her Ph.D. studies at the University of Heidelberg, Letitia started her YouTube channel: AI Coffee Break with Letitia! Her videos break down complex concepts in AI with a creative mix of technical expertise and visualizations unlike anyone else in the space!We began the podcast by discussing our shared background in creating content on YouTube from starting, to plans for the future, and everything else in between!We then discussed the evolution of Deep Learning over the last few years -- from neural network architectures to datasets, tasks, learning algorithms, and more! I think particularly we are at a really interesting time in the future of learning algorithms! We discussed DSPy and new ways of thinking about instruction tuning, example production, gradient descent, and the future of SFT vs. DPO-style techniques!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Deep-Learning-with-Letitia-Parcalabescu---Weaviate-Podcast-96-e2khc19</link>
			<guid isPermaLink="false">50a541f7-1fd4-41ae-8b7b-c449d118aac9</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 05 Jun 2024 14:45:25 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/87649769/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-5%2Fc578df51-b453-0659-3974-85f0bc575321.mp3" length="186912317" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 96th episode of the Weaviate podcast featuring Letitia Parcalabescu! While completing her Ph.D. studies at the University of Heidelberg, Letitia started her YouTube channel: AI Coffee Break with Letitia! Her videos break down complex concepts in AI with a creative mix of technical expertise and visualizations unlike anyone else in the space!We began the podcast by discussing our shared background in creating content on YouTube from starting, to plans for the future, and everything else in between!We then discussed the evolution of Deep Learning over the last few years -- from neural network architectures to datasets, tasks, learning algorithms, and more! I think particularly we are at a really interesting time in the future of learning algorithms! We discussed DSPy and new ways of thinking about instruction tuning, example production, gradient descent, and the future of SFT vs. DPO-style techniques!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:35:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Google Cloud Marketplace with Dai Vu and Bob van Luijt - Weaviate Podcast #95!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 95th Weaviate Podcast! We are beyond honored to feature Dai Vu from Google on this one, alongside Weaviate Co-Founder Bob van Luijt! This podcast dives into all things Google Cloud Marketplace and the state of AI. Beginning with the proliferation of Open-Source models and how Dai sees the evolving landscape with respect to things like Gemini Pro 1.5, Gemini Nano and Gemma, as well as the integration of 3rd party model providers such as Llama 3 on Google Cloud platforms such as Vertex AI. Bob and Dai continue to unpack the next move for open-source infrastructure providers and perspectives around &quot;AI-Native&quot; applications, trends in data gravity, perspectives on benchmarking, and Dai&#39;s &quot;aha&quot; moment in AI!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Google-Cloud-Marketplace-with-Dai-Vu-and-Bob-van-Luijt---Weaviate-Podcast-95-e2jc18b</link>
			<guid isPermaLink="false">6a8533b9-7241-4346-a577-3a0debb93a22</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 07 May 2024 13:51:21 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/86426315/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-4-7%2F09e1d1c4-9fc4-51e4-7faf-89177ebc29f3.mp3" length="81151996" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 95th Weaviate Podcast! We are beyond honored to feature Dai Vu from Google on this one, alongside Weaviate Co-Founder Bob van Luijt! This podcast dives into all things Google Cloud Marketplace and the state of AI. Beginning with the proliferation of Open-Source models and how Dai sees the evolving landscape with respect to things like Gemini Pro 1.5, Gemini Nano and Gemma, as well as the integration of 3rd party model providers such as Llama 3 on Google Cloud platforms such as Vertex AI. Bob and Dai continue to unpack the next move for open-source infrastructure providers and perspectives around &amp;quot;AI-Native&amp;quot; applications, trends in data gravity, perspectives on benchmarking, and Dai&amp;#39;s &amp;quot;aha&amp;quot; moment in AI!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:41:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ParlayANN with Magdalen Dobson Manohar]]></title>
			<description><![CDATA[<p>As you are graduating from ideas to engineering, one of the key concepts to be aware of is Parallel Computing and Concurrency. I am SUPER excited to share our 94th Weaviate podcast with Magdalen Dobson Manohar! Magdalen is one of the most impressive scientists I have ever met, having completed her undergraduate studies at MIT before joining Carnegie Mellon University to study Approximate Nearest Neighbor Search and develop ParlayANN. ParlayANN is one of the most enlightening works I have come across that studies how to build ANN indexes in parallel without the use of locking.</p>
<p>In my opinion, this is the most insightful podcast we have ever produced into Vector Search, the core technology behind Vector Databases. The podcast begins with Magdalen’s journey into ANN science, the issue of Lock Contention in HNSW, further detailing HNSW vs. DiskANN vs. HCNNG and pyNNDescent, ParlayIVF, how Parallel Index Construction is achieved, conclusions from experimentation, Filtered Vector Search, Out of Distribution Vector Search, and exciting directions for the future!</p>
<p>I also want to give a huge thanks to Etienne Dilocker, John Trengrove, Abdel Rodriguez, Asdine El Hrychy, and Zain Hasan. There is no way I would be able to keep up with conversations like this without their leadership and collaboration.</p>
<p>I hope you find the podcast interesting and useful!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/ParlayANN-with-Magdalen-Dobson-Manohar-e2iqqt0</link>
			<guid isPermaLink="false">baf40d89-d71c-4fd1-87e8-1a6321b169b9</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 24 Apr 2024 15:16:18 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/85862752/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-3-24%2Faec1a0c8-27f6-06b9-3510-a7de76aba2c2.mp3" length="125200599" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;As you are graduating from ideas to engineering, one of the key concepts to be aware of is Parallel Computing and Concurrency. I am SUPER excited to share our 94th Weaviate podcast with Magdalen Dobson Manohar! Magdalen is one of the most impressive scientists I have ever met, having completed her undergraduate studies at MIT before joining Carnegie Mellon University to study Approximate Nearest Neighbor Search and develop ParlayANN. ParlayANN is one of the most enlightening works I have come across that studies how to build ANN indexes in parallel without the use of locking.&lt;/p&gt;
&lt;p&gt;In my opinion, this is the most insightful podcast we have ever produced into Vector Search, the core technology behind Vector Databases. The podcast begins with Magdalen’s journey into ANN science, the issue of Lock Contention in HNSW, further detailing HNSW vs. DiskANN vs. HCNNG and pyNNDescent, ParlayIVF, how Parallel Index Construction is achieved, conclusions from experimentation, Filtered Vector Search, Out of Distribution Vector Search, and exciting directions for the future!&lt;/p&gt;
&lt;p&gt;I also want to give a huge thanks to Etienne Dilocker, John Trengrove, Abdel Rodriguez, Asdine El Hrychy, and Zain Hasan. There is no way I would be able to keep up with conversations like this without their leadership and collaboration.&lt;/p&gt;
&lt;p&gt;I hope you find the podcast interesting and useful!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:03:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[RAGKit with Kyle Davis - Weaviate Podcast #93!]]></title>
			<description><![CDATA[<p>Hey everyone! I am SUPER excited to publish our newest Weaviate podcast with Kyle Davis, the creator of RAGKit! At a high-level, the podcast covers our understanding of RAG systems through 4 key areas: (1) Ingest / ETL, (2) Search, (3) Generate / Agents, and (4) Evaluation. Discussing these lead to all sorts of topics from Knowledge Graph RAG, to Function Calling and Tool Selection, Re-ranking, Quantization, and many more!

This discussion forced me to re-think many of my previously held beliefs about the current RAG stack, particularly the definition of “Agents”. I came in believing that the best way of viewing “Agents” is an abstraction on top of multiple pipelines, such as an “Email Agent”, but Kyle presented the idea of looking at “Agents” as scoping the tools each LLM call is connected to, such as `read_email` or `calculator`. Would love to know what people think about this one, as I think getting a consensus definition of “Agents” can clarify a lot of the current confusion for people building with LLMs / Generative AI.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/RAGKit-with-Kyle-Davis---Weaviate-Podcast-93-e2ie2h1</link>
			<guid isPermaLink="false">9d31ba84-823d-41bc-b5ad-08c870a9e31c</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 15 Apr 2024 14:15:59 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/85444577/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-3-15%2Fdf50e00c-e94c-ffd4-2783-8d2413e33d64.mp3" length="170073538" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! I am SUPER excited to publish our newest Weaviate podcast with Kyle Davis, the creator of RAGKit! At a high-level, the podcast covers our understanding of RAG systems through 4 key areas: (1) Ingest / ETL, (2) Search, (3) Generate / Agents, and (4) Evaluation. Discussing these lead to all sorts of topics from Knowledge Graph RAG, to Function Calling and Tool Selection, Re-ranking, Quantization, and many more!

This discussion forced me to re-think many of my previously held beliefs about the current RAG stack, particularly the definition of “Agents”. I came in believing that the best way of viewing “Agents” is an abstraction on top of multiple pipelines, such as an “Email Agent”, but Kyle presented the idea of looking at “Agents” as scoping the tools each LLM call is connected to, such as `read_email` or `calculator`. Would love to know what people think about this one, as I think getting a consensus definition of “Agents” can clarify a lot of the current confusion for people building with LLMs / Generative AI.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:27:03</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[VetRec with David de Matheu - Weaviate Podcast #92!]]></title>
			<description><![CDATA[<p>I&#39;ve seen a lot of interest around RAG for X application domain, Legal, Accounting, Healthcare, .... David and Kevin are maybe the best example of this I have seen so far, pivoting from Neum AI to VetRec!</p>
<p>We begin the podcast by discussing the decision to switch gears, the advice given by Y Combinator, and David&#39;s experience in learning a new application domain.</p>
<p>We then continue to discuss technical opportunities around RAG for Veterinarians, such as SOAP notes and Differential Diagnosis!</p>
<p>We conclude with David&#39;s thoughts on the ETL space, companies like Unstructured and LlamaIndex&#39;s LlamaParse, advice for specific focus in ETL, and general discussions of ETL for Vector DBs / KGs / SQL.</p>
<p>David and Kevin have been two of my favorite entrepreneurs I&#39;ve met during my time at Weaviate! They do an amazing job of writing content that helps you live vicariously through them as they take on this opportunity to apply RAG and AI technologies to help Veterinarians!</p>
<p>I really hope you enjoy the podcast!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/VetRec-with-David-de-Matheu---Weaviate-Podcast-92-e2hm6in</link>
			<guid isPermaLink="false">8865eeee-6407-44b2-b19d-6a35b9736b68</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 28 Mar 2024 12:57:01 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/84662295/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-2-28%2F112e8315-7cf2-0b1a-1562-3720b03d05eb.mp3" length="117082138" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;I&amp;#39;ve seen a lot of interest around RAG for X application domain, Legal, Accounting, Healthcare, .... David and Kevin are maybe the best example of this I have seen so far, pivoting from Neum AI to VetRec!&lt;/p&gt;
&lt;p&gt;We begin the podcast by discussing the decision to switch gears, the advice given by Y Combinator, and David&amp;#39;s experience in learning a new application domain.&lt;/p&gt;
&lt;p&gt;We then continue to discuss technical opportunities around RAG for Veterinarians, such as SOAP notes and Differential Diagnosis!&lt;/p&gt;
&lt;p&gt;We conclude with David&amp;#39;s thoughts on the ETL space, companies like Unstructured and LlamaIndex&amp;#39;s LlamaParse, advice for specific focus in ETL, and general discussions of ETL for Vector DBs / KGs / SQL.&lt;/p&gt;
&lt;p&gt;David and Kevin have been two of my favorite entrepreneurs I&amp;#39;ve met during my time at Weaviate! They do an amazing job of writing content that helps you live vicariously through them as they take on this opportunity to apply RAG and AI technologies to help Veterinarians!&lt;/p&gt;
&lt;p&gt;I really hope you enjoy the podcast!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:59:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Tengyu Ma on Voyage AI - Weaviate Podcast #91!]]></title>
			<description><![CDATA[<p><strong>Voyage AI</strong> is the newest giant in the embedding, reranking, and search model game!</p>
<p>I am SUPER excited to publish our latest Weaviate podcast with Tengyu Ma, Co-Founder of Voyage AI and Assistant Professor at Stanford University!</p>
<p>We began the podcast with a deep dive into everything embedding model training and contrastive learning theory. Tengyu delivered a <strong>masterclass</strong> in everything from scaling laws to multi-vector representations, neural architectures, representation collapse, data augmentation, semantic similarity, and more! I am beyond impressed with Tengyu&#39;s extensive knowledge and explanations of all these topics.</p>
<p>The next chapter dives into a case study Voyage AI did <strong>fine-tuning an embedding model for the LangChain documentation.</strong> This is an absolutely fascinating example of the role of continual fine-tuning with very new concepts (for example, very few people were talking about chaining together LLM calls 2 years ago), as well as the data efficiency advances in fine-tuning.</p>
<p>We concluded by discussing ML systems challenges in serving an embeddings API. Particularly the challenge of detecting if a request is for batch or query inference and the optimizations that go into either say ~100ms latency for a query embedding or maximizing throughput for batch embeddings.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Tengyu-Ma-on-Voyage-AI---Weaviate-Podcast-91-e2hb3f4</link>
			<guid isPermaLink="false">8a626926-c867-43ac-b4b1-1af7204d9b5b</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 20 Mar 2024 14:53:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/84298660/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-2-20%2F5984487f-9167-dead-3773-478ee4fb7dc0.mp3" length="122837979" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;&lt;strong&gt;Voyage AI&lt;/strong&gt; is the newest giant in the embedding, reranking, and search model game!&lt;/p&gt;
&lt;p&gt;I am SUPER excited to publish our latest Weaviate podcast with Tengyu Ma, Co-Founder of Voyage AI and Assistant Professor at Stanford University!&lt;/p&gt;
&lt;p&gt;We began the podcast with a deep dive into everything embedding model training and contrastive learning theory. Tengyu delivered a &lt;strong&gt;masterclass&lt;/strong&gt; in everything from scaling laws to multi-vector representations, neural architectures, representation collapse, data augmentation, semantic similarity, and more! I am beyond impressed with Tengyu&amp;#39;s extensive knowledge and explanations of all these topics.&lt;/p&gt;
&lt;p&gt;The next chapter dives into a case study Voyage AI did &lt;strong&gt;fine-tuning an embedding model for the LangChain documentation.&lt;/strong&gt; This is an absolutely fascinating example of the role of continual fine-tuning with very new concepts (for example, very few people were talking about chaining together LLM calls 2 years ago), as well as the data efficiency advances in fine-tuning.&lt;/p&gt;
&lt;p&gt;We concluded by discussing ML systems challenges in serving an embeddings API. Particularly the challenge of detecting if a request is for batch or query inference and the optimizations that go into either say ~100ms latency for a query embedding or maximizing throughput for batch embeddings.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Self-Discover DSPy with Chris Dossman - Weaviate Podcast #90!]]></title>
			<description><![CDATA[<p>One of the core values of DSPy is the ability to add “reasoning modules” such as Chain-of-Thought to your LLM programs!</p>
<p>For example, Chain-of-Thought describes prompting the LLM with “Let’s think step by step …”. Interestingly, this meta-prompt around asking the LLM to think this way dramatically improves performance in tasks like question answering or document summarization.</p>
<p>Self-Discover is a meta-prompting technique that searches for the optimal thinking primitives to integrate into your program! For example, you could “Let’s think out of the box to arrive at a creative solution” or “Please explain your answer in 4 levels of abstraction: as if you are talking to a five year old, a high school student, a college student studying Computer Science, and a software engineer with years of experience in the topic”.</p>
<p>I am SUPER excited to be publishing our 90th Weaviate Podcast with Chris Dossman! Chris has implemented Self-Discover in DSPy, one of the most fascinating examples so far of what the DSPy framework is capable of!</p>
<p>Chris is also one of the most talented entrepreneurs I have met during my time at Weaviate thanks to introductions from Bob van Luijt and Byron Voorbach. Chris built one of the earliest RAG systems for government information and is now working on LLM opportunities in marketing with his new startup, <a href="http://Dicer.ai">Dicer.ai</a>!</p>
<p>I hope you enjoy the podcast, it was such a fun one and I learned so much!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Self-Discover-DSPy-with-Chris-Dossman---Weaviate-Podcast-90-e2gnehb</link>
			<guid isPermaLink="false">d5b4377d-1114-47d6-a338-07f66774e0c7</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 06 Mar 2024 15:10:49 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/83654635/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-2-6%2F6b4c4f28-6882-1f89-2be3-fa2f7b3cfd40.mp3" length="122403564" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;One of the core values of DSPy is the ability to add “reasoning modules” such as Chain-of-Thought to your LLM programs!&lt;/p&gt;
&lt;p&gt;For example, Chain-of-Thought describes prompting the LLM with “Let’s think step by step …”. Interestingly, this meta-prompt around asking the LLM to think this way dramatically improves performance in tasks like question answering or document summarization.&lt;/p&gt;
&lt;p&gt;Self-Discover is a meta-prompting technique that searches for the optimal thinking primitives to integrate into your program! For example, you could “Let’s think out of the box to arrive at a creative solution” or “Please explain your answer in 4 levels of abstraction: as if you are talking to a five year old, a high school student, a college student studying Computer Science, and a software engineer with years of experience in the topic”.&lt;/p&gt;
&lt;p&gt;I am SUPER excited to be publishing our 90th Weaviate Podcast with Chris Dossman! Chris has implemented Self-Discover in DSPy, one of the most fascinating examples so far of what the DSPy framework is capable of!&lt;/p&gt;
&lt;p&gt;Chris is also one of the most talented entrepreneurs I have met during my time at Weaviate thanks to introductions from Bob van Luijt and Byron Voorbach. Chris built one of the earliest RAG systems for government information and is now working on LLM opportunities in marketing with his new startup, &lt;a href=&quot;http://Dicer.ai&quot;&gt;Dicer.ai&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;I hope you enjoy the podcast, it was such a fun one and I learned so much!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Matryoshka Embeddings with Aditya Kusupati, Zach Nussbaum, and Zain Hasan - Weaviate Podcast #89!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 89th Weaviate Podcast on Matryoshka Representation Learning! I am beyond grateful to be joined by the lead author of Matryoshka Representation Learning, Aditya Kusupati, Zach Nussbaum, a Machine Learning Engineer at Nomic AI bringing these embeddings to production, and my Weaviate colleague, Zain Hasan, who has done amazing research on Matryoshka Embeddings! We think this is a super powerful development for Vector Search! This podcast covers all sorts of details from generally what Matryoshka embeddings are, the challenges of training them, experiences building an embeddings API product from Nomic AI and how it ties with Nomic Atlas, Aditya&#39;s research on differentiable ANN indexes, and many more! This was such a fun one, I really hope you find it useful! Please let us know what you think!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Matryoshka-Embeddings-with-Aditya-Kusupati--Zach-Nussbaum--and-Zain-Hasan---Weaviate-Podcast-89-e2g1ka9</link>
			<guid isPermaLink="false">3a7cf5ca-53fb-440c-ab53-8d4dd4d49c49</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 20 Feb 2024 15:02:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/82939657/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-1-20%2Fae2e1316-a8eb-f61b-ca56-ded484ffcf1e.mp3" length="141410005" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 89th Weaviate Podcast on Matryoshka Representation Learning! I am beyond grateful to be joined by the lead author of Matryoshka Representation Learning, Aditya Kusupati, Zach Nussbaum, a Machine Learning Engineer at Nomic AI bringing these embeddings to production, and my Weaviate colleague, Zain Hasan, who has done amazing research on Matryoshka Embeddings! We think this is a super powerful development for Vector Search! This podcast covers all sorts of details from generally what Matryoshka embeddings are, the challenges of training them, experiences building an embeddings API product from Nomic AI and how it ties with Nomic Atlas, Aditya&amp;#39;s research on differentiable ANN indexes, and many more! This was such a fun one, I really hope you find it useful! Please let us know what you think!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:12:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Instructor with Jason Liu - Weaviate Podcast #88!]]></title>
			<description><![CDATA[<p>Jason Liu is the creator of Instructor, one of the world&#39;s leading LLM frameworks, particularly focused on structured output parsing with LLMs, or as Jason puts it &quot;making LLMs more backwards compatible&quot;. It is hard to understand the impact of Instructor, this is truly leading us to the next era of LLM programming. It was such an honor chatting with Jason, his experience currently as an independent consultant and previously engineering at StitchFix and Meta makes him truly one of the most unique guests we have featured on the Weaviate podcast! I hope you enjoy the podcast!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Instructor-with-Jason-Liu---Weaviate-Podcast-88-e2fp8fn</link>
			<guid isPermaLink="false">470efcb2-6801-4ad3-97e0-abfc66df71b7</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 14 Feb 2024 14:54:57 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/82665399/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-1-14%2F2114dcf6-0555-413a-fff3-31ae4938f60f.mp3" length="108453379" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Jason Liu is the creator of Instructor, one of the world&amp;#39;s leading LLM frameworks, particularly focused on structured output parsing with LLMs, or as Jason puts it &amp;quot;making LLMs more backwards compatible&amp;quot;. It is hard to understand the impact of Instructor, this is truly leading us to the next era of LLM programming. It was such an honor chatting with Jason, his experience currently as an independent consultant and previously engineering at StitchFix and Meta makes him truly one of the most unique guests we have featured on the Weaviate podcast! I hope you enjoy the podcast!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:30</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[XMC.dspy with Karel D'Oosterlinck - Weaviate Podcast #87!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 87th episode of the Weaviate Podcast! I am SUPER excited to welcome Karel D&#39;Oosterlinck! Karel is the creator of IReRa (Infer-Retrieve-Rank)! IReRa is one of the most impressive systems that have been built for Extreme Multi-Label Classification, leveraging the emerging paradigm of DSPy compilation! This podcast dives into all things IReRa, XMC, DSPy compilation, and applications in Biomedical NLP and Recommendation! I hope you find this useful!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/XMC-dspy-with-Karel-DOosterlinck---Weaviate-Podcast-87-e2fehtk</link>
			<guid isPermaLink="false">725d393b-77da-4e0b-98ef-58a342beb60a</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 06 Feb 2024 16:21:08 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/82314612/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-1-6%2F842a6f11-badc-ddb5-14a5-35372ef8c097.mp3" length="134315039" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 87th episode of the Weaviate Podcast! I am SUPER excited to welcome Karel D&amp;#39;Oosterlinck! Karel is the creator of IReRa (Infer-Retrieve-Rank)! IReRa is one of the most impressive systems that have been built for Extreme Multi-Label Classification, leveraging the emerging paradigm of DSPy compilation! This podcast dives into all things IReRa, XMC, DSPy compilation, and applications in Biomedical NLP and Recommendation! I hope you find this useful!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:08:53</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Open-Source AI with Vinod Valloppillil and Bob van Luijt - Weaviate Podcast #86!]]></title>
			<description><![CDATA[<p>Hey everyone! We are super excited to publish this podcast with Vinod Valloppillil and Bob van Luijt on Open-Source AI and future directions for RAG! The podcast begins by discussing Vinod&#39;s &quot;Halloween Documents&quot;, a series of internal strategy writings at Microsoft related to the open-source software movement! The conversation continues to discuss the current state of Open-Source in AI. One of the major points Bob has been making about the business of AI models is that the models themselves are *stateless*, akin to an MP3 file. Vinod pushes back a bit on this definition and jointly it is then settled that these models neither fall into the pure stateful or stateless bucket, rather a &quot;pre-baked&quot; bucket -- presenting completely new opportunities to build business around software. The conversation then continues to discuss the particular details of how people are building RAG systems and many directions for how that may evolve!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Open-Source-AI-with-Vinod-Valloppillil-and-Bob-van-Luijt---Weaviate-Podcast-86-e2eqrpa</link>
			<guid isPermaLink="false">b0264f09-13ea-4def-8fbf-3ab70e6d0697</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 23 Jan 2024 15:00:58 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/81669354/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-23%2F565a7993-4724-1b29-7796-ae39341802db.mp3" length="107918301" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! We are super excited to publish this podcast with Vinod Valloppillil and Bob van Luijt on Open-Source AI and future directions for RAG! The podcast begins by discussing Vinod&amp;#39;s &amp;quot;Halloween Documents&amp;quot;, a series of internal strategy writings at Microsoft related to the open-source software movement! The conversation continues to discuss the current state of Open-Source in AI. One of the major points Bob has been making about the business of AI models is that the models themselves are *stateless*, akin to an MP3 file. Vinod pushes back a bit on this definition and jointly it is then settled that these models neither fall into the pure stateful or stateless bucket, rather a &amp;quot;pre-baked&amp;quot; bucket -- presenting completely new opportunities to build business around software. The conversation then continues to discuss the particular details of how people are building RAG systems and many directions for how that may evolve!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:04</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[DSPy and ColBERT with Omar Khattab! - Weaviate Podcast #85]]></title>
			<description><![CDATA[<p>Hey everyone! I am beyond excited to present our interview with Omar Khattab from Stanford University! Omar is one of the world&#39;s leading scientists on AI and NLP. I highly recommend you check out Omar&#39;s remarkable list of publications linked below! This interview completely transformed my understanding of building RAG and LLM applications! I believe that DSPy will be one of the most impactful software project in LLM development because of the abstractions around *program optimization*. Here is my TLDR of this concept of LLM programs and program optimization with DSPy, I of course encourage you to view the podcast and listen to Omar&#39;s explanation haha.

RAG is one of the most popular LLM programs we have seen. RAG typically consists of two components of retrieve and then generate. Within the generate component we have a prompt like &quot;please ground your answer based on the search results {search_results}&quot;. DSPy gives us a framework to optimize this prompt, bootstrap few-shot examples, or even fine-tune the model if needed. This works by compiling the program based on some evaluation criteria we give DSPy. Now let&#39;s say we add a query re-writer that takes the query and writes a new query before sending it to the retrieval system, and a reranker that takes the search results and re-orders them before handing them to the answer generator. Now we have 4 components of query writer, retrieve, rerank, answer. The 3 components of query writer, rerank, and answer all have a prompt that can be optimized with DSPy to enhance the description of the task or add examples! This optimization is done with DSPy&#39;s Teleprompters.

There are a few other really interesting components to DSPy as well -- such as the formatting of prompts with the docstrings and Signature abstraction, which in my view is quite similar to instructor or LMQL. DSPy also comes with built-in prompts like Chain-of-Thought that offer a really quick way to add this reasoning step and follow a structured output format. I am having so much fun learning about DSPy and I highly recommend you join me in viewing the GitHub repository linked below (with new examples!!):

Omar also discusses ColBERT and late interaction retrieval! Omar describes how this achieves the contextualized attention of cross encoders but in a much more scalable system with the maximum similarity between vectors! Stay tuned for more updates from Weaviate as we are diving into multi vector representations to hopefully support systems like this soon!</p>
<p><br></p>
<p>Chapters</p>
<p>0:00 Weaviate at NeurIPS 2023!</p>
<p>0:38 Omar Khattab</p>
<p>0:57 What is the state of AI?</p>
<p>2:35 DSPy</p>
<p>10:37 Pipelines</p>
<p>14:24 Prompt Tuning and Optimization</p>
<p>18:12 Models for Specific Tasks</p>
<p>21:44 LLM Compiler</p>
<p>23:32 Colbert or ColBERT?</p>
<p>24:02 ColBERT</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/DSPy-and-ColBERT-with-Omar-Khattab----Weaviate-Podcast-85-e2effki</link>
			<guid isPermaLink="false">551c387f-65f4-4749-a0f5-5b02868824e1</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 15 Jan 2024 14:40:26 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/81296466/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-15%2F54d4fdc6-3f24-56f3-0eb7-3b051c660214.mp3" length="61075331" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! I am beyond excited to present our interview with Omar Khattab from Stanford University! Omar is one of the world&amp;#39;s leading scientists on AI and NLP. I highly recommend you check out Omar&amp;#39;s remarkable list of publications linked below! This interview completely transformed my understanding of building RAG and LLM applications! I believe that DSPy will be one of the most impactful software project in LLM development because of the abstractions around *program optimization*. Here is my TLDR of this concept of LLM programs and program optimization with DSPy, I of course encourage you to view the podcast and listen to Omar&amp;#39;s explanation haha.

RAG is one of the most popular LLM programs we have seen. RAG typically consists of two components of retrieve and then generate. Within the generate component we have a prompt like &amp;quot;please ground your answer based on the search results {search_results}&amp;quot;. DSPy gives us a framework to optimize this prompt, bootstrap few-shot examples, or even fine-tune the model if needed. This works by compiling the program based on some evaluation criteria we give DSPy. Now let&amp;#39;s say we add a query re-writer that takes the query and writes a new query before sending it to the retrieval system, and a reranker that takes the search results and re-orders them before handing them to the answer generator. Now we have 4 components of query writer, retrieve, rerank, answer. The 3 components of query writer, rerank, and answer all have a prompt that can be optimized with DSPy to enhance the description of the task or add examples! This optimization is done with DSPy&amp;#39;s Teleprompters.

There are a few other really interesting components to DSPy as well -- such as the formatting of prompts with the docstrings and Signature abstraction, which in my view is quite similar to instructor or LMQL. DSPy also comes with built-in prompts like Chain-of-Thought that offer a really quick way to add this reasoning step and follow a structured output format. I am having so much fun learning about DSPy and I highly recommend you join me in viewing the GitHub repository linked below (with new examples!!):

Omar also discusses ColBERT and late interaction retrieval! Omar describes how this achieves the contextualized attention of cross encoders but in a much more scalable system with the maximum similarity between vectors! Stay tuned for more updates from Weaviate as we are diving into multi vector representations to hopefully support systems like this soon!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chapters&lt;/p&gt;
&lt;p&gt;0:00 Weaviate at NeurIPS 2023!&lt;/p&gt;
&lt;p&gt;0:38 Omar Khattab&lt;/p&gt;
&lt;p&gt;0:57 What is the state of AI?&lt;/p&gt;
&lt;p&gt;2:35 DSPy&lt;/p&gt;
&lt;p&gt;10:37 Pipelines&lt;/p&gt;
&lt;p&gt;14:24 Prompt Tuning and Optimization&lt;/p&gt;
&lt;p&gt;18:12 Models for Specific Tasks&lt;/p&gt;
&lt;p&gt;21:44 LLM Compiler&lt;/p&gt;
&lt;p&gt;23:32 Colbert or ColBERT?&lt;/p&gt;
&lt;p&gt;24:02 ColBERT&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:31:25</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Subjectivity in AI with Dan Shipper: AI-Native Databases #4]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the fourth and final episode of the AI-Native Database series with Dan Shipper! This was another epic one! Dan has had an absolutely remarkable career creating and selling a company and now co-founding and working as the CEO of Every! Every is an incredibly future-looking business focused on content online, both with an amazing newsletter, community of writers and thinkers, an AI-note taking app, and more! I think Dan brings a very unique perspective to the series, as well as the Weaviate podcast broadly, because of his experience with writers and understanding how writers are going to use these new technologies! We heavily discussed the role of personality or subjectivity in AI, amongst many other topics! I really hope you enjoy the podcast, as always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Read writings from Dan Shipper on Every: https://every.to/@danshipper

Chapters
0:00 AI-Native Databases
0:58 Welcome Dan Shipper!
1:37 GPT-4 is a Reasoning Engine
8:40 Subjectivity in LLMs
12:14 AI in Note Taking
16:38 The opinions of LLMs
25:50 Cookbooks for you
31:16 Overdrive in LLMs
34:50 Tweaking the voice of AI
40:45 Multi-Agent Personalities</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Subjectivity-in-AI-with-Dan-Shipper-AI-Native-Databases-4-e2e1u5n</link>
			<guid isPermaLink="false">d82e103e-ff67-459f-91a4-aa06b0e01279</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 21 Dec 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/80852599/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-5%2F362309790-44100-2-13e1339fb3f95.mp3" length="40741824" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the fourth and final episode of the AI-Native Database series with Dan Shipper! This was another epic one! Dan has had an absolutely remarkable career creating and selling a company and now co-founding and working as the CEO of Every! Every is an incredibly future-looking business focused on content online, both with an amazing newsletter, community of writers and thinkers, an AI-note taking app, and more! I think Dan brings a very unique perspective to the series, as well as the Weaviate podcast broadly, because of his experience with writers and understanding how writers are going to use these new technologies! We heavily discussed the role of personality or subjectivity in AI, amongst many other topics! I really hope you enjoy the podcast, as always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Read writings from Dan Shipper on Every: https://every.to/@danshipper

Chapters
0:00 AI-Native Databases
0:58 Welcome Dan Shipper!
1:37 GPT-4 is a Reasoning Engine
8:40 Subjectivity in LLMs
12:14 AI in Note Taking
16:38 The opinions of LLMs
25:50 Cookbooks for you
31:16 Overdrive in LLMs
34:50 Tweaking the voice of AI
40:45 Multi-Agent Personalities&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:42:26</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Humans and AI with John Maeda: AI-Native Databases #3]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 3rd episode of the AI-Native Database series featuring John Maeda and Bob van Luijt! This one dives into how humans perceive AI, from Anthroaormorphization to Doomsday scenario thinking and how important understanding how AI actually work is to the engineering of these systems. Bob and John discuss the evolution of the design in tech report, 3 categories of design, and many others! I hope you enjoy the podcast! As always, we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Links:

Design in Tech Report: https://designintech.report/

3 Kinds of Design: https://qz.com/1585165/john-maeda-on-the-importance-of-computational-design

Microsoft Semantic Kernel: https://github.com/microsoft/semantic-kernel

Chapters
0:00 AI-Native Databases
0:58 Welcome John Maeda!
1:35 Design in Tech Report
4:07 Anthropomorphizing AI
15:30 3 Types of Design
19:30 The ChatGPT Shift
22:58 Explaining Technology
32:54 Impact of AI on the Creative Industries
39:00 Semantic Kernel</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Humans-and-AI-with-John-Maeda-AI-Native-Databases-3-e2e1u1r</link>
			<guid isPermaLink="false">4ea3277e-9a85-4e51-ba1c-615e40b4a78f</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 20 Dec 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/80852475/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-5%2F362309362-44100-2-9af2a8a4d1b16.mp3" length="38506578" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 3rd episode of the AI-Native Database series featuring John Maeda and Bob van Luijt! This one dives into how humans perceive AI, from Anthroaormorphization to Doomsday scenario thinking and how important understanding how AI actually work is to the engineering of these systems. Bob and John discuss the evolution of the design in tech report, 3 categories of design, and many others! I hope you enjoy the podcast! As always, we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Links:

Design in Tech Report: https://designintech.report/

3 Kinds of Design: https://qz.com/1585165/john-maeda-on-the-importance-of-computational-design

Microsoft Semantic Kernel: https://github.com/microsoft/semantic-kernel

Chapters
0:00 AI-Native Databases
0:58 Welcome John Maeda!
1:35 Design in Tech Report
4:07 Anthropomorphizing AI
15:30 3 Types of Design
19:30 The ChatGPT Shift
22:58 Explaining Technology
32:54 Impact of AI on the Creative Industries
39:00 Semantic Kernel&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:40:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Structure in Data with Paul Groth: AI-Native Databases #2]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the second episode of AI-Native Databases with Paul Groth! This was another epic one, diving deep into the role of structure in our data! Beginning with Knowledge Graphs and LLMs, there are two perspectives: LLMs for Knowledge Graphs (using LLMs to extract relationships or predict missing links) and then Knowledge Graph for LLMs (to provide factual information in RAG). There is another intersection that sits in the middle of both LLMs for KGs and KGs for LLMs, which is using LLMs to query Knowledge Graphs, e.g. Text-to-Cypher/SPARQL/... From there I think the conversation evolves in a really fascinating way exploring the ability to structure data on-the-fly. Paul says &quot;Unstructured data is now becoming a peer to structured data&quot;! I think in addition to RAG, Generative Search is another underrated use case -- where we use LLMs to summarize search results or parse out the structure. Super interesting ideas, I hope you enjoy the podcast -- as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Learn more about Professor Groth&#39;s research here: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa3F1di02akZEQ2E3RGtpT29BZ2dQck1jMlZid3xBQ3Jtc0tsYmJ6XzlHS0VOZktob0dPUGZKT2J1clhLS2RNZF80TGtFbldrRWYzODJjQnduLVVya0tpdVlxUzdhWFZGOElpQUhFUVk3SmpDSnFPNmVZa2pKRmFNTnQyM3NLajdaYjNLOFd1N1N4YUdoZWxpSUNRbw&q=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fuser%3D0tHSHCIAAAAJ%26hl%3Den&v=3ET69F7smk8" target="_blank" rel="nofollow">https://scholar.google.com/citations?...</a>

Knowledge Engineering using Large Language Models: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbDVSUVE1Z0dlX2JUZXNkVkZ4dFNDOWpfOGFIQXxBQ3Jtc0traUdqSUpvdnVuUERqX0gwVHhjRzdFc0lRZ2k2UU1HdUVEbTktTFhNU01qeUM3eDBBRFY0S2c1NDlyLUhkcmRDUnJ2MjEzczcwNnFfZWljUW5JM0hPWWMxN0pFUmtfYUJzRnRGbHktdHdDNHRMb0xXUQ&q=https%3A%2F%2Farxiv.org%2Fpdf%2F2310.00637.pdf&v=3ET69F7smk8" target="_blank" rel="nofollow">https://arxiv.org/pdf/2310.00637.pdf</a>

How Much Knowledge Can You Pack into the Parameters of a Language Model? <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqazZVcnVKZEFlVWthRkMwNXY5TDdLTzhCTTY4Z3xBQ3Jtc0trNWZkVF9KaXNYM3Z6Zml1djV3eXROejUxVjl1MFRxMXhsdHpCYVhla1V3X1A5M0J5ZkR1TlRBLXlKS1FYU3lROG9tbDFONUU1XzdycVl6X1ZTM2lyd1JQakRNZnhObENZUzdHZlVJWEpfN1NyUldXNA&q=https%3A%2F%2Farxiv.org%2Fabs%2F2002.08910&v=3ET69F7smk8" target="_blank" rel="nofollow">https://arxiv.org/abs/2002.08910</a>

Chapters

<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=0s" rel="nofollow">0:00</a> AI-Native Databases!
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=58s" rel="nofollow">0:58</a> Welcome Paul!
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=85s" rel="nofollow">1:25</a> Bob’s overview of the series
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=150s" rel="nofollow">2:30</a> How do we build great datasets?
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=268s" rel="nofollow">4:28</a> Defining Knowledge Graphs
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=435s" rel="nofollow">7:15</a> LLM as a Knowledge Graph
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=918s" rel="nofollow">15:18</a> Adding CRUD Support to Models
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=1690s" rel="nofollow">28:10</a> Database of Model Weights
<a href="https://www.youtube.com/watch?v=3ET69F7smk8&t=1970s" rel="nofollow">32:50</a> Structuring Data On-the-Fly</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Structure-in-Data-with-Paul-Groth-AI-Native-Databases-2-e2e1tvq</link>
			<guid isPermaLink="false">59a43f71-fedf-4586-9de0-b7f5c8d52ae2</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 19 Dec 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/80852410/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-5%2F362309160-44100-2-1117b082e9744.mp3" length="43448528" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the second episode of AI-Native Databases with Paul Groth! This was another epic one, diving deep into the role of structure in our data! Beginning with Knowledge Graphs and LLMs, there are two perspectives: LLMs for Knowledge Graphs (using LLMs to extract relationships or predict missing links) and then Knowledge Graph for LLMs (to provide factual information in RAG). There is another intersection that sits in the middle of both LLMs for KGs and KGs for LLMs, which is using LLMs to query Knowledge Graphs, e.g. Text-to-Cypher/SPARQL/... From there I think the conversation evolves in a really fascinating way exploring the ability to structure data on-the-fly. Paul says &amp;quot;Unstructured data is now becoming a peer to structured data&amp;quot;! I think in addition to RAG, Generative Search is another underrated use case -- where we use LLMs to summarize search results or parse out the structure. Super interesting ideas, I hope you enjoy the podcast -- as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Learn more about Professor Groth&amp;#39;s research here: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa3F1di02akZEQ2E3RGtpT29BZ2dQck1jMlZid3xBQ3Jtc0tsYmJ6XzlHS0VOZktob0dPUGZKT2J1clhLS2RNZF80TGtFbldrRWYzODJjQnduLVVya0tpdVlxUzdhWFZGOElpQUhFUVk3SmpDSnFPNmVZa2pKRmFNTnQyM3NLajdaYjNLOFd1N1N4YUdoZWxpSUNRbw&amp;q=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fuser%3D0tHSHCIAAAAJ%26hl%3Den&amp;v=3ET69F7smk8&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://scholar.google.com/citations?...&lt;/a&gt;

Knowledge Engineering using Large Language Models: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbDVSUVE1Z0dlX2JUZXNkVkZ4dFNDOWpfOGFIQXxBQ3Jtc0traUdqSUpvdnVuUERqX0gwVHhjRzdFc0lRZ2k2UU1HdUVEbTktTFhNU01qeUM3eDBBRFY0S2c1NDlyLUhkcmRDUnJ2MjEzczcwNnFfZWljUW5JM0hPWWMxN0pFUmtfYUJzRnRGbHktdHdDNHRMb0xXUQ&amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F2310.00637.pdf&amp;v=3ET69F7smk8&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/pdf/2310.00637.pdf&lt;/a&gt;

How Much Knowledge Can You Pack into the Parameters of a Language Model? &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazZVcnVKZEFlVWthRkMwNXY5TDdLTzhCTTY4Z3xBQ3Jtc0trNWZkVF9KaXNYM3Z6Zml1djV3eXROejUxVjl1MFRxMXhsdHpCYVhla1V3X1A5M0J5ZkR1TlRBLXlKS1FYU3lROG9tbDFONUU1XzdycVl6X1ZTM2lyd1JQakRNZnhObENZUzdHZlVJWEpfN1NyUldXNA&amp;q=https%3A%2F%2Farxiv.org%2Fabs%2F2002.08910&amp;v=3ET69F7smk8&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://arxiv.org/abs/2002.08910&lt;/a&gt;

Chapters

&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=0s&quot; rel=&quot;nofollow&quot;&gt;0:00&lt;/a&gt; AI-Native Databases!
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=58s&quot; rel=&quot;nofollow&quot;&gt;0:58&lt;/a&gt; Welcome Paul!
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=85s&quot; rel=&quot;nofollow&quot;&gt;1:25&lt;/a&gt; Bob’s overview of the series
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=150s&quot; rel=&quot;nofollow&quot;&gt;2:30&lt;/a&gt; How do we build great datasets?
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=268s&quot; rel=&quot;nofollow&quot;&gt;4:28&lt;/a&gt; Defining Knowledge Graphs
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=435s&quot; rel=&quot;nofollow&quot;&gt;7:15&lt;/a&gt; LLM as a Knowledge Graph
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=918s&quot; rel=&quot;nofollow&quot;&gt;15:18&lt;/a&gt; Adding CRUD Support to Models
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=1690s&quot; rel=&quot;nofollow&quot;&gt;28:10&lt;/a&gt; Database of Model Weights
&lt;a href=&quot;https://www.youtube.com/watch?v=3ET69F7smk8&amp;t=1970s&quot; rel=&quot;nofollow&quot;&gt;32:50&lt;/a&gt; Structuring Data On-the-Fly&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:45:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Self-Driving Databases with Andy Pavlo: AI-Native Databases #1]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the first episode of AI-Native Databases with Andy Pavlo! This was an epic one! We began by explaining the &quot;Self-Driving Database&quot; and all the opportunities to optimize DBs with AI and ML at both the low-level, as well as how we query and interact with them. We also discussed new opportunities with DBs + LLMs, such as bringing the data to the model (such as ROME, MEMIT, GRACE), in addition to bringing the model to the data (such as RAG). We also discuss the subjective &quot;opinion&quot; of these models and many more!

I hope you enjoy the podcast! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast! This one means a lot to me. Andy Pavlo&#39;s CMU DB course was one of the most impactful resources in my personal education, and I love the vision for the future outlined by OtterTune! It was amazing to see Etienne Dilocker featured in the ML for DBs, DBs for ML series at CMU. I am so grateful to Andy for joining the Weaviate Podcast!

Links:
CMU Database Group on YouTube: https://www.youtube.com/@CMUDatabaseGroup/videos

Self-Driving Database Management Systems - Pavlo et al. - https://db.cs.cmu.edu/papers/2017/p42-pavlo-cidr17.pdf

Database of Databases: https://dbdb.io/

Generative Feedback Loops: https://weaviate.io/blog/generative-feedback-loops-with-llms

Weaviate Gorilla: https://weaviate.io/blog/weaviate-gorilla-part-1

Chapters

0:00 AI-Native Databases
0:58 Welcome Andy
1:58 Bob’s overview of the series
3:20 Self-Driving Databases
8:18 Why isn’t there just 1 Database?
12:46 Collaboration of Models and Databases
20:05 LLM Schema Tuning
23:44 The Opinion of the System
28:20 PyTorchDB - Moving the Data to the Model
33:30 Database APIs
38:15 Learning to operate Databases
42:54 Vector DBs and the DB Hype Cycle
51:38 SQL in Weaviate? 
1:07:40 The Future of DBs
1:14:00 Thank you Andy!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Self-Driving-Databases-with-Andy-Pavlo-AI-Native-Databases-1-e2e1u3j</link>
			<guid isPermaLink="false">51aa3995-9951-4956-aebf-609d68bde8ae</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 18 Dec 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/80852531/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-5%2F362309643-44100-2-7b8f3969e51a7.mp3" length="71560880" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the first episode of AI-Native Databases with Andy Pavlo! This was an epic one! We began by explaining the &amp;quot;Self-Driving Database&amp;quot; and all the opportunities to optimize DBs with AI and ML at both the low-level, as well as how we query and interact with them. We also discussed new opportunities with DBs + LLMs, such as bringing the data to the model (such as ROME, MEMIT, GRACE), in addition to bringing the model to the data (such as RAG). We also discuss the subjective &amp;quot;opinion&amp;quot; of these models and many more!

I hope you enjoy the podcast! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast! This one means a lot to me. Andy Pavlo&amp;#39;s CMU DB course was one of the most impactful resources in my personal education, and I love the vision for the future outlined by OtterTune! It was amazing to see Etienne Dilocker featured in the ML for DBs, DBs for ML series at CMU. I am so grateful to Andy for joining the Weaviate Podcast!

Links:
CMU Database Group on YouTube: https://www.youtube.com/@CMUDatabaseGroup/videos

Self-Driving Database Management Systems - Pavlo et al. - https://db.cs.cmu.edu/papers/2017/p42-pavlo-cidr17.pdf

Database of Databases: https://dbdb.io/

Generative Feedback Loops: https://weaviate.io/blog/generative-feedback-loops-with-llms

Weaviate Gorilla: https://weaviate.io/blog/weaviate-gorilla-part-1

Chapters

0:00 AI-Native Databases
0:58 Welcome Andy
1:58 Bob’s overview of the series
3:20 Self-Driving Databases
8:18 Why isn’t there just 1 Database?
12:46 Collaboration of Models and Databases
20:05 LLM Schema Tuning
23:44 The Opinion of the System
28:20 PyTorchDB - Moving the Data to the Model
33:30 Database APIs
38:15 Learning to operate Databases
42:54 Vector DBs and the DB Hype Cycle
51:38 SQL in Weaviate? 
1:07:40 The Future of DBs
1:14:00 Thank you Andy!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:14:32</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Weaviate 1.23 Release Podcast with Etienne Dilocker!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the Weaviate 1.23 Release Podcast with Weaviate Co-Founder and CTO Etienne Dilocker! Weaviate 1.23 is a massive step forward for managing multi-tenancy with vector databases. For most RAG and Vector DB applications, you will have an uneven distribution in the # of vectors per user. Some users have 10k docs, others 10M+! Weaviate now offers a flat index with binary quantization to efficiently balance when you need an HNSW graph for the 10M doc users and when brute force is all you need for the 10k doc users!

Weaviate also comes with some other &quot;self-driving database&quot; features like lazy shard loading for faster startup times with multi-tenancy and automatic resource limiting with the GOMEMLIMIT and other details Etienne shares in the podcast!

I am also beyond excited to present our new integration with Anyscale (@anyscalecompute)! Anyscale has amazing pricing for serving and fine-tuning popular open-source LLMs. At the time of this release we are now integrating the Llama 70B/13B/7B, Mistral 7B, and Code Llama 34B into Weaviate -- but we expect much further development with adding support for fine-tuned models, the super cool new function calling models Anyscale announced yesterday. and other model such as Diffusion and multimodal models!

Chapters
0:00 Weaviate 1.23
1:08 Lazy Shard Loading
8:20 Flat Index + BQ
33:15 Default Segments for PQ
38:55 AutoPQ
42:20 Auto Resource Limiting
46:04 Node Endpoint Update
47:25 Generative Anyscale

Links:
Etienne Dilocker on Native Multi-Tenancy at the AI Conference in SF:
https://www.youtube.com/watch?v=KT2RFMTJKGs

Etienne Dilocker in the CMU DB Series: 
https://www.youtube.com/watch?v=4sLJapXEPd4

Self-Driving Databases by Andy Pavlo: https://www.cs.cmu.edu/~pavlo/blog/2018/04/what-is-a-self-driving-database-management-system.html</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Weaviate-1-23-Release-Podcast-with-Etienne-Dilocker-e2e1u82</link>
			<guid isPermaLink="false">5e003923-1195-49dc-a1ab-d5a626adf539</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 14 Dec 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/80852674/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-5%2F362309951-44100-2-b0cde4e97bcd8.mp3" length="52924498" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the Weaviate 1.23 Release Podcast with Weaviate Co-Founder and CTO Etienne Dilocker! Weaviate 1.23 is a massive step forward for managing multi-tenancy with vector databases. For most RAG and Vector DB applications, you will have an uneven distribution in the # of vectors per user. Some users have 10k docs, others 10M+! Weaviate now offers a flat index with binary quantization to efficiently balance when you need an HNSW graph for the 10M doc users and when brute force is all you need for the 10k doc users!

Weaviate also comes with some other &amp;quot;self-driving database&amp;quot; features like lazy shard loading for faster startup times with multi-tenancy and automatic resource limiting with the GOMEMLIMIT and other details Etienne shares in the podcast!

I am also beyond excited to present our new integration with Anyscale (@anyscalecompute)! Anyscale has amazing pricing for serving and fine-tuning popular open-source LLMs. At the time of this release we are now integrating the Llama 70B/13B/7B, Mistral 7B, and Code Llama 34B into Weaviate -- but we expect much further development with adding support for fine-tuned models, the super cool new function calling models Anyscale announced yesterday. and other model such as Diffusion and multimodal models!

Chapters
0:00 Weaviate 1.23
1:08 Lazy Shard Loading
8:20 Flat Index + BQ
33:15 Default Segments for PQ
38:55 AutoPQ
42:20 Auto Resource Limiting
46:04 Node Endpoint Update
47:25 Generative Anyscale

Links:
Etienne Dilocker on Native Multi-Tenancy at the AI Conference in SF:
https://www.youtube.com/watch?v=KT2RFMTJKGs

Etienne Dilocker in the CMU DB Series: 
https://www.youtube.com/watch?v=4sLJapXEPd4

Self-Driving Databases by Andy Pavlo: https://www.cs.cmu.edu/~pavlo/blog/2018/04/what-is-a-self-driving-database-management-system.html&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:07</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Rudy Lai on Tactic Generate - Weaviate Podcast #78!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 78th episode of the Weaviate podcast featuring Rudy Lai, the founder and CEO of Tactic Generate! Tactic Generate has developed a user experience around applying LLMs in parallel to multiple documents, or even folders / collections / databases. Rudy discussed the user research that lead the company to this direction and how he sees the opportunities in building AI products with new LLM and Vector Database technologies! I hope you enjoy the podcast, as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Learn more about Tactic Generate here: https://tactic.fyi/generative-insights/

Weaviate Podcast #69 with Charles Pierse: https://www.youtube.com/watch?v=L_nyz1xs9AU

Chapters
0:00 Welcome Rudy!
0:48 Story of Tactic Generate
7:45 Finding Common Workflows
19:30 Multiple Document RAG UIs
26:14 Parallel LLM Execution
32:40 Aggregating Parallel LLM Analysis
38:25 Pretty Reports
44:28 Research Agents</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Rudy-Lai-on-Tactic-Generate---Weaviate-Podcast-78-e2crnf2</link>
			<guid isPermaLink="false">969a4936-c1de-40dd-a8be-f63062ec51cf</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 29 Nov 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/79600546/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-11-5%2F358606746-44100-2-956a0bc7e423a.mp3" length="53917151" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 78th episode of the Weaviate podcast featuring Rudy Lai, the founder and CEO of Tactic Generate! Tactic Generate has developed a user experience around applying LLMs in parallel to multiple documents, or even folders / collections / databases. Rudy discussed the user research that lead the company to this direction and how he sees the opportunities in building AI products with new LLM and Vector Database technologies! I hope you enjoy the podcast, as always more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Learn more about Tactic Generate here: https://tactic.fyi/generative-insights/

Weaviate Podcast #69 with Charles Pierse: https://www.youtube.com/watch?v=L_nyz1xs9AU

Chapters
0:00 Welcome Rudy!
0:48 Story of Tactic Generate
7:45 Finding Common Workflows
19:30 Multiple Document RAG UIs
26:14 Parallel LLM Execution
32:40 Aggregating Parallel LLM Analysis
38:25 Pretty Reports
44:28 Research Agents&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:56:09</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[RAGAS with Jithin James, Shahul Es, and Erika Cardenas - Weaviate Podcast #77!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 77th Weaviate Podcast on RAGAS, featuring Jithin James, Shahul ES, and Erika Cardenas! RAGAS is one of the hottest rising startups in Retrieval-Augmented Generation! RAGAS began it&#39;s journey with the RAGAS score, a matrix of evaluations for generation and retrieval. Generation evaluated on Faithfulness (is the response grounded in the context) as well as Relevancy (is the response useful). Retrieval is then evaluated on Precision (How many of the search results are relevant to the question?) and Recall (How many of the relevant search results are captured in the retrieved results?). Now, the super novel thing about this is that an LLM is used to determine these metrics. So we circumvent painstaking manual labeling effort with the RAGAS score! This podcast dives into the development of the RAGAS score as well as how RAG application builders should think about the knobs to tune for optimizing their RAGAS score: embedding models, chunking strategies, hybrid search tuning, rerankers, ... ?!? We also discussed tons of exciting directions for the future such as fine-tuning smaller LLMs for these metrics, agents that use tuning APIs, and long context RAG!

Check out the docs here for getting started with RAGAS! https://docs.ragas.io/en/latest/getstarted/index.html#get-started

Chapters
0:00 Welcome Jithin and Shahul!
0:44 Welcome Erika!
0:56 RAGAS, Founding Story
2:38 Weaviate + RAGAS integration plans
4:44 RAG Knobs to Tune
25:50 RAG Experiment Tracking
34:52 LangSmith and RAGAS
38:55 LLM Evaluation
40:25 RAGAS Agents
44:00 Long Context RAG Evaluation</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/RAGAS-with-Jithin-James--Shahul-Es--and-Erika-Cardenas---Weaviate-Podcast-77-e2crnba</link>
			<guid isPermaLink="false">42cf8fbf-804f-4e09-9f3b-d80f07bc73df</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 20 Nov 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/79600426/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-11-5%2F358606246-44100-2-c35bb52ff3eda.mp3" length="47995923" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 77th Weaviate Podcast on RAGAS, featuring Jithin James, Shahul ES, and Erika Cardenas! RAGAS is one of the hottest rising startups in Retrieval-Augmented Generation! RAGAS began it&amp;#39;s journey with the RAGAS score, a matrix of evaluations for generation and retrieval. Generation evaluated on Faithfulness (is the response grounded in the context) as well as Relevancy (is the response useful). Retrieval is then evaluated on Precision (How many of the search results are relevant to the question?) and Recall (How many of the relevant search results are captured in the retrieved results?). Now, the super novel thing about this is that an LLM is used to determine these metrics. So we circumvent painstaking manual labeling effort with the RAGAS score! This podcast dives into the development of the RAGAS score as well as how RAG application builders should think about the knobs to tune for optimizing their RAGAS score: embedding models, chunking strategies, hybrid search tuning, rerankers, ... ?!? We also discussed tons of exciting directions for the future such as fine-tuning smaller LLMs for these metrics, agents that use tuning APIs, and long context RAG!

Check out the docs here for getting started with RAGAS! https://docs.ragas.io/en/latest/getstarted/index.html#get-started

Chapters
0:00 Welcome Jithin and Shahul!
0:44 Welcome Erika!
0:56 RAGAS, Founding Story
2:38 Weaviate + RAGAS integration plans
4:44 RAG Knobs to Tune
25:50 RAG Experiment Tracking
34:52 LangSmith and RAGAS
38:55 LLM Evaluation
40:25 RAGAS Agents
44:00 Long Context RAG Evaluation&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Patrick Lewis on Retrieval-Augmented Generation - Weaviate Podcast #76!]]></title>
			<description><![CDATA[<p>Hey everyone, I am SUPER excited to present our 76th Weaviate Podcast featuring Patrick Lewis, an NLP Research Scientist at Cohere! Patrick has had an absolutely massive impact on Natural Language Processing with AI and Deep Learning! Especially notable for the current climate in AI and Weaviate is that Patrick is the lead author of the original &quot;Retrieval-Augmented Generation&quot; paper!! Patrick has contributed to many other profoundly impactful papers in the space as well such as DPR, Atlas, Task-Aware Retrieval with Instruction, and many many others! This was such an illuminating conversation, here is a quick overview of the chapters in the podcast!

1. Origin of RAG - Patrick explains the build-up that lead to the RAG paper, AskJeeves, IBM Watson, conceptual shift to retrieve-read in mainstream connectionist approaches to AI.

2. Atlas - Atlas shows that a much smaller LLM when paired with Retrieval-Augmentation can still achieve competitive few-shot and zero-shot task performance. This is super impactful because this few-shot and zero-shot capability has been a massive evangelist for AI broadly, and the fact that smaller Retrieval-Augmented models can do this is massive for the economically unlocking these applications.

Teasing apart some architectural details of RAG:

3. Fusion In-Decoder - Interesting encoder-decoder transformer design in which each document + the query is encoded separately, then concatenated and passed to the LM.
4. End-to-End RAG - How to think about jointly training an embedding model and an LLM augmented with retrieval?
5. Query Routers - How to route queries from say SQL or Vector DBs? (More nuance on this later with Multi-Index Retrieval)
6. ConcurrentQA - Super interesting work on the privacy of multi-index routers. For example, if you ask &quot;Who is the father of our new CEO&quot; - this may reveal the private information of the new CEO with the public query of their father.
7. Multi-Index Retrieval
8. New APIs for LLMs
9. Self-Instructed Gorillas
10. Task-Aware Retrieval with Instructions
11. Editing Text, EditEval and PEER
12. What future direction excites you the most?

Links:

Learn more about Patrick Lewis: https://www.patricklewis.io/

Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: https://arxiv.org/abs/2005.11401

Atlas: https://arxiv.org/pdf/2208.03299.pdf

Fusion In-Decoder: https://arxiv.org/pdf/2007.01282.pdf

Chapters
0:00 Welcome Patrick Lewis!
0:36 Origin of RAG
5:20 Atlas
10:43 Fusion In-Decoder
17:50 End-to-End RAG
27:05 Query Routers
32:05 ConcurrentQA
37:30 Multi-Index Retrieval
40:05 New APIs for LLMs
41:50 Self-Instructed Gorillas
44:35 Task-Aware Retrieval with Instructions
52:00 Editing Text, EditEval and PEER
55:35 What future direction excites you the most?</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Patrick-Lewis-on-Retrieval-Augmented-Generation---Weaviate-Podcast-76-e2crn90</link>
			<guid isPermaLink="false">de98648f-9711-4c76-a070-23d405bd73b0</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 14 Nov 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/79600352/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-11-5%2F358606051-44100-2-56238b4357f07.mp3" length="56619257" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, I am SUPER excited to present our 76th Weaviate Podcast featuring Patrick Lewis, an NLP Research Scientist at Cohere! Patrick has had an absolutely massive impact on Natural Language Processing with AI and Deep Learning! Especially notable for the current climate in AI and Weaviate is that Patrick is the lead author of the original &amp;quot;Retrieval-Augmented Generation&amp;quot; paper!! Patrick has contributed to many other profoundly impactful papers in the space as well such as DPR, Atlas, Task-Aware Retrieval with Instruction, and many many others! This was such an illuminating conversation, here is a quick overview of the chapters in the podcast!

1. Origin of RAG - Patrick explains the build-up that lead to the RAG paper, AskJeeves, IBM Watson, conceptual shift to retrieve-read in mainstream connectionist approaches to AI.

2. Atlas - Atlas shows that a much smaller LLM when paired with Retrieval-Augmentation can still achieve competitive few-shot and zero-shot task performance. This is super impactful because this few-shot and zero-shot capability has been a massive evangelist for AI broadly, and the fact that smaller Retrieval-Augmented models can do this is massive for the economically unlocking these applications.

Teasing apart some architectural details of RAG:

3. Fusion In-Decoder - Interesting encoder-decoder transformer design in which each document + the query is encoded separately, then concatenated and passed to the LM.
4. End-to-End RAG - How to think about jointly training an embedding model and an LLM augmented with retrieval?
5. Query Routers - How to route queries from say SQL or Vector DBs? (More nuance on this later with Multi-Index Retrieval)
6. ConcurrentQA - Super interesting work on the privacy of multi-index routers. For example, if you ask &amp;quot;Who is the father of our new CEO&amp;quot; - this may reveal the private information of the new CEO with the public query of their father.
7. Multi-Index Retrieval
8. New APIs for LLMs
9. Self-Instructed Gorillas
10. Task-Aware Retrieval with Instructions
11. Editing Text, EditEval and PEER
12. What future direction excites you the most?

Links:

Learn more about Patrick Lewis: https://www.patricklewis.io/

Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: https://arxiv.org/abs/2005.11401

Atlas: https://arxiv.org/pdf/2208.03299.pdf

Fusion In-Decoder: https://arxiv.org/pdf/2007.01282.pdf

Chapters
0:00 Welcome Patrick Lewis!
0:36 Origin of RAG
5:20 Atlas
10:43 Fusion In-Decoder
17:50 End-to-End RAG
27:05 Query Routers
32:05 ConcurrentQA
37:30 Multi-Index Retrieval
40:05 New APIs for LLMs
41:50 Self-Instructed Gorillas
44:35 Task-Aware Retrieval with Instructions
52:00 Editing Text, EditEval and PEER
55:35 What future direction excites you the most?&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:58:58</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Tanmay Chopra on Emissary - Weaviate Podcast #75!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 75th Weaviate Podcast featuring Tanmay Chopra! The podcast details Tanmay&#39;s incredible career in Machine Learning from Tik Tok to Neeva and now building his own startup, Emissary! Tanmay shared some amazing insights into Search AI such as how to process Temporal Queries, how to think about diversity in Retrieval, and Query Recommendation products! We then dove into the opportunity Tanmay sees in fine-tuning LLMs and knowledge distillation that motivated Tanmay to build Emissary! I thought Tanmay&#39;s analogy of GPT-4 to 3D printers was really interesting, tons of great nuggets in here! I really hope you enjoy the podcast, as always more than happy to answer any questions or discuss any ideas with you related to the content in the podcast!

Chapters
0:00 Welcome Tanmay!
0:23 Early Career Story
2:02 Tik Tok
4:10 Neeva
8:45 Temporal Queries
11:40 Retrieval Diversity
17:22 Query Recommendation
23:20 Emissary, starting a company!
30:20 A Simple API for Custom Models
35:42 GPT-4 = 3D Printer?</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Tanmay-Chopra-on-Emissary---Weaviate-Podcast-75-e2crn6i</link>
			<guid isPermaLink="false">086848b4-aa20-42ff-bf5e-f27a9e68fb2d</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 08 Nov 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/79600274/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-11-5%2F358605796-44100-2-b3ab7ba35d4d7.mp3" length="48290167" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 75th Weaviate Podcast featuring Tanmay Chopra! The podcast details Tanmay&amp;#39;s incredible career in Machine Learning from Tik Tok to Neeva and now building his own startup, Emissary! Tanmay shared some amazing insights into Search AI such as how to process Temporal Queries, how to think about diversity in Retrieval, and Query Recommendation products! We then dove into the opportunity Tanmay sees in fine-tuning LLMs and knowledge distillation that motivated Tanmay to build Emissary! I thought Tanmay&amp;#39;s analogy of GPT-4 to 3D printers was really interesting, tons of great nuggets in here! I really hope you enjoy the podcast, as always more than happy to answer any questions or discuss any ideas with you related to the content in the podcast!

Chapters
0:00 Welcome Tanmay!
0:23 Early Career Story
2:02 Tik Tok
4:10 Neeva
8:45 Temporal Queries
11:40 Retrieval Diversity
17:22 Query Recommendation
23:20 Emissary, starting a company!
30:20 A Simple API for Custom Models
35:42 GPT-4 = 3D Printer?&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:50:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Simba Khadder on FeatureForm - Weaviate Podcast #74!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 74th Weaviate Podcast feature Simba Khadder, the CEO and Co-Founder of FeatureForm! To begin, &quot;features&quot; broadly describe the inputs to machine learning models that they use to produce outputs, or predictions. Feature stores orchestrate the construction of features, whether that be transformations for tabular machine learning models such as XGBoost, to chunking for vector embedding inference, and now features for LLM inference in RAG. Right out of the gate, Simba really opened my eyes to the role that feature engineering plays in RAG. Further touching on this at the very end under the &quot;Exciting future for RAG with Features&quot; chapter, Simba further describes how we can use more advanced features to provide better context to LLMs. In addition to these insights on RAG, there are so many nuggets in the podcast, Simba is a world class professional when it comes to building distributed systems, production scale recommendation systems, and more! I learned so much from chatting with Simba, I hope you enjoy listening to the podcast! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

FeatureForm: https://www.featureform.com/

Highly Recommend!! Simba Khadder at the CMU DB Seminar series: https://www.youtube.com/watch?v=ZsWa6XiBc-U

FeatureForm and Weaviate demo! https://docs.featureform.com/providers/weaviate

Chapters
0:00 Simba Khadder
0:35 RAG and Feature Stores
4:30 Experience building Recommendation Systems
9:47 The End-to-End Feature Lifecycle
15:08 Virtual Feature Store Orchestration
26:45 RAG Evaluation
31:27 Feature Engineering
34:15 LLM Tuning and Features
39:55 Streaming Features
51:15 Data Drift Detection
54:20 Exciting future for RAG with Features</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Simba-Khadder-on-FeatureForm---Weaviate-Podcast-74-e2crn3v</link>
			<guid isPermaLink="false">b04c07c2-cf63-43f4-a803-625faa0aea80</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 07 Nov 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/79600191/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-11-5%2F358605484-44100-2-1f9ea828285a7.mp3" length="54437510" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 74th Weaviate Podcast feature Simba Khadder, the CEO and Co-Founder of FeatureForm! To begin, &amp;quot;features&amp;quot; broadly describe the inputs to machine learning models that they use to produce outputs, or predictions. Feature stores orchestrate the construction of features, whether that be transformations for tabular machine learning models such as XGBoost, to chunking for vector embedding inference, and now features for LLM inference in RAG. Right out of the gate, Simba really opened my eyes to the role that feature engineering plays in RAG. Further touching on this at the very end under the &amp;quot;Exciting future for RAG with Features&amp;quot; chapter, Simba further describes how we can use more advanced features to provide better context to LLMs. In addition to these insights on RAG, there are so many nuggets in the podcast, Simba is a world class professional when it comes to building distributed systems, production scale recommendation systems, and more! I learned so much from chatting with Simba, I hope you enjoy listening to the podcast! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

FeatureForm: https://www.featureform.com/

Highly Recommend!! Simba Khadder at the CMU DB Seminar series: https://www.youtube.com/watch?v=ZsWa6XiBc-U

FeatureForm and Weaviate demo! https://docs.featureform.com/providers/weaviate

Chapters
0:00 Simba Khadder
0:35 RAG and Feature Stores
4:30 Experience building Recommendation Systems
9:47 The End-to-End Feature Lifecycle
15:08 Virtual Feature Store Orchestration
26:45 RAG Evaluation
31:27 Feature Engineering
34:15 LLM Tuning and Features
39:55 Streaming Features
51:15 Data Drift Detection
54:20 Exciting future for RAG with Features&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:56:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Charles Packer on MemGPT - Weaviate Podcast #73!]]></title>
			<description><![CDATA[<p>Hey everyone! I am SUPER excited to publish our 73rd Weaviate Podcast with Charles Packer, the lead author of MemGPT at UC Berkeley! MemGPT presents the &quot;Operating System for LLMs&quot;, an incredibly exciting idea to explicitly prompt the LLM with the information that it has a limited context window and give it memory management tools to behave accordingly! This was such a fun discussion with Charles diving into all things related to the paper! I hope you enjoy the podcast!!

Check out MemGPT here! https://memgpt.ai/

Chapters
0:00 Welcome Charles!
0:27 LLM Operating System
4:47 Memory Management Tools
6:50 Interrupts in LLM Applications
10:15 LLM Tools
17:45 Self-Instruct Data Creation
20:50 Cost of Experiments
24:28 Explicit Context Annotation
29:40 Recall vs. Archival Storage
33:12 Page Replacement Inspiration
38:00 Creativity in AI
43:40 Evolutionary Perspective
46:18 Inspiring Future Directions
48:45 Multi-Threaded LLM Processing</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Charles-Packer-on-MemGPT---Weaviate-Podcast-73-e2bi0k7</link>
			<guid isPermaLink="false">c46fa143-61eb-4ee1-85a4-d36f87734886</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 06 Nov 2023 12:05:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/78233671/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-6%2F354309427-44100-2-60ad78a068139.mp3" length="49447078" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! I am SUPER excited to publish our 73rd Weaviate Podcast with Charles Packer, the lead author of MemGPT at UC Berkeley! MemGPT presents the &amp;quot;Operating System for LLMs&amp;quot;, an incredibly exciting idea to explicitly prompt the LLM with the information that it has a limited context window and give it memory management tools to behave accordingly! This was such a fun discussion with Charles diving into all things related to the paper! I hope you enjoy the podcast!!

Check out MemGPT here! https://memgpt.ai/

Chapters
0:00 Welcome Charles!
0:27 LLM Operating System
4:47 Memory Management Tools
6:50 Interrupts in LLM Applications
10:15 LLM Tools
17:45 Self-Instruct Data Creation
20:50 Cost of Experiments
24:28 Explicit Context Annotation
29:40 Recall vs. Archival Storage
33:12 Page Replacement Inspiration
38:00 Creativity in AI
43:40 Evolutionary Perspective
46:18 Inspiring Future Directions
48:45 Multi-Threaded LLM Processing&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:30</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Madelon Hulsebos on Tabular Machine Learning - Weaviate Podcast #72!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 72nd episode of the Weaviate Podcast with Madelon Hulsebos!! Madelon is one of the world&#39;s experts on Machine Learning with Tables and Tabular-Structured Data, this was such an eye-opening conversation! We discussed all sorts of topics from the relationship of tabular data and embeddings, to searching through tables, semantic joins, more complex Text-to-SQL, using machine learning for query execution, using tabular data in search and recommendation reranking, and many more! This was easily one of the most knowledge packed episodes of the Weaviate podcast so far, please don&#39;t hesitate to leave any questions or ideas you have related to the content discussed!

You can learn more about Madelon&#39;s incredible research career and publications / talks here: https://www.madelonhulsebos.com/! Papers such as GitTables are listed here!

Another nice nugget form the podcast - Madelon introduced me to the BIRD-SQL benchmark which really expanded my understanding of Text-to-SQL (https://arxiv.org/pdf/2305.03111.pdf.

Chapters
0:00 Welcome Madelon!
0:58 Tabular Data and Embeddings
3:10 Tabular Representation Learning
5:48 Semantic Type Detection
9:50 Pandas as an LLM Tool
11:52 Table-Based Question Answering and Text-to-SQL
19:35 Joins with Machine Learning
21:38 Query Execution with Machine Learning
22:45 Graph Neural Networks
24:07 XGBoost
28:28 Merging Tables
32:10 Fact Representation
35:50 GPT-4V and Tables
39:00 Metadata in Embeddings
42:45 Table Retrieval in Weaviate
46:25 Exciting future directions!!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Madelon-Hulsebos-on-Tabular-Machine-Learning---Weaviate-Podcast-72-e2bhrec</link>
			<guid isPermaLink="false">8f6f71b0-819b-48e0-a242-8b3be5c8fd36</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 01 Nov 2023 03:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/78228364/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-6%2F354295784-44100-2-ac63f7de32931.mp3" length="47675349" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 72nd episode of the Weaviate Podcast with Madelon Hulsebos!! Madelon is one of the world&amp;#39;s experts on Machine Learning with Tables and Tabular-Structured Data, this was such an eye-opening conversation! We discussed all sorts of topics from the relationship of tabular data and embeddings, to searching through tables, semantic joins, more complex Text-to-SQL, using machine learning for query execution, using tabular data in search and recommendation reranking, and many more! This was easily one of the most knowledge packed episodes of the Weaviate podcast so far, please don&amp;#39;t hesitate to leave any questions or ideas you have related to the content discussed!

You can learn more about Madelon&amp;#39;s incredible research career and publications / talks here: https://www.madelonhulsebos.com/! Papers such as GitTables are listed here!

Another nice nugget form the podcast - Madelon introduced me to the BIRD-SQL benchmark which really expanded my understanding of Text-to-SQL (https://arxiv.org/pdf/2305.03111.pdf.

Chapters
0:00 Welcome Madelon!
0:58 Tabular Data and Embeddings
3:10 Tabular Representation Learning
5:48 Semantic Type Detection
9:50 Pandas as an LLM Tool
11:52 Table-Based Question Answering and Text-to-SQL
19:35 Joins with Machine Learning
21:38 Query Execution with Machine Learning
22:45 Graph Neural Networks
24:07 XGBoost
28:28 Merging Tables
32:10 Fact Representation
35:50 GPT-4V and Tables
39:00 Metadata in Embeddings
42:45 Table Retrieval in Weaviate
46:25 Exciting future directions!!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:39</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Vibs Abhishek on Alltius AI - Weaviate Podcast #71!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 71st Weaviate Podcast with Vibs Abhishek! Vibs is the CEO and Founder of Alltius AI, as well as a professor at UC Irvine business school! In order to tame the somewhat chaotic emerging landscape of RAG and LLM applications, Alltius has settled on 3 core pillars of Knowledge, Skills, and Deployment Channels! Vibs further explained how he sees the distinction between Assistants and Agents and many more topics important to Enterprise deployment of RAG applications such as reducing hallucinations and employing classifiers to route skills and knowledge sources! I learned so much from this conversation, I hope you enjoy the podcast!

Alltius KNO Plus Demo Video: https://www.loom.com/share/fcfe516b75ea4f069b1a8d6a3510fa4c?sid=5f43317f-c20b-4dd9-91d3-2cde993fd91f

Chapters
0:00 Welcome Vibs
0:22 Background
2:30 Alltius’ UI for Assistants
7:15 The Knowledge Pillar
12:05 SQL Router and Intent Management
14:10 Classifying a Pipeline / Skill
17:30 Flexibility of Zero-Shot versus Fine-Tuning
21:00 The Channels Pillar
23:00 Connecting the Warehouse / Lakehouse
24:50 Assistant versus Agent
28:30 MemGPT
31:25 Offline LLM Research
35:50 Multi-Agent Role-Playing Assistants
39:25 From Clicks to Conversations
44:10 CEO / Professor and Evolution of the Field</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Vibs-Abhishek-on-Alltius-AI---Weaviate-Podcast-71-e2bhrf7</link>
			<guid isPermaLink="false">23d96284-49cf-4586-9a5f-193aa77287c3</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 26 Oct 2023 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/78228391/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-6%2F354295861-44100-2-77d7b601559f4.mp3" length="53397628" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 71st Weaviate Podcast with Vibs Abhishek! Vibs is the CEO and Founder of Alltius AI, as well as a professor at UC Irvine business school! In order to tame the somewhat chaotic emerging landscape of RAG and LLM applications, Alltius has settled on 3 core pillars of Knowledge, Skills, and Deployment Channels! Vibs further explained how he sees the distinction between Assistants and Agents and many more topics important to Enterprise deployment of RAG applications such as reducing hallucinations and employing classifiers to route skills and knowledge sources! I learned so much from this conversation, I hope you enjoy the podcast!

Alltius KNO Plus Demo Video: https://www.loom.com/share/fcfe516b75ea4f069b1a8d6a3510fa4c?sid=5f43317f-c20b-4dd9-91d3-2cde993fd91f

Chapters
0:00 Welcome Vibs
0:22 Background
2:30 Alltius’ UI for Assistants
7:15 The Knowledge Pillar
12:05 SQL Router and Intent Management
14:10 Classifying a Pipeline / Skill
17:30 Flexibility of Zero-Shot versus Fine-Tuning
21:00 The Channels Pillar
23:00 Connecting the Warehouse / Lakehouse
24:50 Assistant versus Agent
28:30 MemGPT
31:25 Offline LLM Research
35:50 Multi-Agent Role-Playing Assistants
39:25 From Clicks to Conversations
44:10 CEO / Professor and Evolution of the Field&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:37</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[MemGPT Explained!]]></title>
			<description><![CDATA[<p>Thank you so much for watching our paper summary video on MemGPT! MemGPT is a super exciting new work bridging together concepts in how Operating Systems manage memory and LLMs! 

Links:
Paper: https://arxiv.org/pdf/2310.08560.pdf
Andrej Karpathy on Operating Systems and LLMs: https://twitter.com/karpathy/status/1707437820045062561
Run LLM Podcast with Charles Packer: https://www.youtube.com/watch?v=4aOLxPdx1Dg
SciPhi: https://github.com/SciPhi-AI/sciphi/tree/main

Our perspectives on Database Agents that WRITE to Vector Databases: https://weaviate.io/blog/generative-feedback-loops-with-llms

Chapters
0:00 Introduction to MemGPT
2:45 MemGPT Architecture
6:15 Operating System for LLMs
11:48 Types of Context and Storage
15:42 Control Flow
18:00 Experiments
22:04 Future Work
24:46 Personal Takeaways
30:34 Thank you for watching!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/MemGPT-Explained-e2bhrdn</link>
			<guid isPermaLink="false">211cbb0e-a1a4-49b3-9e44-d67fdeaee122</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 24 Oct 2023 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/78228343/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-6%2F354295690-44100-2-240bb7c50b041.mp3" length="29746154" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Thank you so much for watching our paper summary video on MemGPT! MemGPT is a super exciting new work bridging together concepts in how Operating Systems manage memory and LLMs! 

Links:
Paper: https://arxiv.org/pdf/2310.08560.pdf
Andrej Karpathy on Operating Systems and LLMs: https://twitter.com/karpathy/status/1707437820045062561
Run LLM Podcast with Charles Packer: https://www.youtube.com/watch?v=4aOLxPdx1Dg
SciPhi: https://github.com/SciPhi-AI/sciphi/tree/main

Our perspectives on Database Agents that WRITE to Vector Databases: https://weaviate.io/blog/generative-feedback-loops-with-llms

Chapters
0:00 Introduction to MemGPT
2:45 MemGPT Architecture
6:15 Operating System for LLMs
11:48 Types of Context and Storage
15:42 Control Flow
18:00 Experiments
22:04 Future Work
24:46 Personal Takeaways
30:34 Thank you for watching!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:30:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Kevin Cohen on Neum AI - Weaviate Podcast #70!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 70th episode of the Weaviate podcast with Neum AI CTO and Co-Founder Kevin Cohen! I first met Kevin when he was debugging an issue with his distributed node utilization and have since learned so much from him about how he sees the space of Data Ingestion, also commonly referenced as ETL for LLMs! There are so many interesting parts to this from the general flow of data connectors, chunkers and metadata extractors, embedding inference, and the last leg of the mile of importing the vectors to a Vector DB such as Weaviate! I really loved how Kevin broke down the distributed messaging queue and system design for orchestrating data ingestion at massive scale such as dealing with failures and optimizing the infrastructure as code setup. We also discussed things like new use cases with quadrillion scale vector indexes and the role of knowledge graphs in all this! I really hope you enjoy the podcast, please check out this amazing article below from Neum AI!

https://medium.com/@neum_ai/retrieval-augmented-generation-at-scale-building-a-distributed-system-for-synchronizing-and-eaa29162521

Chapters
0:00 Check this out!
1:18 Welcome Kevin!
1:58 Founding Neum AI
6:55 Data Ingestion, End-to-End Overview
9:10 Chunking and Metadata Extraction
14:20 Embedding Cache
16:57 Distributed Messaging Queues
22:15 Embeddings Cache ELI5
25:30 Customizing Weaviate Kubernetes
38:10 Multi-Tenancy and Resource Allocation
39:20 Billion-Scale Vector Search
45:05 Knowledge Graphs
52:10 Y Combinator Experience</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Kevin-Cohen-on-Neum-AI---Weaviate-Podcast-70-e2bhrg9</link>
			<guid isPermaLink="false">62894674-6661-43f6-b199-ca125a6b256c</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 18 Oct 2023 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/78228425/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-6%2F354295938-44100-2-0a5400064816d.mp3" length="52833383" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 70th episode of the Weaviate podcast with Neum AI CTO and Co-Founder Kevin Cohen! I first met Kevin when he was debugging an issue with his distributed node utilization and have since learned so much from him about how he sees the space of Data Ingestion, also commonly referenced as ETL for LLMs! There are so many interesting parts to this from the general flow of data connectors, chunkers and metadata extractors, embedding inference, and the last leg of the mile of importing the vectors to a Vector DB such as Weaviate! I really loved how Kevin broke down the distributed messaging queue and system design for orchestrating data ingestion at massive scale such as dealing with failures and optimizing the infrastructure as code setup. We also discussed things like new use cases with quadrillion scale vector indexes and the role of knowledge graphs in all this! I really hope you enjoy the podcast, please check out this amazing article below from Neum AI!

https://medium.com/@neum_ai/retrieval-augmented-generation-at-scale-building-a-distributed-system-for-synchronizing-and-eaa29162521

Chapters
0:00 Check this out!
1:18 Welcome Kevin!
1:58 Founding Neum AI
6:55 Data Ingestion, End-to-End Overview
9:10 Chunking and Metadata Extraction
14:20 Embedding Cache
16:57 Distributed Messaging Queues
22:15 Embeddings Cache ELI5
25:30 Customizing Weaviate Kubernetes
38:10 Multi-Tenancy and Resource Allocation
39:20 Billion-Scale Vector Search
45:05 Knowledge Graphs
52:10 Y Combinator Experience&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:02</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Charles Pierse on Tactic Generate - Weaviate Podcast #69!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 69th episode of the Weaviate Podcast featuring Charles Pierse from Tactic! Tactic has recently launched their new Tactic Generate project, an incredible UI for conducting research across multiple documents. I think there is a massive opportunity to pair these prompts and LLM workflows with User Interfaces and take more of a holistic User Experience perspective. Tactic Generate has done an incredible job of that, please take a look from the link below! I had such a fun conversation catching up with Charles (Charles was our 2nd Weaviate Podcast guest!), I hope you enjoy the podcast!

Tactic Generate: https://tactic.fyi/generative-insights/

Chapters
0:00 Tactic Generate
1:40 Welcome Charles!
2:38 Charles’ work at Tactic
4:40 LLMs comparing documents
9:10 LLM Chaining
17:30 Discovering LLM Chains
20:28 Moats in ML Products
28:48 Fine-Tuning vs. RAG
34:30 Fine-Tuning Search Models
39:45 Skepticism on RLHF
41:52 Gorilla, Integrations, and CRM
45:40 Query Routers
47:55 CRM and Tree-of-Thoughts
55:54 Graph Embeddings
1:02:20 Llama CPP / GGML
1:04:28 What are you looking forward to most in AI?</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Charles-Pierse-on-Tactic-Generate---Weaviate-Podcast-69-e2afn4b</link>
			<guid isPermaLink="false">be3f902f-d9b8-4f67-b1c2-1982e70d7da0</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 04 Oct 2023 02:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/77109835/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-9-12%2F350802557-44100-2-9ed7b0f978736.mp3" length="66035042" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 69th episode of the Weaviate Podcast featuring Charles Pierse from Tactic! Tactic has recently launched their new Tactic Generate project, an incredible UI for conducting research across multiple documents. I think there is a massive opportunity to pair these prompts and LLM workflows with User Interfaces and take more of a holistic User Experience perspective. Tactic Generate has done an incredible job of that, please take a look from the link below! I had such a fun conversation catching up with Charles (Charles was our 2nd Weaviate Podcast guest!), I hope you enjoy the podcast!

Tactic Generate: https://tactic.fyi/generative-insights/

Chapters
0:00 Tactic Generate
1:40 Welcome Charles!
2:38 Charles’ work at Tactic
4:40 LLMs comparing documents
9:10 LLM Chaining
17:30 Discovering LLM Chains
20:28 Moats in ML Products
28:48 Fine-Tuning vs. RAG
34:30 Fine-Tuning Search Models
39:45 Skepticism on RLHF
41:52 Gorilla, Integrations, and CRM
45:40 Query Routers
47:55 CRM and Tree-of-Thoughts
55:54 Graph Embeddings
1:02:20 Llama CPP / GGML
1:04:28 What are you looking forward to most in AI?&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:08:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Weights and Biases on Fine-Tuning LLMs - Weaviate Podcast #68!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 68th episode of the Weaviate Podcast! We are super excited to welcome Morgan McGuire, Darek Kleczek, and Thomas Capelle! This was such a fun discussion beginning with generally how see the space of fine-tuning from why you would want to do it, to the available tooling, intersection with RAG and more!

Check out W&amp;B Prompts! https://wandb.ai/site/prompts

Check out the W&amp;B Tiny Llama Report! https://wandb.ai/capecape/llamac/reports/Training-Tiny-Llamas-for-Fun-and-Science--Vmlldzo1MDM2MDg0

Chapters
0:00 Tiny Llamas!
1:53 Welcome!
2:22 LLM Fine-Tuning
5:25 Tooling for Fine-Tuning
7:55 Why Fine-Tune?
9:55 RAG vs. Fine-Tuning
12:25 Knowledge Distillation
14:40 Gorilla LLMs
18:25 Open-Source LLMs
22:48 Jonathan Frankle on W&amp;B
23:45 Data Quality for LLM Training
25:55 W&amp;B for Data Versioning
27:25 Curriculum Learning
29:28 GPU Rich and Data Quality
30:30 Vector DBs and Data Quality
32:50 Tuning Training with Weights &amp; Biases
35:47 Training Reports
42:28 HF Collections and W&amp;B Sweeps
44:50 Exciting Directions for AI</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Weights-and-Biases-on-Fine-Tuning-LLMs---Weaviate-Podcast-68-e29q46m</link>
			<guid isPermaLink="false">9733d45e-9dc3-49ee-aa11-101acae6c529</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 20 Sep 2023 13:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/76402326/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-8-26%2F348591164-44100-2-a4c580ef19cb6.mp3" length="50069837" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 68th episode of the Weaviate Podcast! We are super excited to welcome Morgan McGuire, Darek Kleczek, and Thomas Capelle! This was such a fun discussion beginning with generally how see the space of fine-tuning from why you would want to do it, to the available tooling, intersection with RAG and more!

Check out W&amp;amp;B Prompts! https://wandb.ai/site/prompts

Check out the W&amp;amp;B Tiny Llama Report! https://wandb.ai/capecape/llamac/reports/Training-Tiny-Llamas-for-Fun-and-Science--Vmlldzo1MDM2MDg0

Chapters
0:00 Tiny Llamas!
1:53 Welcome!
2:22 LLM Fine-Tuning
5:25 Tooling for Fine-Tuning
7:55 Why Fine-Tune?
9:55 RAG vs. Fine-Tuning
12:25 Knowledge Distillation
14:40 Gorilla LLMs
18:25 Open-Source LLMs
22:48 Jonathan Frankle on W&amp;amp;B
23:45 Data Quality for LLM Training
25:55 W&amp;amp;B for Data Versioning
27:25 Curriculum Learning
29:28 GPU Rich and Data Quality
30:30 Vector DBs and Data Quality
32:50 Tuning Training with Weights &amp;amp; Biases
35:47 Training Reports
42:28 HF Collections and W&amp;amp;B Sweeps
44:50 Exciting Directions for AI&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:52:09</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Farshad Farahbakhshian and Etienne Dilocker on Weaviate and AWS - Weaviate Podcast #67!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 67th Weaviate Podcast, announcing Weaviate on the AWS Marketplace! This was one of my favorite podcasts to date with a deep dive on the details of running RAG applications in the cloud, our general understanding of LLM Fine-Tuning and RAG, as well as a really interesting discussion on VPCs and Hybrid SaaS! I hope you find the podcast useful, as always we are more than happy to answer any questions or discuss any ideas you have about the content presented in the podcast!

Learn more here: https://aws.amazon.com/marketplace/seller-profile?id=seller-jxgfug62rvpxs

As well as here: https://weaviate.io/developers/weaviate/installation/aws-marketplace

Chapters
0:00 Welcome Farshad
0:38 Weaviate’s Journey to AWS
2:05 Retrieval-Augmented Generation and Vector DBs
3:44 Running AI in the Cloud
9:40 Fine-Tuning LLMs vs. RAG
10:30 Skill vs. Knowledge (Lawyer Example)
14:28 Continual Learning of LLMs
16:50 Searching through multiple sources
19:58 Hybrid Search controlled by LLMs
22:10 Classes versus Filters
25:00 SQL and Vector Search
25:55 Favorite RAG Use Cases
31:55 Cloud Benchmarking
37:00 Price Performance
38:20 Tuning HNSW
42:15 Horizontal Scalability on AWS Marketplace
47:00 Privacy Requirements
54:45 Weaviate Hybrid SaaS
59:00 AWS Marketplace</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Farshad-Farahbakhshian-and-Etienne-Dilocker-on-Weaviate-and-AWS---Weaviate-Podcast-67-e29q447</link>
			<guid isPermaLink="false">3a7cce77-ea0a-462c-b62e-028af0b88002</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 13 Sep 2023 13:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/76402247/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-8-26%2F348590980-44100-2-8323e766edb81.mp3" length="59011656" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 67th Weaviate Podcast, announcing Weaviate on the AWS Marketplace! This was one of my favorite podcasts to date with a deep dive on the details of running RAG applications in the cloud, our general understanding of LLM Fine-Tuning and RAG, as well as a really interesting discussion on VPCs and Hybrid SaaS! I hope you find the podcast useful, as always we are more than happy to answer any questions or discuss any ideas you have about the content presented in the podcast!

Learn more here: https://aws.amazon.com/marketplace/seller-profile?id=seller-jxgfug62rvpxs

As well as here: https://weaviate.io/developers/weaviate/installation/aws-marketplace

Chapters
0:00 Welcome Farshad
0:38 Weaviate’s Journey to AWS
2:05 Retrieval-Augmented Generation and Vector DBs
3:44 Running AI in the Cloud
9:40 Fine-Tuning LLMs vs. RAG
10:30 Skill vs. Knowledge (Lawyer Example)
14:28 Continual Learning of LLMs
16:50 Searching through multiple sources
19:58 Hybrid Search controlled by LLMs
22:10 Classes versus Filters
25:00 SQL and Vector Search
25:55 Favorite RAG Use Cases
31:55 Cloud Benchmarking
37:00 Price Performance
38:20 Tuning HNSW
42:15 Horizontal Scalability on AWS Marketplace
47:00 Privacy Requirements
54:45 Weaviate Hybrid SaaS
59:00 AWS Marketplace&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:01:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Hybrid SaaS in Weaviate Explained!]]></title>
			<description><![CDATA[<p>Hey everyone! Here is a clip from our newest Weaviate podcast with Farshad Farahbakhshian, Gen AI specialist at AWS and Etienne Dilocker, CTO and Co-Founder of Weaviate! This podcast announces Weaviate on the AWS marketplace and is packed with info on running Weaviate in the cloud such as this clip explaining how Hybrid SaaS works! I hope you find the clip useful, we are more than happy to answer any questions you have about the content in this clip!

Chapters
0:00 Quick Intro for Context
0:29 Etienne Dilocker on Hybrid SaaS</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Hybrid-SaaS-in-Weaviate-Explained-e29q42n</link>
			<guid isPermaLink="false">04388a71-13d1-4b65-a22b-f4b8f8454fb5</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 12 Sep 2023 13:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/76402199/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-8-26%2F348590609-44100-2-ca775eca1eba6.mp3" length="3928397" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Here is a clip from our newest Weaviate podcast with Farshad Farahbakhshian, Gen AI specialist at AWS and Etienne Dilocker, CTO and Co-Founder of Weaviate! This podcast announces Weaviate on the AWS marketplace and is packed with info on running Weaviate in the cloud such as this clip explaining how Hybrid SaaS works! I hope you find the clip useful, we are more than happy to answer any questions you have about the content in this clip!

Chapters
0:00 Quick Intro for Context
0:29 Etienne Dilocker on Hybrid SaaS&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:04:05</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[David Garnitz on VectorFlow - Weaviate Podcast #66!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 66th Weaviate Podcast with David Garnitz, the creator of VectorFlow! VectorFlow (open-sourced on GH and linked below) is a new tool for ingesting data into Vector Databases such as Weaviate! There is quite an interesting End-to-End stack emerging at the ingestion layer, from retrieving data from misc. sources such as Slack, Salesforce, GitHub, Google Drive, Notion, ... to then Chunking the Text (maybe with the use of Visual Document Layout parsers like what Unstructured is imagining), extracting Metadata potentially (say the &quot;age&quot; of an NBA player as in the Evaporate-Code+ research) -- then sending this data off to embedding model inference and unpacking that can of worms from inference acceleration to load balancing, and finally -- importing the vectors themselves to Weaviate! I learned so much from this conversation, I really hope you enjoy listening and please check out VectorFlow below!

VectorFlow: https://github.com/dgarnitz/vectorflow

Chapters
0:00 VectorFlow on GitHub!
0:52 Welcome David Garnitz!
1:17 Vector Flow, Founding Vision
2:00 Billions of Vectors in Weaviate!
4:20 End-to-end data importing
6:30 Metadata Extraction in Vector Database Flows
10:15 Vectorizing 100s of millions of billions of chunks
15:58 Fine-Tuning Embedding Models
23:50 Zero-Shot Models in Metadata and Chunking
36:36 Vector + SQL
42:45 Self-Driving Databases
49:23 Generative Feedback Loop REST API
51:38 GPT Cache
55:55 Building VectorFlow</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/David-Garnitz-on-VectorFlow---Weaviate-Podcast-66-e297jp6</link>
			<guid isPermaLink="false">99cf1839-8f1e-4f4c-99a8-75180e89b81b</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 07 Sep 2023 13:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/75795686/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-8-12%2F346714234-44100-2-962652a93027b.mp3" length="62013021" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 66th Weaviate Podcast with David Garnitz, the creator of VectorFlow! VectorFlow (open-sourced on GH and linked below) is a new tool for ingesting data into Vector Databases such as Weaviate! There is quite an interesting End-to-End stack emerging at the ingestion layer, from retrieving data from misc. sources such as Slack, Salesforce, GitHub, Google Drive, Notion, ... to then Chunking the Text (maybe with the use of Visual Document Layout parsers like what Unstructured is imagining), extracting Metadata potentially (say the &amp;quot;age&amp;quot; of an NBA player as in the Evaporate-Code+ research) -- then sending this data off to embedding model inference and unpacking that can of worms from inference acceleration to load balancing, and finally -- importing the vectors themselves to Weaviate! I learned so much from this conversation, I really hope you enjoy listening and please check out VectorFlow below!

VectorFlow: https://github.com/dgarnitz/vectorflow

Chapters
0:00 VectorFlow on GitHub!
0:52 Welcome David Garnitz!
1:17 Vector Flow, Founding Vision
2:00 Billions of Vectors in Weaviate!
4:20 End-to-end data importing
6:30 Metadata Extraction in Vector Database Flows
10:15 Vectorizing 100s of millions of billions of chunks
15:58 Fine-Tuning Embedding Models
23:50 Zero-Shot Models in Metadata and Chunking
36:36 Vector + SQL
42:45 Self-Driving Databases
49:23 Generative Feedback Loop REST API
51:38 GPT Cache
55:55 Building VectorFlow&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:04:35</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Ofir Press on AliBi and Self-Ask - Weaviate Podcast #65!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the Weaviate Podcast! I am SUPER excited to publish my conversation with Ofir Press! Ofir has done incredible work pioneering AliBi attention and Self-Ask prompting and I learned so much from speaking with him! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

+Huge Congratulations on your Ph.D. Ofir!

AliBi Attention: https://arxiv.org/abs/2108.12409

Self-Ask Prompting: https://arxiv.org/abs/2210.03350

Ofir Pres on YouTube: https://www.youtube.com/@ofirpress

Chapters
0:00 Welcome Ofir Press
0:41 Large Context LLMs
12:38 Quadratic Complexity of Attention
19:12 AliBi Attention, Visual Demo!
24:53 Recency Bias in LLMs
28:57 RAG in Long Context LLM Training
36:27 Self-Ask Prompting
46:07 Chain-of-Thought and Self-Ask
50:47 Gorilla LLMs
58:42 New Directions for New Training Data</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Ofir-Press-on-AliBi-and-Self-Ask---Weaviate-Podcast-65-e28ni6r</link>
			<guid isPermaLink="false">3646de25-da84-4c8f-8e26-7233869652a9</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 31 Aug 2023 08:01:13 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/75269787/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-30%2F345071480-44100-2-809539096f43a.mp3" length="64500714" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the Weaviate Podcast! I am SUPER excited to publish my conversation with Ofir Press! Ofir has done incredible work pioneering AliBi attention and Self-Ask prompting and I learned so much from speaking with him! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

+Huge Congratulations on your Ph.D. Ofir!

AliBi Attention: https://arxiv.org/abs/2108.12409

Self-Ask Prompting: https://arxiv.org/abs/2210.03350

Ofir Pres on YouTube: https://www.youtube.com/@ofirpress

Chapters
0:00 Welcome Ofir Press
0:41 Large Context LLMs
12:38 Quadratic Complexity of Attention
19:12 AliBi Attention, Visual Demo!
24:53 Recency Bias in LLMs
28:57 RAG in Long Context LLM Training
36:27 Self-Ask Prompting
46:07 Chain-of-Thought and Self-Ask
50:47 Gorilla LLMs
58:42 New Directions for New Training Data&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:11</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Shishir Patil and Tianjun Zhang on Gorilla - Weaviate Podcast #64!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 64th Weaviate Podcast with Shishir Patil and Tianjun Zhang, co-authors of Gorilla: Large Language Models Connected with Massive APIs! I learned so much about Gorilla from Shishir and Tianjun, from the APIBench dataset to the continually evolving APIZoo, how the models are trained with Retrieval-Aware Training, Self-Instruct Training data and how the authors think of fine-tuning LLaMA-7B models for tasks such as this, and many more! I hope you enjoy the podcast! As always I am more than happy to answer any questions or discuss any ideas you have about the content in the podcast!


Please check out the paper here! https://arxiv.org/abs/2305.15334

Chapters

0:00 Welcome Shishir and Tianjun
0:25 Gorilla LLM Story
1:50 API Examples
7:40 The APIZoo
10:55 Gorilla vs. OpenAI Funcs
12:50 Retrieval-Aware Training
19:55 Mixing APIs, Gorilla for Integration
25:12 LlaMA-7B Fine-Tuning vs. GPT-4
29:08 Weaviate Gorilla
33:52 Gorilla and Baby Gorillas
35:40 Gorilla vs. HuggingFace
38:32 Structured Output Parsing
41:14 Reflexion Prompting for Debugging
44:00 Directions for the Future</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Shishir-Patil-and-Tianjun-Zhang-on-Gorilla---Weaviate-Podcast-64-e28nf7b</link>
			<guid isPermaLink="false">b29fbff1-b220-435f-ae52-3013a698d013</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 30 Aug 2023 17:23:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/75266731/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-30%2F345062491-44100-2-c83883b4cea3f.mp3" length="47280795" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 64th Weaviate Podcast with Shishir Patil and Tianjun Zhang, co-authors of Gorilla: Large Language Models Connected with Massive APIs! I learned so much about Gorilla from Shishir and Tianjun, from the APIBench dataset to the continually evolving APIZoo, how the models are trained with Retrieval-Aware Training, Self-Instruct Training data and how the authors think of fine-tuning LLaMA-7B models for tasks such as this, and many more! I hope you enjoy the podcast! As always I am more than happy to answer any questions or discuss any ideas you have about the content in the podcast!


Please check out the paper here! https://arxiv.org/abs/2305.15334

Chapters

0:00 Welcome Shishir and Tianjun
0:25 Gorilla LLM Story
1:50 API Examples
7:40 The APIZoo
10:55 Gorilla vs. OpenAI Funcs
12:50 Retrieval-Aware Training
19:55 Mixing APIs, Gorilla for Integration
25:12 LlaMA-7B Fine-Tuning vs. GPT-4
29:08 Weaviate Gorilla
33:52 Gorilla and Baby Gorillas
35:40 Gorilla vs. HuggingFace
38:32 Structured Output Parsing
41:14 Reflexion Prompting for Debugging
44:00 Directions for the Future&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Nils Reimers on Cohere Search AI - Weaviate Podcast #63!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 63rd Weaviate Podcast, I couldn&#39;t be more excited to welcome Nils Reimers back to the podcast!! Similar to our debut episode together, we began by describing the latest collaboration of Weaviate and Cohere (episode 1, new multilingual embedding models; episode 2, rerankers!), and then continued into some of the key questions around search technology. In this one, we discussed the importance of temporal queries and metadata extraction, long document representation, and future directions for Retrieval-Augmented Generation! I hope you enjoy the podcast, as always I am more than happy to answer any questions or discuss any ideas you have about the content in the podcast! Thank you so much for watching!

Learn more about Cohere Rerankers and how to use it in Weaviate here: https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere

Chapters
0:00 Introduction
1:30 Cohere Rerankers
7:02 Dataset Curation at Cohere
10:30 New Rerankers and XGBoost
14:35 Temporal Queries
17:55 Metadata Extraction from Unstructured Text Chunks
21:52 Soft Filters
24:58 Chunking and Long Document Representation
38:00 Retrieval-Augmented Generation
45:40 Retrieval-Aware Training to solve Hallucinations
49:50 Learning to Search and End-to-End RAG
54:35 RETRO
59:25 Foundation Model for Search</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Nils-Reimers-on-Cohere-Search-AI---Weaviate-Podcast-63-e2876ki</link>
			<guid isPermaLink="false">cb941300-953e-4a8b-b03d-80fb7525edd3</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 17 Aug 2023 11:58:45 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/74733650/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-17%2F343403838-44100-2-761daacdb7958.mp3" length="62573086" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 63rd Weaviate Podcast, I couldn&amp;#39;t be more excited to welcome Nils Reimers back to the podcast!! Similar to our debut episode together, we began by describing the latest collaboration of Weaviate and Cohere (episode 1, new multilingual embedding models; episode 2, rerankers!), and then continued into some of the key questions around search technology. In this one, we discussed the importance of temporal queries and metadata extraction, long document representation, and future directions for Retrieval-Augmented Generation! I hope you enjoy the podcast, as always I am more than happy to answer any questions or discuss any ideas you have about the content in the podcast! Thank you so much for watching!

Learn more about Cohere Rerankers and how to use it in Weaviate here: https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere

Chapters
0:00 Introduction
1:30 Cohere Rerankers
7:02 Dataset Curation at Cohere
10:30 New Rerankers and XGBoost
14:35 Temporal Queries
17:55 Metadata Extraction from Unstructured Text Chunks
21:52 Soft Filters
24:58 Chunking and Long Document Representation
38:00 Retrieval-Augmented Generation
45:40 Retrieval-Aware Training to solve Hallucinations
49:50 Learning to Search and End-to-End RAG
54:35 RETRO
59:25 Foundation Model for Search&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:05:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Atai Barkai on PodcastGPT - Weaviate Podcast #62!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 62nd Weaviate Podcast with Atai Barkai! We are stepping into the meta with this one for a podcast about podcasts! Podcasts are one of the biggest opportunities of new technologies, starting with Whisper&#39;s ability to transcribe audio to text and advances with speaker diarization, .. the question to be explored is, What Vector Database and LLM applications can we build with this data?! What is the future of podcasting with these new technologies?! I had so much fun discussing all these ideas with Atai! As always we are more than happy to answer any questions or discuss any ideas you have about content discussed in the podcast! Thank you so much for watching!

Chapters
0:00 Welcome Atai!
1:04 TawkitAI and PodcastGPT!
2:20 Chat with Podcast

PodcastGPT - https://www.podcastgpt.ai/

Tawkit AI - https://twitter.com/tawkitapp

Weaviate Podcast Search Demo!
https://github.com/weaviate/weaviate-podcast-search</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Atai-Barkai-on-PodcastGPT---Weaviate-Podcast-62-e27tseu</link>
			<guid isPermaLink="false">b785bbe9-cd16-4bb7-b23c-a63e0d177491</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 09 Aug 2023 14:18:02 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/74428318/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-9%2F342454850-44100-2-d56d080f3b552.mp3" length="53451963" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 62nd Weaviate Podcast with Atai Barkai! We are stepping into the meta with this one for a podcast about podcasts! Podcasts are one of the biggest opportunities of new technologies, starting with Whisper&amp;#39;s ability to transcribe audio to text and advances with speaker diarization, .. the question to be explored is, What Vector Database and LLM applications can we build with this data?! What is the future of podcasting with these new technologies?! I had so much fun discussing all these ideas with Atai! As always we are more than happy to answer any questions or discuss any ideas you have about content discussed in the podcast! Thank you so much for watching!

Chapters
0:00 Welcome Atai!
1:04 TawkitAI and PodcastGPT!
2:20 Chat with Podcast

PodcastGPT - https://www.podcastgpt.ai/

Tawkit AI - https://twitter.com/tawkitapp

Weaviate Podcast Search Demo!
https://github.com/weaviate/weaviate-podcast-search&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Rohit Agarwal on Portkey - Weaviate Podcast #61!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 61st episode of the Weaviate Podcast! I am beyond excited to publish this one! I first met Rohit at the Cal Hacks event hosted by UC Berkeley where we had a debate about the impact of Semantic Caching! Rohit taught me a ton about the topic and I think it&#39;s going to be one of the most impactful early applications of Generative Feedback Loops! Rohit is building Portkey, a SUPER interesting LLM middleware that does things like load balancing between LLM APIs, and as discussed in the podcast there are all sorts of opportunities for this kind of space whether it be routing to tool-specific LLMs, different cost / accuracy requirements, or multiple models in the HuggingGPT sense. It was amazing chatting with Rohit, this was the best dive into LLMOps I have personally been apart of! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Check out portkey here! https://portkey.ai/blog

Chapters
0:00 Introduction
0:24 Portkey, Founding Vision
2:20 LLMOps vs. MLOps
4:00 Inference Hosting Options
7:05 3 Layers of LLM Use
8:35 LLM Load Balancers
12:45 Fine-Tuning LLMs
17:08 Retrieval-Aware Tuning
21:16 Portkey Cost Savings
23:08 HuggingGPT
26:28 Semantic Caching
32:40 Frequently Asked Questions
34:00 Embeddings vs. Generative Tasks
35:30 AI Moats, GPT Wrappers
39:56 Unlocks from Cheaper LLM Inference</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Rohit-Agarwal-on-Portkey---Weaviate-Podcast-61-e27tsho</link>
			<guid isPermaLink="false">50f39c9f-d09b-4851-a15d-6ef60e0e1c26</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 03 Aug 2023 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/74428408/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-9%2F342455168-44100-2-0aea57e87e6c7.mp3" length="47427499" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 61st episode of the Weaviate Podcast! I am beyond excited to publish this one! I first met Rohit at the Cal Hacks event hosted by UC Berkeley where we had a debate about the impact of Semantic Caching! Rohit taught me a ton about the topic and I think it&amp;#39;s going to be one of the most impactful early applications of Generative Feedback Loops! Rohit is building Portkey, a SUPER interesting LLM middleware that does things like load balancing between LLM APIs, and as discussed in the podcast there are all sorts of opportunities for this kind of space whether it be routing to tool-specific LLMs, different cost / accuracy requirements, or multiple models in the HuggingGPT sense. It was amazing chatting with Rohit, this was the best dive into LLMOps I have personally been apart of! As always we are more than happy to answer any questions or discuss any ideas you have about the content in the podcast!

Check out portkey here! https://portkey.ai/blog

Chapters
0:00 Introduction
0:24 Portkey, Founding Vision
2:20 LLMOps vs. MLOps
4:00 Inference Hosting Options
7:05 3 Layers of LLM Use
8:35 LLM Load Balancers
12:45 Fine-Tuning LLMs
17:08 Retrieval-Aware Tuning
21:16 Portkey Cost Savings
23:08 HuggingGPT
26:28 Semantic Caching
32:40 Frequently Asked Questions
34:00 Embeddings vs. Generative Tasks
35:30 AI Moats, GPT Wrappers
39:56 Unlocks from Cheaper LLM Inference&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:24</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Patrice Bourgougnon on WPSolr - Weaviate Podcast #60]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 60th Weaviate podcast with Patrice Bourgougnon! Patrice is the creator of WPSolr, integrating AI search capabilities with Wordpress and Woocommerce. Patrice is one of the most active contributors to Weaviate, filing issues and poking holes in new releases! Patrice shared incredible feedback on Weaviate and how he sees the state of Vector Databases and Search! As always, we are more than happy to answer any questions or ideas you have about the content discussed in the podcast! Thanks for watching!

Chapters
0:00 Introduction
0:45 Vector Databases and Wordpress
4:50 Weaviate Client Languages
10:00 Inference and Database Container Management
21:30 Business Opportunities for Search in Production
26:40 Testing Search Performance, “Something to sleep on”
30:50 Zero-Shot Model Ability
36:05 Make LLMs Stateful
43:46 Chatbots and Search Boxes
44:55 Mixing Models in Applications
47:00 BM25 vs. Vector Search in RETRO RAG</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Patrice-Bourgougnon-on-WPSolr---Weaviate-Podcast-60-e27lbgi</link>
			<guid isPermaLink="false">0f8328e9-d788-47b9-94d7-27925d14e71c</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 02 Aug 2023 08:10:18 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/74148818/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-2%2F341589769-44100-2-fe765a2f17152.mp3" length="82105572" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 60th Weaviate podcast with Patrice Bourgougnon! Patrice is the creator of WPSolr, integrating AI search capabilities with Wordpress and Woocommerce. Patrice is one of the most active contributors to Weaviate, filing issues and poking holes in new releases! Patrice shared incredible feedback on Weaviate and how he sees the state of Vector Databases and Search! As always, we are more than happy to answer any questions or ideas you have about the content discussed in the podcast! Thanks for watching!

Chapters
0:00 Introduction
0:45 Vector Databases and Wordpress
4:50 Weaviate Client Languages
10:00 Inference and Database Container Management
21:30 Business Opportunities for Search in Production
26:40 Testing Search Performance, “Something to sleep on”
30:50 Zero-Shot Model Ability
36:05 Make LLMs Stateful
43:46 Chatbots and Search Boxes
44:55 Mixing Models in Applications
47:00 BM25 vs. Vector Search in RETRO RAG&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:25:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Andriy Mulyar on Nomic AI, Atlas, and GPT4All - Weaviate Podcast #58]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 58th episode of the Weaviate Podcast! I am SUPER excited to welcome Andriy Muylar! Andriy is the Co-Founder of Nomic AI, a company fresh off a $17M series A raise! Nomic has created some incredible products such as Atlas and GPT4All! I was really impressed by Andriy&#39;s vision of the state and forecasted evolution of these topics! I hope you enjoy the podcast! As always, we are more than happy to answer any questions or discuss any ideas you have about the content discussed in the podcast!

Integration Tutorial for Weaviate and Nomic AI Atlas! https://docs.nomic.ai/vector_database.html

This example worked for me if you want to clone it with the podcast transcription dataset: https://github.com/weaviate/weaviate-podcast-search/blob/main/atlas-visualizer.py

Check out Nomic AI here! https://home.nomic.ai/blog

Chapters
0:00 Congrats Nomic and Weaviate Integration!
2:35 Welcome Andriy Mulyar!
3:05 Founding Story of Nomic AI
6:55 Understanding Massive Scale Text Data
10:14 Topic Modeling
16:30 Monitoring Model Training</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Andriy-Mulyar-on-Nomic-AI--Atlas--and-GPT4All---Weaviate-Podcast-58-e2736k9</link>
			<guid isPermaLink="false">230c8bb2-f239-48bd-8068-b12bb3d3417e</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 18 Jul 2023 14:54:29 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/73553993/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-6-18%2F339753890-44100-2-a03d966ec10f4.mp3" length="55866095" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 58th episode of the Weaviate Podcast! I am SUPER excited to welcome Andriy Muylar! Andriy is the Co-Founder of Nomic AI, a company fresh off a $17M series A raise! Nomic has created some incredible products such as Atlas and GPT4All! I was really impressed by Andriy&amp;#39;s vision of the state and forecasted evolution of these topics! I hope you enjoy the podcast! As always, we are more than happy to answer any questions or discuss any ideas you have about the content discussed in the podcast!

Integration Tutorial for Weaviate and Nomic AI Atlas! https://docs.nomic.ai/vector_database.html

This example worked for me if you want to clone it with the podcast transcription dataset: https://github.com/weaviate/weaviate-podcast-search/blob/main/atlas-visualizer.py

Check out Nomic AI here! https://home.nomic.ai/blog

Chapters
0:00 Congrats Nomic and Weaviate Integration!
2:35 Welcome Andriy Mulyar!
3:05 Founding Story of Nomic AI
6:55 Understanding Massive Scale Text Data
10:14 Topic Modeling
16:30 Monitoring Model Training&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:58:11</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Charles Frye on Full Stack Deep Learning - Weaviate Podcast #57!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 57th Weaviate podcast with Charles Frye! Charles is an educator at Full Stack Deep Learning, one of the world&#39;s top courses on Deep Learning with lectures available on YouTube (link below)! This was one of the most thorough Weaviate podcasts published so far, covering all sorts of topics around the evolution of Deep Learning! Particularly we discussed the Retrieval-Augmented Generation stack with Vector Databases and Zero-Shot Large Language Models and how that compares to more conventional machine learning workflows and the MLOPs stack! I really enjoyed chatting with Charles and am more than happy to answer any questions or discuss any ideas you have about the content in the podcast! Thank you so much for listening!

Check out Full Stack Deep Learning! https://fullstackdeeplearning.com/

Full Stack Deep Learning on YouTube! https://www.youtube.com/@The_Full_Stack

Chapters
0:00 Welcome Charles Frye!
0:52 Charles’ journey into Deep Learning
3:00 Weights &amp; Biases and MLOps
5:30 Retrieval-Augmented Generation Stack
8:58 Data Engines and AI Products
13:50 Fine-Tuning
16:35 Information Retrieval Techniques
20:10 RAG as Tool Use and RETRO
23:33 Gorilla and Fine-Tuned Tool Use
27:36 Text-to-SQL Tool Use
30:46 Generative Data Augmentation
33:05 LLM generated queries for embeddings
38:04 Long-Tail and Data Imbalance
41:45 LoRA LLM Fine-Tuning
44:50 Eigenvectors and Disentaglement
50:00 LLM for Each User
55:00 Embedding Visualization and ML Observability
58:40 GPU Utilization
1:05:05 Discord Q&amp;A Bot App
1:16:10 Data Schema Design
1:21:25 Graph and Vector Databases
1:28:35 Future Directions in AI</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Charles-Frye-on-Full-Stack-Deep-Learning---Weaviate-Podcast-57-e26td3u</link>
			<guid isPermaLink="false">003f4f0a-cb2b-46ba-acb7-a6097f6db291</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 13 Jul 2023 15:34:25 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/73364030/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-6-13%2F339147346-44100-2-85cc5fea7d47d.mp3" length="95347773" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 57th Weaviate podcast with Charles Frye! Charles is an educator at Full Stack Deep Learning, one of the world&amp;#39;s top courses on Deep Learning with lectures available on YouTube (link below)! This was one of the most thorough Weaviate podcasts published so far, covering all sorts of topics around the evolution of Deep Learning! Particularly we discussed the Retrieval-Augmented Generation stack with Vector Databases and Zero-Shot Large Language Models and how that compares to more conventional machine learning workflows and the MLOPs stack! I really enjoyed chatting with Charles and am more than happy to answer any questions or discuss any ideas you have about the content in the podcast! Thank you so much for listening!

Check out Full Stack Deep Learning! https://fullstackdeeplearning.com/

Full Stack Deep Learning on YouTube! https://www.youtube.com/@The_Full_Stack

Chapters
0:00 Welcome Charles Frye!
0:52 Charles’ journey into Deep Learning
3:00 Weights &amp;amp; Biases and MLOps
5:30 Retrieval-Augmented Generation Stack
8:58 Data Engines and AI Products
13:50 Fine-Tuning
16:35 Information Retrieval Techniques
20:10 RAG as Tool Use and RETRO
23:33 Gorilla and Fine-Tuned Tool Use
27:36 Text-to-SQL Tool Use
30:46 Generative Data Augmentation
33:05 LLM generated queries for embeddings
38:04 Long-Tail and Data Imbalance
41:45 LoRA LLM Fine-Tuning
44:50 Eigenvectors and Disentaglement
50:00 LLM for Each User
55:00 Embedding Visualization and ML Observability
58:40 GPU Utilization
1:05:05 Discord Q&amp;amp;A Bot App
1:16:10 Data Schema Design
1:21:25 Graph and Vector Databases
1:28:35 Future Directions in AI&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:39:19</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Etienne Dilocker on Weaviate 1.20 - Weaviate Podcast #56!]]></title>
			<description><![CDATA[<p>Chapters
0:00 Weaviate 1.20!!!
0:40 Multi-Tenancy
35:36 PQ Rescoring
47:20 Re-Ranking, AutoCut, Rank Fusion
58:58 Cloud Monitoring Metrics</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Etienne-Dilocker-on-Weaviate-1-20---Weaviate-Podcast-56-e26rttv</link>
			<guid isPermaLink="false">f04cc14c-d638-4fcf-9af8-b5e917ecea9e</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 12 Jul 2023 13:06:45 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/73315711/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-6-12%2F338989824-44100-2-57d6b7c8dd749.mp3" length="60347871" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Chapters
0:00 Weaviate 1.20!!!
0:40 Multi-Tenancy
35:36 PQ Rescoring
47:20 Re-Ranking, AutoCut, Rank Fusion
58:58 Cloud Monitoring Metrics&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:51</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Aleksa Gordcic - Weaviate Podcast #55!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 55th episode of the Weaviate Podcast with Aleksa Gordcic! This episodes dives into Aleksa&#39;s incredible story from Deep Learning YouTube to DeepMind and now creating Ortus! We dived into all sorts of topics, I loved hearing about the latest updates on Ortus and how Aleksa is sees the current state of AI development! We are more than happy to answer any questions or discuss any ideas you might have about the content in the podcast! Thanks so much for watching!

Check out Ortus here! - https://www.ortusbuddy.ai/welcome

Chapters
0:00 Introduction
1:08 Deep Learning YouTube
5:40 DeepMind
9:40 Ortus
19:50 LangChain and LlamaIndex
23:10 Software 2.0 and Full Stack DL
29:20 Training Embedding Models
32:23 Text Chunking for Vector DBs
34:35 Visual Information in YouTube
38:15 Simulating Conversations
42:46 Aidan Gomez Quote on Synthetic Data
44:40 Tree of Thoughts
47:40 New Ortus Features
49:00 Embedding Marketplace
54:00 Personal Organization</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Aleksa-Gordcic---Weaviate-Podcast-55-e26j0lr</link>
			<guid isPermaLink="false">37a1e909-4de8-42a6-8291-413b63304e1f</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 05 Jul 2023 14:31:08 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/73023611/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-6-5%2F338100710-44100-2-387f08424dc97.mp3" length="64663718" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 55th episode of the Weaviate Podcast with Aleksa Gordcic! This episodes dives into Aleksa&amp;#39;s incredible story from Deep Learning YouTube to DeepMind and now creating Ortus! We dived into all sorts of topics, I loved hearing about the latest updates on Ortus and how Aleksa is sees the current state of AI development! We are more than happy to answer any questions or discuss any ideas you might have about the content in the podcast! Thanks so much for watching!

Check out Ortus here! - https://www.ortusbuddy.ai/welcome

Chapters
0:00 Introduction
1:08 Deep Learning YouTube
5:40 DeepMind
9:40 Ortus
19:50 LangChain and LlamaIndex
23:10 Software 2.0 and Full Stack DL
29:20 Training Embedding Models
32:23 Text Chunking for Vector DBs
34:35 Visual Information in YouTube
38:15 Simulating Conversations
42:46 Aidan Gomez Quote on Synthetic Data
44:40 Tree of Thoughts
47:40 New Ortus Features
49:00 Embedding Marketplace
54:00 Personal Organization&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Stephanie Horbaczewski and Gunjan Bhattarai on Vody - Weaviate Podcast #53!]]></title>
			<description><![CDATA[<p>Chapters
0:00 Introduction
0:38 Founding Story of Vody
8:15 Custom Embedding Models
12:42 Movie Genre Vectors
13:42 Classification and Contrastive Learning
15:45 Foundation Model Tuning
21:13 Multimodal Generative Models
25:08 Training Embedding Models
33:20 Tabular Data Ranking Models
36:00 RoomGPT
41:36 Diversity in Recommendations
48:25 Future Directions in Multimodal AI
51:15 Open-Source
55:45 Keeping up with Vody!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Stephanie-Horbaczewski-and-Gunjan-Bhattarai-on-Vody---Weaviate-Podcast-53-e2626r3</link>
			<guid isPermaLink="false">e0cdbd3a-1788-472d-9292-cc1ad27ed302</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 22 Jun 2023 13:00:16 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/72472867/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-22%2F336317249-44100-2-da4e19d00a229.mp3" length="53936377" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Chapters
0:00 Introduction
0:38 Founding Story of Vody
8:15 Custom Embedding Models
12:42 Movie Genre Vectors
13:42 Classification and Contrastive Learning
15:45 Foundation Model Tuning
21:13 Multimodal Generative Models
25:08 Training Embedding Models
33:20 Tabular Data Ranking Models
36:00 RoomGPT
41:36 Diversity in Recommendations
48:25 Future Directions in Multimodal AI
51:15 Open-Source
55:45 Keeping up with Vody!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:56:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Yana Welinder on Kraftful - Weaviate Podcast #52!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 52nd episode of the Weaviate Podcast with Yana Welinder! Yana is the Founder and CEO of Kratful (https://www.kraftful.com/). Kratful is an incredibly interesting &quot;ChatGPT but for Product Research&quot; -- curating specific skills for Product Managers into a collection of prompts. We discussed all sorts of things from the latest innovations in LLMs to the ChatGPT marketplace and product management, I really hope you enjoy the podcast!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Yana-Welinder-on-Kraftful---Weaviate-Podcast-52-e25nb71</link>
			<guid isPermaLink="false">f557c5de-a213-4de9-a84b-71640c52f0cd</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 14 Jun 2023 14:00:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/72116897/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-14%2F335045029-44100-2-7dfee9d26baae.mp3" length="40014575" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 52nd episode of the Weaviate Podcast with Yana Welinder! Yana is the Founder and CEO of Kratful (https://www.kraftful.com/). Kratful is an incredibly interesting &amp;quot;ChatGPT but for Product Research&amp;quot; -- curating specific skills for Product Managers into a collection of prompts. We discussed all sorts of things from the latest innovations in LLMs to the ChatGPT marketplace and product management, I really hope you enjoy the podcast!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:41:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Greg Kamradt and Colin Harmon on LLM Agents - Weaviate Podcast #51]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 51st episode of the Weaviate Podcast with Greg Kamradt and Colin Harmon! Greg and Colin are both entrepreneurs in the space of new AI tools powered by LLMs! This podcast is about keeping up with the evolution of LLM Agents from AutoGPT to connecting LLMs with Vector Databases or Wolfram Alpha, as well as the ChatGPT Marketplace, Personalized LLMs, Private LLMs, and many more! I think there are so many interesting nuggets from this podcast, thank you so much to Greg and Colin for joining, really enjoyed this one!

Data Independent: https://www.youtube.com/@DataIndependent

Greg Kamradt on Twitter: https://twitter.com/GregKamradt

Nesh: https://hellonesh.io/

Colin Harmon on LinkedIn: https://www.linkedin.com/in/coluha/

Colin Harmon Blog: https://colinharman.substack.com/

Colin Harmon at Haystack US 2023: https://www.youtube.com/watch?v=LO3U5iqnTpk

Chapters
0:00 Introduction
0:42 Backgrounds
2:43 Defining “LLM Agents”
6:12 Data-Aware LLMs
13:04 Tool Use
13:38 ChatGPT API vs. Marketplace
17:40 Personalized LLMs, LLM for Greg
19:20 PrivateGPT
25:14 AutoGPT and Chain-of-Thought Prompting
32:30 Few-Shot Examples
35:30 Early AI Signals and Open-Source
43:10 Multi-Agent LLMs
47:14 Fine-Tuning and Long Input Lengths
52:20 Greg’s LLM Wishlist Hierarchy
53:15 Keeping up with Greg and Colin!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Greg-Kamradt-and-Colin-Harmon-on-LLM-Agents---Weaviate-Podcast-51-e25d18o</link>
			<guid isPermaLink="false">f88bf891-0534-4a79-a0fa-1f76a0b725c4</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 07 Jun 2023 14:00:59 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/71779032/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-7%2F333899098-44100-2-f0132c338748d.mp3" length="52352312" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 51st episode of the Weaviate Podcast with Greg Kamradt and Colin Harmon! Greg and Colin are both entrepreneurs in the space of new AI tools powered by LLMs! This podcast is about keeping up with the evolution of LLM Agents from AutoGPT to connecting LLMs with Vector Databases or Wolfram Alpha, as well as the ChatGPT Marketplace, Personalized LLMs, Private LLMs, and many more! I think there are so many interesting nuggets from this podcast, thank you so much to Greg and Colin for joining, really enjoyed this one!

Data Independent: https://www.youtube.com/@DataIndependent

Greg Kamradt on Twitter: https://twitter.com/GregKamradt

Nesh: https://hellonesh.io/

Colin Harmon on LinkedIn: https://www.linkedin.com/in/coluha/

Colin Harmon Blog: https://colinharman.substack.com/

Colin Harmon at Haystack US 2023: https://www.youtube.com/watch?v=LO3U5iqnTpk

Chapters
0:00 Introduction
0:42 Backgrounds
2:43 Defining “LLM Agents”
6:12 Data-Aware LLMs
13:04 Tool Use
13:38 ChatGPT API vs. Marketplace
17:40 Personalized LLMs, LLM for Greg
19:20 PrivateGPT
25:14 AutoGPT and Chain-of-Thought Prompting
32:30 Few-Shot Examples
35:30 Early AI Signals and Open-Source
43:10 Multi-Agent LLMs
47:14 Fine-Tuning and Long Input Lengths
52:20 Greg’s LLM Wishlist Hierarchy
53:15 Keeping up with Greg and Colin!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:54:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Retrieving Texts based on Abstract Descriptions Explained!]]></title>
			<description><![CDATA[<p>This video explores a new paper exploring the use of summarization chains to represent long texts and use (original text, summary) pairs for optimizing text embeddings models! Here are 3 main takeaways I think everyone working with Weaviate may get value from:

1. Understanding of Summary Indexing and the Prompts (as well as Prompt Chains) used to build them.

2. Continued development of LLM-generated data for search -- creating (full text, summary) pairs gives you (1) data to build a summary index with as mentioned, (2) data to compare different embedding models with, and (3) data to train your own embedding model.

3. Tournament style evaluation with human annotators -- the top 5 retrieved texts from one model are concatenated with the top 5 from another model, these 10 are given to human annotators to pick 5 and this is how the authors are reporting the performance of their models rather than traditional benchmarks. This m ay be a more productive evaluation technique for most real world search applications.

Thank you so much for watching, here are some links mentioned in the video!

Retrieving Texts based on Abstract Descriptions: https://arxiv.org/abs/2305.12517

Weaviate Blog - Combining LangChain and Weaviate: https://weaviate.io/blog/combining-langchain-and-weaviate

Weaviate Blog - Generative Feedback Loops: https://weaviate.io/blog/generative-feedback-loops-with-llms

Jerry Liu in Llama Index Blog - A New Document Summary Index for LLM-powered QA Systems: https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec

Learning to Retrieve Passages without Supervision (Spider): https://arxiv.org/pdf/2112.07708.pdf

Weaviate Blog - Analysis of Spider - https://weaviate.io/blog/research-insights-spider

Chapters
0:00 Introduction
0:13 Quick Overview
7:30 How to use in Weaviate!
7:50 Background
12:08 Motivation
14:20 Prompts Used
18:14 More Details of training
21:12 Human Evaluation Study
22:40 My Takeaways from the Paper</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Retrieving-Texts-based-on-Abstract-Descriptions-Explained-e258anl</link>
			<guid isPermaLink="false">9dc87442-f341-4496-b484-0a48a3118fac</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Fri, 02 Jun 2023 14:00:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/71624885/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-5%2F333454930-44100-2-ab7c3bbf0e255.mp3" length="27305272" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This video explores a new paper exploring the use of summarization chains to represent long texts and use (original text, summary) pairs for optimizing text embeddings models! Here are 3 main takeaways I think everyone working with Weaviate may get value from:

1. Understanding of Summary Indexing and the Prompts (as well as Prompt Chains) used to build them.

2. Continued development of LLM-generated data for search -- creating (full text, summary) pairs gives you (1) data to build a summary index with as mentioned, (2) data to compare different embedding models with, and (3) data to train your own embedding model.

3. Tournament style evaluation with human annotators -- the top 5 retrieved texts from one model are concatenated with the top 5 from another model, these 10 are given to human annotators to pick 5 and this is how the authors are reporting the performance of their models rather than traditional benchmarks. This m ay be a more productive evaluation technique for most real world search applications.

Thank you so much for watching, here are some links mentioned in the video!

Retrieving Texts based on Abstract Descriptions: https://arxiv.org/abs/2305.12517

Weaviate Blog - Combining LangChain and Weaviate: https://weaviate.io/blog/combining-langchain-and-weaviate

Weaviate Blog - Generative Feedback Loops: https://weaviate.io/blog/generative-feedback-loops-with-llms

Jerry Liu in Llama Index Blog - A New Document Summary Index for LLM-powered QA Systems: https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec

Learning to Retrieve Passages without Supervision (Spider): https://arxiv.org/pdf/2112.07708.pdf

Weaviate Blog - Analysis of Spider - https://weaviate.io/blog/research-insights-spider

Chapters
0:00 Introduction
0:13 Quick Overview
7:30 How to use in Weaviate!
7:50 Background
12:08 Motivation
14:20 Prompts Used
18:14 More Details of training
21:12 Human Evaluation Study
22:40 My Takeaways from the Paper&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:28:26</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Kapa AI with Emil Sorensen and Finn Bauer - Weaviate Podcast #50!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 50th (!!!) Weaviate Podcast with Emil Sorensen and Finn Bauer from Kapa AI! Are you curious about taking either your, or your company&#39;s, specific information and putting into a Vector DB + LLM system? Emil and Finn are doing this at the highest level, taking the documentation of software companies like Weaviate and building these LLM-augmetnted assistant systems for them. This podcast takes a complete tour from Data Ingestion to Cleaning, Chunking, LLM latency, and emerging trends in LLMs such as cheap fine-tuning with LoRA or Long Context Windows such as GPT-4 32K, MPT-7B 65K, or Anthropic Claude&#39;s 100k. I learned so much from speaking with Emil and Finn! Please let us know any questions you have or ideas you would like to discuss!

Check out Kapa here! https://www.kapa.ai/

Chapters
0:00 Welcome Emil and Finn!
0:42 Origin Story of Kapa
2:08 Data Ingestion
5:10 Data Cleaning
6:20 Slack / Discord / Forum Ingestion
9:05 Testing Models on Support QA
11:14 Selling Kapa to Weaviate and friends
12:37 Hallucinations in LLMs
14:06 Trends in Open-Source LLMs
15:20 Long Input LLMs (32K, 65K, 100K, …)
16:54 Retrieval-Augmentation for Long Input LLMs
18:08 Fine-Tuning LLMs
23:00 As much or as refined content as possible?
24:40 Adding Docs from Integrations
26:15 Generative Feedback Loops
29:00 What in AI excites you the most?</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Kapa-AI-with-Emil-Sorensen-and-Finn-Bauer---Weaviate-Podcast-50-e258am6</link>
			<guid isPermaLink="false">633cfa56-047d-47bc-996c-24a402d8b3be</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 31 May 2023 14:00:23 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/71624838/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-5%2F333454868-44100-2-6428bb5a8e0ca.mp3" length="34280175" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 50th (!!!) Weaviate Podcast with Emil Sorensen and Finn Bauer from Kapa AI! Are you curious about taking either your, or your company&amp;#39;s, specific information and putting into a Vector DB + LLM system? Emil and Finn are doing this at the highest level, taking the documentation of software companies like Weaviate and building these LLM-augmetnted assistant systems for them. This podcast takes a complete tour from Data Ingestion to Cleaning, Chunking, LLM latency, and emerging trends in LLMs such as cheap fine-tuning with LoRA or Long Context Windows such as GPT-4 32K, MPT-7B 65K, or Anthropic Claude&amp;#39;s 100k. I learned so much from speaking with Emil and Finn! Please let us know any questions you have or ideas you would like to discuss!

Check out Kapa here! https://www.kapa.ai/

Chapters
0:00 Welcome Emil and Finn!
0:42 Origin Story of Kapa
2:08 Data Ingestion
5:10 Data Cleaning
6:20 Slack / Discord / Forum Ingestion
9:05 Testing Models on Support QA
11:14 Selling Kapa to Weaviate and friends
12:37 Hallucinations in LLMs
14:06 Trends in Open-Source LLMs
15:20 Long Input LLMs (32K, 65K, 100K, …)
16:54 Retrieval-Augmentation for Long Input LLMs
18:08 Fine-Tuning LLMs
23:00 As much or as refined content as possible?
24:40 Adding Docs from Integrations
26:15 Generative Feedback Loops
29:00 What in AI excites you the most?&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:35:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Neurosymbolic AI in Search with Professor Laura Dietz - Weaviate Podcast #49!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 49th episode of the Weaviate Podcast!! This podcast features Professor Laura Dietz from the University of New Hampshire! I came across Dr. Dietz&#39;s tutorial at ECIR on Neuro-Symbolic Approaches for Information Retrieval and am so grateful that she was interested in joining the Weaviate Podcast! I learned so much about Neurosymbolic Search, especially around the role of Entity Linking and Entity Re-Ranking -- as well as the topic of Knowledge Graphs and Vector Search. We also discussed Prof. Dietz and collaborators latest perspectives paper on Large Language Models for Relevance Judgment. TLDR this describes the idea of using LLMs to either generate synthetic queries for documents or say annotate the relevance for query, document pairs. We discussed this kind of idea with Leo Boytsov and his work on InPars, and have presented Promptagator on past episodes of the Weaviate Air show. Although this idea comes with a lot of potential, Dr. Dietz explains the potentials for bias and poor judgements, as well as generally diving more into the details of this kind of idea! I really hope you enjoy the podcast, we are more than happy to answer any questions you might have about these ideas, or discuss any of your ideas! Thanks so much for watching!

Check out Laura Dietz&#39;s Publications here: https://scholar.google.com/citations?user=IIXpJ8oAAAAJ&amp;hl=en&amp;oi=ao

ECIR 23 Tutorial: Neuro-Symbolic Approaches
for Information Retrieval: https://www.cs.unh.edu/~dietz/appendix/dietz2023neurosymbolic.pdf

Please check this paper out below, I think this is a severely underrated work in the Search / Information Retrieval community:

Perspectives on Large Language Models for Relevance Judgment: https://arxiv.org/pdf/2304.09161.pdf

Chapters
0:00 Introduction
0:15 Neurosymbolic Search
4:50 Entity Parsing and Vector Semantics
10:56 Query Intent Understanding
15:35 Knowledge Graphs and Vector Search
17:37 Symbolic Re-Ranking
22:10 ColBERT and Entity Ranking
26:25 Example - South America and Zika Virus IR
29:15 Knowledge Graph Query Languages with LLMs
35:10 We need more Knowledge Graphs!!
37:30 PrimeKG from Harvard BMI
39:40 Filtered Vector Search
42:20 LLM Entity Linking - “The” example
47:30 Cross Encoder Entity Focus?
48:25 Perspectives on LLMs for Relevance Judgments
55:28 Spectrum of Human-Machine Collaboration for Labeling
57:30 Use LLM to Create Relevance Labeling Interfaces
1:02:30 Importance for Weaviate
1:03:45 12 Authors’ 3 Conclusions
1:04:40 IR Research Community Challenge
1:06:55 Query Generation for Weaviate Users
1:13:05 Clustering Queries
1:17:30 Final Thoughts</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Neurosymbolic-AI-in-Search-with-Professor-Laura-Dietz---Weaviate-Podcast-49-e258ail</link>
			<guid isPermaLink="false">09b2df74-5992-46fe-bc91-2866dbc5817f</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 25 May 2023 14:00:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/71624725/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-5%2F333454758-44100-2-69678e84be968.mp3" length="86706885" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 49th episode of the Weaviate Podcast!! This podcast features Professor Laura Dietz from the University of New Hampshire! I came across Dr. Dietz&amp;#39;s tutorial at ECIR on Neuro-Symbolic Approaches for Information Retrieval and am so grateful that she was interested in joining the Weaviate Podcast! I learned so much about Neurosymbolic Search, especially around the role of Entity Linking and Entity Re-Ranking -- as well as the topic of Knowledge Graphs and Vector Search. We also discussed Prof. Dietz and collaborators latest perspectives paper on Large Language Models for Relevance Judgment. TLDR this describes the idea of using LLMs to either generate synthetic queries for documents or say annotate the relevance for query, document pairs. We discussed this kind of idea with Leo Boytsov and his work on InPars, and have presented Promptagator on past episodes of the Weaviate Air show. Although this idea comes with a lot of potential, Dr. Dietz explains the potentials for bias and poor judgements, as well as generally diving more into the details of this kind of idea! I really hope you enjoy the podcast, we are more than happy to answer any questions you might have about these ideas, or discuss any of your ideas! Thanks so much for watching!

Check out Laura Dietz&amp;#39;s Publications here: https://scholar.google.com/citations?user=IIXpJ8oAAAAJ&amp;amp;hl=en&amp;amp;oi=ao

ECIR 23 Tutorial: Neuro-Symbolic Approaches
for Information Retrieval: https://www.cs.unh.edu/~dietz/appendix/dietz2023neurosymbolic.pdf

Please check this paper out below, I think this is a severely underrated work in the Search / Information Retrieval community:

Perspectives on Large Language Models for Relevance Judgment: https://arxiv.org/pdf/2304.09161.pdf

Chapters
0:00 Introduction
0:15 Neurosymbolic Search
4:50 Entity Parsing and Vector Semantics
10:56 Query Intent Understanding
15:35 Knowledge Graphs and Vector Search
17:37 Symbolic Re-Ranking
22:10 ColBERT and Entity Ranking
26:25 Example - South America and Zika Virus IR
29:15 Knowledge Graph Query Languages with LLMs
35:10 We need more Knowledge Graphs!!
37:30 PrimeKG from Harvard BMI
39:40 Filtered Vector Search
42:20 LLM Entity Linking - “The” example
47:30 Cross Encoder Entity Focus?
48:25 Perspectives on LLMs for Relevance Judgments
55:28 Spectrum of Human-Machine Collaboration for Labeling
57:30 Use LLM to Create Relevance Labeling Interfaces
1:02:30 Importance for Weaviate
1:03:45 12 Authors’ 3 Conclusions
1:04:40 IR Research Community Challenge
1:06:55 Query Generation for Weaviate Users
1:13:05 Clustering Queries
1:17:30 Final Thoughts&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:30:19</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Unstructured with Brian Raymond - Weaviate Podcast #48!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the 48th episode of the Weaviate Podcast!! This is a SUPER exciting one, welcoming Brian Raymond the CEO / Founder of Unstructured! Unstructured is a perfect complimenting technology for Weaviate, helping people get their Unstructured data into Weaviate! The podcast dives into the nuances of this task, but it generally revolves around Unstructured&#39;s abstraction of Partitioning, Cleaning, and Staging! Unstructured is making groundbreaking innovations on using Visual Document Layout models for Partitioning, for example saying that this part of the PDF is the header, body, image caption, and so on. Cleaning then describes removing pesky details like whitespaces or odd characters. Staging then describes the transformations of say formatting a text chunk with it&#39;s metadata into the JSON for a Weaviate object upload! I really hope you find this podcast interesting! We are publishing a blog post as well showing an example of how to use Unstructured to get PDF data into Weaviate, please please check that out and let us know if it works for your data and how we can improve it! This blog post can be found on weaviate.io and we will be managing discussions around it both in the Weaviate slack, as well as Unstructured! Thank you so much for listening!

Check out Unstructured here! https://www.unstructured.io/

Chapters
0:00 Welcome Brian!!
0:27 What is Unstructured?
5:42 Why now? New Advancements in Unstructured
8:02 Thoughts on Data Connectors Hub
10:55 PDFs to Weaviate with Unstructured
13:53 State-of-the-Art in OCR and Document Parsing
16:10 How to get the data from Weaviate.io?
18:06 Foundation Models from Unstructured
20:45 Evaporate-Code+
23:15 CSV, Parquet, JSON transformations in Staging
25:08 Cleaning Bricks
28:02 Visual Document Examples
30:45 Text Chunking with Metadata
33:25 Knowledge Graphs with Goldman Sachs example
39:10 LLM Hallucinations
42:10 Announcements from Brian!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Unstructured-with-Brian-Raymond---Weaviate-Podcast-48-e24iabl</link>
			<guid isPermaLink="false">6a31da6b-5d4c-4406-9ab1-5c0bed125686</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 23 May 2023 14:00:49 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/70903605/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-23%2F331154756-44100-2-28723fbf2bd79.mp3" length="41343685" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the 48th episode of the Weaviate Podcast!! This is a SUPER exciting one, welcoming Brian Raymond the CEO / Founder of Unstructured! Unstructured is a perfect complimenting technology for Weaviate, helping people get their Unstructured data into Weaviate! The podcast dives into the nuances of this task, but it generally revolves around Unstructured&amp;#39;s abstraction of Partitioning, Cleaning, and Staging! Unstructured is making groundbreaking innovations on using Visual Document Layout models for Partitioning, for example saying that this part of the PDF is the header, body, image caption, and so on. Cleaning then describes removing pesky details like whitespaces or odd characters. Staging then describes the transformations of say formatting a text chunk with it&amp;#39;s metadata into the JSON for a Weaviate object upload! I really hope you find this podcast interesting! We are publishing a blog post as well showing an example of how to use Unstructured to get PDF data into Weaviate, please please check that out and let us know if it works for your data and how we can improve it! This blog post can be found on weaviate.io and we will be managing discussions around it both in the Weaviate slack, as well as Unstructured! Thank you so much for listening!

Check out Unstructured here! https://www.unstructured.io/

Chapters
0:00 Welcome Brian!!
0:27 What is Unstructured?
5:42 Why now? New Advancements in Unstructured
8:02 Thoughts on Data Connectors Hub
10:55 PDFs to Weaviate with Unstructured
13:53 State-of-the-Art in OCR and Document Parsing
16:10 How to get the data from Weaviate.io?
18:06 Foundation Models from Unstructured
20:45 Evaporate-Code+
23:15 CSV, Parquet, JSON transformations in Staging
25:08 Cleaning Bricks
28:02 Visual Document Examples
30:45 Text Chunking with Metadata
33:25 Knowledge Graphs with Goldman Sachs example
39:10 LLM Hallucinations
42:10 Announcements from Brian!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:43:03</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ChatArena with Yuxiang Wu - Weaviate Podcast #47!]]></title>
			<description><![CDATA[<p>Hey everyone, thank you so much for watching the Weaviate podcast! I am so excited about this episode! ChatArena is a software framework for multi-agent chat games. There are quite a few interesting applications of this, firstly we can use this kind of system to evaluate the intelligence of an LLM based on how intelligent it sounds in conversation with another LLM! Another interesting idea is to have the LLM impersonate people such as Lex Fridman or Sam Altman and simulate conversations between these people -- retrieving from their digital content to facilitate the impersonation. I thought there was so many interesting ideas in this podcast, please let us know what you think!

Links:

ChatArena on GitHub (please give it a star!) - https://github.com/chatarena/chatarena

Twitter thread from Yuxiang describing the launch of ChatArena - https://twitter.com/YuxiangJWu/status/1643633046208249856

Chapters
0:00 Welcome Yuxiang!
0:38 What is ChatArena?
2:38 Impersonating People with LLMs
4:58 Weaviate and ChatArena
8:14 Generative Feedback Loops
11:10 Chat Games
16:30 Scientific Peer Review Discussions
20:05 Code Repos and Multi-Agent LLMs
23:05 Scaling Multi-Agent LLMs
25:16 Role Evolution in Startups
26:00 Evolution of Multi-Agent RL Research
29:22 AlphaGo and MCTS Text Generation
36:55 Hallucination in Role Maintenance
41:15 Evaluating LLMs with ChatArena
45:40 ChatGPT Marketplace and Tool Use
50:30 Upcoming work from Yuxiang and ChatArena!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/ChatArena-with-Yuxiang-Wu---Weaviate-Podcast-47-e247cg7</link>
			<guid isPermaLink="false">760fa96c-085d-4965-bad4-6815819d9b19</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 17 May 2023 14:00:29 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/70545351/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-17%2F330051665-44100-2-4c6b295646a59.mp3" length="49717915" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone, thank you so much for watching the Weaviate podcast! I am so excited about this episode! ChatArena is a software framework for multi-agent chat games. There are quite a few interesting applications of this, firstly we can use this kind of system to evaluate the intelligence of an LLM based on how intelligent it sounds in conversation with another LLM! Another interesting idea is to have the LLM impersonate people such as Lex Fridman or Sam Altman and simulate conversations between these people -- retrieving from their digital content to facilitate the impersonation. I thought there was so many interesting ideas in this podcast, please let us know what you think!

Links:

ChatArena on GitHub (please give it a star!) - https://github.com/chatarena/chatarena

Twitter thread from Yuxiang describing the launch of ChatArena - https://twitter.com/YuxiangJWu/status/1643633046208249856

Chapters
0:00 Welcome Yuxiang!
0:38 What is ChatArena?
2:38 Impersonating People with LLMs
4:58 Weaviate and ChatArena
8:14 Generative Feedback Loops
11:10 Chat Games
16:30 Scientific Peer Review Discussions
20:05 Code Repos and Multi-Agent LLMs
23:05 Scaling Multi-Agent LLMs
25:16 Role Evolution in Startups
26:00 Evolution of Multi-Agent RL Research
29:22 AlphaGo and MCTS Text Generation
36:55 Hallucination in Role Maintenance
41:15 Evaluating LLMs with ChatArena
45:40 ChatGPT Marketplace and Tool Use
50:30 Upcoming work from Yuxiang and ChatArena!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[HyperDB with John Dagdelen, Bob van Luijt, and Etienne Dilocker - Weaviate Podcast #46!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the Weaviate Podcast! This is pretty novel episode featuring both Weaviate Co-Founders Bob van Luijt and Etienne Dilocker! This is also extremely novel because we are featuring a competitor vector database, HyperDB! John Dagdelen is the founder of HyperDB which is a hyper-fast local vector database for use with LLM Agents. Now accepting SAFEs at $135M cap.

HyperDB: https://github.com/jdagdelen/hyperDB

More seriously, John has produced an incredible body of research - https://scholar.google.com/citations?user=TiCS5FEAAAAJ&amp;hl=en&amp;oi=ao. John&#39;s work on Scientific Literature Mining for Materials Science literature has played an enormous role in my personal education of this technology and what it is capable. Please also follow John on twitter @jmdagdelen.

Chapters

0:00 Introduction
0:26 HyperDB!
3:58 Initial Discovery of Vector Dos
15:00 Search Engine versus Databases
18:40 Scientific Literature Mining
21:42 Structured Information Extraction
27:47 Generative Feedback Loops</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/HyperDB-with-John-Dagdelen--Bob-van-Luijt--and-Etienne-Dilocker---Weaviate-Podcast-46-e243mid</link>
			<guid isPermaLink="false">808bd2bb-9702-4d9c-9a81-39be67331bf6</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 10 May 2023 14:00:45 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/70424589/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-15%2F329660707-44100-2-ca737cd0af72b.mp3" length="63737938" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the Weaviate Podcast! This is pretty novel episode featuring both Weaviate Co-Founders Bob van Luijt and Etienne Dilocker! This is also extremely novel because we are featuring a competitor vector database, HyperDB! John Dagdelen is the founder of HyperDB which is a hyper-fast local vector database for use with LLM Agents. Now accepting SAFEs at $135M cap.

HyperDB: https://github.com/jdagdelen/hyperDB

More seriously, John has produced an incredible body of research - https://scholar.google.com/citations?user=TiCS5FEAAAAJ&amp;amp;hl=en&amp;amp;oi=ao. John&amp;#39;s work on Scientific Literature Mining for Materials Science literature has played an enormous role in my personal education of this technology and what it is capable. Please also follow John on twitter @jmdagdelen.

Chapters

0:00 Introduction
0:26 HyperDB!
3:58 Initial Discovery of Vector Dos
15:00 Search Engine versus Databases
18:40 Scientific Literature Mining
21:42 Structured Information Extraction
27:47 Generative Feedback Loops&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:06:23</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Generative Feedback Loops with Bob van Luijt - Weaviate Podcast #45!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the Generative Feedback Loops Podcast! We have also created a blog post and GitHub repository for more information!

Chapters
0:00 Bob the Podcast Host
1:20 Retrieval-Augmented Generation
4:10 Hallucination in LLMs
6:15 Solving Hallucination with RLHF
7:44 LLM Monster - Reasoning and Knowledge
10:12 Feedback Loops
11:00 Hands-on Code Demo
26:00 Demo Analysis from Bob and Connor
30:35 Star Wars Wes Anderson Generated Video
34:12 Multimodal Vector Databases
36:00 Speculative Design Theory

Links:

John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges: https://www.youtube.com/watch?v=hhiLw5Q_UFg

Colin Nesh (HaystackUS 2023 slide deck) - Ground is NOT all you need, Stop hallucinations &amp; defects in generative search: https://docs.google.com/presentation/d/1uycLEUeRuF8A85Uso_A3OU6EF-qq4aYswWVxkPWHBKI/edit#slide=id.p

Generative Starwars video source - https://twitter.com/CuriousRefuge/status/1652412004626497536

Speculative Design Theory - https://readings.design/PDF/speculative-everything.pdf

Aggregation Theory - https://stratechery.com/aggregation-theory/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Generative-Feedback-Loops-with-Bob-van-Luijt---Weaviate-Podcast-45-e243mf7</link>
			<guid isPermaLink="false">6e1e82ec-9d25-43e4-bfb6-1ecf713748cc</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Fri, 05 May 2023 14:00:08 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/70424487/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-15%2F329660226-44100-2-0da2a55d19588.mp3" length="52658676" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the Generative Feedback Loops Podcast! We have also created a blog post and GitHub repository for more information!

Chapters
0:00 Bob the Podcast Host
1:20 Retrieval-Augmented Generation
4:10 Hallucination in LLMs
6:15 Solving Hallucination with RLHF
7:44 LLM Monster - Reasoning and Knowledge
10:12 Feedback Loops
11:00 Hands-on Code Demo
26:00 Demo Analysis from Bob and Connor
30:35 Star Wars Wes Anderson Generated Video
34:12 Multimodal Vector Databases
36:00 Speculative Design Theory

Links:

John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges: https://www.youtube.com/watch?v=hhiLw5Q_UFg

Colin Nesh (HaystackUS 2023 slide deck) - Ground is NOT all you need, Stop hallucinations &amp;amp; defects in generative search: https://docs.google.com/presentation/d/1uycLEUeRuF8A85Uso_A3OU6EF-qq4aYswWVxkPWHBKI/edit#slide=id.p

Generative Starwars video source - https://twitter.com/CuriousRefuge/status/1652412004626497536

Speculative Design Theory - https://readings.design/PDF/speculative-everything.pdf

Aggregation Theory - https://stratechery.com/aggregation-theory/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:54:51</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Weaviate 1.19 Release with Etienne Dilocker - Weaviate Podcast #44!]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the Weaviate 1.19 release podcast! We have all sorts of cool new features, in addition to the database and module features, I really want to encourage readers to see the `groupBy` search discussed at 14:32, quite an interesting idea for improving search performance!

Chapters
0:00 Welcome Etienne!
0:38 gRPC API
9:50 Generative Cohere
14:32 groupBy search
19:33 Bitmap or BM25 index tuning
22:20 Additional Tokenization Options
24:05 Tunable Consistency</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Weaviate-1-19-Release-with-Etienne-Dilocker---Weaviate-Podcast-44-e243mbk</link>
			<guid isPermaLink="false">31a421dd-aad5-4613-a132-5d0d4693f93a</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 04 May 2023 14:00:07 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/70424372/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-15%2F329659763-44100-2-0dcd4687c6403.mp3" length="25737925" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the Weaviate 1.19 release podcast! We have all sorts of cool new features, in addition to the database and module features, I really want to encourage readers to see the `groupBy` search discussed at 14:32, quite an interesting idea for improving search performance!

Chapters
0:00 Welcome Etienne!
0:38 gRPC API
9:50 Generative Cohere
14:32 groupBy search
19:33 Bitmap or BM25 index tuning
22:20 Additional Tokenization Options
24:05 Tunable Consistency&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:26:48</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Erika Cardenas, Roman Grebennikov, and Vsevolod Goloviznin on Recommendation and Metarank - Pod #43!]]></title>
			<description><![CDATA[<p>Thank you so much for watching the 43rd episode of the Weaviate Podcast with Roman Grebennikov and Vesvolod Goloviznin from Metarank, as well as Erika Cardenas from Weaviate! This podcast is a masterclass on Ranking models, additionally touching on the connection between Search and Recommendation. Learning-to-rank is an exciting idea where we use models that produce more fine-grained relevance scores than the offline indexing techniques of vector search and bm25, however with the tradeoff of the speed of these inferences. Romand and Vsevolod touched on another extremely interesting part of these ranking models which is the estimation of features such as Click-through-Rates and how they use streaming technology to do this. I learned so much from this podcast about the directions in ranking, I hope you enjoy it as well! As always, we are more than happy to answer any questions or discuss any ideas with you!

In reflecting on this podcast, Erika and I wrote up our latest thoughts on Ranking Models in a Weaviate blogpost, check it out here if interested: https://weaviate.io/blog/ranking-models-for-better-search.

Chapters
0:00 Welcome Everyone!
0:40 Recommendation with Weaviate
4:20 Metarank - Founding Story
8:20 Ranking MLOps
9:52 User Friendliness Perspective
15:10 Retrieval vs. Ranking
17:45 Ranking Optimization
25:20 Multi-Vector Object Representations
27:55 Click-through-Rate Feature Streaming
33:06 Weaviate Properties vs. Feature Stores
40:06 Cold-Start Recommendation Problem
46:04 Ranklens Demo - RecSys Datasets
52:02 Cross Encoders</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Erika-Cardenas--Roman-Grebennikov--and-Vsevolod-Goloviznin-on-Recommendation-and-Metarank---Pod-43-e2280u3</link>
			<guid isPermaLink="false">2aa2641a-6537-442c-bf60-6824d354c55c</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 12 Apr 2023 12:57:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/68469123/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-12%2F323518810-44100-2-1b13bdaf03e5c.mp3" length="58255150" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Thank you so much for watching the 43rd episode of the Weaviate Podcast with Roman Grebennikov and Vesvolod Goloviznin from Metarank, as well as Erika Cardenas from Weaviate! This podcast is a masterclass on Ranking models, additionally touching on the connection between Search and Recommendation. Learning-to-rank is an exciting idea where we use models that produce more fine-grained relevance scores than the offline indexing techniques of vector search and bm25, however with the tradeoff of the speed of these inferences. Romand and Vsevolod touched on another extremely interesting part of these ranking models which is the estimation of features such as Click-through-Rates and how they use streaming technology to do this. I learned so much from this podcast about the directions in ranking, I hope you enjoy it as well! As always, we are more than happy to answer any questions or discuss any ideas with you!

In reflecting on this podcast, Erika and I wrote up our latest thoughts on Ranking Models in a Weaviate blogpost, check it out here if interested: https://weaviate.io/blog/ranking-models-for-better-search.

Chapters
0:00 Welcome Everyone!
0:40 Recommendation with Weaviate
4:20 Metarank - Founding Story
8:20 Ranking MLOps
9:52 User Friendliness Perspective
15:10 Retrieval vs. Ranking
17:45 Ranking Optimization
25:20 Multi-Vector Object Representations
27:55 Click-through-Rate Feature Streaming
33:06 Weaviate Properties vs. Feature Stores
40:06 Cold-Start Recommendation Problem
46:04 Ranklens Demo - RecSys Datasets
52:02 Cross Encoders&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Ethan Steininger on Mixpeek and the AI Landscape - Weaviate Podcast #42!]]></title>
			<description><![CDATA[<p>Thank you so much for watching the 42nd episode of the Weaviate Podcast! Ethan Steininger is the founder of Mixpeek, an intelligence layer that sits on top of your S3 bucket, so you can search and analyze unstructured data at scale. Ethan has also created Collie with the headline of &quot;Enter your website and Collie will fetch every asset, then give you an embedded search bar that wows your users&quot;. Ethan began the podcast by describing his background at MongoDB and integrating the database with full text search functionality. Ethan then presented the founding vision of Mixpeek and some of the most outstanding problems with adapting the latest AI technologies to solve business problems. This lead us to discuss a massive range of topics around the AI landscape from the Llama / Alpaca models to ChatGPT Plugins, the paradigm shift in coding and serverless GPUs. I really enjoyed speaking with Ethan about all these things, I hope you enjoy listening! We would more than happy to discuss any ideas you have with you or answer any questions, thanks again for watching!

Chapters
0:00 Welcome Ethan Steininger!
0:50 Entry into Search from MongoDB
6:45 Founding Vision of Mixpeek
10:15 Data Ingestion
13:45 ChatGPT Plugins
16:25 Paradigm shift in Coding with GPT-4
18:54 Alpaca Models
22:42 Tuning LLMs with Retrieval
31:45 Adding Structure to Code Repo Search
35:06 Re-Ranking / Learning-to-Rank
43:30 AGI Monopoly
49:10 Hybrid Search! Zero-Shot + BM25
54:20 Open-Source Business
59:35 Serverless GPUs
1:11:18 Ethan’s Advice for Stress Management
1:13:00 Existential AI Fear

Links:

Mixpeek - https://mixpeek.com/

Collie - https://collie.ai/

An Open-Source, Personalized Generative Model Framework - https://esteininger.medium.com/an-open-source-personalized-generative-model-framework-6df865de51bf

Teaching GPT-4 to write code from research papers - https://esteininger.medium.com/teaching-gpt-4-to-write-code-from-research-papers-889a880fb4f0

The Need for an AI Content Verification Layer - https://esteininger.medium.com/the-need-for-an-ai-content-verification-layer-10be9379b354

Building the ML Stack of the Future - https://esteininger.medium.com/building-the-ml-stack-of-the-future-d66c8a8b566a

Vertical Integration is Key to Winning the AI Race - https://esteininger.medium.com/vertical-integration-is-key-to-winning-the-ai-race-44c8e4bd3b30</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Ethan-Steininger-on-Mixpeek-and-the-AI-Landscape---Weaviate-Podcast-42-e21qc2q</link>
			<guid isPermaLink="false">037d35b9-cb5d-4c1e-966a-a63d20274027</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 05 Apr 2023 15:44:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/68021786/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-5%2F322337749-44100-2-2f166b81712fa.mp3" length="79597817" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Thank you so much for watching the 42nd episode of the Weaviate Podcast! Ethan Steininger is the founder of Mixpeek, an intelligence layer that sits on top of your S3 bucket, so you can search and analyze unstructured data at scale. Ethan has also created Collie with the headline of &amp;quot;Enter your website and Collie will fetch every asset, then give you an embedded search bar that wows your users&amp;quot;. Ethan began the podcast by describing his background at MongoDB and integrating the database with full text search functionality. Ethan then presented the founding vision of Mixpeek and some of the most outstanding problems with adapting the latest AI technologies to solve business problems. This lead us to discuss a massive range of topics around the AI landscape from the Llama / Alpaca models to ChatGPT Plugins, the paradigm shift in coding and serverless GPUs. I really enjoyed speaking with Ethan about all these things, I hope you enjoy listening! We would more than happy to discuss any ideas you have with you or answer any questions, thanks again for watching!

Chapters
0:00 Welcome Ethan Steininger!
0:50 Entry into Search from MongoDB
6:45 Founding Vision of Mixpeek
10:15 Data Ingestion
13:45 ChatGPT Plugins
16:25 Paradigm shift in Coding with GPT-4
18:54 Alpaca Models
22:42 Tuning LLMs with Retrieval
31:45 Adding Structure to Code Repo Search
35:06 Re-Ranking / Learning-to-Rank
43:30 AGI Monopoly
49:10 Hybrid Search! Zero-Shot + BM25
54:20 Open-Source Business
59:35 Serverless GPUs
1:11:18 Ethan’s Advice for Stress Management
1:13:00 Existential AI Fear

Links:

Mixpeek - https://mixpeek.com/

Collie - https://collie.ai/

An Open-Source, Personalized Generative Model Framework - https://esteininger.medium.com/an-open-source-personalized-generative-model-framework-6df865de51bf

Teaching GPT-4 to write code from research papers - https://esteininger.medium.com/teaching-gpt-4-to-write-code-from-research-papers-889a880fb4f0

The Need for an AI Content Verification Layer - https://esteininger.medium.com/the-need-for-an-ai-content-verification-layer-10be9379b354

Building the ML Stack of the Future - https://esteininger.medium.com/building-the-ml-stack-of-the-future-d66c8a8b566a

Vertical Integration is Key to Winning the AI Race - https://esteininger.medium.com/vertical-integration-is-key-to-winning-the-ai-race-44c8e4bd3b30&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:22:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dennis Xu on Mem and LLMs! - Weaviate Podcast #41]]></title>
			<description><![CDATA[<p>Chapters
0:00 Welcome Dennis Xu!
0:30 Founding Vision of Mem
4:18 Personalized Embeddings
6:02 GPT-4, How will this change everything?
11:00 Writing code with LLMs
13:18 Embeddings at Mem
17:10 Structure in Vector Search
19:10 Zero-Shot vs. Fine-Tuned Models
25:05 Ranking Models and LLM Distillation</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Dennis-Xu-on-Mem-and-LLMs----Weaviate-Podcast-41-e21bfra</link>
			<guid isPermaLink="false">13e5e729-1a7f-4a5a-8054-331502863f97</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 29 Mar 2023 14:10:01 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/67534122/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-29%2F320979434-44100-2-c38468c7bc947.mp3" length="42740923" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Chapters
0:00 Welcome Dennis Xu!
0:30 Founding Vision of Mem
4:18 Personalized Embeddings
6:02 GPT-4, How will this change everything?
11:00 Writing code with LLMs
13:18 Embeddings at Mem
17:10 Structure in Vector Search
19:10 Zero-Shot vs. Fine-Tuned Models
25:05 Ranking Models and LLM Distillation&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:44:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/34794122/34794122-1704376570442-9264e631e6abf.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Weaviate 1.18 Release Podcast - Weaviate Podcast #40!]]></title>
			<description><![CDATA[<p>Chapters<br>
0:00 Weaviate 1.18!!!<br>
0:32 Bitmap Indexing!<br>
11:40 HNSW PQ<br>
25:33 Cursor API<br>
30:03 Filters in Hybrid Search<br>
32:55 WAND Scoring<br>
40:35 Replication<br>
49:10 Building a Database in Golang<br>
1:00:55 Thank you!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Weaviate-1-18-Release-Podcast---Weaviate-Podcast-40-e200jh1</link>
			<guid isPermaLink="false">26f63e2c-674d-4fe7-bd7a-2029bfc85732</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 07 Mar 2023 17:00:21 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/66128865/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-7%2F316637991-44100-2-f84bd3692725c.mp3" length="60241710" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Chapters&lt;br&gt;
0:00 Weaviate 1.18!!!&lt;br&gt;
0:32 Bitmap Indexing!&lt;br&gt;
11:40 HNSW PQ&lt;br&gt;
25:33 Cursor API&lt;br&gt;
30:03 Filters in Hybrid Search&lt;br&gt;
32:55 WAND Scoring&lt;br&gt;
40:35 Replication&lt;br&gt;
49:10 Building a Database in Golang&lt;br&gt;
1:00:55 Thank you!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Floris Hoogenboom on OpenVerkiezingenNL - Weaviate Podcast #39]]></title>
			<description><![CDATA[<p>Check out the website here! https://openverkiezingen.nl/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Floris-Hoogenboom-on-OpenVerkiezingenNL---Weaviate-Podcast-39-e1vuhv0</link>
			<guid isPermaLink="false">e2fcc5f7-6fd9-45ab-94dc-2143f99383ec</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 06 Mar 2023 18:35:36 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/66061728/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-6%2F316411847-44100-2-398b8b7977bf3.mp3" length="35958699" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Check out the website here! https://openverkiezingen.nl/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:37:27</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Leo Boystov on Information Retrieval Science - Weaviate Podcast #38]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 38th episode of the Weaviate podcast! This episode features Leo Boystov, an expert in Information Retrieval technology! We discussed a very wide range of topics from an overview of IR methods such as BM25, Neural Bi-Encoder and Cross-Encoder rankers, and a super exciting new work Leo has co-authored on using Large Language Models to generate training data for Neural Ranking models titled "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers." We also discussed Leo's work on Non-Metric Space Search, the challenge of long document ranking, Robustness in Generalization Testing, and ended with some thoughts on Hybrid Rank Fusion. I really hope you enjoy the podcast, more than happy to answer any questions you have or clarify anything!<br>
<br>
In-Pars Light: Cost-Effective Unsupervised Training of Efficient Rankers - <a href="https://arxiv.org/abs/2301.02998" rel="nofollow" target="_blank">https://arxiv.org/abs/2301.02998</a><br>
<br>
Google Scholar Leo Boystov - <a href="https://scholar.google.com/citations?authuser=0" rel="nofollow" target="_blank">https://scholar.google.com/citations</a>?...<br>
<br>
Chapters<br>
0:00 Introduction<br>
1:08 Information Retrieval Research<br>
25:20 Ranker Inference Requirements<br>
40:40 Non Metric Space Search<br>
52:38 Code Libraries for IR Research<br>
59:40 Long Document Ranking<br>
1:07:00 Robustness Generalization<br>
1:15:40 Hybrid Rank Fusion</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Leo-Boystov-on-Information-Retrieval-Science---Weaviate-Podcast-38-e1vmiqr</link>
			<guid isPermaLink="false">d8434382-6ac1-4e6d-a689-8c49dd88039e</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 01 Mar 2023 13:30:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/65800475/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-1%2F315479158-44100-2-5859141458d09.mp3" length="84789706" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 38th episode of the Weaviate podcast! This episode features Leo Boystov, an expert in Information Retrieval technology! We discussed a very wide range of topics from an overview of IR methods such as BM25, Neural Bi-Encoder and Cross-Encoder rankers, and a super exciting new work Leo has co-authored on using Large Language Models to generate training data for Neural Ranking models titled &quot;InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers.&quot; We also discussed Leo&apos;s work on Non-Metric Space Search, the challenge of long document ranking, Robustness in Generalization Testing, and ended with some thoughts on Hybrid Rank Fusion. I really hope you enjoy the podcast, more than happy to answer any questions you have or clarify anything!&lt;br&gt;
&lt;br&gt;
In-Pars Light: Cost-Effective Unsupervised Training of Efficient Rankers - &lt;a href=&quot;https://arxiv.org/abs/2301.02998&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://arxiv.org/abs/2301.02998&lt;/a&gt;&lt;br&gt;
&lt;br&gt;
Google Scholar Leo Boystov - &lt;a href=&quot;https://scholar.google.com/citations?authuser=0&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://scholar.google.com/citations&lt;/a&gt;?...&lt;br&gt;
&lt;br&gt;
Chapters&lt;br&gt;
0:00 Introduction&lt;br&gt;
1:08 Information Retrieval Research&lt;br&gt;
25:20 Ranker Inference Requirements&lt;br&gt;
40:40 Non Metric Space Search&lt;br&gt;
52:38 Code Libraries for IR Research&lt;br&gt;
59:40 Long Document Ranking&lt;br&gt;
1:07:00 Robustness Generalization&lt;br&gt;
1:15:40 Hybrid Rank Fusion&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:28:19</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[GPT Index and Weaviate with Jerry Liu and Bob van Luijt - Weaviate Podcast #37]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 37th episode of the Weaviate podcast! This episode discusses some of the ideas behind GPT Index. GPT Index presents really exciting ideas about how we use LLMs to index our data and then traverse these data structures. We began the podcast by discussing the origins of the tool and the ideas behind the Tree Index. We then discussed generalizing these trees to graphs and whether we are headed to the Knowledge Graph 2.0. Another really interesting topic we covered is the inference cost of building and traversing LLM indices like this! I really hope you enjoy this podcast I think these are some of the most cutting edge ideas in AI and Search!</p>
<p>Check out GPT Index (now LlamaIndex here -<a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbmcyd2xBT1JkQ2tHNm96S2RJNGZPcEVLTzJzUXxBQ3Jtc0tudGc2Ty1TRjBTMFZ6ZzNQVEQ3d0dIWEktVlBUVG00ekJXdHpWLTBUZ3JVYnhYYWJmeEhkdXlNUGlqRlJMX18yVVI4dTF3bmxGZUNPVFlFVHo5NHloZGx4em1wR01NUEU1cGx3UWwtcFFXYXNxcDhsYw&amp;q=https%3A%2F%2Fgpt-index.readthedocs.io%2Fen%2Flatest%2Findex.html&amp;v=jbQ2UbnU7vQ"> <u>https://gpt-index.readthedocs.io/en/l...</u></a>)</p>
<p>Chapters</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=0s"><u>0:00</u></a> Introduction</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=18s"><u>0:18</u></a> Origin Story of GPT Index</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=142s"><u>2:22</u></a> GPT Tree Index</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=353s"><u>5:53</u></a> Search Examples - Podcast Clips</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=682s"><u>11:22</u></a> Knowledge Graph 2.0?</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=965s"><u>16:05</u></a> LLM Writing Data to DB</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=1218s"><u>20:18</u></a> Weaviate Classes and Index Hierarchy</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=1433s"><u>23:53</u></a> Subindices vs. Tool Use</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=1730s"><u>28:50</u></a> Inference Requirements for GPT Index</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=2153s"><u>35:53</u></a> Design of GPT Index</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=2260s"><u>37:40</u></a> Impact of Cheaper LLMs for this</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=2402s"><u>40:02</u></a> Name Change for GPT Index?</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=2524s"><u>42:04</u></a> Llama Hub</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=2707s"><u>45:07</u></a> Relationship in Software Stack</p>
<p><a href="https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;t=2895s"><u>48:15</u></a> Extension to Multimodal, e.g. Vision-Language</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/GPT-Index-and-Weaviate-with-Jerry-Liu-and-Bob-van-Luijt---Weaviate-Podcast-37-e1vc5i5</link>
			<guid isPermaLink="false">c3304626-3249-4050-9f9d-114d2db6c3e7</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 22 Feb 2023 14:17:51 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/65459205/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-22%2F314190516-44100-2-a0c7b3531b1a4.mp3" length="49741321" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 37th episode of the Weaviate podcast! This episode discusses some of the ideas behind GPT Index. GPT Index presents really exciting ideas about how we use LLMs to index our data and then traverse these data structures. We began the podcast by discussing the origins of the tool and the ideas behind the Tree Index. We then discussed generalizing these trees to graphs and whether we are headed to the Knowledge Graph 2.0. Another really interesting topic we covered is the inference cost of building and traversing LLM indices like this! I really hope you enjoy this podcast I think these are some of the most cutting edge ideas in AI and Search!&lt;/p&gt;
&lt;p&gt;Check out GPT Index (now LlamaIndex here -&lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbmcyd2xBT1JkQ2tHNm96S2RJNGZPcEVLTzJzUXxBQ3Jtc0tudGc2Ty1TRjBTMFZ6ZzNQVEQ3d0dIWEktVlBUVG00ekJXdHpWLTBUZ3JVYnhYYWJmeEhkdXlNUGlqRlJMX18yVVI4dTF3bmxGZUNPVFlFVHo5NHloZGx4em1wR01NUEU1cGx3UWwtcFFXYXNxcDhsYw&amp;amp;q=https%3A%2F%2Fgpt-index.readthedocs.io%2Fen%2Flatest%2Findex.html&amp;amp;v=jbQ2UbnU7vQ&quot;&gt; &lt;u&gt;https://gpt-index.readthedocs.io/en/l...&lt;/u&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Chapters&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=0s&quot;&gt;&lt;u&gt;0:00&lt;/u&gt;&lt;/a&gt; Introduction&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=18s&quot;&gt;&lt;u&gt;0:18&lt;/u&gt;&lt;/a&gt; Origin Story of GPT Index&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=142s&quot;&gt;&lt;u&gt;2:22&lt;/u&gt;&lt;/a&gt; GPT Tree Index&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=353s&quot;&gt;&lt;u&gt;5:53&lt;/u&gt;&lt;/a&gt; Search Examples - Podcast Clips&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=682s&quot;&gt;&lt;u&gt;11:22&lt;/u&gt;&lt;/a&gt; Knowledge Graph 2.0?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=965s&quot;&gt;&lt;u&gt;16:05&lt;/u&gt;&lt;/a&gt; LLM Writing Data to DB&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=1218s&quot;&gt;&lt;u&gt;20:18&lt;/u&gt;&lt;/a&gt; Weaviate Classes and Index Hierarchy&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=1433s&quot;&gt;&lt;u&gt;23:53&lt;/u&gt;&lt;/a&gt; Subindices vs. Tool Use&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=1730s&quot;&gt;&lt;u&gt;28:50&lt;/u&gt;&lt;/a&gt; Inference Requirements for GPT Index&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=2153s&quot;&gt;&lt;u&gt;35:53&lt;/u&gt;&lt;/a&gt; Design of GPT Index&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=2260s&quot;&gt;&lt;u&gt;37:40&lt;/u&gt;&lt;/a&gt; Impact of Cheaper LLMs for this&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=2402s&quot;&gt;&lt;u&gt;40:02&lt;/u&gt;&lt;/a&gt; Name Change for GPT Index?&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=2524s&quot;&gt;&lt;u&gt;42:04&lt;/u&gt;&lt;/a&gt; Llama Hub&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=2707s&quot;&gt;&lt;u&gt;45:07&lt;/u&gt;&lt;/a&gt; Relationship in Software Stack&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jbQ2UbnU7vQ&amp;amp;t=2895s&quot;&gt;&lt;u&gt;48:15&lt;/u&gt;&lt;/a&gt; Extension to Multimodal, e.g. Vision-Language&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:48</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36]]></title>
			<description><![CDATA[<p>Hey everyone! Thank you so much for watching the 36th episode of the Weaviate podcast! This episode continues on the marriage between LLMs and Semantic Search, welcoming back Weaviate CEO and Co-Founder Bob van Luijt! Enter LangChain and its creator, Harrison Chase, providing the glue between LLMs and tools, such as semantic search. LangChain provides a set of abstractions around chaining multiple language model calls with different prompts, strategies for overcoming the 4096 token limit, and connecting LLMs with their tools. LangChain Hub is a collection of these chains if you want to check it out yourself! Huge thank you to Harrison and Bob for joining the podcast, this was such an information packed podcast with some great predictions for the future of LLMs + Vector Databases! &nbsp;</p>
<p><br></p>
<p>Check out LangChain here! https://langchain.readthedocs.io/en/latest/</p>
<p><br></p>
<p>Chapters</p>
<p>0:00 Welcome<br>
0:14 Origin Story of LangChain<br>
1:27 What are LLM Chains?<br>
4:00 Adding Weaviate Search<br>
7:30 LLM Orchestration and Tool Use<br>
11:24 Extension to Multi-Modal<br>
14:00 Natural Language Interaction with Software<br>
20:36 Will Prompt Engineering Last?<br>
21:00 More on Tool Use<br>
25:47 Favorite Prompts<br>
29:54 Temperature in LLMs<br>
31:00 Reasoning and Knowledge<br>
32:50 LLM as Router<br>
35:50 Model Diversity<br>
39:45 No GPUs before PMF<br>
41:35 Virality of LangChain<br>
43:40 Future of LangChain</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/LangChain-and-Weaviate-with-Harrison-Chase-and-Bob-van-Luijt---Weaviate-Podcast-36-e1v0shq</link>
			<guid isPermaLink="false">6469b17f-73d5-4af2-90e7-db7cf89cbfe4</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 15 Feb 2023 14:28:31 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/65089530/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-15%2F312954774-44100-2-02dcd9b4a9fc9.mp3" length="45984704" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Hey everyone! Thank you so much for watching the 36th episode of the Weaviate podcast! This episode continues on the marriage between LLMs and Semantic Search, welcoming back Weaviate CEO and Co-Founder Bob van Luijt! Enter LangChain and its creator, Harrison Chase, providing the glue between LLMs and tools, such as semantic search. LangChain provides a set of abstractions around chaining multiple language model calls with different prompts, strategies for overcoming the 4096 token limit, and connecting LLMs with their tools. LangChain Hub is a collection of these chains if you want to check it out yourself! Huge thank you to Harrison and Bob for joining the podcast, this was such an information packed podcast with some great predictions for the future of LLMs + Vector Databases! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Check out LangChain here! https://langchain.readthedocs.io/en/latest/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chapters&lt;/p&gt;
&lt;p&gt;0:00 Welcome&lt;br&gt;
0:14 Origin Story of LangChain&lt;br&gt;
1:27 What are LLM Chains?&lt;br&gt;
4:00 Adding Weaviate Search&lt;br&gt;
7:30 LLM Orchestration and Tool Use&lt;br&gt;
11:24 Extension to Multi-Modal&lt;br&gt;
14:00 Natural Language Interaction with Software&lt;br&gt;
20:36 Will Prompt Engineering Last?&lt;br&gt;
21:00 More on Tool Use&lt;br&gt;
25:47 Favorite Prompts&lt;br&gt;
29:54 Temperature in LLMs&lt;br&gt;
31:00 Reasoning and Knowledge&lt;br&gt;
32:50 LLM as Router&lt;br&gt;
35:50 Model Diversity&lt;br&gt;
39:45 No GPUs before PMF&lt;br&gt;
41:35 Virality of LangChain&lt;br&gt;
43:40 Future of LangChain&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:47:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Bob van Luijt on Generative Search with Weaviate - Weaviate Podcast #35]]></title>
			<description><![CDATA[<p>This podcast debuts a huge new release from Weaviate... the generate module! The generate module is a new API in Weaviate that facilitates passing YOUR data from the Weaviate database to ChatGPT. This enables ChatGPT to become knowledgeable about your particular business or interests! Here is a great snippet from Bob around the 43 minute mark that describes how this kind of LLM technology is changing the world of database technology, "Yeah so, what I’m really excited about and this is something that it’s just so funny right because if you see it, you have this huge epiphany. I’ve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector search database. So now what we’re adding is not only the input in the database but the output. So we’re basically saying we’re going to give you relevant information coming from the database, but that’s not per se stored inside the database. That’s new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what’s in there. It makes sense. Because that’s how you use it. But now what we’re saying, is that’s fine you can do that, but also it can give you information, give you data that’s generated based on a task or prompt that you’re giving it. Having databases that make sense of it at input and generate new relevant content if that’s something you want as a user is amazing, and it’s just getting started. We should do this podcast like a half a year from now again and see how it's evolved because this is just too exciting man.". I really hope you enjoy the podcast, we are more than happy to answer any questions or help you get started with Weaviate!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Bob-van-Luijt-on-Generative-Search-with-Weaviate---Weaviate-Podcast-35-e1ujuof</link>
			<guid isPermaLink="false">64e8c5d1-e397-4f07-b8a5-51828bd77d9e</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 07 Feb 2023 17:00:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/64665807/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-7%2F311554086-44100-2-a85b73c121be.mp3" length="42084727" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This podcast debuts a huge new release from Weaviate... the generate module! The generate module is a new API in Weaviate that facilitates passing YOUR data from the Weaviate database to ChatGPT. This enables ChatGPT to become knowledgeable about your particular business or interests! Here is a great snippet from Bob around the 43 minute mark that describes how this kind of LLM technology is changing the world of database technology, &quot;Yeah so, what I’m really excited about and this is something that it’s just so funny right because if you see it, you have this huge epiphany. I’ve always been thinking of working with these models on input. Right so that they we can solve the problem of not having 100% keyword based search, so that we can have semantic search, image search, and those kind of things. I saw that as this beautiful uniqueness coming from a vector search engine or vector search database. So now what we’re adding is not only the input in the database but the output. So we’re basically saying we’re going to give you relevant information coming from the database, but that’s not per se stored inside the database. That’s new! I mean, just think about the most used databases in the world, Postgres, or MySQL, those kind of databases. It only outputs what’s in there. It makes sense. Because that’s how you use it. But now what we’re saying, is that’s fine you can do that, but also it can give you information, give you data that’s generated based on a task or prompt that you’re giving it. Having databases that make sense of it at input and generate new relevant content if that’s something you want as a user is amazing, and it’s just getting started. We should do this podcast like a half a year from now again and see how it&apos;s evolved because this is just too exciting man.&quot;. I really hope you enjoy the podcast, we are more than happy to answer any questions or help you get started with Weaviate!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:43:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Our Mad Journey of Building a Vector Database in Go - Weaviate at FOSDEM 2023]]></title>
			<description><![CDATA[<p>Chapters<br>
1:00 Introduction<br>
1:26 Why does the world need yet another database?<br>
3:57 Memory Allocations<br>
9:40 Delayed Decoding<br>
16:05 SIMD<br>
22:04 Demo Time!<br>
24:38 Mad at Go?<br>
26:00 Audience Questions</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Our-Mad-Journey-of-Building-a-Vector-Database-in-Go---Weaviate-at-FOSDEM-2023-e1ugj9q</link>
			<guid isPermaLink="false">0cc2c624-ddc7-4b17-a546-75ee86559e72</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 06 Feb 2023 17:00:58 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/64555770/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-5%2F311215552-44100-2-f637e69ebf6ae.mp3" length="27423554" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Chapters&lt;br&gt;
1:00 Introduction&lt;br&gt;
1:26 Why does the world need yet another database?&lt;br&gt;
3:57 Memory Allocations&lt;br&gt;
9:40 Delayed Decoding&lt;br&gt;
16:05 SIMD&lt;br&gt;
22:04 Demo Time!&lt;br&gt;
24:38 Mad at Go?&lt;br&gt;
26:00 Audience Questions&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:28:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dmitry Kan on Neural Search Frameworks - Weaviate Podcast #34]]></title>
			<description><![CDATA[<p>I am so excited to host Dmitry Kan on the Weaviate Podcast!! Dmitry is a world class expert on emerging trends in search technology! This podcast reflects on Dmitry's latest characterization of the field, the Neural Search Pyramid. This describes the different components involved with building a Deep Learning-powered Search experience from the Approximate Nearest Neighbor index algorithms, to Database functionality, &nbsp;LLM orchestration, Vectorization optimization, Data preprocessing, User Interface, and many more! We also concluded the podcast with an interesting debate around renaming "Vector Search" to something else that reaches a broader audience. I really hope you enjoy the podcast, thank you so much for listening! Please see the links below to Dmitry's recent content and the Weaviate Podcast Search App!</p>
<p>Links:</p>
<p>Dmitry's Keynote at Haystack Europe 2022, Where Vector Search is Taking Us - https://www.youtube.com/watch?v=2o8-dX__EgU</p>
<p>Dmitry's latest blog post on Neural Search Frameworks: A Head-to-Head Comparison - https://dmitry-kan.medium.com/neural-search-frameworks-a-head-to-head-comparison-976aa6662d20.</p>
<p>Search through this episode of the Weaviate Podcast! - https://github.com/weaviate/weaviate-podcast-search</p>
<p>Chapters</p>
<p>0:00 Neural Search Pyramid Visual</p>
<p>0:40 Weaviate Podcast Search!</p>
<p>1:35 Welcome Dmitry!!</p>
<p>2:02 Where is Vector Search taking us?</p>
<p>5:40 Retail and Search</p>
<p>11:02 Neural Search Frameworks</p>
<p>17:10 Data Preprocessing, e.g. PDF to Text / OCR</p>
<p>24:15 Vectorizing Data</p>
<p>31:18 ANN Index and Database Entanglement</p>
<p>37:25 Hardware Accelerators for Vector Search</p>
<p>46:02 Reader Layers, Q&amp;A, Ranking, …</p>
<p>51:20 ChatGPT in Neural Search Frameworks</p>
<p>1:03:40 Search Result Summarization with ChatGPT</p>
<p>1:12:55 User Interfaces for Neural Search</p>
<p>1:26:30 Renaming “Vector Search”</p>
<p>1:46:10 Thank you Dmitry!!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Dmitry-Kan-on-Neural-Search-Frameworks---Weaviate-Podcast-34-e1u06g1</link>
			<guid isPermaLink="false">ac6588d4-1628-4a3b-b818-cea6066687dd</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 25 Jan 2023 15:04:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/64018369/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-0-25%2F309324656-44100-2-47c4b96f1b2ed.mp3" length="103260577" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;I am so excited to host Dmitry Kan on the Weaviate Podcast!! Dmitry is a world class expert on emerging trends in search technology! This podcast reflects on Dmitry&apos;s latest characterization of the field, the Neural Search Pyramid. This describes the different components involved with building a Deep Learning-powered Search experience from the Approximate Nearest Neighbor index algorithms, to Database functionality, &amp;nbsp;LLM orchestration, Vectorization optimization, Data preprocessing, User Interface, and many more! We also concluded the podcast with an interesting debate around renaming &quot;Vector Search&quot; to something else that reaches a broader audience. I really hope you enjoy the podcast, thank you so much for listening! Please see the links below to Dmitry&apos;s recent content and the Weaviate Podcast Search App!&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;p&gt;Dmitry&apos;s Keynote at Haystack Europe 2022, Where Vector Search is Taking Us - https://www.youtube.com/watch?v=2o8-dX__EgU&lt;/p&gt;
&lt;p&gt;Dmitry&apos;s latest blog post on Neural Search Frameworks: A Head-to-Head Comparison - https://dmitry-kan.medium.com/neural-search-frameworks-a-head-to-head-comparison-976aa6662d20.&lt;/p&gt;
&lt;p&gt;Search through this episode of the Weaviate Podcast! - https://github.com/weaviate/weaviate-podcast-search&lt;/p&gt;
&lt;p&gt;Chapters&lt;/p&gt;
&lt;p&gt;0:00 Neural Search Pyramid Visual&lt;/p&gt;
&lt;p&gt;0:40 Weaviate Podcast Search!&lt;/p&gt;
&lt;p&gt;1:35 Welcome Dmitry!!&lt;/p&gt;
&lt;p&gt;2:02 Where is Vector Search taking us?&lt;/p&gt;
&lt;p&gt;5:40 Retail and Search&lt;/p&gt;
&lt;p&gt;11:02 Neural Search Frameworks&lt;/p&gt;
&lt;p&gt;17:10 Data Preprocessing, e.g. PDF to Text / OCR&lt;/p&gt;
&lt;p&gt;24:15 Vectorizing Data&lt;/p&gt;
&lt;p&gt;31:18 ANN Index and Database Entanglement&lt;/p&gt;
&lt;p&gt;37:25 Hardware Accelerators for Vector Search&lt;/p&gt;
&lt;p&gt;46:02 Reader Layers, Q&amp;amp;A, Ranking, …&lt;/p&gt;
&lt;p&gt;51:20 ChatGPT in Neural Search Frameworks&lt;/p&gt;
&lt;p&gt;1:03:40 Search Result Summarization with ChatGPT&lt;/p&gt;
&lt;p&gt;1:12:55 User Interfaces for Neural Search&lt;/p&gt;
&lt;p&gt;1:26:30 Renaming “Vector Search”&lt;/p&gt;
&lt;p&gt;1:46:10 Thank you Dmitry!!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:47:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Nils Reimers on Cohere Embedding Models]]></title>
			<description><![CDATA[<p>Weaviate podcast #33.</p>
<p>Thank you so much for watching the 33rd Weaviate Podcast! This episode features one of the heroes of Deep Learning for Search, Nils Reimers! Nils' work on SentenceBERT is one of the foundational works for applying Deep Representation Learning to text search. This is the idea that personally inspired me to work in this field. Having seen the successes of Contrastive Representation Learning for Computer Vision, I was mind-blown by the possibility of this for NLP and text search. In addition to the scientific foundation, the software development of the Sentence Transformers library and BEIR benchmarks has been enormously impactful! It was an honor getting to ask Nils the questions I have about these things, from the role of Data Quality to Intent, Sparse Vectors, Long Document Encoding, Distribution Shift, and many more. I really hope you enjoy the podcast! We are so excited about the Cohere Multilingual embedding model and can't wait to see what else comes out of Cohere and their amazing team!<br>
<br>
Cohere Multilingual ML Models with Weaviate: <a href="https://weaviate.io/blog/2022/12/Cohe" rel="nofollow" target="_blank">https://weaviate.io/blog/2022/12/Cohe</a>...<br>
<br>
Nils Reimers: <a href="https://scholar.google.com/citations?authuser=0" rel="nofollow" target="_blank">https://scholar.google.com/citations</a>?...<br>
<br>
Mentioned in the podcast,<br>
<br>
Cross-Encoders: <a href="https://weaviate.io/blog/2022/08/Usin" rel="nofollow" target="_blank">https://weaviate.io/blog/2022/08/Usin</a>...<br>
<br>
How to choose a Sentence Transformer from HuggingFace: <a href="https://weaviate.io/blog/2022/10/How-" rel="nofollow" target="_blank">https://weaviate.io/blog/2022/10/How-</a>...<br>
<br>
Chapters<br>
0:00 Cohere X Weaviate<br>
0:22 Welcome Nils Reimers!<br>
1:18 Origin Story<br>
3:15 Learning Text Embeddings<br>
6:54 Positive and Negative Sampling in Contrastive Learning<br>
13:32 1 Billion Pairs for Text Embedding Optimization<br>
15:44 Impact of Data Quality<br>
18:40 New Cohere Multilingual Model!<br>
24:50 Challenge of Debugging Multilingual Models<br>
28:30 Intent in Search<br>
30:40 Thoughts on ColBERT<br>
33:50 Sparse Vectors in Search<br>
36:17 Long Documents and Multi-Discourse<br>
43:40 Entity Parsing in Query Understanding<br>
46:08 Unknown Words and Distribution Shift<br>
50:07 Re-Vectorizing with Fine-Tuning<br>
53:07 More on Search Interfaces and Intent in Search<br>
55:15 Thank you Nils!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Nils-Reimers-on-Cohere-Embedding-Models-e1temsm</link>
			<guid isPermaLink="false">4c0ee876-a0df-45b3-9a10-2a07e46b95c5</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 11 Jan 2023 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/63445334/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-0-13%2F307350441-44100-2-38e578cc63baa.mp3" length="53482892" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate podcast #33.&lt;/p&gt;
&lt;p&gt;Thank you so much for watching the 33rd Weaviate Podcast! This episode features one of the heroes of Deep Learning for Search, Nils Reimers! Nils&apos; work on SentenceBERT is one of the foundational works for applying Deep Representation Learning to text search. This is the idea that personally inspired me to work in this field. Having seen the successes of Contrastive Representation Learning for Computer Vision, I was mind-blown by the possibility of this for NLP and text search. In addition to the scientific foundation, the software development of the Sentence Transformers library and BEIR benchmarks has been enormously impactful! It was an honor getting to ask Nils the questions I have about these things, from the role of Data Quality to Intent, Sparse Vectors, Long Document Encoding, Distribution Shift, and many more. I really hope you enjoy the podcast! We are so excited about the Cohere Multilingual embedding model and can&apos;t wait to see what else comes out of Cohere and their amazing team!&lt;br&gt;
&lt;br&gt;
Cohere Multilingual ML Models with Weaviate: &lt;a href=&quot;https://weaviate.io/blog/2022/12/Cohe&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://weaviate.io/blog/2022/12/Cohe&lt;/a&gt;...&lt;br&gt;
&lt;br&gt;
Nils Reimers: &lt;a href=&quot;https://scholar.google.com/citations?authuser=0&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://scholar.google.com/citations&lt;/a&gt;?...&lt;br&gt;
&lt;br&gt;
Mentioned in the podcast,&lt;br&gt;
&lt;br&gt;
Cross-Encoders: &lt;a href=&quot;https://weaviate.io/blog/2022/08/Usin&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://weaviate.io/blog/2022/08/Usin&lt;/a&gt;...&lt;br&gt;
&lt;br&gt;
How to choose a Sentence Transformer from HuggingFace: &lt;a href=&quot;https://weaviate.io/blog/2022/10/How-&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://weaviate.io/blog/2022/10/How-&lt;/a&gt;...&lt;br&gt;
&lt;br&gt;
Chapters&lt;br&gt;
0:00 Cohere X Weaviate&lt;br&gt;
0:22 Welcome Nils Reimers!&lt;br&gt;
1:18 Origin Story&lt;br&gt;
3:15 Learning Text Embeddings&lt;br&gt;
6:54 Positive and Negative Sampling in Contrastive Learning&lt;br&gt;
13:32 1 Billion Pairs for Text Embedding Optimization&lt;br&gt;
15:44 Impact of Data Quality&lt;br&gt;
18:40 New Cohere Multilingual Model!&lt;br&gt;
24:50 Challenge of Debugging Multilingual Models&lt;br&gt;
28:30 Intent in Search&lt;br&gt;
30:40 Thoughts on ColBERT&lt;br&gt;
33:50 Sparse Vectors in Search&lt;br&gt;
36:17 Long Documents and Multi-Discourse&lt;br&gt;
43:40 Entity Parsing in Query Understanding&lt;br&gt;
46:08 Unknown Words and Distribution Shift&lt;br&gt;
50:07 Re-Vectorizing with Fine-Tuning&lt;br&gt;
53:07 More on Search Interfaces and Intent in Search&lt;br&gt;
55:15 Thank you Nils!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sam Bean, Zain Hasan, and John Trengrove on You.com and Spark ]]></title>
			<description><![CDATA[<p>Weaviate Podcast #32.</p>
<p><br>
Thank you so much for watching the Weaviate podcast! We are super excited to host Sam Bean from You.com! As well as welcome Zain Hasan and John Trengrove to the Weaviate podcast for the first time! Sam begins by describing You.com, and then we dive into the Weaviate Spark Connector that Sam played a massive role in creating. I thought this was such a masterclass in the Spark big data technology; John, Sam, and Zain are all data engineering pros and I've never learned more about a new technology from a podcast than this one. I really hope you enjoy listening to it, please let us know any questions or ideas you have. Also, please see Zain's blog post on "The Details Behind the Sphere Dataset in Weaviate" - <a href="https://weaviate.io/blog/2022/12/deta" rel="nofollow" target="_blank">https://weaviate.io/blog/2022/12/deta</a>.... This provides great detail on exactly how to use the Spark connector in Weaviate! In this case for a billion-scale dataset upload!!!<br>
<br>
Chapters<br>
0:00 Thanks for Watching!<br>
0:18 Welcome Zain and John<br>
0:28 Welcome Sam Bean, You.com!<br>
1:48 Search Interface and Search Apps / Widgets<br>
3:40 Searching through Specific Websites<br>
4:00 Origin Story of <a href="http://you.com/" rel="nofollow" target="_blank">You.com</a><br>
6:53 How did you come across Weaviate?<br>
8:33 Text, Image, Audio Search<br>
10:28 What do you use Spark for?<br>
14:20 Datasets used with Weaviate<br>
16:14 Creating a Spark Connector to Weaviate<br>
21:05 Adding Streaming support<br>
22:50 Vectorizing Data at <a href="http://you.com/" rel="nofollow" target="_blank">You.com</a><br>
27:15 More on ONNX + Spark<br>
29:52 Performance Questions, Spark + Weaviate<br>
34:35 Parquet for HuggingFace Dataset Files<br>
34:54 What is Parquet? Spark Pushdown Filters Explained<br>
39:04 Similar to HDF5?<br>
39:45 Spark for Extracting Ranking Features<br>
43:25 Hybrid Search<br>
46:48 Collecting Search Relevance Data<br>
51:07 Thank you for watching! Thanks Sam, Zain, and John!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Sam-Bean--Zain-Hasan--and-John-Trengrove-on-You-com-and-Spark-e1t8gh9</link>
			<guid isPermaLink="false">9587bd7c-f827-4be8-ac44-b7e936299ea5</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 09 Jan 2023 08:47:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/63242217/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-0-9%2F306624975-44100-2-08d72584e11c6.mp3" length="49505592" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #32.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
Thank you so much for watching the Weaviate podcast! We are super excited to host Sam Bean from You.com! As well as welcome Zain Hasan and John Trengrove to the Weaviate podcast for the first time! Sam begins by describing You.com, and then we dive into the Weaviate Spark Connector that Sam played a massive role in creating. I thought this was such a masterclass in the Spark big data technology; John, Sam, and Zain are all data engineering pros and I&apos;ve never learned more about a new technology from a podcast than this one. I really hope you enjoy listening to it, please let us know any questions or ideas you have. Also, please see Zain&apos;s blog post on &quot;The Details Behind the Sphere Dataset in Weaviate&quot; - &lt;a href=&quot;https://weaviate.io/blog/2022/12/deta&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://weaviate.io/blog/2022/12/deta&lt;/a&gt;.... This provides great detail on exactly how to use the Spark connector in Weaviate! In this case for a billion-scale dataset upload!!!&lt;br&gt;
&lt;br&gt;
Chapters&lt;br&gt;
0:00 Thanks for Watching!&lt;br&gt;
0:18 Welcome Zain and John&lt;br&gt;
0:28 Welcome Sam Bean, You.com!&lt;br&gt;
1:48 Search Interface and Search Apps / Widgets&lt;br&gt;
3:40 Searching through Specific Websites&lt;br&gt;
4:00 Origin Story of &lt;a href=&quot;http://you.com/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;You.com&lt;/a&gt;&lt;br&gt;
6:53 How did you come across Weaviate?&lt;br&gt;
8:33 Text, Image, Audio Search&lt;br&gt;
10:28 What do you use Spark for?&lt;br&gt;
14:20 Datasets used with Weaviate&lt;br&gt;
16:14 Creating a Spark Connector to Weaviate&lt;br&gt;
21:05 Adding Streaming support&lt;br&gt;
22:50 Vectorizing Data at &lt;a href=&quot;http://you.com/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;You.com&lt;/a&gt;&lt;br&gt;
27:15 More on ONNX + Spark&lt;br&gt;
29:52 Performance Questions, Spark + Weaviate&lt;br&gt;
34:35 Parquet for HuggingFace Dataset Files&lt;br&gt;
34:54 What is Parquet? Spark Pushdown Filters Explained&lt;br&gt;
39:04 Similar to HDF5?&lt;br&gt;
39:45 Spark for Extracting Ranking Features&lt;br&gt;
43:25 Hybrid Search&lt;br&gt;
46:48 Collecting Search Relevance Data&lt;br&gt;
51:07 Thank you for watching! Thanks Sam, Zain, and John!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:34</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Weaviate 1.17 Release with Etienne Dilocker and Parker Duckworth ]]></title>
			<description><![CDATA[<p>Weaviate Podcast #31.&nbsp;</p>
<p>Weaviate 1.17!! This is a massive release for Weaviate, debuting Replication, Hybrid Search, BM25, Faster Startup and Import Times, as well as other fixes! Replication and Hybrid Search are two massive features for Weaviate, and we really hope you enjoy the description of them from the podcast. Please also check out the Weaviate 1.17 release blog post for more information as well - https://weaviate.io/blog/2022/12/Weaviate-release-1-17.html! &nbsp;</p>
<p>This is also a very special podcast as we welcome Parker Duckworth for the first time to the podcast! Parker gave an excellent explanation of Replication and unpacked some of the questions we are seeing around Ref2Vec! &nbsp;Thank you so much for listening to the podcast! Please check out the newest version of Weaviate! &nbsp;</p>
<p><br></p>
<p>Chapters&nbsp;</p>
<p>0:00 Weaviate 1.17! Welcome Parker!&nbsp;</p>
<p>0:28 From Italy to 1.17&nbsp;</p>
<p>2:04 Replication work in Italy&nbsp;</p>
<p>3:58 Replication Details&nbsp;</p>
<p>6:28 Use Cases of Replication&nbsp;</p>
<p>13:12 Product Engineering&nbsp;</p>
<p>16:24 Hybrid Search&nbsp;</p>
<p>21:30 Open Question around Hybrid Search&nbsp;</p>
<p>23:15 Rank Fusion 24:00 BEIR Benchmarks&nbsp;</p>
<p>27:28 What is Ref2Vec?&nbsp;</p>
<p>29:08 Bipartite Graph Ref2Vec Example&nbsp;</p>
<p>29:30 Graphs in Weaviate&nbsp;</p>
<p>34:25 Ref2Vec Cascading Updates&nbsp;</p>
<p>37:45 Custom Aggregation Functions in Ref2Vec&nbsp;</p>
<p>39:08 Adding Recency Bias in Ref2Vec&nbsp;</p>
<p>41:18 Startup Time Improvements&nbsp;</p>
<p>41:50 Batch Latency Improvement</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Weaviate-1-17-Release-with-Etienne-Dilocker-and-Parker-Duckworth-e1si0ar</link>
			<guid isPermaLink="false">30a7383f-06d4-4b71-ace0-3688c4aa639a</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 21 Dec 2022 15:00:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/62504731/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-21%2F304216490-44100-2-8885d48118b2c.mp3" length="41044426" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #31.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Weaviate 1.17!! This is a massive release for Weaviate, debuting Replication, Hybrid Search, BM25, Faster Startup and Import Times, as well as other fixes! Replication and Hybrid Search are two massive features for Weaviate, and we really hope you enjoy the description of them from the podcast. Please also check out the Weaviate 1.17 release blog post for more information as well - https://weaviate.io/blog/2022/12/Weaviate-release-1-17.html! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;This is also a very special podcast as we welcome Parker Duckworth for the first time to the podcast! Parker gave an excellent explanation of Replication and unpacked some of the questions we are seeing around Ref2Vec! &amp;nbsp;Thank you so much for listening to the podcast! Please check out the newest version of Weaviate! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chapters&amp;nbsp;&lt;/p&gt;
&lt;p&gt;0:00 Weaviate 1.17! Welcome Parker!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;0:28 From Italy to 1.17&amp;nbsp;&lt;/p&gt;
&lt;p&gt;2:04 Replication work in Italy&amp;nbsp;&lt;/p&gt;
&lt;p&gt;3:58 Replication Details&amp;nbsp;&lt;/p&gt;
&lt;p&gt;6:28 Use Cases of Replication&amp;nbsp;&lt;/p&gt;
&lt;p&gt;13:12 Product Engineering&amp;nbsp;&lt;/p&gt;
&lt;p&gt;16:24 Hybrid Search&amp;nbsp;&lt;/p&gt;
&lt;p&gt;21:30 Open Question around Hybrid Search&amp;nbsp;&lt;/p&gt;
&lt;p&gt;23:15 Rank Fusion 24:00 BEIR Benchmarks&amp;nbsp;&lt;/p&gt;
&lt;p&gt;27:28 What is Ref2Vec?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;29:08 Bipartite Graph Ref2Vec Example&amp;nbsp;&lt;/p&gt;
&lt;p&gt;29:30 Graphs in Weaviate&amp;nbsp;&lt;/p&gt;
&lt;p&gt;34:25 Ref2Vec Cascading Updates&amp;nbsp;&lt;/p&gt;
&lt;p&gt;37:45 Custom Aggregation Functions in Ref2Vec&amp;nbsp;&lt;/p&gt;
&lt;p&gt;39:08 Adding Recency Bias in Ref2Vec&amp;nbsp;&lt;/p&gt;
&lt;p&gt;41:18 Startup Time Improvements&amp;nbsp;&lt;/p&gt;
&lt;p&gt;41:50 Batch Latency Improvement&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:42:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Bob van Luijt, Chris Dossman and Marco Bianco on the future of search]]></title>
			<description><![CDATA[<p>Weaviate Podcast #30.&nbsp;</p>
<p>Chapters</p>
<p>0:00 The future of search!<br>
0:42 Welcome Marco and Chris!<br>
4:28 Solving Hallucination with External Memory LLMs<br>
8:16 Bob van Luijt on Weaviate and LLMs, Collaborations<br>
14:48 What we have is not yet what the technology is capable of<br>
16:45 Everything is Search!<br>
18:55 The Magic of Machine Learning<br>
20:30 Asking follow up questions<br>
22:28 Meaning in LLMs and RLHF<br>
27:10 How ChatGPT is Evangelizing the Technology<br>
29:45 What is the future of search from a user perspective?<br>
34:38 Integration with Existing Businesses<br>
35:20 Impact on Creativity<br>
37:37 Data Visualization from Natural Language Questions<br>
39:00 Thought experiment - is this notebook an extension of the brain?<br>
42:20 More on “always on” interface<br>
43:42 General vs. Specific Intelligence<br>
45:25 Software Business Impact<br>
48:12 Open-Source Models<br>
49:25 Finding Niches<br>
56:30 Pride in Humans + AI<br>
57:20 Exploring more Prompts<br>
59:20 Personalized Embedding Key<br>
1:02:40 Concluding Thoughts</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Bob-van-Luijt--Chris-Dossman-and-Marco-Bianco-on-the-future-of-search-e1s8aq9</link>
			<guid isPermaLink="false">542ee33b-ad06-4703-b6c0-96ab8edcb655</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 14 Dec 2022 15:00:49 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/62187785/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-14%2F303032216-44100-2-c1511e74785c2.mp3" length="62905363" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #30.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Chapters&lt;/p&gt;
&lt;p&gt;0:00 The future of search!&lt;br&gt;
0:42 Welcome Marco and Chris!&lt;br&gt;
4:28 Solving Hallucination with External Memory LLMs&lt;br&gt;
8:16 Bob van Luijt on Weaviate and LLMs, Collaborations&lt;br&gt;
14:48 What we have is not yet what the technology is capable of&lt;br&gt;
16:45 Everything is Search!&lt;br&gt;
18:55 The Magic of Machine Learning&lt;br&gt;
20:30 Asking follow up questions&lt;br&gt;
22:28 Meaning in LLMs and RLHF&lt;br&gt;
27:10 How ChatGPT is Evangelizing the Technology&lt;br&gt;
29:45 What is the future of search from a user perspective?&lt;br&gt;
34:38 Integration with Existing Businesses&lt;br&gt;
35:20 Impact on Creativity&lt;br&gt;
37:37 Data Visualization from Natural Language Questions&lt;br&gt;
39:00 Thought experiment - is this notebook an extension of the brain?&lt;br&gt;
42:20 More on “always on” interface&lt;br&gt;
43:42 General vs. Specific Intelligence&lt;br&gt;
45:25 Software Business Impact&lt;br&gt;
48:12 Open-Source Models&lt;br&gt;
49:25 Finding Niches&lt;br&gt;
56:30 Pride in Humans + AI&lt;br&gt;
57:20 Exploring more Prompts&lt;br&gt;
59:20 Personalized Embedding Key&lt;br&gt;
1:02:40 Concluding Thoughts&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:05:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Matthijs Douze on Quantization and FAISS]]></title>
			<description><![CDATA[<p>Weaviate Podcast #29. Hey everyone, thank you so much for watching another episode of the Weaviate podcast! This episode features Matthijs Douze, one of the most talented and accomplished scientists we've hosted on the Weaviate podcast! Matthijs has pioneered the use of Product Quantization to compress vector representations and enable even faster and more efficient approximate nearest neighbor vector search. Matthijs told an incredible story about the history of this research, from searching from SIFT vectors for Computer Vision Search applications like real-time CD Cover album search to the problems facing modern IVF-PQ systems and the use of PQ in graph-based HNSW search. This is also a very special episode as Abdel Rodriguez makes his debut on the Weaviate podcast to discuss Weaviate's efforts in integrating PQ support and the unique challenges with this algorithm and the incremental updates required for a Vector Database. On this topic, Etienne Dilocker also returned to discuss the topic of Vector Database vs. Library with Matthijs, who is one of the lead developers of the Faiss library. This was a really information-heavy podcast, please don't hesitate to ask us any questions or present any of your ideas! Thanks again for listening!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Matthijs-Douze-on-Quantization-and-FAISS-e1rhgg3</link>
			<guid isPermaLink="false">153e1ff6-8cdc-4431-8f3f-1caf12fbf7d3</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 30 Nov 2022 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61439939/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-30%2F300150496-44100-2-d14d27cc8b7f3.mp3" length="69631999" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #29. Hey everyone, thank you so much for watching another episode of the Weaviate podcast! This episode features Matthijs Douze, one of the most talented and accomplished scientists we&apos;ve hosted on the Weaviate podcast! Matthijs has pioneered the use of Product Quantization to compress vector representations and enable even faster and more efficient approximate nearest neighbor vector search. Matthijs told an incredible story about the history of this research, from searching from SIFT vectors for Computer Vision Search applications like real-time CD Cover album search to the problems facing modern IVF-PQ systems and the use of PQ in graph-based HNSW search. This is also a very special episode as Abdel Rodriguez makes his debut on the Weaviate podcast to discuss Weaviate&apos;s efforts in integrating PQ support and the unique challenges with this algorithm and the incremental updates required for a Vector Database. On this topic, Etienne Dilocker also returned to discuss the topic of Vector Database vs. Library with Matthijs, who is one of the lead developers of the Faiss library. This was a really information-heavy podcast, please don&apos;t hesitate to ask us any questions or present any of your ideas! Thanks again for listening!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:12:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Maarten Grootendorst on BERTopic]]></title>
			<description><![CDATA[<p>Weaviate Podcast #28. Thank you so much for watching the 28th Weaviate Podcast! This episode features Maarten Grootendorst, developer of the BERTopic python library and an active evangelist of this exciting cluster analysis technology, (Maarten has written some incredible articles here - <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa2c4UTdDUU5HSkd0bGZnS2F2aGE1UEVrdndNQXxBQ3Jtc0ttRl83TWdFNm1SNnhEcjFCWks2aVlqamlpdEY2bUVTQTVHcHE4d3Jtdzd5eV9heC1JazNXeFFSRnVOeWl6cThaZlZxS0lTVnE2ZF85ajVDcDhWNExQbnhidC1zXzZ6N0x3QlU2MzFDcDlPOHl1akEtNA&amp;q=https%3A%2F%2Fmedium.com%2F%40maartengrootendorst%29&amp;v=IwXOaHanfUU" rel="nofollow" target="_blank">https://medium.com/@maartengrootendorst)</a>! In this podcast, Maarten did an incredible job explaining how BERTopic works, with particular details such as k-Means clustering vs. HDBSCAN, Semi-Supervised topic modeling, Dynamic topic modeling, and many more! I was amazed at Maarten's expertise in the miscellaneous details of these algorithms! We are extremely excited about adding BERTopic to Weaviate, please see this proposal if interested in contributing to the discussion: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa0k5a1lGcnF5eFE2dHk4R2JHbElUbGVWREZzZ3xBQ3Jtc0ttam9xWXpBR2otSlhlMGdvNC12blpheTdDUUsxdGVZVWwtcEg3QjRZVEpQWkZZNlVWUjNoeXNmQjNGOHJuSENiVUtwdkVjUk40eEozRzE4ZzQ3a3NUTzVjRG55QjlRNEc3RE1nVkZGanhxMi1QWHVGQQ&amp;q=https%3A%2F%2Fgithub.com%2Fsemi-technologies%2Fweaviate%2Fissues%2F2304&amp;v=IwXOaHanfUU" rel="nofollow" target="_blank">https://github.com/semi-technologies/...</a>!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Maarten-Grootendorst-on-BERTopic-e1ra9nh</link>
			<guid isPermaLink="false">917a11f7-b1cc-446c-84dd-64c0625656eb</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 17 Nov 2022 15:00:39 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61203633/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299226392-44100-2-7c681098d9766.mp3" length="51104704" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #28. Thank you so much for watching the 28th Weaviate Podcast! This episode features Maarten Grootendorst, developer of the BERTopic python library and an active evangelist of this exciting cluster analysis technology, (Maarten has written some incredible articles here - &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqa2c4UTdDUU5HSkd0bGZnS2F2aGE1UEVrdndNQXxBQ3Jtc0ttRl83TWdFNm1SNnhEcjFCWks2aVlqamlpdEY2bUVTQTVHcHE4d3Jtdzd5eV9heC1JazNXeFFSRnVOeWl6cThaZlZxS0lTVnE2ZF85ajVDcDhWNExQbnhidC1zXzZ6N0x3QlU2MzFDcDlPOHl1akEtNA&amp;amp;q=https%3A%2F%2Fmedium.com%2F%40maartengrootendorst%29&amp;amp;v=IwXOaHanfUU&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://medium.com/@maartengrootendorst)&lt;/a&gt;! In this podcast, Maarten did an incredible job explaining how BERTopic works, with particular details such as k-Means clustering vs. HDBSCAN, Semi-Supervised topic modeling, Dynamic topic modeling, and many more! I was amazed at Maarten&apos;s expertise in the miscellaneous details of these algorithms! We are extremely excited about adding BERTopic to Weaviate, please see this proposal if interested in contributing to the discussion: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqa0k5a1lGcnF5eFE2dHk4R2JHbElUbGVWREZzZ3xBQ3Jtc0ttam9xWXpBR2otSlhlMGdvNC12blpheTdDUUsxdGVZVWwtcEg3QjRZVEpQWkZZNlVWUjNoeXNmQjNGOHJuSENiVUtwdkVjUk40eEozRzE4ZzQ3a3NUTzVjRG55QjlRNEc3RE1nVkZGanhxMi1QWHVGQQ&amp;amp;q=https%3A%2F%2Fgithub.com%2Fsemi-technologies%2Fweaviate%2Fissues%2F2304&amp;amp;v=IwXOaHanfUU&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://github.com/semi-technologies/...&lt;/a&gt;!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Michael Goin on Neural Magic]]></title>
			<description><![CDATA[<p>Weaviate Podcast #27. Thank you so much for watching the 27th episode of the Weaviate Podcast! This is truly one of my favorite podcasts we have published so far, I think the way Weaviate and Neural Magic fit together is really exciting! Michael did an amazing job explaining the concepts behind how Neural Magic delivers and tests inference acceleration, as well as the vision for the future of Deep Learning with Sparsity and CPU inference. I really hope you enjoy the podcast, more than happy to answer any questions or entertain any ideas/discussion! Thanks again for watching! Weaviate users can begin using Neural Magic's text vectorization pipeline as a custom text2vec-transformers docker image here - cshorten/experimental-text2vec-neuralmagic. Please note this is an experimental build and we will be releasing the full integration with thorough testing very soon!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Michael-Goin-on-Neural-Magic-e1ra9k4</link>
			<guid isPermaLink="false">836587a2-afae-4cad-b542-70fcf5f895c2</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 26 Oct 2022 14:00:15 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61203524/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299225954-44100-2-085519ef8855.mp3" length="42291199" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #27. Thank you so much for watching the 27th episode of the Weaviate Podcast! This is truly one of my favorite podcasts we have published so far, I think the way Weaviate and Neural Magic fit together is really exciting! Michael did an amazing job explaining the concepts behind how Neural Magic delivers and tests inference acceleration, as well as the vision for the future of Deep Learning with Sparsity and CPU inference. I really hope you enjoy the podcast, more than happy to answer any questions or entertain any ideas/discussion! Thanks again for watching! Weaviate users can begin using Neural Magic&apos;s text vectorization pipeline as a custom text2vec-transformers docker image here - cshorten/experimental-text2vec-neuralmagic. Please note this is an experimental build and we will be releasing the full integration with thorough testing very soon!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:44:03</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jonathan Frankle on MosaicML Cloud]]></title>
			<description><![CDATA[<p>Weaviate Podcast #26. Thank you so much for watching the 26th episode of the Weaviate Podcast! This is another really special episode! Jonathan Frankle is one of the world's experts in Deep Learning and is making incredible advances at MosaicML in efficient Deep Learning training. The headline event is the release of MosaicML Cloud and a set of new cost estimates for GPT language models at different scales (linked below). Jonathan explains that these numbers are a baseline and he predicts they could get to as low as $100K as they seek opportunities for efficiency optimizations. This story has already played out in the realm of ResNet ImageNet training as MosaicML has demolished expectations of how fast we can train these models and it seems highly likely they will do the same for large language model costs. Jonathan and I also discussed the general space of Language Models and their applications, especially discussing their role as Databases in things like the Weaviate Vector Search Engine. We also discussed Self-Ask, Chain-of-thought Prompting, and tool use in Language Models. I had an awesome time picking Jonathan's brain about these topics and I hope you all enjoy the podcast, more than happy to answer any questions or entertain any ideas / discussion! Thanks again for watching! &nbsp;Blog post: GPT-3 Quality for less than $500K - <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazE2cUtocEdHNVd4TjFzMGV0c0F2cks5b3FwUXxBQ3Jtc0ttcVRGSTdtVTI0RUkzSFhfbHFpeHRLOWRYWWhjYjZDXzViNmV1b2pXSmk0UGxOVGg2b0duY0lRamlTSmxUZmNRZmF1M0tUaV9oZER2SGxnanpVZDVpWHk0WGdiVXVLZzd5QThxaEVHMnhvOWhWOWRiMA&amp;q=https%3A%2F%2Fwww.mosaicml.com%2Fblog%2Fgpt-3-quality-for-500k&amp;v=oFyYaZbRviY" rel="nofollow" target="_blank">https://www.mosaicml.com/blog/gpt-3-q...</a></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Jonathan-Frankle-on-MosaicML-Cloud-e1ra9gk</link>
			<guid isPermaLink="false">9c047161-a900-45f3-b1e5-3ed95d651828</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 19 Oct 2022 14:00:26 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61203412/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299225425-44100-2-11abc34d6e992.mp3" length="42892224" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #26. Thank you so much for watching the 26th episode of the Weaviate Podcast! This is another really special episode! Jonathan Frankle is one of the world&apos;s experts in Deep Learning and is making incredible advances at MosaicML in efficient Deep Learning training. The headline event is the release of MosaicML Cloud and a set of new cost estimates for GPT language models at different scales (linked below). Jonathan explains that these numbers are a baseline and he predicts they could get to as low as $100K as they seek opportunities for efficiency optimizations. This story has already played out in the realm of ResNet ImageNet training as MosaicML has demolished expectations of how fast we can train these models and it seems highly likely they will do the same for large language model costs. Jonathan and I also discussed the general space of Language Models and their applications, especially discussing their role as Databases in things like the Weaviate Vector Search Engine. We also discussed Self-Ask, Chain-of-thought Prompting, and tool use in Language Models. I had an awesome time picking Jonathan&apos;s brain about these topics and I hope you all enjoy the podcast, more than happy to answer any questions or entertain any ideas / discussion! Thanks again for watching! &amp;nbsp;Blog post: GPT-3 Quality for less than $500K - &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqazE2cUtocEdHNVd4TjFzMGV0c0F2cks5b3FwUXxBQ3Jtc0ttcVRGSTdtVTI0RUkzSFhfbHFpeHRLOWRYWWhjYjZDXzViNmV1b2pXSmk0UGxOVGg2b0duY0lRamlTSmxUZmNRZmF1M0tUaV9oZER2SGxnanpVZDVpWHk0WGdiVXVLZzd5QThxaEVHMnhvOWhWOWRiMA&amp;amp;q=https%3A%2F%2Fwww.mosaicml.com%2Fblog%2Fgpt-3-quality-for-500k&amp;amp;v=oFyYaZbRviY&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://www.mosaicml.com/blog/gpt-3-q...&lt;/a&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:44:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Erik Bernhardsson and Etienne Dilocker on Vector Search in Production. ]]></title>
			<description><![CDATA[<p>Weaviate Podcast #25. Thank you so much for watching the 25th episode of the Weaviate Podcast! This is a really special episode with Erik Bernhardsson! Erik is one of the early thought leaders on Approximate Nearest Neighbor (ANN) Search, creating the ANNOY library at Spotify. Erik shared incredible insights about vector search at Spotify such as the role of Offline and Online Machine Learning inference and the role of multi-stage re-ranking pipelines. Erik has also done massively impactful work on benchmarking ANN algorithms! We really hope you enjoy the podcast and would be thrilled to answer any questions you have about the conversation topics! Thanks again for watching!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Erik-Bernhardsson-and-Etienne-Dilocker-on-Vector-Search-in-Production-e1ra9er</link>
			<guid isPermaLink="false">df81a66a-e251-4faf-b849-922435eba359</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 06 Oct 2022 14:00:41 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61203355/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299225107-44100-2-d73daaf9755a.mp3" length="42891388" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #25. Thank you so much for watching the 25th episode of the Weaviate Podcast! This is a really special episode with Erik Bernhardsson! Erik is one of the early thought leaders on Approximate Nearest Neighbor (ANN) Search, creating the ANNOY library at Spotify. Erik shared incredible insights about vector search at Spotify such as the role of Offline and Online Machine Learning inference and the role of multi-stage re-ranking pipelines. Erik has also done massively impactful work on benchmarking ANN algorithms! We really hope you enjoy the podcast and would be thrilled to answer any questions you have about the conversation topics! Thanks again for watching!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:44:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Weaviate v1.15 Release with Etienne Dilocker and Dirk Kulawiak]]></title>
			<description><![CDATA[<p>Weaviate Podcast #24. Weaviate v1.15 Release! Thank you so much for checking out the Weaviate podcast -- here is a summary of what is new in Weaviate 1.15: &nbsp;1. Cloud-native backups – allows you to configure your environment to create backups – of selected classes or the whole database – straight into AWS S3, GCS or local filesystem 2. Reduced memory usage - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 3. Better control over Garbage Collector – with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 4. Faster imports for ordered data – by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n) 5. More efficient filtered aggregations – thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 6. Two new distance metrics – with the addition of Hamming and Manhattan distance metrics, you can choose the metric (or a combination of) to best suit your data and use case. 7. Two new Weaviate modules – with the Summarization module, you can summarize any text on the fly, while with the HuggingFace module, you can use compatible transformers from the HuggingFace 8. Other improvements and bug fixes – it goes without saying that with every Weaviate release, we strive to make Weaviate more stable – through bug fixes – and more efficient – through many optimizations. &nbsp;Please check out this awesome blog post from Sebastian Witalec and the team describing these further - <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqblIwV3phdDN3VERFTGJ3YWlIei1NcGdGeE1fUXxBQ3Jtc0tudnpKVHB0ZVFaeF9DZnVfR2ZpWG96aHBLc3lMa25MT1Q3OHJTczB6S2pHWlRORGlrYnIxVVRONGtaZkFlZDM0S05laGVZNHhmMTZwanNzSGhxSWw5VGoyZk1kNnY5YWtuR2RTZkRoT25Kb0NwT0lDMA&amp;q=https%3A%2F%2Fweaviate.io%2Fblog%2F2022%2F09%2FWeaviate-release-1-15.html&amp;v=8lyA3mf7FjY" rel="nofollow" target="_blank">https://weaviate.io/blog/2022/09/Weav...</a>.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Weaviate-v1-15-Release-with-Etienne-Dilocker-and-Dirk-Kulawiak-e1ra9ak</link>
			<guid isPermaLink="false">5d69c65a-6dd1-4bbc-9533-690b134c3ddd</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 08 Sep 2022 14:00:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61203220/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299224670-44100-2-3e0ed1c00d5ea.mp3" length="64223607" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #24. Weaviate v1.15 Release! Thank you so much for checking out the Weaviate podcast -- here is a summary of what is new in Weaviate 1.15: &amp;nbsp;1. Cloud-native backups – allows you to configure your environment to create backups – of selected classes or the whole database – straight into AWS S3, GCS or local filesystem 2. Reduced memory usage - we found new ways to optimize memory usage, reducing RAM usage by 10-30%. 3. Better control over Garbage Collector – with the introduction of GOMEMLIMIT we gained more control over the garbage collector, which significantly reduced the chances of OOM kills for your Weaviate setups. 4. Faster imports for ordered data – by extending the Binary Search Tree structure with a self-balancing Red-black tree, we were able to speed up imports from O(n) to O(log n) 5. More efficient filtered aggregations – thanks to optimization to a library reading binary data, filtered aggregations are now 10-20 faster and require a lot less memory. 6. Two new distance metrics – with the addition of Hamming and Manhattan distance metrics, you can choose the metric (or a combination of) to best suit your data and use case. 7. Two new Weaviate modules – with the Summarization module, you can summarize any text on the fly, while with the HuggingFace module, you can use compatible transformers from the HuggingFace 8. Other improvements and bug fixes – it goes without saying that with every Weaviate release, we strive to make Weaviate more stable – through bug fixes – and more efficient – through many optimizations. &amp;nbsp;Please check out this awesome blog post from Sebastian Witalec and the team describing these further - &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqblIwV3phdDN3VERFTGJ3YWlIei1NcGdGeE1fUXxBQ3Jtc0tudnpKVHB0ZVFaeF9DZnVfR2ZpWG96aHBLc3lMa25MT1Q3OHJTczB6S2pHWlRORGlrYnIxVVRONGtaZkFlZDM0S05laGVZNHhmMTZwanNzSGhxSWw5VGoyZk1kNnY5YWtuR2RTZkRoT25Kb0NwT0lDMA&amp;amp;q=https%3A%2F%2Fweaviate.io%2Fblog%2F2022%2F09%2FWeaviate-release-1-15.html&amp;amp;v=8lyA3mf7FjY&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://weaviate.io/blog/2022/09/Weav...&lt;/a&gt;.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:06:53</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Ori Ram on Learning to Retrieve Passages without Supervision]]></title>
			<description><![CDATA[<p>Weaviate Podcast #23. Thank you so much for watching the 23rd episode of the Weaviate Podcast! This episode dives into a new technique for Self-Supervised retrieval in NLP with some incredible results!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Ori-Ram-on-Learning-to-Retrieve-Passages-without-Supervision-e1ra924</link>
			<guid isPermaLink="false">82f4acd4-bafb-42d8-8b85-8f2cf64c0e69</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 31 Aug 2022 14:00:13 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61202948/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299224069-44100-2-2ad54f9ed4cec.mp3" length="59643610" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #23. Thank you so much for watching the 23rd episode of the Weaviate Podcast! This episode dives into a new technique for Self-Supervised retrieval in NLP with some incredible results!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:07</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Yaoshiang Ho on Masterful AI]]></title>
			<description><![CDATA[<p>Weaviate Podcast #22. Thank you so much for watching the 22nd Weaviate Podcast with Yaoshiang Ho! Yaoshiang is a Co-Founder of Masterful AI, a company doing incredible work in the Computer Vision model training and deployment space (<a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa2M3N1ZJYURhZF9wcFBsRXV2c1gybW5fX043QXxBQ3Jtc0tudkdjdG1OXzF3Yk5mV2U5dV9wa1V3OFB2TmhHdjYwczU1RWFwbFhaMmVLNTctVmI2Vi1NRTgwS2lqZW12bTVGYVJ5Vl82VElFS05JVnhuNUw2NFBFaFFkcFZSMmdsN0JDcmlIWTJ5cW1CUjN3MFotZw&amp;q=https%3A%2F%2Fwww.masterfulai.com%2F%29&amp;v=dHhQjlrLu9k" rel="nofollow" target="_blank">https://www.masterfulai.com/)</a>. I really hope you enjoy this podcast, Yaoshiang and I went deep into some of the cutting edge Computer Vision algorithms such as Noisy Student, SimCLR, and Barlow Twins -- as well as the broader topic of Semi-Supervised Learning in which we have a small labeled dataset and a large unlabelled dataset. I am so excited about model training tools like Masterful and the integration with the Weaviate Vector Search Indexes and other Database features! More than happy to answer any questions/host any discussion on topics mentioned in the podcast! Please hit the like and subscribe to support more content like this!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Yaoshiang-Ho-on-Masterful-AI-e1ra8t3</link>
			<guid isPermaLink="false">f91b5df3-e412-46a5-a1ad-9bead71711ab</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 10 Aug 2022 14:00:26 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61202787/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299223280-44100-2-522899106626c.mp3" length="55448972" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #22. Thank you so much for watching the 22nd Weaviate Podcast with Yaoshiang Ho! Yaoshiang is a Co-Founder of Masterful AI, a company doing incredible work in the Computer Vision model training and deployment space (&lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqa2M3N1ZJYURhZF9wcFBsRXV2c1gybW5fX043QXxBQ3Jtc0tudkdjdG1OXzF3Yk5mV2U5dV9wa1V3OFB2TmhHdjYwczU1RWFwbFhaMmVLNTctVmI2Vi1NRTgwS2lqZW12bTVGYVJ5Vl82VElFS05JVnhuNUw2NFBFaFFkcFZSMmdsN0JDcmlIWTJ5cW1CUjN3MFotZw&amp;amp;q=https%3A%2F%2Fwww.masterfulai.com%2F%29&amp;amp;v=dHhQjlrLu9k&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://www.masterfulai.com/)&lt;/a&gt;. I really hope you enjoy this podcast, Yaoshiang and I went deep into some of the cutting edge Computer Vision algorithms such as Noisy Student, SimCLR, and Barlow Twins -- as well as the broader topic of Semi-Supervised Learning in which we have a small labeled dataset and a large unlabelled dataset. I am so excited about model training tools like Masterful and the integration with the Weaviate Vector Search Indexes and other Database features! More than happy to answer any questions/host any discussion on topics mentioned in the podcast! Please hit the like and subscribe to support more content like this!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Laura Ham on Weaviate User Experience]]></title>
			<description><![CDATA[<p>Weaviate Podcast #21. Thank you for watching the 21st Weaviate Podcast with Laura Ham! Laura Ham has worked on Weaviate at SeMI Technologies for a little over 5 years. She has had a heavy influence on all things from the GraphQL User Experience design to the Graph data model, and the creation of educational content! I really enjoyed this podcast, please see the list of topics under “chapters”! Here are some examples of recent coding tutorial videos Laura has made on “How to add custom modules to Weaviate” and integrations of Weaviate with Jina AI and Haystack.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Laura-Ham-on-Weaviate-User-Experience-e1ra8k1</link>
			<guid isPermaLink="false">bcb04c8c-817a-48d1-a3af-262c382da3ce</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 27 Jul 2022 14:00:25 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61202497/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299222220-44100-2-9113ad6ac679f.mp3" length="50815058" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #21. Thank you for watching the 21st Weaviate Podcast with Laura Ham! Laura Ham has worked on Weaviate at SeMI Technologies for a little over 5 years. She has had a heavy influence on all things from the GraphQL User Experience design to the Graph data model, and the creation of educational content! I really enjoyed this podcast, please see the list of topics under “chapters”! Here are some examples of recent coding tutorial videos Laura has made on “How to add custom modules to Weaviate” and integrations of Weaviate with Jina AI and Haystack.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:52:55</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Tuana Celik on Question Answering with Haystack]]></title>
			<description><![CDATA[<p>Weaviate Podcast #20. Tuana Celik, a Developer Advocate at Deepset, presented many exciting ideas around Question Answering! We began with her Game of Thrones Question Answering Demo on HuggingFace Spaces and continued to discuss all topics QA from Extractive to Abstractive, benefits of Retrieve-then-Read, and Zero-Shot Generalization, to give a quick preview. For our Weaviate users, please check out this demo from Laura Ham on how to use Haystack QA in tandem with the Weaviate Vector Search Database: <a href="https://www.youtube.com/watch?v=BkozaOnZpJI&amp;t=0s">https://www.youtube.com/watch?v=Bkoza...</a>. I really hope you enjoy this podcast, please don't forget to check the Chapters to see if any topics appeal particularly to what you are working on! Please also see the links below with referenced content in the podcast!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Tuana-Celik-on-Question-Answering-with-Haystack-e1ra8fr</link>
			<guid isPermaLink="false">54f47042-7852-421f-ad51-1e2111d5b9a5</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 13 Jul 2022 14:00:55 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61202363/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299221738-44100-2-e89a87b7e9c1c.mp3" length="44966973" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #20. Tuana Celik, a Developer Advocate at Deepset, presented many exciting ideas around Question Answering! We began with her Game of Thrones Question Answering Demo on HuggingFace Spaces and continued to discuss all topics QA from Extractive to Abstractive, benefits of Retrieve-then-Read, and Zero-Shot Generalization, to give a quick preview. For our Weaviate users, please check out this demo from Laura Ham on how to use Haystack QA in tandem with the Weaviate Vector Search Database: &lt;a href=&quot;https://www.youtube.com/watch?v=BkozaOnZpJI&amp;amp;t=0s&quot;&gt;https://www.youtube.com/watch?v=Bkoza...&lt;/a&gt;. I really hope you enjoy this podcast, please don&apos;t forget to check the Chapters to see if any topics appeal particularly to what you are working on! Please also see the links below with referenced content in the podcast!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:46:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Etienne Dilocker on Weaviate v1.14 Release!]]></title>
			<description><![CDATA[<p>Weaviate Podcast #19. SeMI Technologies Co-Founder and CTO Etienne Dilocker returns to the Weaviate podcast to describe what's new with Weaviate v1.14! Please see the chapter outlines if you would like to skip ahead to the update most relevant to you! Please also see this blog post lead by Sebastian Witalec describing the new changes to Weaviate! &nbsp;Weaviate v1.14 Blog Post: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbTB0azRwWjdqTll3VmFzWlJqbkYtbkxDaEdYQXxBQ3Jtc0tsUElNNWVfZkxPS2lqUnFPbXNwc05tX3RjWlNhR0NzZ252WGtKNTE1VG5oUkVWVkZaT29DVFVRMGxRMFFIaEtYSEhXQ2tjWDJLRWlSOE1QNDAxYTMtd2JrTk9PZEpuT0pPTWphZGktUXFaWmZ3OUlydw&amp;q=https%3A%2F%2Fweaviate.io%2Fblog%2F2022%2F07%2FWeaviate-release-1-14.html&amp;v=eiQaZIhUS_o" rel="nofollow" target="_blank">https://weaviate.io/blog/2022/07/Weav...</a></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Etienne-Dilocker-on-Weaviate-v1-14-Release-e1ra8nl</link>
			<guid isPermaLink="false">c9e981c2-572c-4eea-ad2f-05441925834c</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Fri, 08 Jul 2022 14:00:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61202613/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299222616-44100-2-cc57092b302d4.mp3" length="45916159" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #19. SeMI Technologies Co-Founder and CTO Etienne Dilocker returns to the Weaviate podcast to describe what&apos;s new with Weaviate v1.14! Please see the chapter outlines if you would like to skip ahead to the update most relevant to you! Please also see this blog post lead by Sebastian Witalec describing the new changes to Weaviate! &amp;nbsp;Weaviate v1.14 Blog Post: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbTB0azRwWjdqTll3VmFzWlJqbkYtbkxDaEdYQXxBQ3Jtc0tsUElNNWVfZkxPS2lqUnFPbXNwc05tX3RjWlNhR0NzZ252WGtKNTE1VG5oUkVWVkZaT29DVFVRMGxRMFFIaEtYSEhXQ2tjWDJLRWlSOE1QNDAxYTMtd2JrTk9PZEpuT0pPTWphZGktUXFaWmZ3OUlydw&amp;amp;q=https%3A%2F%2Fweaviate.io%2Fblog%2F2022%2F07%2FWeaviate-release-1-14.html&amp;amp;v=eiQaZIhUS_o&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://weaviate.io/blog/2022/07/Weav...&lt;/a&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:47:49</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Vincent D. Warmerdam on Applications of Nearest Neighbor Search]]></title>
			<description><![CDATA[<p>Weaviate Podcast #18. Thank you for watching the 18th Weaviate Podcast with Vincent D. Warmerdam! Vincent is an engineer at Spacy working on exciting tools such as Prodigy! Vincent describes how nearest neighbor search can aid in tasks such as Data De-Duplication and Data Labeling! Vincent shared many interesting ideas from representations of text, challenges with annotator disagreement, lessons from hosting data labeling workshops to train data scientists, and many more!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Vincent-D--Warmerdam-on-Applications-of-Nearest-Neighbor-Search-e1ra7ss</link>
			<guid isPermaLink="false">6ef64cf1-44e4-419f-bd6c-cdb82095b968</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 28 Jun 2022 14:00:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61201756/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299219780-44100-2-a21c97b9cdff9.mp3" length="55459003" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #18. Thank you for watching the 18th Weaviate Podcast with Vincent D. Warmerdam! Vincent is an engineer at Spacy working on exciting tools such as Prodigy! Vincent describes how nearest neighbor search can aid in tasks such as Data De-Duplication and Data Labeling! Vincent shared many interesting ideas from representations of text, challenges with annotator disagreement, lessons from hosting data labeling workshops to train data scientists, and many more!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Kyle Lo on Scientific Literature Mining]]></title>
			<description><![CDATA[<p>Weaviate Podcast #17. Thank you for watching the 17th Weaviate Podcast with Kyle Lo! Vector Search enables us to find semantically similar items in large collections. Scientific Literature Mining is an extremely interesting case of this where we search through enormous collections of scientific papers to find evidence and ideas. Kyle has an extremely impressive resume in this application domain, tackling tasks such as Question Answering, Summarization, Fact Verification, and more! We really hope you enjoy the lessons Kyle describes from building these systems. Further, we hope that this inspires excitement in your own Vector Search applications!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Kyle-Lo-on-Scientific-Literature-Mining-e1ra7vg</link>
			<guid isPermaLink="false">e691dd57-4116-41ef-a97b-8a61c1a99629</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 31 May 2022 14:00:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61201840/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299220213-44100-2-a8a1446acc7a3.mp3" length="64331440" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #17. Thank you for watching the 17th Weaviate Podcast with Kyle Lo! Vector Search enables us to find semantically similar items in large collections. Scientific Literature Mining is an extremely interesting case of this where we search through enormous collections of scientific papers to find evidence and ideas. Kyle has an extremely impressive resume in this application domain, tackling tasks such as Question Answering, Summarization, Fact Verification, and more! We really hope you enjoy the lessons Kyle describes from building these systems. Further, we hope that this inspires excitement in your own Vector Search applications!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:00</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Etienne Dilocker on ANN Benchmarks]]></title>
			<description><![CDATA[<p>Weaviate Podcast #16. ANN Benchmarks are a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. Etienne Dilocker, the CTO of SeMI Technologies, the company behind Weaviate shares some insight knowledge about this interesting topic.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Etienne-Dilocker-on-ANN-Benchmarks-e1ra5uo</link>
			<guid isPermaLink="false">c3313771-65d4-45ee-b1eb-108ac74b0a7c</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 24 May 2022 14:00:41 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61199768/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299212643-44100-2-65be83e2947e5.mp3" length="43336932" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #16. ANN Benchmarks are a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. Etienne Dilocker, the CTO of SeMI Technologies, the company behind Weaviate shares some insight knowledge about this interesting topic.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:45:08</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Maximilian Werk on Jina AI's Neural Search Framework]]></title>
			<description><![CDATA[<p>Weaviate Podcast #15. Weaviate is used as a database for Jina AI's Neural Search Framework. In this podcast, Maximilian Werk, Engineering Director at Jina AI, will talk about all things related to this neural search framework together with Connor Shorten. Also, Maximilian will give a Jina Example Walkthrough... Enjoy!!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Maximilian-Werk-on-Jina-AIs-Neural-Search-Framework-e1ra5rf</link>
			<guid isPermaLink="false">e1325096-8b53-4817-a6ac-75eaff4fe64f</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 03 May 2022 14:00:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61199663/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299212395-44100-2-caf1de0cc02f7.mp3" length="68391078" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #15. Weaviate is used as a database for Jina AI&apos;s Neural Search Framework. In this podcast, Maximilian Werk, Engineering Director at Jina AI, will talk about all things related to this neural search framework together with Connor Shorten. Also, Maximilian will give a Jina Example Walkthrough... Enjoy!!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:11:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[UNC research team on VL Adapter for Efficient CLIP Transfer]]></title>
			<description><![CDATA[<p>Weaviate Podcast #14. Thanks for watching the Weaviate podcast! Our 14th episode welcomes Yi-Lin Sung, Jaemin Cho, and Professor Mohit Bansal, a research team from UNC! Our guests present their work on VL Adapter, a technique to achieve full fine-tuning performance while only updating 4% of original parameters!! This is an incredibly interesting finding for the sake of cost-effective tuning of Vision and Language models based on CLIP. We additionally discussed topics around compression bottlenecks in neural architectures, V&amp;L datasets, and the tricky question of compositional generalization. If you are curious about using CLIP in Weaviate, please check out this text-to-image search example with Unsplash images and a React frontend!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/UNC-research-team-on-VL-Adapter-for-Efficient-CLIP-Transfer-e1ra5lp</link>
			<guid isPermaLink="false">a2d66a69-6763-4a67-a9be-443ba216f6e5</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 26 Apr 2022 14:00:46 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61199481/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299211447-44100-2-0f76d004e39ce.mp3" length="27225442" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #14. Thanks for watching the Weaviate podcast! Our 14th episode welcomes Yi-Lin Sung, Jaemin Cho, and Professor Mohit Bansal, a research team from UNC! Our guests present their work on VL Adapter, a technique to achieve full fine-tuning performance while only updating 4% of original parameters!! This is an incredibly interesting finding for the sake of cost-effective tuning of Vision and Language models based on CLIP. We additionally discussed topics around compression bottlenecks in neural architectures, V&amp;amp;L datasets, and the tricky question of compositional generalization. If you are curious about using CLIP in Weaviate, please check out this text-to-image search example with Unsplash images and a React frontend!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:28:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Data Science with Rick Lamers from Orchest]]></title>
			<description><![CDATA[<p>Weaviate Podcast #13. Rick Lamers, CEO, and Founder of Orchest.io. Orchest is a tool targeted at data scientists and this software simplifies building data pipelines.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Data-Science-with-Rick-Lamers-from-Orchest-e1ra547</link>
			<guid isPermaLink="false">8b417143-d17b-478a-8394-dd06788f64da</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 05 Apr 2022 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198919/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299209947-44100-2-027e1f17c5a7a.mp3" length="45415861" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #13. Rick Lamers, CEO, and Founder of Orchest.io. Orchest is a tool targeted at data scientists and this software simplifies building data pipelines.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:47:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jonathan Frankle, Research Scientist in Deep Learning]]></title>
			<description><![CDATA[<p>Weaviate Podcast #12. Please check out Composer from MosaicML! <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazhhSXBfRE9PWlRqbTFUc2cySmVkVWcyOXNkZ3xBQ3Jtc0tucXFPUnAyUmdXUzlzbjhaTGtuM3hvRncwUnRFZDlsdVpURXZ5UmstbzV6dXpoN2dLT2hvVjBNV1lqOUZWRTVYWjhyLXp2amJBUDA3VE84OVV1YmdiaVRyUGJlMXZhczY1bS1fRWtPenVxVTNWUms0VQ&amp;q=https%3A%2F%2Fgithub.com%2Fmosaicml%2Fcomposer&amp;v=ZiBkspwrICA" rel="nofollow" target="_blank">https://github.com/mosaicml/composer</a> Jonathan Frankle is the Chief Scientist at MosaicML and a PhD student in Machine Learning at MIT. Jonathan is the first author of “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks” which won an ICLR best paper award. You can learn more about Jonathan Frankle here: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbGhnMDctZzVyYkxWWUJONkRHRXd0NVZRQ3NvQXxBQ3Jtc0tsQ0x0QXNDTGxRLVk2N3pjSnR1UVJTeVhqbml3d1BDQ21WaU1CY1E3clRLelRVdUZJTkFTV3NxZno1T1RlY1djaXVnZHlJbS0zMk84aTR5UGhySFkxUzUzRHVRaFlXN1drb0Jsa21JdktVZDNmRnZVbw&amp;q=http%3A%2F%2Fwww.jfrankle.com%2F&amp;v=ZiBkspwrICA" rel="nofollow" target="_blank">http://www.jfrankle.com/</a>. &nbsp;Here is an explanation of how to use the Python library of Composer: <a href="https://www.youtube.com/watch?v=Xi_5wq2MpOw&amp;t=0s">https://www.youtube.com/watch?v=Xi_5w...</a></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Jonathan-Frankle--Research-Scientist-in-Deep-Learning-e1ra4uk</link>
			<guid isPermaLink="false">9b7e8c9b-8d88-4888-8c25-5e693329ccce</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 29 Mar 2022 14:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198740/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299209382-44100-2-4c0cc09e94112.mp3" length="57760286" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #12. Please check out Composer from MosaicML! &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqazhhSXBfRE9PWlRqbTFUc2cySmVkVWcyOXNkZ3xBQ3Jtc0tucXFPUnAyUmdXUzlzbjhaTGtuM3hvRncwUnRFZDlsdVpURXZ5UmstbzV6dXpoN2dLT2hvVjBNV1lqOUZWRTVYWjhyLXp2amJBUDA3VE84OVV1YmdiaVRyUGJlMXZhczY1bS1fRWtPenVxVTNWUms0VQ&amp;amp;q=https%3A%2F%2Fgithub.com%2Fmosaicml%2Fcomposer&amp;amp;v=ZiBkspwrICA&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://github.com/mosaicml/composer&lt;/a&gt; Jonathan Frankle is the Chief Scientist at MosaicML and a PhD student in Machine Learning at MIT. Jonathan is the first author of “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks” which won an ICLR best paper award. You can learn more about Jonathan Frankle here: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbGhnMDctZzVyYkxWWUJONkRHRXd0NVZRQ3NvQXxBQ3Jtc0tsQ0x0QXNDTGxRLVk2N3pjSnR1UVJTeVhqbml3d1BDQ21WaU1CY1E3clRLelRVdUZJTkFTV3NxZno1T1RlY1djaXVnZHlJbS0zMk84aTR5UGhySFkxUzUzRHVRaFlXN1drb0Jsa21JdktVZDNmRnZVbw&amp;amp;q=http%3A%2F%2Fwww.jfrankle.com%2F&amp;amp;v=ZiBkspwrICA&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;http://www.jfrankle.com/&lt;/a&gt;. &amp;nbsp;Here is an explanation of how to use the Python library of Composer: &lt;a href=&quot;https://www.youtube.com/watch?v=Xi_5wq2MpOw&amp;amp;t=0s&quot;&gt;https://www.youtube.com/watch?v=Xi_5w...&lt;/a&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:09</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[CEO Han Xiao From Jina AI]]></title>
			<description><![CDATA[<p>Weaviate Podcast #11. You can now use Weaviate as the document store for DocumentArray in Jina AI. We had the pleasure to talk with their CEO Han Xiao. See the timestamps below what it is all about, or check out the recap from Henry AI!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/CEO-Han-Xiao-From-Jina-AI-e1ra4qa</link>
			<guid isPermaLink="false">99c1c139-7dff-42ba-8ea6-0e8607accfa8</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 15 Mar 2022 15:00:39 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198602/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299208780-44100-2-3a623d37a7ed8.mp3" length="74398823" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #11. You can now use Weaviate as the document store for DocumentArray in Jina AI. We had the pleasure to talk with their CEO Han Xiao. See the timestamps below what it is all about, or check out the recap from Henry AI!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:17:29</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Yury Malkov and Etienne Dilocker about HNSW in Vector Search and Weaviate]]></title>
			<description><![CDATA[<p>Weaviate Podcast #10. A guided conversation about HNSW by Connor Shorten between Yury Malkov, Staff ML Engineer at Twitter and the co-inventor of HNSW, and Etienne Dilocker, the co-founder of Weaviate. Check the timestamps below!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Yury-Malkov-and-Etienne-Dilocker-about-HNSW-in-Vector-Search-and-Weaviate-e1ra4ns</link>
			<guid isPermaLink="false">f4e2a4eb-ecd7-48c0-a52b-1dacbb6d10a4</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 10 Mar 2022 15:00:18 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198524/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299208303-44100-2-4f8ccbd7bcdc1.mp3" length="58221713" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #10. A guided conversation about HNSW by Connor Shorten between Yury Malkov, Staff ML Engineer at Twitter and the co-inventor of HNSW, and Etienne Dilocker, the co-founder of Weaviate. Check the timestamps below!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:38</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Karen Beckers about The Role of Vector Search in eCommerce]]></title>
			<description><![CDATA[<p>Weaviate Podcast #9. Karen Beckers, Data Scientist from Squadra Machine Learning Company, gives insightful information about how to use vector search in eCommerce in this podcast with Connor Shorten. Some topics are image-based datasets, vector search for data scientists, the future of eCommerce, and many more! See the timestamps below for more information.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Karen-Beckers-about-The-Role-of-Vector-Search-in-eCommerce-e1ra4j6</link>
			<guid isPermaLink="false">d7ff72f8-59d7-4600-9390-8a80008e50cd</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Thu, 03 Mar 2022 15:00:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198374/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299207748-44100-2-19383666f3059.mp3" length="51537710" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #9. Karen Beckers, Data Scientist from Squadra Machine Learning Company, gives insightful information about how to use vector search in eCommerce in this podcast with Connor Shorten. Some topics are image-based datasets, vector search for data scientists, the future of eCommerce, and many more! See the timestamps below for more information.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:41</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Brady Neal about Causal Inference in Vector Search]]></title>
			<description><![CDATA[<p>Weaviate Podcast #8. Brady Neal from Oogway talks with Connor Shorten from Henry AI Labs about causal inference and many more. See the timestamps below to check out what this podcast is all about.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Brady-Neal-about-Causal-Inference-in-Vector-Search-e1ra4dk</link>
			<guid isPermaLink="false">3d5e32ce-91c2-4ecf-ad9b-d5ff9ec118b2</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 28 Feb 2022 15:00:06 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198196/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299207205-44100-2-25b420f0a1605.mp3" length="77008142" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #8. Brady Neal from Oogway talks with Connor Shorten from Henry AI Labs about causal inference and many more. See the timestamps below to check out what this podcast is all about.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:20:12</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Arvind Neelakantan of OpenAI • Embeddings API in Weaviate]]></title>
			<description><![CDATA[<p>Weaviate Podcast #7. Arvind Neelakantan, Research Lead at Open AI, talks with Connor Shorten about their newly released embeddings API, his work at Open AI, the integration into Weaviate, and more.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Arvind-Neelakantan-of-OpenAI--Embeddings-API-in-Weaviate-e1ra480</link>
			<guid isPermaLink="false">16d22732-ad3b-409a-bed0-705639a9b78a</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Fri, 11 Feb 2022 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61198016/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299206543-44100-2-d48b35586edf.mp3" length="45731839" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #7. Arvind Neelakantan, Research Lead at Open AI, talks with Connor Shorten about their newly released embeddings API, his work at Open AI, the integration into Weaviate, and more.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:47:38</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[How Zencastr Searches through their Podcast Transcriptions with Weaviate]]></title>
			<description><![CDATA[<p>Weaviate Podcast #6. Alex Cannan, a Machine Learning engineer at Zencastr, talks with Connor Shorten about a really exciting use case of applying search to look through podcast transcription. Topics discussed are the need for fine-tuning, building your own vector database versus Weaviate, data privacy for Deep Learning applications, and many more!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/How-Zencastr-Searches-through-their-Podcast-Transcriptions-with-Weaviate-e1ra433</link>
			<guid isPermaLink="false">4a1a1453-e6d0-4d0b-9da0-d6bd49e6023e</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Wed, 02 Feb 2022 15:00:01 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61197859/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299205854-44100-2-f61d53dc025f4.mp3" length="53343293" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #6. Alex Cannan, a Machine Learning engineer at Zencastr, talks with Connor Shorten about a really exciting use case of applying search to look through podcast transcription. Topics discussed are the need for fine-tuning, building your own vector database versus Weaviate, data privacy for Deep Learning applications, and many more!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[How The Knowledge Management Bot Katie leverages Weaviate]]></title>
			<description><![CDATA[<p>Weaviate Podcast #5. Katie is a knowledge management bot, continuously improving, self-learning, and trained by humans. Under the hood, Katie is powered by the Weaviate vector search engine, during this podcast, Katie's Michael Wechner will talk about all things vector search and more!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/How-The-Knowledge-Management-Bot-Katie-leverages-Weaviate-e1ra2vb</link>
			<guid isPermaLink="false">965dcd8d-848a-4516-beaa-76c3f06c59d2</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Fri, 21 Jan 2022 15:00:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61196715/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299205576-44100-2-6668c9115808d.mp3" length="82183731" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #5. Katie is a knowledge management bot, continuously improving, self-learning, and trained by humans. Under the hood, Katie is powered by the Weaviate vector search engine, during this podcast, Katie&apos;s Michael Wechner will talk about all things vector search and more!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:25:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[On Deepset's Haystack and how they leverage The Weaviate Vector Search Engine]]></title>
			<description><![CDATA[<p>Weaviate Podcast #4. NLP frameworks like Deepset's Haystack are powerful tools to help data scientists and software engineers work with the latest and greatest in natural language processing. In this interview, Malte Pietsch will be talking about Haystack and how they leverage the Weaviate vector search engine as a persistent storage engine for their data and vector representations.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/On-Deepsets-Haystack-and-how-they-leverage-The-Weaviate-Vector-Search-Engine-e1ra2ko</link>
			<guid isPermaLink="false">b2da6538-f4a5-4816-ac80-cc0359d21d7a</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Tue, 11 Jan 2022 15:00:02 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61196376/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299203770-44100-2-3540791057f.mp3" length="54828302" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #4. NLP frameworks like Deepset&apos;s Haystack are powerful tools to help data scientists and software engineers work with the latest and greatest in natural language processing. In this interview, Malte Pietsch will be talking about Haystack and how they leverage the Weaviate vector search engine as a persistent storage engine for their data and vector representations.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[A Vision for The Future of Vector Search]]></title>
			<description><![CDATA[<p>Weaviate Podcast #3. Join ‪Connor Shorten and Bob van Luijt (SeMI Technologies) for the third Weaviate vector search engine Podcast. During the show, they will be discussing use cases, the GraphQL API, knowledge graphs, Weaviate as a product, vector search engine use cases, and a vision for the future of vector search.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/A-Vision-for-The-Future-of-Vector-Search-e1r9voi</link>
			<guid isPermaLink="false">874604a7-0c9b-4a28-a80f-d1d9a239f579</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 20 Dec 2021 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61193426/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299194194-44100-2-02c38311a83b8.mp3" length="57619016" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #3. Join ‪Connor Shorten and Bob van Luijt (SeMI Technologies) for the third Weaviate vector search engine Podcast. During the show, they will be discussing use cases, the GraphQL API, knowledge graphs, Weaviate as a product, vector search engine use cases, and a vision for the future of vector search.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:01</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/34794122/34794122-1669387908946-be7ea7955a1f6.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[How Keenious uses Weaviate to Enable Semantic Search through 60M+ Academic PUBs]]></title>
			<description><![CDATA[<p>Weaviate Podcast #2. Join ‪Connor Shorten (Henry AI Labs) and Charles Pierse (Keenious) for the second Weaviate vector search engine Podcast. During the show, they will be discussing how Keenious uses Weaviate and broader, all things NLP!</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/How-Keenious-uses-Weaviate-to-Enable-Semantic-Search-through-60M-Academic-PUBs-e1ra1tf</link>
			<guid isPermaLink="false">6e149d80-0b42-4fa2-b7fc-f7acd1905e78</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Mon, 13 Dec 2021 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61195631/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299200983-44100-2-c161cf02b1d1b.mp3" length="80105220" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #2. Join ‪Connor Shorten (Henry AI Labs) and Charles Pierse (Keenious) for the second Weaviate vector search engine Podcast. During the show, they will be discussing how Keenious uses Weaviate and broader, all things NLP!&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:23:26</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Community and Weaviate Core Update]]></title>
			<description><![CDATA[<p>Weaviate Podcast #1. Join ‪Connor Shorten and Etienne Dilocker (SeMI Technologies) for the first Weaviate Podcast. During the show, they will be discussing Weaviate's horizontal scalability features in the v1.8.0 release and a wide variety of topics surrounding the Weaviate Slack channel.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/weaviate/episodes/Community-and-Weaviate-Core-Update-e1ra2e7</link>
			<guid isPermaLink="false">1fbb6d12-84a2-492b-b98b-a32bfc321118</guid>
			<dc:creator><![CDATA[Weaviate]]></dc:creator>
			<pubDate>Sun, 05 Dec 2021 15:00:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/cffc3468/podcast/play/61196167/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-25%2F299202951-44100-2-42b171dc2c9d3.mp3" length="45039280" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Weaviate Podcast #1. Join ‪Connor Shorten and Etienne Dilocker (SeMI Technologies) for the first Weaviate Podcast. During the show, they will be discussing Weaviate&apos;s horizontal scalability features in the v1.8.0 release and a wide variety of topics surrounding the Weaviate Slack channel.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:46:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/34794122/34794122-1669388195788-2433cc8b9a1d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
	</channel>
</rss>