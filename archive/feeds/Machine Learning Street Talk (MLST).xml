<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:anchor="https://anchor.fm/xmlns" xmlns:podcast="https://podcastindex.org/namespace/1.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:psc="http://podlove.org/simple-chapters">
	<channel>
		<title><![CDATA[Machine Learning Street Talk (MLST)]]></title>
		<description><![CDATA[Welcome! We engage in fascinating discussions with pre-eminent figures in the AI field. Our flagship show covers current affairs in AI, cognitive science, neuroscience and philosophy of mind with in-depth analysis. Our approach is unrivalled in terms of scope and rigour – we believe in intellectual diversity in AI, and we touch on all of the main ideas in the field with the hype surgically removed. MLST is run by Tim Scarfe, Ph.D (https://www.linkedin.com/in/ecsquizor/) and features regular appearances from MIT Doctor of Philosophy Keith Duggar (https://www.linkedin.com/in/dr-keith-duggar/). ]]></description>
		<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk</link>
		<generator>Anchor Podcasts</generator>
		<lastBuildDate>Mon, 09 Jun 2025 21:30:38 GMT</lastBuildDate>
		<atom:link href="https://anchor.fm/s/1e4a0eac/podcast/rss" rel="self" type="application/rss+xml"/>
		<author><![CDATA[Machine Learning Street Talk (MLST)]]></author>
		<copyright><![CDATA[Machine Learning Street Talk (MLST)]]></copyright>
		<language><![CDATA[en]]></language>
		<atom:link rel="hub" href="https://pubsubhubbub.appspot.com/"/>
		<itunes:author>Machine Learning Street Talk (MLST)</itunes:author>
		<itunes:summary>Welcome! We engage in fascinating discussions with pre-eminent figures in the AI field. Our flagship show covers current affairs in AI, cognitive science, neuroscience and philosophy of mind with in-depth analysis. Our approach is unrivalled in terms of scope and rigour – we believe in intellectual diversity in AI, and we touch on all of the main ideas in the field with the hype surgically removed. MLST is run by Tim Scarfe, Ph.D (https://www.linkedin.com/in/ecsquizor/) and features regular appearances from MIT Doctor of Philosophy Keith Duggar (https://www.linkedin.com/in/dr-keith-duggar/). </itunes:summary>
		<itunes:type>episodic</itunes:type>
		<itunes:owner>
			<itunes:name>Machine Learning Street Talk (MLST)</itunes:name>
			<itunes:email>podcasts33+1e4a0eac@anchor.fm</itunes:email>
		</itunes:owner>
		<itunes:explicit>false</itunes:explicit>
		<itunes:category text="Technology"/>
		<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
		<item>
			<title><![CDATA["Blurring Reality" - Chai's Social AI Platform (SPONSORED)]]></title>
			<description><![CDATA[<p>&quot;Blurring Reality&quot; - Chai&#39;s Social AI Platform - <strong>sponsored</strong></p><p><br></p><p>This episode of MLST explores the groundbreaking work of Chai, a social AI platform that quietly built one of the world&#39;s largest AI companion ecosystems before ChatGPT&#39;s mainstream adoption. With over 10 million active users and just 13 engineers serving 2 trillion tokens per day, Chai discovered the massive appetite for AI companionship through serendipity while searching for product-market fit.</p><p><br></p><p>CHAI sponsored this show *because they want to hire amazing engineers* -- </p><p><br></p><p><strong>CAREER OPPORTUNITIES AT CHAI</strong></p><p>Chai is actively hiring in Palo Alto with competitive compensation ($300K-$800K+ equity) for roles including AI Infrastructure Engineers, Software Engineers, Applied AI Researchers, and more. Fast-track qualification available for candidates with significant product launches, open source contributions, or entrepreneurial success.</p><p>https://www.chai-research.com/jobs/</p><p><br></p><p>The conversation with founder William Beauchamp and engineers Tom Lu and Nischay Dhankhar covers Chai&#39;s innovative technical approaches including reinforcement learning from human feedback (RLHF), model blending techniques that combine smaller models to outperform larger ones, and their unique infrastructure challenges running exaflop-class compute.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p><strong>Tufa AI Labs</strong> is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers in Zurich and SF. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>Key themes explored include:</p><p>- The ethics of AI engagement optimization and attention hacking</p><p>- Content moderation at scale with a lean engineering team</p><p>- The shift from AI as utility tool to AI as social companion</p><p>- How users form deep emotional bonds with artificial intelligence</p><p>- The broader implications of AI becoming a social medium</p><p><br></p><p>We also examine OpenAI&#39;s recent pivot toward companion AI with April&#39;s new GPT-4o, suggesting a fundamental shift in how we interact with artificial intelligence - from utility-focused tools to companion-like experiences that blur the lines between human and artificial intimacy.</p><p><br></p><p>The episode also covers Chai&#39;s unconventional approach to hiring only top-tier engineers, their bootstrap funding strategy focused on user revenue over VC funding, and their rapid experimentation culture where one in five experiments succeed.</p><p><br></p><p>TOC:</p><p>00:00:00 - Introduction: Steve Jobs&#39; AI Vision &amp; Chai&#39;s Scale</p><p>00:04:02 - Chapter 1: Simulators - The Birth of Social AI</p><p>00:13:34 - Chapter 2: Engineering at Chai - RLHF &amp; Model Blending</p><p>00:21:49 - Chapter 3: Social Impact of GenAI - Ethics &amp; Safety</p><p>00:33:55 - Chapter 4: The Lean Machine - 13 Engineers, Millions of Users</p><p>00:42:38 - Chapter 5: GPT-4o Becoming a Companion - OpenAI&#39;s Pivot</p><p>00:50:10 - Chapter 6: What Comes Next - The Future of AI Intimacy </p><p><br></p><p>TRANSCRIPT: https://www.dropbox.com/scl/fi/yz2ewkzmwz9rbbturfbap/CHAI.pdf?rlkey=uuyk2nfhjzezucwdgntg5ubqb&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Blurring-Reality---Chais-Social-AI-Platform-SPONSORED-e33d0jm</link>
			<guid isPermaLink="false">eb30c5f8-b47a-48e1-b8c0-1673c92849ef</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 26 May 2025 21:18:57 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/103235638/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-26%2F401041586-44100-2-c1c7c54714968.mp3" length="48946781" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;&amp;quot;Blurring Reality&amp;quot; - Chai&amp;#39;s Social AI Platform - &lt;strong&gt;sponsored&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;This episode of MLST explores the groundbreaking work of Chai, a social AI platform that quietly built one of the world&amp;#39;s largest AI companion ecosystems before ChatGPT&amp;#39;s mainstream adoption. With over 10 million active users and just 13 engineers serving 2 trillion tokens per day, Chai discovered the massive appetite for AI companionship through serendipity while searching for product-market fit.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;CHAI sponsored this show *because they want to hire amazing engineers* -- &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;CAREER OPPORTUNITIES AT CHAI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Chai is actively hiring in Palo Alto with competitive compensation ($300K-$800K+ equity) for roles including AI Infrastructure Engineers, Software Engineers, Applied AI Researchers, and more. Fast-track qualification available for candidates with significant product launches, open source contributions, or entrepreneurial success.&lt;/p&gt;&lt;p&gt;https://www.chai-research.com/jobs/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;The conversation with founder William Beauchamp and engineers Tom Lu and Nischay Dhankhar covers Chai&amp;#39;s innovative technical approaches including reinforcement learning from human feedback (RLHF), model blending techniques that combine smaller models to outperform larger ones, and their unique infrastructure challenges running exaflop-class compute.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tufa AI Labs&lt;/strong&gt; is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers in Zurich and SF. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Key themes explored include:&lt;/p&gt;&lt;p&gt;- The ethics of AI engagement optimization and attention hacking&lt;/p&gt;&lt;p&gt;- Content moderation at scale with a lean engineering team&lt;/p&gt;&lt;p&gt;- The shift from AI as utility tool to AI as social companion&lt;/p&gt;&lt;p&gt;- How users form deep emotional bonds with artificial intelligence&lt;/p&gt;&lt;p&gt;- The broader implications of AI becoming a social medium&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;We also examine OpenAI&amp;#39;s recent pivot toward companion AI with April&amp;#39;s new GPT-4o, suggesting a fundamental shift in how we interact with artificial intelligence - from utility-focused tools to companion-like experiences that blur the lines between human and artificial intimacy.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;The episode also covers Chai&amp;#39;s unconventional approach to hiring only top-tier engineers, their bootstrap funding strategy focused on user revenue over VC funding, and their rapid experimentation culture where one in five experiments succeed.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;00:00:00 - Introduction: Steve Jobs&amp;#39; AI Vision &amp;amp; Chai&amp;#39;s Scale&lt;/p&gt;&lt;p&gt;00:04:02 - Chapter 1: Simulators - The Birth of Social AI&lt;/p&gt;&lt;p&gt;00:13:34 - Chapter 2: Engineering at Chai - RLHF &amp;amp; Model Blending&lt;/p&gt;&lt;p&gt;00:21:49 - Chapter 3: Social Impact of GenAI - Ethics &amp;amp; Safety&lt;/p&gt;&lt;p&gt;00:33:55 - Chapter 4: The Lean Machine - 13 Engineers, Millions of Users&lt;/p&gt;&lt;p&gt;00:42:38 - Chapter 5: GPT-4o Becoming a Companion - OpenAI&amp;#39;s Pivot&lt;/p&gt;&lt;p&gt;00:50:10 - Chapter 6: What Comes Next - The Future of AI Intimacy &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT: https://www.dropbox.com/scl/fi/yz2ewkzmwz9rbbturfbap/CHAI.pdf?rlkey=uuyk2nfhjzezucwdgntg5ubqb&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:50:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1748294290796-aaff907693466.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Google AlphaEvolve - Discovering new science (exclusive interview)]]></title>
			<description><![CDATA[<p>Today GoogleDeepMind released AlphaEvolve: a Gemini coding agent for algorithm discovery. It beat the famous Strassen algorithm for matrix multiplication set 56 years ago. Google has been killing it recently. We had early access to the paper and interviewed the researchers behind the work.</p><p><br></p><p>AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms</p><p>https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/</p><p>Authors: Alexander Novikov*, Ngân Vũ*, Marvin Eisenberger*, Emilien Dupont*, Po-Sen Huang*, Adam Zsolt Wagner*, Sergey Shirobokov*, Borislav Kozlovskii*, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, Matej Balog*</p><p>(* indicates equal contribution or special designation, if defined elsewhere)</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>AlphaEvolve works like a very smart, tireless programmer. It uses powerful AI language models (like Gemini) to generate ideas for computer code. Then, it uses an &quot;evolutionary&quot; process – like survival of the fittest for programs. It tries out many different program ideas, automatically tests how well they solve a problem, and then uses the best ones to inspire new, even better programs.</p><p><br></p><p>Beyond this mathematical breakthrough, AlphaEvolve has already been used to improve real-world systems at Google, such as making their massive data centers run more efficiently and even speeding up the training of the AI models that power AlphaEvolve itself. The discussion also covers how humans work with AlphaEvolve, the challenges of making AI discover things, and the exciting future of AI helping scientists make new discoveries.</p><p><br></p><p>In short, AlphaEvolve is a powerful new AI tool that can invent new algorithms and solve complex problems, showing how AI can be a creative partner in science and engineering.</p><p><br></p><p>Guests:</p><p>Matej Balog: https://x.com/matejbalog</p><p>Alexander Novikov: https://x.com/SashaVNovikov</p><p><br></p><p>REFS:</p><p>MAP Elites [Jean-Baptiste Mouret, Jeff Clune]</p><p>https://arxiv.org/abs/1504.04909</p><p><br></p><p>FunSearch [Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli &amp; Alhussein Fawzi]</p><p>https://www.nature.com/articles/s41586-023-06924-6</p><p><br></p><p>TOC:</p><p><br></p><p>[00:00:00] Introduction: Alpha Evolve&#39;s Breakthroughs, DeepMind&#39;s Lineage, and Real-World Impact</p><p>[00:12:06] Introducing AlphaEvolve: Concept, Evolutionary Algorithms, and Architecture</p><p>[00:16:56] Search Challenges: The Halting Problem and Enabling Creative Leaps</p><p>[00:23:20] Knowledge Augmentation: Self-Generated Data, Meta-Prompting, and Library Learning</p><p>[00:29:08] Matrix Multiplication Breakthrough: From Strassen to AlphaEvolve&#39;s 48 Multiplications</p><p>[00:39:11] Problem Representation: Direct Solutions, Constructors, and Search Algorithms</p><p>[00:46:06] Developer Reflections: Surprising Outcomes and Superiority over Simple LLM Sampling</p><p>[00:51:42] Algorithmic Improvement: Hill Climbing, Program Synthesis, and Intelligibility</p><p>[01:00:24] Real-World Application: Complex Evaluations and Robotics</p><p>[01:05:39] Role of LLMs &amp; Future: Advanced Models, Recursive Self-Improvement, and Human-AI Collaboration</p><p>[01:11:22] Resource Considerations: Compute Costs of AlphaEvolve</p><p><br></p><p>This is a trial of posting videos on Spotify, thoughts? Email me or chat in our Discord</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Google-AlphaEvolve---Discovering-new-science-exclusive-interview-e32rjpn</link>
			<guid isPermaLink="false">f75eb49b-6bb6-4013-82fa-863e1a947ea3</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 14 May 2025 18:45:06 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/102665463/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-4-14%2F400288116-44100-2-bf56074113cbc.mp3" length="71015861" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today GoogleDeepMind released AlphaEvolve: a Gemini coding agent for algorithm discovery. It beat the famous Strassen algorithm for matrix multiplication set 56 years ago. Google has been killing it recently. We had early access to the paper and interviewed the researchers behind the work.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms&lt;/p&gt;&lt;p&gt;https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/&lt;/p&gt;&lt;p&gt;Authors: Alexander Novikov*, Ngân Vũ*, Marvin Eisenberger*, Emilien Dupont*, Po-Sen Huang*, Adam Zsolt Wagner*, Sergey Shirobokov*, Borislav Kozlovskii*, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, Matej Balog*&lt;/p&gt;&lt;p&gt;(* indicates equal contribution or special designation, if defined elsewhere)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;AlphaEvolve works like a very smart, tireless programmer. It uses powerful AI language models (like Gemini) to generate ideas for computer code. Then, it uses an &amp;quot;evolutionary&amp;quot; process – like survival of the fittest for programs. It tries out many different program ideas, automatically tests how well they solve a problem, and then uses the best ones to inspire new, even better programs.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Beyond this mathematical breakthrough, AlphaEvolve has already been used to improve real-world systems at Google, such as making their massive data centers run more efficiently and even speeding up the training of the AI models that power AlphaEvolve itself. The discussion also covers how humans work with AlphaEvolve, the challenges of making AI discover things, and the exciting future of AI helping scientists make new discoveries.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;In short, AlphaEvolve is a powerful new AI tool that can invent new algorithms and solve complex problems, showing how AI can be a creative partner in science and engineering.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Guests:&lt;/p&gt;&lt;p&gt;Matej Balog: https://x.com/matejbalog&lt;/p&gt;&lt;p&gt;Alexander Novikov: https://x.com/SashaVNovikov&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;MAP Elites [Jean-Baptiste Mouret, Jeff Clune]&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1504.04909&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;FunSearch [Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli &amp;amp; Alhussein Fawzi]&lt;/p&gt;&lt;p&gt;https://www.nature.com/articles/s41586-023-06924-6&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:00:00] Introduction: Alpha Evolve&amp;#39;s Breakthroughs, DeepMind&amp;#39;s Lineage, and Real-World Impact&lt;/p&gt;&lt;p&gt;[00:12:06] Introducing AlphaEvolve: Concept, Evolutionary Algorithms, and Architecture&lt;/p&gt;&lt;p&gt;[00:16:56] Search Challenges: The Halting Problem and Enabling Creative Leaps&lt;/p&gt;&lt;p&gt;[00:23:20] Knowledge Augmentation: Self-Generated Data, Meta-Prompting, and Library Learning&lt;/p&gt;&lt;p&gt;[00:29:08] Matrix Multiplication Breakthrough: From Strassen to AlphaEvolve&amp;#39;s 48 Multiplications&lt;/p&gt;&lt;p&gt;[00:39:11] Problem Representation: Direct Solutions, Constructors, and Search Algorithms&lt;/p&gt;&lt;p&gt;[00:46:06] Developer Reflections: Surprising Outcomes and Superiority over Simple LLM Sampling&lt;/p&gt;&lt;p&gt;[00:51:42] Algorithmic Improvement: Hill Climbing, Program Synthesis, and Intelligibility&lt;/p&gt;&lt;p&gt;[01:00:24] Real-World Application: Complex Evaluations and Robotics&lt;/p&gt;&lt;p&gt;[01:05:39] Role of LLMs &amp;amp; Future: Advanced Models, Recursive Self-Improvement, and Human-AI Collaboration&lt;/p&gt;&lt;p&gt;[01:11:22] Resource Considerations: Compute Costs of AlphaEvolve&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;This is a trial of posting videos on Spotify, thoughts? Email me or chat in our Discord&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:13:58</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1747248237340-719d44e773765.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Randall Balestriero - LLMs without pretraining and SSL]]></title>
			<description><![CDATA[<p>Randall Balestriero joins the show to discuss some counterintuitive findings in AI. He shares research showing that huge language models, even when started from scratch (randomly initialized) without massive pre-training, can learn specific tasks like sentiment analysis surprisingly well, train stably, and avoid severe overfitting, sometimes matching the performance of costly pre-trained models. This raises questions about when giant pre-training efforts are truly worth it.</p><p><br></p><p>He also talks about how self-supervised learning (where models learn from data structure itself) and traditional supervised learning (using labeled data) are fundamentally similar, allowing researchers to apply decades of supervised learning theory to improve newer self-supervised methods.</p><p><br></p><p>Finally, Randall touches on fairness in AI models used for Earth data (like climate prediction), revealing that these models can be biased, performing poorly in specific locations like islands or coastlines even if they seem accurate overall, which has important implications for policy decisions based on this data.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + SHOWNOTES:</p><p>https://www.dropbox.com/scl/fi/n7yev71nsjso71jyjz1fy/RANDALLNEURIPS.pdf?rlkey=0dn4injp1sc4ts8njwf3wfmxv&amp;dl=0</p><p><br></p><p>TOC:</p><p>1. Model Training Efficiency and Scale</p><p> [00:00:00] 1.1 Training Stability of Large Models on Small Datasets</p><p>   [00:04:09] 1.2 Pre-training vs Random Initialization Performance Comparison</p><p>   [00:07:58] 1.3 Task-Specific Models vs General LLMs Efficiency</p><p><br></p><p>2. Learning Paradigms and Data Distribution</p><p>   [00:10:35] 2.1 Fair Language Model Paradox and Token Frequency Issues</p><p>   [00:12:02] 2.2 Pre-training vs Single-task Learning Spectrum</p><p>   [00:16:04] 2.3 Theoretical Equivalence of Supervised and Self-supervised Learning</p><p>   [00:19:40] 2.4 Self-Supervised Learning and Supervised Learning Relationships</p><p>   [00:21:25] 2.5 SSL Objectives and Heavy-tailed Data Distribution Challenges</p><p><br></p><p>3. Geographic Representation in ML Systems</p><p>   [00:25:20] 3.1 Geographic Bias in Earth Data Models and Neural Representations</p><p>   [00:28:10] 3.2 Mathematical Limitations and Model Improvements</p><p>   [00:30:24] 3.3 Data Quality and Geographic Bias in ML Datasets</p><p><br></p><p>REFS:</p><p>[00:01:40] Research on training large language models from scratch on small datasets, Randall Balestriero et al.</p><p>https://openreview.net/forum?id=wYGBWOjq1Q</p><p>[00:10:35] The Fair Language Model Paradox (2024), Andrea Pinto, Tomer Galanti, Randall Balestriero</p><p>https://arxiv.org/abs/2410.11985</p><p>[00:12:20] Muppet: Massive Multi-task Representations with Pre-Finetuning (2021), Armen Aghajanyan et al.</p><p>https://arxiv.org/abs/2101.11038</p><p>[00:14:30] Dissociating language and thought in large language models (2023), Kyle Mahowald et al.</p><p>https://arxiv.org/abs/2301.06627</p><p>[00:16:05] The Birth of Self-Supervised Learning: A Supervised Theory, Randall Balestriero et al.</p><p>https://openreview.net/forum?id=NhYAjAAdQT</p><p>[00:21:25] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning, Adrien Bardes, Jean Ponce, Yann LeCun</p><p>https://arxiv.org/abs/2105.04906</p><p>[00:25:20] No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data (2025), Daniel Cai, Randall Balestriero, et al.</p><p>https://arxiv.org/abs/2502.06831</p><p>[00:33:45] Mark Ibrahim et al.&#39;s work on geographic bias in computer vision datasets, Mark Ibrahim</p><p>https://arxiv.org/pdf/2304.12210</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Randall-Balestriero---LLMs-without-pretraining-and-SSL-e31ta64</link>
			<guid isPermaLink="false">2cd3f44d-a967-42fe-9389-401f42fbd759</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 23 Apr 2025 14:16:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/101672580/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-3-23%2Fc31e9f0d-1a79-4a2a-df8d-c8608ab858e1.mp3" length="49952920" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Randall Balestriero joins the show to discuss some counterintuitive findings in AI. He shares research showing that huge language models, even when started from scratch (randomly initialized) without massive pre-training, can learn specific tasks like sentiment analysis surprisingly well, train stably, and avoid severe overfitting, sometimes matching the performance of costly pre-trained models. This raises questions about when giant pre-training efforts are truly worth it.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;He also talks about how self-supervised learning (where models learn from data structure itself) and traditional supervised learning (using labeled data) are fundamentally similar, allowing researchers to apply decades of supervised learning theory to improve newer self-supervised methods.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Finally, Randall touches on fairness in AI models used for Earth data (like climate prediction), revealing that these models can be biased, performing poorly in specific locations like islands or coastlines even if they seem accurate overall, which has important implications for policy decisions based on this data.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + SHOWNOTES:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/n7yev71nsjso71jyjz1fy/RANDALLNEURIPS.pdf?rlkey=0dn4injp1sc4ts8njwf3wfmxv&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Model Training Efficiency and Scale&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Training Stability of Large Models on Small Datasets&lt;/p&gt;&lt;p&gt;   [00:04:09] 1.2 Pre-training vs Random Initialization Performance Comparison&lt;/p&gt;&lt;p&gt;   [00:07:58] 1.3 Task-Specific Models vs General LLMs Efficiency&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Learning Paradigms and Data Distribution&lt;/p&gt;&lt;p&gt;   [00:10:35] 2.1 Fair Language Model Paradox and Token Frequency Issues&lt;/p&gt;&lt;p&gt;   [00:12:02] 2.2 Pre-training vs Single-task Learning Spectrum&lt;/p&gt;&lt;p&gt;   [00:16:04] 2.3 Theoretical Equivalence of Supervised and Self-supervised Learning&lt;/p&gt;&lt;p&gt;   [00:19:40] 2.4 Self-Supervised Learning and Supervised Learning Relationships&lt;/p&gt;&lt;p&gt;   [00:21:25] 2.5 SSL Objectives and Heavy-tailed Data Distribution Challenges&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Geographic Representation in ML Systems&lt;/p&gt;&lt;p&gt;   [00:25:20] 3.1 Geographic Bias in Earth Data Models and Neural Representations&lt;/p&gt;&lt;p&gt;   [00:28:10] 3.2 Mathematical Limitations and Model Improvements&lt;/p&gt;&lt;p&gt;   [00:30:24] 3.3 Data Quality and Geographic Bias in ML Datasets&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:01:40] Research on training large language models from scratch on small datasets, Randall Balestriero et al.&lt;/p&gt;&lt;p&gt;https://openreview.net/forum?id=wYGBWOjq1Q&lt;/p&gt;&lt;p&gt;[00:10:35] The Fair Language Model Paradox (2024), Andrea Pinto, Tomer Galanti, Randall Balestriero&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2410.11985&lt;/p&gt;&lt;p&gt;[00:12:20] Muppet: Massive Multi-task Representations with Pre-Finetuning (2021), Armen Aghajanyan et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2101.11038&lt;/p&gt;&lt;p&gt;[00:14:30] Dissociating language and thought in large language models (2023), Kyle Mahowald et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2301.06627&lt;/p&gt;&lt;p&gt;[00:16:05] The Birth of Self-Supervised Learning: A Supervised Theory, Randall Balestriero et al.&lt;/p&gt;&lt;p&gt;https://openreview.net/forum?id=NhYAjAAdQT&lt;/p&gt;&lt;p&gt;[00:21:25] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning, Adrien Bardes, Jean Ponce, Yann LeCun&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2105.04906&lt;/p&gt;&lt;p&gt;[00:25:20] No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data (2025), Daniel Cai, Randall Balestriero, et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2502.06831&lt;/p&gt;&lt;p&gt;[00:33:45] Mark Ibrahim et al.&amp;#39;s work on geographic bias in computer vision datasets, Mark Ibrahim&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/2304.12210&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:34:30</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1745417771226-735bd1993a9d5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[How Machines Learn to Ignore the Noise (Kevin Ellis + Zenna Tavares)]]></title>
			<description><![CDATA[<p>Prof. Kevin Ellis and Dr. Zenna Tavares talk about making AI smarter, like humans. They want AI to learn from just a little bit of information by actively trying things out, not just by looking at tons of data.</p><p><br></p><p>They discuss two main ways AI can &quot;think&quot;: one way is like following specific rules or steps (like a computer program), and the other is more intuitive, like guessing based on patterns (like modern AI often does). They found combining both methods works well for solving complex puzzles like ARC.</p><p><br></p><p>A key idea is &quot;compositionality&quot; - building big ideas from small ones, like LEGOs. This is powerful but can also be overwhelming. Another important idea is &quot;abstraction&quot; - understanding things simply, without getting lost in details, and knowing there are different levels of understanding.</p><p><br></p><p>Ultimately, they believe the best AI will need to explore, experiment, and build models of the world, much like humans do when learning something new.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT:</p><p>https://www.dropbox.com/scl/fi/3ngggvhb3tnemw879er5y/BASIS.pdf?rlkey=lr2zbj3317mex1q5l0c2rsk0h&amp;dl=0</p><p> </p><p>Zenna Tavares:</p><p>http://www.zenna.org/</p><p>Kevin Ellis:</p><p>https://www.cs.cornell.edu/~ellisk/</p><p><br></p><p>TOC:</p><p>1. Compositionality and Learning Foundations</p><p> [00:00:00] 1.1 Compositional Search and Learning Challenges</p><p> [00:03:55] 1.2 Bayesian Learning and World Models</p><p>   [00:12:05] 1.3 Programming Languages and Compositionality Trade-offs</p><p>   [00:15:35] 1.4 Inductive vs Transductive Approaches in AI Systems</p><p><br></p><p>2. Neural-Symbolic Program Synthesis</p><p>   [00:27:20] 2.1 Integration of LLMs with Traditional Programming and Meta-Programming</p><p>   [00:30:43] 2.2 Wake-Sleep Learning and DreamCoder Architecture</p><p>   [00:38:26] 2.3 Program Synthesis from Interactions and Hidden State Inference</p><p>   [00:41:36] 2.4 Abstraction Mechanisms and Resource Rationality</p><p>   [00:48:38] 2.5 Inductive Biases and Causal Abstraction in AI Systems</p><p><br></p><p>3. Abstract Reasoning Systems</p><p>   [00:52:10] 3.1 Abstract Concepts and Grid-Based Transformations in ARC</p><p>   [00:56:08] 3.2 Induction vs Transduction Approaches in Abstract Reasoning</p><p>   [00:59:12] 3.3 ARC Limitations and Interactive Learning Extensions</p><p>   [01:06:30] 3.4 Wake-Sleep Program Learning and Hybrid Approaches</p><p>   [01:11:37] 3.5 Project MARA and Future Research Directions</p><p><br></p><p>REFS:</p><p>[00:00:25] DreamCoder, Kevin Ellis et al.</p><p>https://arxiv.org/abs/2006.08381</p><p><br></p><p>[00:01:10] Mind Your Step, Ryan Liu et al.</p><p>https://arxiv.org/abs/2410.21333</p><p><br></p><p>[00:06:05] Bayesian inference, Griffiths, T. L., Kemp, C., &amp; Tenenbaum, J. B.</p><p>https://psycnet.apa.org/record/2008-06911-003</p><p><br></p><p>[00:13:00] Induction and Transduction, Wen-Ding Li, Zenna Tavares, Yewen Pu, Kevin Ellis</p><p>https://arxiv.org/abs/2411.02272</p><p><br></p><p>[00:23:15] Neurosymbolic AI, Garcez, Artur d&#39;Avila et al.</p><p>https://arxiv.org/abs/2012.05876</p><p><br></p><p>[00:33:50] Induction and Transduction (II), Wen-Ding Li, Kevin Ellis et al.</p><p>https://arxiv.org/abs/2411.02272</p><p><br></p><p>[00:38:35] ARC, François Chollet</p><p>https://arxiv.org/abs/1911.01547</p><p><br></p><p>[00:39:20] Causal Reactive Programs, Ria Das, Joshua B. Tenenbaum, Armando Solar-Lezama, Zenna Tavares</p><p>http://www.zenna.org/publications/autumn2022.pdf</p><p><br></p><p>[00:42:50] MuZero, Julian Schrittwieser et al.</p><p>http://arxiv.org/pdf/1911.08265</p><p><br></p><p>[00:43:20] VisualPredicator, Yichao Liang</p><p>https://arxiv.org/abs/2410.23156</p><p><br></p><p>[00:48:55] Bayesian models of cognition, Joshua B. Tenenbaum</p><p>https://mitpress.mit.edu/9780262049412/bayesian-models-of-cognition/</p><p><br></p><p>[00:49:30] The Bitter Lesson, Rich Sutton</p><p>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</p><p><br></p><p>[01:06:35] Program induction, Kevin Ellis, Wen-Ding Li</p><p>https://arxiv.org/pdf/2411.02272</p><p><br></p><p>[01:06:50] DreamCoder (II), Kevin Ellis et al.</p><p>https://arxiv.org/abs/2006.08381</p><p><br></p><p>[01:11:55] Project MARA, Zenna Tavares, Kevin Ellis</p><p>https://www.basis.ai/blog/mara/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/How-Machines-Learn-to-Ignore-the-Noise-Kevin-Ellis--Zenna-Tavares-e319n04</link>
			<guid isPermaLink="false">e55c63f4-2976-4c7f-a4ca-eb1b309745b9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 08 Apr 2025 21:03:31 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/101030340/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-3-8%2Feca73139-5076-05da-1c71-0c4421ae52e9.mp3" length="111218294" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof. Kevin Ellis and Dr. Zenna Tavares talk about making AI smarter, like humans. They want AI to learn from just a little bit of information by actively trying things out, not just by looking at tons of data.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;They discuss two main ways AI can &amp;quot;think&amp;quot;: one way is like following specific rules or steps (like a computer program), and the other is more intuitive, like guessing based on patterns (like modern AI often does). They found combining both methods works well for solving complex puzzles like ARC.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;A key idea is &amp;quot;compositionality&amp;quot; - building big ideas from small ones, like LEGOs. This is powerful but can also be overwhelming. Another important idea is &amp;quot;abstraction&amp;quot; - understanding things simply, without getting lost in details, and knowing there are different levels of understanding.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Ultimately, they believe the best AI will need to explore, experiment, and build models of the world, much like humans do when learning something new.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/3ngggvhb3tnemw879er5y/BASIS.pdf?rlkey=lr2zbj3317mex1q5l0c2rsk0h&amp;amp;dl=0&lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;p&gt;Zenna Tavares:&lt;/p&gt;&lt;p&gt;http://www.zenna.org/&lt;/p&gt;&lt;p&gt;Kevin Ellis:&lt;/p&gt;&lt;p&gt;https://www.cs.cornell.edu/~ellisk/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Compositionality and Learning Foundations&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Compositional Search and Learning Challenges&lt;/p&gt;&lt;p&gt; [00:03:55] 1.2 Bayesian Learning and World Models&lt;/p&gt;&lt;p&gt;   [00:12:05] 1.3 Programming Languages and Compositionality Trade-offs&lt;/p&gt;&lt;p&gt;   [00:15:35] 1.4 Inductive vs Transductive Approaches in AI Systems&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Neural-Symbolic Program Synthesis&lt;/p&gt;&lt;p&gt;   [00:27:20] 2.1 Integration of LLMs with Traditional Programming and Meta-Programming&lt;/p&gt;&lt;p&gt;   [00:30:43] 2.2 Wake-Sleep Learning and DreamCoder Architecture&lt;/p&gt;&lt;p&gt;   [00:38:26] 2.3 Program Synthesis from Interactions and Hidden State Inference&lt;/p&gt;&lt;p&gt;   [00:41:36] 2.4 Abstraction Mechanisms and Resource Rationality&lt;/p&gt;&lt;p&gt;   [00:48:38] 2.5 Inductive Biases and Causal Abstraction in AI Systems&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Abstract Reasoning Systems&lt;/p&gt;&lt;p&gt;   [00:52:10] 3.1 Abstract Concepts and Grid-Based Transformations in ARC&lt;/p&gt;&lt;p&gt;   [00:56:08] 3.2 Induction vs Transduction Approaches in Abstract Reasoning&lt;/p&gt;&lt;p&gt;   [00:59:12] 3.3 ARC Limitations and Interactive Learning Extensions&lt;/p&gt;&lt;p&gt;   [01:06:30] 3.4 Wake-Sleep Program Learning and Hybrid Approaches&lt;/p&gt;&lt;p&gt;   [01:11:37] 3.5 Project MARA and Future Research Directions&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:00:25] DreamCoder, Kevin Ellis et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2006.08381&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:01:10] Mind Your Step, Ryan Liu et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2410.21333&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:06:05] Bayesian inference, Griffiths, T. L., Kemp, C., &amp;amp; Tenenbaum, J. B.&lt;/p&gt;&lt;p&gt;https://psycnet.apa.org/record/2008-06911-003&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:13:00] Induction and Transduction, Wen-Ding Li, Zenna Tavares, Yewen Pu, Kevin Ellis&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.02272&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:23:15] Neurosymbolic AI, Garcez, Artur d&amp;#39;Avila et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2012.05876&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:33:50] Induction and Transduction (II), Wen-Ding Li, Kevin Ellis et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.02272&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:38:35] ARC, François Chollet&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:39:20] Causal Reactive Programs, Ria Das, Joshua B. Tenenbaum, Armando Solar-Lezama, Zenna Tavares&lt;/p&gt;&lt;p&gt;http://www.zenna.org/publications/autumn2022.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:42:50] MuZero, Julian Schrittwieser et al.&lt;/p&gt;&lt;p&gt;http://arxiv.org/pdf/1911.08265&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:43:20] VisualPredicator, Yichao Liang&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2410.23156&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:48:55] Bayesian models of cognition, Joshua B. Tenenbaum&lt;/p&gt;&lt;p&gt;https://mitpress.mit.edu/9780262049412/bayesian-models-of-cognition/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:49:30] The Bitter Lesson, Rich Sutton&lt;/p&gt;&lt;p&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[01:06:35] Program induction, Kevin Ellis, Wen-Ding Li&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/2411.02272&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[01:06:50] DreamCoder (II), Kevin Ellis et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2006.08381&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[01:11:55] Project MARA, Zenna Tavares, Kevin Ellis&lt;/p&gt;&lt;p&gt;https://www.basis.ai/blog/mara/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:16:55</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1744146173685-9d1942902c30f.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Eiso Kant (CTO poolside) - Superhuman Coding Is Coming!]]></title>
			<description><![CDATA[<p>Eiso Kant, CTO of poolside AI, discusses the company&#39;s approach to building frontier AI foundation models, particularly focused on software development. Their unique strategy is reinforcement learning from code execution feedback which is an important axis for scaling AI capabilities beyond just increasing model size or data volume. Kant predicts human-level AI in knowledge work could be achieved within 18-36 months, outlining poolside&#39;s vision to dramatically increase software development productivity and accessibility. </p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>Eiso Kant:</p><p>https://x.com/eisokant</p><p>https://poolside.ai/</p><p><br></p><p>TRANSCRIPT:</p><p>https://www.dropbox.com/scl/fi/szepl6taqziyqie9wgmk9/poolside.pdf?rlkey=iqar7dcwshyrpeoz0xa76k422&amp;dl=0</p><p><br></p><p>TOC:</p><p>1. Foundation Models and AI Strategy</p><p>  [00:00:00] 1.1 Foundation Models and Timeline Predictions for AI Development</p><p>   [00:02:55] 1.2 Poolside AI&#39;s Corporate History and Strategic Vision</p><p>   [00:06:48] 1.3 Foundation Models vs Enterprise Customization Trade-offs</p><p><br></p><p>2. Reinforcement Learning and Model Economics</p><p>   [00:15:42] 2.1 Reinforcement Learning and Code Execution Feedback Approaches</p><p>   [00:22:06] 2.2 Model Economics and Experimental Optimization</p><p><br></p><p>3. Enterprise AI Implementation</p><p>   [00:25:20] 3.1 Poolside&#39;s Enterprise Deployment Strategy and Infrastructure</p><p>   [00:26:00] 3.2 Enterprise-First Business Model and Market Focus</p><p>   [00:27:05] 3.3 Foundation Models and AGI Development Approach</p><p>   [00:29:24] 3.4 DeepSeek Case Study and Infrastructure Requirements</p><p><br></p><p>4. LLM Architecture and Performance</p><p>   [00:30:15] 4.1 Distributed Training and Hardware Architecture Optimization</p><p>   [00:33:01] 4.2 Model Scaling Strategies and Chinchilla Optimality Trade-offs</p><p>   [00:36:04] 4.3 Emergent Reasoning and Model Architecture Comparisons</p><p>   [00:43:26] 4.4 Balancing Creativity and Determinism in AI Models</p><p>   [00:50:01] 4.5 AI-Assisted Software Development Evolution</p><p><br></p><p>5. AI Systems Engineering and Scalability</p><p>   [00:58:31] 5.1 Enterprise AI Productivity and Implementation Challenges</p><p>   [00:58:40] 5.2 Low-Code Solutions and Enterprise Hiring Trends</p><p>   [01:01:25] 5.3 Distributed Systems and Engineering Complexity</p><p>   [01:01:50] 5.4 GenAI Architecture and Scalability Patterns</p><p>   [01:01:55] 5.5 Scaling Limitations and Architectural Patterns in AI Code Generation</p><p><br></p><p>6. AI Safety and Future Capabilities</p><p>   [01:06:23] 6.1 Semantic Understanding and Language Model Reasoning Approaches</p><p>   [01:12:42] 6.2 Model Interpretability and Safety Considerations in AI Systems</p><p>   [01:16:27] 6.3 AI vs Human Capabilities in Software Development</p><p>   [01:33:45] 6.4 Enterprise Deployment and Security Architecture</p><p><br></p><p>CORE REFS (see shownotes for URLs/more refs):</p><p><br></p><p>[00:15:45] Research demonstrating how training on model-generated content leads to distribution collapse in AI models, Ilia Shumailov et al. (Key finding on synthetic data risk)</p><p><br></p><p>[00:20:05] Foundational paper introducing Word2Vec for computing word vector representations, Tomas Mikolov et al. (Seminal NLP technique)</p><p><br></p><p>[00:22:15] OpenAI O3 model&#39;s breakthrough performance on ARC Prize Challenge, OpenAI (Significant AI reasoning benchmark achievement)</p><p><br></p><p>[00:22:40] Seminal paper proposing a formal definition of intelligence as skill-acquisition efficiency, François Chollet (Influential AI definition/philosophy)</p><p><br></p><p>[00:30:30] Technical documentation of DeepSeek&#39;s V3 model architecture and capabilities, DeepSeek AI (Details on a major new model)</p><p><br></p><p>[00:34:30] Foundational paper establishing optimal scaling laws for LLM training, Jordan Hoffmann et al. (Key paper on LLM scaling)</p><p><br></p><p>[00:45:45] Seminal essay arguing that scaling computation consistently trumps human-engineered solutions in AI, Richard S. Sutton (Influential &quot;Bitter Lesson&quot; perspective)</p><p>&lt;trunc - see PDF shownotes&gt;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Eiso-Kant-CTO-poolside---Superhuman-Coding-Is-Coming-e3117gu</link>
			<guid isPermaLink="false">fd6f4efb-7967-4f83-853f-f70c04b0185d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 02 Apr 2025 19:58:20 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/100752350/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-3-2%2F9a64e682-db0b-c9ec-bed7-1469af13f755.mp3" length="139459991" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Eiso Kant, CTO of poolside AI, discusses the company&amp;#39;s approach to building frontier AI foundation models, particularly focused on software development. Their unique strategy is reinforcement learning from code execution feedback which is an important axis for scaling AI capabilities beyond just increasing model size or data volume. Kant predicts human-level AI in knowledge work could be achieved within 18-36 months, outlining poolside&amp;#39;s vision to dramatically increase software development productivity and accessibility. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Eiso Kant:&lt;/p&gt;&lt;p&gt;https://x.com/eisokant&lt;/p&gt;&lt;p&gt;https://poolside.ai/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/szepl6taqziyqie9wgmk9/poolside.pdf?rlkey=iqar7dcwshyrpeoz0xa76k422&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Foundation Models and AI Strategy&lt;/p&gt;&lt;p&gt;  [00:00:00] 1.1 Foundation Models and Timeline Predictions for AI Development&lt;/p&gt;&lt;p&gt;   [00:02:55] 1.2 Poolside AI&amp;#39;s Corporate History and Strategic Vision&lt;/p&gt;&lt;p&gt;   [00:06:48] 1.3 Foundation Models vs Enterprise Customization Trade-offs&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Reinforcement Learning and Model Economics&lt;/p&gt;&lt;p&gt;   [00:15:42] 2.1 Reinforcement Learning and Code Execution Feedback Approaches&lt;/p&gt;&lt;p&gt;   [00:22:06] 2.2 Model Economics and Experimental Optimization&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Enterprise AI Implementation&lt;/p&gt;&lt;p&gt;   [00:25:20] 3.1 Poolside&amp;#39;s Enterprise Deployment Strategy and Infrastructure&lt;/p&gt;&lt;p&gt;   [00:26:00] 3.2 Enterprise-First Business Model and Market Focus&lt;/p&gt;&lt;p&gt;   [00:27:05] 3.3 Foundation Models and AGI Development Approach&lt;/p&gt;&lt;p&gt;   [00:29:24] 3.4 DeepSeek Case Study and Infrastructure Requirements&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;4. LLM Architecture and Performance&lt;/p&gt;&lt;p&gt;   [00:30:15] 4.1 Distributed Training and Hardware Architecture Optimization&lt;/p&gt;&lt;p&gt;   [00:33:01] 4.2 Model Scaling Strategies and Chinchilla Optimality Trade-offs&lt;/p&gt;&lt;p&gt;   [00:36:04] 4.3 Emergent Reasoning and Model Architecture Comparisons&lt;/p&gt;&lt;p&gt;   [00:43:26] 4.4 Balancing Creativity and Determinism in AI Models&lt;/p&gt;&lt;p&gt;   [00:50:01] 4.5 AI-Assisted Software Development Evolution&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;5. AI Systems Engineering and Scalability&lt;/p&gt;&lt;p&gt;   [00:58:31] 5.1 Enterprise AI Productivity and Implementation Challenges&lt;/p&gt;&lt;p&gt;   [00:58:40] 5.2 Low-Code Solutions and Enterprise Hiring Trends&lt;/p&gt;&lt;p&gt;   [01:01:25] 5.3 Distributed Systems and Engineering Complexity&lt;/p&gt;&lt;p&gt;   [01:01:50] 5.4 GenAI Architecture and Scalability Patterns&lt;/p&gt;&lt;p&gt;   [01:01:55] 5.5 Scaling Limitations and Architectural Patterns in AI Code Generation&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;6. AI Safety and Future Capabilities&lt;/p&gt;&lt;p&gt;   [01:06:23] 6.1 Semantic Understanding and Language Model Reasoning Approaches&lt;/p&gt;&lt;p&gt;   [01:12:42] 6.2 Model Interpretability and Safety Considerations in AI Systems&lt;/p&gt;&lt;p&gt;   [01:16:27] 6.3 AI vs Human Capabilities in Software Development&lt;/p&gt;&lt;p&gt;   [01:33:45] 6.4 Enterprise Deployment and Security Architecture&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;CORE REFS (see shownotes for URLs/more refs):&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:15:45] Research demonstrating how training on model-generated content leads to distribution collapse in AI models, Ilia Shumailov et al. (Key finding on synthetic data risk)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:20:05] Foundational paper introducing Word2Vec for computing word vector representations, Tomas Mikolov et al. (Seminal NLP technique)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:22:15] OpenAI O3 model&amp;#39;s breakthrough performance on ARC Prize Challenge, OpenAI (Significant AI reasoning benchmark achievement)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:22:40] Seminal paper proposing a formal definition of intelligence as skill-acquisition efficiency, François Chollet (Influential AI definition/philosophy)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:30:30] Technical documentation of DeepSeek&amp;#39;s V3 model architecture and capabilities, DeepSeek AI (Details on a major new model)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:34:30] Foundational paper establishing optimal scaling laws for LLM training, Jordan Hoffmann et al. (Key paper on LLM scaling)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:45:45] Seminal essay arguing that scaling computation consistently trumps human-engineered solutions in AI, Richard S. Sutton (Influential &amp;quot;Bitter Lesson&amp;quot; perspective)&lt;/p&gt;&lt;p&gt;&amp;lt;trunc - see PDF shownotes&amp;gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:36:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1743623841303-715d3eb54e55.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Compendium - Connor Leahy and Gabriel Alfour]]></title>
			<description><![CDATA[<p>Connor Leahy and Gabriel Alfour, AI researchers from Conjecture and authors of &quot;The Compendium,&quot; joinus for a critical discussion centered on Artificial Superintelligence (ASI) safety and governance. Drawing from their comprehensive analysis in &quot;The Compendium,&quot; they articulate a stark warning about the existential risks inherent in uncontrolled AI development, framing it through the lens of &quot;intelligence domination&quot;—where a sufficiently advanced AI could subordinate humanity, much like humans dominate less intelligent species.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + REFS + NOTES:</p><p>https://www.dropbox.com/scl/fi/p86l75y4o2ii40df5t7no/Compendium.pdf?rlkey=tukczgf3flw133sr9rgss0pnj&amp;dl=0</p><p><br></p><p>https://www.thecompendium.ai/</p><p>https://en.wikipedia.org/wiki/Connor_Leahy</p><p>https://www.conjecture.dev/about</p><p>https://substack.com/@gabecc​</p><p><br></p><p>TOC:</p><p>1. AI Intelligence and Safety Fundamentals</p><p> [00:00:00] 1.1 Understanding Intelligence and AI Capabilities</p><p>   [00:06:20] 1.2 Emergence of Intelligence and Regulatory Challenges</p><p>   [00:10:18] 1.3 Human vs Animal Intelligence Debate</p><p>   [00:18:00] 1.4 AI Regulation and Risk Assessment Approaches</p><p>   [00:26:14] 1.5 Competing AI Development Ideologies</p><p><br></p><p>2. Economic and Social Impact</p><p>   [00:29:10] 2.1 Labor Market Disruption and Post-Scarcity Scenarios</p><p>   [00:32:40] 2.2 Institutional Frameworks and Tech Power Dynamics</p><p>   [00:37:40] 2.3 Ethical Frameworks and AI Governance Debates</p><p>   [00:40:52] 2.4 AI Alignment Evolution and Technical Challenges</p><p><br></p><p>3. Technical Governance Framework</p><p>   [00:55:07] 3.1 Three Levels of AI Safety: Alignment, Corrigibility, and Boundedness</p><p>   [00:55:30] 3.2 Challenges of AI System Corrigibility and Constitutional Models</p><p>   [00:57:35] 3.3 Limitations of Current Boundedness Approaches</p><p>   [00:59:11] 3.4 Abstract Governance Concepts and Policy Solutions</p><p><br></p><p>4. Democratic Implementation and Coordination</p><p>   [00:59:20] 4.1 Governance Design and Measurement Challenges</p><p>   [01:00:10] 4.2 Democratic Institutions and Experimental Governance</p><p>   [01:14:10] 4.3 Political Engagement and AI Safety Advocacy</p><p>   [01:25:30] 4.4 Practical AI Safety Measures and International Coordination</p><p><br></p><p>CORE REFS:</p><p>[00:01:45] The Compendium (2023), Leahy et al.</p><p>https://pdf.thecompendium.ai/the_compendium.pdf</p><p><br></p><p>[00:06:50] Geoffrey Hinton Leaves Google, BBC News</p><p>https://www.bbc.com/news/world-us-canada-65452940</p><p><br></p><p>[00:10:00] ARC-AGI, Chollet</p><p>https://arcprize.org/arc-agi</p><p><br></p><p>[00:13:25] A Brief History of Intelligence, Bennett</p><p>https://www.amazon.com/Brief-History-Intelligence-Humans-Breakthroughs/dp/0063286343</p><p><br></p><p>[00:25:35] Statement on AI Risk, Center for AI Safety</p><p>https://www.safe.ai/work/statement-on-ai-risk</p><p><br></p><p>[00:26:15] Machines of Love and Grace, Amodei</p><p>https://darioamodei.com/machines-of-loving-grace</p><p><br></p><p>[00:26:35] The Techno-Optimist Manifesto, Andreessen</p><p>https://a16z.com/the-techno-optimist-manifesto/</p><p><br></p><p>[00:31:55] Techno-Feudalism, Varoufakis</p><p>https://www.amazon.co.uk/Technofeudalism-Killed-Capitalism-Yanis-Varoufakis/dp/1847927270</p><p><br></p><p>[00:42:40] Introducing Superalignment, OpenAI</p><p>https://openai.com/index/introducing-superalignment/</p><p><br></p><p>[00:47:20] Three Laws of Robotics, Asimov</p><p>https://www.britannica.com/topic/Three-Laws-of-Robotics</p><p><br></p><p>[00:50:00] Symbolic AI (GOFAI), Haugeland</p><p>https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence</p><p><br></p><p>[00:52:30] Intent Alignment, Christiano</p><p>https://www.alignmentforum.org/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety</p><p><br></p><p>[00:55:10] Large Language Model Alignment: A Survey, Jiang et al.</p><p>http://arxiv.org/pdf/2309.15025</p><p><br></p><p>[00:55:40] Constitutional Checks and Balances, Bok</p><p>https://plato.stanford.edu/entries/montesquieu/</p><p>&lt;trunc, see PDF&gt;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Compendium---Connor-Leahy-and-Gabriel-Alfour-e30s7s0</link>
			<guid isPermaLink="false">e4571571-99cb-4274-9fe3-0c5ce7e92fe4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 30 Mar 2025 17:16:37 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/100588864/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-30%2F2f821d64-3ab9-3534-5bc2-ec24a7e20e6a.mp3" length="140413268" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Connor Leahy and Gabriel Alfour, AI researchers from Conjecture and authors of &amp;quot;The Compendium,&amp;quot; joinus for a critical discussion centered on Artificial Superintelligence (ASI) safety and governance. Drawing from their comprehensive analysis in &amp;quot;The Compendium,&amp;quot; they articulate a stark warning about the existential risks inherent in uncontrolled AI development, framing it through the lens of &amp;quot;intelligence domination&amp;quot;—where a sufficiently advanced AI could subordinate humanity, much like humans dominate less intelligent species.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + REFS + NOTES:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/p86l75y4o2ii40df5t7no/Compendium.pdf?rlkey=tukczgf3flw133sr9rgss0pnj&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;https://www.thecompendium.ai/&lt;/p&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Connor_Leahy&lt;/p&gt;&lt;p&gt;https://www.conjecture.dev/about&lt;/p&gt;&lt;p&gt;https://substack.com/@gabecc​&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. AI Intelligence and Safety Fundamentals&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Understanding Intelligence and AI Capabilities&lt;/p&gt;&lt;p&gt;   [00:06:20] 1.2 Emergence of Intelligence and Regulatory Challenges&lt;/p&gt;&lt;p&gt;   [00:10:18] 1.3 Human vs Animal Intelligence Debate&lt;/p&gt;&lt;p&gt;   [00:18:00] 1.4 AI Regulation and Risk Assessment Approaches&lt;/p&gt;&lt;p&gt;   [00:26:14] 1.5 Competing AI Development Ideologies&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Economic and Social Impact&lt;/p&gt;&lt;p&gt;   [00:29:10] 2.1 Labor Market Disruption and Post-Scarcity Scenarios&lt;/p&gt;&lt;p&gt;   [00:32:40] 2.2 Institutional Frameworks and Tech Power Dynamics&lt;/p&gt;&lt;p&gt;   [00:37:40] 2.3 Ethical Frameworks and AI Governance Debates&lt;/p&gt;&lt;p&gt;   [00:40:52] 2.4 AI Alignment Evolution and Technical Challenges&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Technical Governance Framework&lt;/p&gt;&lt;p&gt;   [00:55:07] 3.1 Three Levels of AI Safety: Alignment, Corrigibility, and Boundedness&lt;/p&gt;&lt;p&gt;   [00:55:30] 3.2 Challenges of AI System Corrigibility and Constitutional Models&lt;/p&gt;&lt;p&gt;   [00:57:35] 3.3 Limitations of Current Boundedness Approaches&lt;/p&gt;&lt;p&gt;   [00:59:11] 3.4 Abstract Governance Concepts and Policy Solutions&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;4. Democratic Implementation and Coordination&lt;/p&gt;&lt;p&gt;   [00:59:20] 4.1 Governance Design and Measurement Challenges&lt;/p&gt;&lt;p&gt;   [01:00:10] 4.2 Democratic Institutions and Experimental Governance&lt;/p&gt;&lt;p&gt;   [01:14:10] 4.3 Political Engagement and AI Safety Advocacy&lt;/p&gt;&lt;p&gt;   [01:25:30] 4.4 Practical AI Safety Measures and International Coordination&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;CORE REFS:&lt;/p&gt;&lt;p&gt;[00:01:45] The Compendium (2023), Leahy et al.&lt;/p&gt;&lt;p&gt;https://pdf.thecompendium.ai/the_compendium.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:06:50] Geoffrey Hinton Leaves Google, BBC News&lt;/p&gt;&lt;p&gt;https://www.bbc.com/news/world-us-canada-65452940&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:10:00] ARC-AGI, Chollet&lt;/p&gt;&lt;p&gt;https://arcprize.org/arc-agi&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:13:25] A Brief History of Intelligence, Bennett&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Brief-History-Intelligence-Humans-Breakthroughs/dp/0063286343&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:25:35] Statement on AI Risk, Center for AI Safety&lt;/p&gt;&lt;p&gt;https://www.safe.ai/work/statement-on-ai-risk&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:26:15] Machines of Love and Grace, Amodei&lt;/p&gt;&lt;p&gt;https://darioamodei.com/machines-of-loving-grace&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:26:35] The Techno-Optimist Manifesto, Andreessen&lt;/p&gt;&lt;p&gt;https://a16z.com/the-techno-optimist-manifesto/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:31:55] Techno-Feudalism, Varoufakis&lt;/p&gt;&lt;p&gt;https://www.amazon.co.uk/Technofeudalism-Killed-Capitalism-Yanis-Varoufakis/dp/1847927270&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:42:40] Introducing Superalignment, OpenAI&lt;/p&gt;&lt;p&gt;https://openai.com/index/introducing-superalignment/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:47:20] Three Laws of Robotics, Asimov&lt;/p&gt;&lt;p&gt;https://www.britannica.com/topic/Three-Laws-of-Robotics&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:50:00] Symbolic AI (GOFAI), Haugeland&lt;/p&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:52:30] Intent Alignment, Christiano&lt;/p&gt;&lt;p&gt;https://www.alignmentforum.org/posts/HEZgGBZTpT4Bov7nH/mapping-the-conceptual-territory-in-ai-existential-safety&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:55:10] Large Language Model Alignment: A Survey, Jiang et al.&lt;/p&gt;&lt;p&gt;http://arxiv.org/pdf/2309.15025&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:55:40] Constitutional Checks and Balances, Bok&lt;/p&gt;&lt;p&gt;https://plato.stanford.edu/entries/montesquieu/&lt;/p&gt;&lt;p&gt;&amp;lt;trunc, see PDF&amp;gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:37:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1743354958188-f122fe9e527cb.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ARC Prize v2 Launch! (Francois Chollet and Mike Knoop)]]></title>
			<description><![CDATA[<p> We are joined by Francois Chollet and Mike Knoop, to launch the new version of the ARC prize! In version 2, the challenges have been calibrated with humans such that at least 2 humans could solve each task in a reasonable task, but also adversarially selected so that frontier reasoning models can&#39;t solve them. The best LLMs today get negligible performance on this challenge. </p><p><br></p><p>https://arcprize.org/</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT:</p><p>https://www.dropbox.com/scl/fi/0v9o8xcpppdwnkntj59oi/ARCv2.pdf?rlkey=luqb6f141976vra6zdtptv5uj&amp;dl=0</p><p><br></p><p>TOC:</p><p>1. ARC v2 Core Design &amp; Objectives</p><p> [00:00:00] 1.1 ARC v2 Launch and Benchmark Architecture</p><p>   [00:03:16] 1.2 Test-Time Optimization and AGI Assessment</p><p>   [00:06:24] 1.3 Human-AI Capability Analysis</p><p>   [00:13:02] 1.4 OpenAI o3 Initial Performance Results</p><p><br></p><p>2. ARC Technical Evolution</p><p>   [00:17:20] 2.1 ARC-v1 to ARC-v2 Design Improvements</p><p>   [00:21:12] 2.2 Human Validation Methodology</p><p>   [00:26:05] 2.3 Task Design and Gaming Prevention</p><p>   [00:29:11] 2.4 Intelligence Measurement Framework</p><p><br></p><p>3. O3 Performance &amp; Future Challenges</p><p>   [00:38:50] 3.1 O3 Comprehensive Performance Analysis</p><p>   [00:43:40] 3.2 System Limitations and Failure Modes</p><p>   [00:49:30] 3.3 Program Synthesis Applications</p><p>   [00:53:00] 3.4 Future Development Roadmap</p><p><br></p><p>REFS:</p><p>[00:00:15] On the Measure of Intelligence, François Chollet</p><p>https://arxiv.org/abs/1911.01547</p><p>[00:06:45] ARC Prize Foundation, François Chollet, Mike Knoop</p><p>https://arcprize.org/</p><p>[00:12:50] OpenAI o3 model performance on ARC v1, ARC Prize Team</p><p>https://arcprize.org/blog/oai-o3-pub-breakthrough</p><p>[00:18:30] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Jason Wei et al.</p><p>https://arxiv.org/abs/2201.11903</p><p>[00:21:45] ARC-v2 benchmark tasks, Mike Knoop</p><p>https://arcprize.org/blog/introducing-arc-agi-public-leaderboard</p><p>[00:26:05] ARC Prize 2024: Technical Report, Francois Chollet et al.</p><p>https://arxiv.org/html/2412.04604v2</p><p>[00:32:45] ARC Prize 2024 Technical Report, Francois Chollet, Mike Knoop, Gregory Kamradt</p><p>https://arxiv.org/abs/2412.04604</p><p>[00:48:55] The Bitter Lesson, Rich Sutton</p><p>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</p><p>[00:53:30] Decoding strategies in neural text generation, Sina Zarrieß</p><p>https://www.mdpi.com/2078-2489/12/9/355/pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/ARC-Prize-v2-Launch--Francois-Chollet-and-Mike-Knoop-e30k12p</link>
			<guid isPermaLink="false">2c463a67-2add-4475-8bbd-b992de11b4f5</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 24 Mar 2025 20:26:47 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/100319769/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-24%2F103fc293-4801-926b-df34-d8027ffb69d0.mp3" length="78476572" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt; We are joined by Francois Chollet and Mike Knoop, to launch the new version of the ARC prize! In version 2, the challenges have been calibrated with humans such that at least 2 humans could solve each task in a reasonable task, but also adversarially selected so that frontier reasoning models can&amp;#39;t solve them. The best LLMs today get negligible performance on this challenge. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;https://arcprize.org/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/0v9o8xcpppdwnkntj59oi/ARCv2.pdf?rlkey=luqb6f141976vra6zdtptv5uj&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. ARC v2 Core Design &amp;amp; Objectives&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 ARC v2 Launch and Benchmark Architecture&lt;/p&gt;&lt;p&gt;   [00:03:16] 1.2 Test-Time Optimization and AGI Assessment&lt;/p&gt;&lt;p&gt;   [00:06:24] 1.3 Human-AI Capability Analysis&lt;/p&gt;&lt;p&gt;   [00:13:02] 1.4 OpenAI o3 Initial Performance Results&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. ARC Technical Evolution&lt;/p&gt;&lt;p&gt;   [00:17:20] 2.1 ARC-v1 to ARC-v2 Design Improvements&lt;/p&gt;&lt;p&gt;   [00:21:12] 2.2 Human Validation Methodology&lt;/p&gt;&lt;p&gt;   [00:26:05] 2.3 Task Design and Gaming Prevention&lt;/p&gt;&lt;p&gt;   [00:29:11] 2.4 Intelligence Measurement Framework&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. O3 Performance &amp;amp; Future Challenges&lt;/p&gt;&lt;p&gt;   [00:38:50] 3.1 O3 Comprehensive Performance Analysis&lt;/p&gt;&lt;p&gt;   [00:43:40] 3.2 System Limitations and Failure Modes&lt;/p&gt;&lt;p&gt;   [00:49:30] 3.3 Program Synthesis Applications&lt;/p&gt;&lt;p&gt;   [00:53:00] 3.4 Future Development Roadmap&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:00:15] On the Measure of Intelligence, François Chollet&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;&lt;p&gt;[00:06:45] ARC Prize Foundation, François Chollet, Mike Knoop&lt;/p&gt;&lt;p&gt;https://arcprize.org/&lt;/p&gt;&lt;p&gt;[00:12:50] OpenAI o3 model performance on ARC v1, ARC Prize Team&lt;/p&gt;&lt;p&gt;https://arcprize.org/blog/oai-o3-pub-breakthrough&lt;/p&gt;&lt;p&gt;[00:18:30] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Jason Wei et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2201.11903&lt;/p&gt;&lt;p&gt;[00:21:45] ARC-v2 benchmark tasks, Mike Knoop&lt;/p&gt;&lt;p&gt;https://arcprize.org/blog/introducing-arc-agi-public-leaderboard&lt;/p&gt;&lt;p&gt;[00:26:05] ARC Prize 2024: Technical Report, Francois Chollet et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2412.04604v2&lt;/p&gt;&lt;p&gt;[00:32:45] ARC Prize 2024 Technical Report, Francois Chollet, Mike Knoop, Gregory Kamradt&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2412.04604&lt;/p&gt;&lt;p&gt;[00:48:55] The Bitter Lesson, Rich Sutton&lt;/p&gt;&lt;p&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/p&gt;&lt;p&gt;[00:53:30] Decoding strategies in neural text generation, Sina Zarrieß&lt;/p&gt;&lt;p&gt;https://www.mdpi.com/2078-2489/12/9/355/pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:54:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1742847994818-f9d32896407bb.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Test-Time Adaptation: the key to reasoning with DL (Mohamed Osman)]]></title>
			<description><![CDATA[<p>Mohamed Osman joins to discuss MindsAI&#39;s highest scoring entry to the ARC challenge 2024 and the paradigm of test-time fine-tuning. They explore how the team, now part of Tufa Labs in Zurich, achieved state-of-the-art results using a combination of pre-training techniques, a unique meta-learning strategy, and an ensemble voting mechanism. Mohamed emphasizes the importance of raw data input and flexibility of the network.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + REFS:</p><p>https://www.dropbox.com/scl/fi/jeavyqidsjzjgjgd7ns7h/MoFInal.pdf?rlkey=cjjmo7rgtenxrr3b46nk6yq2e&amp;dl=0</p><p><br></p><p>Mohamed Osman (Tufa Labs)</p><p>https://x.com/MohamedOsmanML</p><p><br></p><p>Jack Cole (Tufa Labs)</p><p>https://x.com/MindsAI_Jack</p><p><br></p><p>How and why deep learning for ARC paper:</p><p>https://github.com/MohamedOsman1998/deep-learning-for-arc/blob/main/deep_learning_for_arc.pdf</p><p><br></p><p>TOC:</p><p>1. Abstract Reasoning Foundations</p><p> [00:00:00] 1.1 Test-Time Fine-Tuning and ARC Challenge Overview</p><p>   [00:10:20] 1.2 Neural Networks vs Programmatic Approaches to Reasoning</p><p>   [00:13:23] 1.3 Code-Based Learning and Meta-Model Architecture</p><p>   [00:20:26] 1.4 Technical Implementation with Long T5 Model</p><p><br></p><p>2. ARC Solution Architectures</p><p>   [00:24:10] 2.1 Test-Time Tuning and Voting Methods for ARC Solutions</p><p>   [00:27:54] 2.2 Model Generalization and Function Generation Challenges</p><p>   [00:32:53] 2.3 Input Representation and VLM Limitations</p><p>   [00:36:21] 2.4 Architecture Innovation and Cross-Modal Integration</p><p>   [00:40:05] 2.5 Future of ARC Challenge and Program Synthesis Approaches</p><p><br></p><p>3. Advanced Systems Integration</p><p>   [00:43:00] 3.1 DreamCoder Evolution and LLM Integration</p><p>   [00:50:07] 3.2 MindsAI Team Progress and Acquisition by Tufa Labs</p><p>   [00:54:15] 3.3 ARC v2 Development and Performance Scaling</p><p>   [00:58:22] 3.4 Intelligence Benchmarks and Transformer Limitations</p><p>   [01:01:50] 3.5 Neural Architecture Optimization and Processing Distribution</p><p><br></p><p>REFS:</p><p>[00:01:32] Original ARC challenge paper, François Chollet</p><p>https://arxiv.org/abs/1911.01547</p><p><br></p><p>[00:06:55] DreamCoder, Kevin Ellis et al.</p><p>https://arxiv.org/abs/2006.08381</p><p><br></p><p>[00:12:50] Deep Learning with Python, François Chollet</p><p>https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438</p><p><br></p><p>[00:13:35] Deep Learning with Python, François Chollet</p><p>https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438</p><p><br></p><p>[00:13:35] Influence of pretraining data for reasoning, Laura Ruis</p><p>https://arxiv.org/abs/2411.12580</p><p><br></p><p>[00:17:50] Latent Program Networks, Clement Bonnet</p><p>https://arxiv.org/html/2411.08706v1</p><p><br></p><p>[00:20:50] T5, Colin Raffel et al.</p><p>https://arxiv.org/abs/1910.10683</p><p><br></p><p>[00:30:30] Combining Induction and Transduction for Abstract Reasoning, Wen-Ding Li, Kevin Ellis et al.</p><p>https://arxiv.org/abs/2411.02272</p><p><br></p><p>[00:34:15] Six finger problem, Chen et al.</p><p>https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf</p><p><br></p><p>[00:38:15] DeepSeek-R1-Distill-Llama, DeepSeek AI</p><p>https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B</p><p><br></p><p>[00:40:10] ARC Prize 2024 Technical Report, François Chollet et al.</p><p>https://arxiv.org/html/2412.04604v2</p><p><br></p><p>[00:45:20] LLM-Guided Compositional Program Synthesis, Wen-Ding Li and Kevin Ellis</p><p>https://arxiv.org/html/2503.15540</p><p><br></p><p>[00:54:25] Abstraction and Reasoning Corpus, François Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p><br></p><p>[00:57:10] O3 breakthrough on ARC-AGI, OpenAI</p><p>https://arcprize.org/</p><p><br></p><p>[00:59:35] ConceptARC Benchmark, Arseny Moskvichev, Melanie Mitchell</p><p>https://arxiv.org/abs/2305.07141</p><p><br></p><p>[01:02:05] Mixtape: Breaking the Softmax Bottleneck Efficiently, Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.</p><p>http://papers.neurips.cc/paper/9723-mixtape-breaking-the-softmax-bottleneck-efficiently.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Test-Time-Adaptation-the-key-to-reasoning-with-DL-Mohamed-Osman-e30hd8t</link>
			<guid isPermaLink="false">8a9426f0-cc35-48c2-aecc-6c3cc7c284e7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 22 Mar 2025 22:48:25 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/100233949/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-22%2Fed4795dc-93de-d0ab-94b3-c2face88c2fd.mp3" length="92128832" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Mohamed Osman joins to discuss MindsAI&amp;#39;s highest scoring entry to the ARC challenge 2024 and the paradigm of test-time fine-tuning. They explore how the team, now part of Tufa Labs in Zurich, achieved state-of-the-art results using a combination of pre-training techniques, a unique meta-learning strategy, and an ensemble voting mechanism. Mohamed emphasizes the importance of raw data input and flexibility of the network.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + REFS:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/jeavyqidsjzjgjgd7ns7h/MoFInal.pdf?rlkey=cjjmo7rgtenxrr3b46nk6yq2e&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Mohamed Osman (Tufa Labs)&lt;/p&gt;&lt;p&gt;https://x.com/MohamedOsmanML&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Jack Cole (Tufa Labs)&lt;/p&gt;&lt;p&gt;https://x.com/MindsAI_Jack&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;How and why deep learning for ARC paper:&lt;/p&gt;&lt;p&gt;https://github.com/MohamedOsman1998/deep-learning-for-arc/blob/main/deep_learning_for_arc.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Abstract Reasoning Foundations&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Test-Time Fine-Tuning and ARC Challenge Overview&lt;/p&gt;&lt;p&gt;   [00:10:20] 1.2 Neural Networks vs Programmatic Approaches to Reasoning&lt;/p&gt;&lt;p&gt;   [00:13:23] 1.3 Code-Based Learning and Meta-Model Architecture&lt;/p&gt;&lt;p&gt;   [00:20:26] 1.4 Technical Implementation with Long T5 Model&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. ARC Solution Architectures&lt;/p&gt;&lt;p&gt;   [00:24:10] 2.1 Test-Time Tuning and Voting Methods for ARC Solutions&lt;/p&gt;&lt;p&gt;   [00:27:54] 2.2 Model Generalization and Function Generation Challenges&lt;/p&gt;&lt;p&gt;   [00:32:53] 2.3 Input Representation and VLM Limitations&lt;/p&gt;&lt;p&gt;   [00:36:21] 2.4 Architecture Innovation and Cross-Modal Integration&lt;/p&gt;&lt;p&gt;   [00:40:05] 2.5 Future of ARC Challenge and Program Synthesis Approaches&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Advanced Systems Integration&lt;/p&gt;&lt;p&gt;   [00:43:00] 3.1 DreamCoder Evolution and LLM Integration&lt;/p&gt;&lt;p&gt;   [00:50:07] 3.2 MindsAI Team Progress and Acquisition by Tufa Labs&lt;/p&gt;&lt;p&gt;   [00:54:15] 3.3 ARC v2 Development and Performance Scaling&lt;/p&gt;&lt;p&gt;   [00:58:22] 3.4 Intelligence Benchmarks and Transformer Limitations&lt;/p&gt;&lt;p&gt;   [01:01:50] 3.5 Neural Architecture Optimization and Processing Distribution&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:01:32] Original ARC challenge paper, François Chollet&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:06:55] DreamCoder, Kevin Ellis et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2006.08381&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:12:50] Deep Learning with Python, François Chollet&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:13:35] Deep Learning with Python, François Chollet&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:13:35] Influence of pretraining data for reasoning, Laura Ruis&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.12580&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:17:50] Latent Program Networks, Clement Bonnet&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2411.08706v1&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:20:50] T5, Colin Raffel et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1910.10683&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:30:30] Combining Induction and Transduction for Abstract Reasoning, Wen-Ding Li, Kevin Ellis et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.02272&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:34:15] Six finger problem, Chen et al.&lt;/p&gt;&lt;p&gt;https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:38:15] DeepSeek-R1-Distill-Llama, DeepSeek AI&lt;/p&gt;&lt;p&gt;https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:40:10] ARC Prize 2024 Technical Report, François Chollet et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2412.04604v2&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:45:20] LLM-Guided Compositional Program Synthesis, Wen-Ding Li and Kevin Ellis&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2503.15540&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:54:25] Abstraction and Reasoning Corpus, François Chollet&lt;/p&gt;&lt;p&gt;https://github.com/fchollet/ARC-AGI&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:57:10] O3 breakthrough on ARC-AGI, OpenAI&lt;/p&gt;&lt;p&gt;https://arcprize.org/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:59:35] ConceptARC Benchmark, Arseny Moskvichev, Melanie Mitchell&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2305.07141&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[01:02:05] Mixtape: Breaking the Softmax Bottleneck Efficiently, Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.&lt;/p&gt;&lt;p&gt;http://papers.neurips.cc/paper/9723-mixtape-breaking-the-softmax-bottleneck-efficiently.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:03:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1742683609426-b76a38d2c357c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[GSMSymbolic paper - Iman Mirzadeh (Apple)]]></title>
			<description><![CDATA[<p>Iman Mirzadeh from Apple, who recently published the GSM-Symbolic paper discusses the crucial distinction between intelligence and achievement in AI systems. He critiques current AI research methodologies, highlighting the limitations of Large Language Models (LLMs) in reasoning and knowledge representation. </p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + RESEARCH:</p><p>https://www.dropbox.com/scl/fi/mlcjl9cd5p1kem4l0vqd3/IMAN.pdf?rlkey=dqfqb74zr81a5gqr8r6c8isg3&amp;dl=0</p><p><br></p><p>TOC:</p><p>1. Intelligence vs Achievement in AI Systems</p><p> [00:00:00] 1.1 Intelligence vs Achievement Metrics in AI Systems</p><p>   [00:03:27] 1.2 AlphaZero and Abstract Understanding in Chess</p><p>   [00:10:10] 1.3 Language Models and Distribution Learning Limitations</p><p>   [00:14:47] 1.4 Research Methodology and Theoretical Frameworks</p><p><br></p><p>2. Intelligence Measurement and Learning</p><p>   [00:24:24] 2.1 LLM Capabilities: Interpolation vs True Reasoning</p><p>   [00:29:00] 2.2 Intelligence Definition and Measurement Approaches</p><p>   [00:34:35] 2.3 Learning Capabilities and Agency in AI Systems</p><p>   [00:39:26] 2.4 Abstract Reasoning and Symbol Understanding</p><p><br></p><p>3. LLM Performance and Evaluation</p><p>   [00:47:15] 3.1 Scaling Laws and Fundamental Limitations</p><p>   [00:54:33] 3.2 Connectionism vs Symbolism Debate in Neural Networks</p><p>   [00:58:09] 3.3 GSM-Symbolic: Testing Mathematical Reasoning in LLMs</p><p>   [01:08:38] 3.4 Benchmark Evaluation and Model Performance Assessment</p><p><br></p><p>REFS:</p><p>[00:01:00] AlphaZero chess AI system, Silver et al.</p><p>https://arxiv.org/abs/1712.01815</p><p>[00:07:10] Game Changer: AlphaZero&#39;s Groundbreaking Chess Strategies, Sadler &amp; Regan</p><p>https://www.amazon.com/Game-Changer-AlphaZeros-Groundbreaking-Strategies/dp/9056918184</p><p>[00:11:35] Cross-entropy loss in language modeling, Voita</p><p>http://lena-voita.github.io/nlp_course/language_modeling.html</p><p>[00:17:20] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in LLMs, Mirzadeh et al.</p><p>https://arxiv.org/abs/2410.05229</p><p>[00:21:25] Connectionism and Cognitive Architecture: A Critical Analysis, Fodor &amp; Pylyshyn</p><p>https://www.sciencedirect.com/science/article/pii/001002779090014B</p><p>[00:28:55] Brain-to-body mass ratio scaling laws, Sutskever</p><p>https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training</p><p>[00:29:40] On the Measure of Intelligence, Chollet</p><p>https://arxiv.org/abs/1911.01547</p><p>[00:33:30] On definition of intelligence, Gignac et al.</p><p>https://www.sciencedirect.com/science/article/pii/S0160289624000266</p><p>[00:35:30] Defining intelligence, Wang</p><p>https://cis.temple.edu/~wangp/papers.html</p><p>[00:37:40] How We Learn: Why Brains Learn Better Than Any Machine... for Now, Dehaene</p><p>https://www.amazon.com/How-We-Learn-Brains-Machine/dp/0525559884</p><p>[00:39:35] Surfaces and Essences: Analogy as the Fuel and Fire of Thinking, Hofstadter and Sander</p><p>https://www.amazon.com/Surfaces-Essences-Analogy-Fuel-Thinking/dp/0465018475</p><p>[00:43:15] Chain-of-thought prompting, Wei et al.</p><p>https://arxiv.org/abs/2201.11903</p><p>[00:47:20] Test-time scaling laws in machine learning, Brown</p><p>https://podcasts.apple.com/mv/podcast/openais-noam-brown-ilge-akkaya-and-hunter-lightman-on/id1750736528?i=1000671532058</p><p>[00:47:50] Scaling Laws for Neural Language Models, Kaplan et al.</p><p>https://arxiv.org/abs/2001.08361</p><p>[00:55:15] Tensor product variable binding, Smolensky</p><p>https://www.sciencedirect.com/science/article/abs/pii/000437029090007M</p><p>[01:08:45] GSM-8K dataset, OpenAI</p><p>https://huggingface.co/datasets/openai/gsm8k</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/GSMSymbolic-paper---Iman-Mirzadeh-Apple-e30dhvp</link>
			<guid isPermaLink="false">9bd7543c-4462-497e-9f60-7dc1dca826d1</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 19 Mar 2025 22:33:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/100107705/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-19%2F22dc98c2-78df-ec59-226d-b38b3fd4bd0e.mp3" length="103282360" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Iman Mirzadeh from Apple, who recently published the GSM-Symbolic paper discusses the crucial distinction between intelligence and achievement in AI systems. He critiques current AI research methodologies, highlighting the limitations of Large Language Models (LLMs) in reasoning and knowledge representation. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + RESEARCH:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/mlcjl9cd5p1kem4l0vqd3/IMAN.pdf?rlkey=dqfqb74zr81a5gqr8r6c8isg3&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Intelligence vs Achievement in AI Systems&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Intelligence vs Achievement Metrics in AI Systems&lt;/p&gt;&lt;p&gt;   [00:03:27] 1.2 AlphaZero and Abstract Understanding in Chess&lt;/p&gt;&lt;p&gt;   [00:10:10] 1.3 Language Models and Distribution Learning Limitations&lt;/p&gt;&lt;p&gt;   [00:14:47] 1.4 Research Methodology and Theoretical Frameworks&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Intelligence Measurement and Learning&lt;/p&gt;&lt;p&gt;   [00:24:24] 2.1 LLM Capabilities: Interpolation vs True Reasoning&lt;/p&gt;&lt;p&gt;   [00:29:00] 2.2 Intelligence Definition and Measurement Approaches&lt;/p&gt;&lt;p&gt;   [00:34:35] 2.3 Learning Capabilities and Agency in AI Systems&lt;/p&gt;&lt;p&gt;   [00:39:26] 2.4 Abstract Reasoning and Symbol Understanding&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. LLM Performance and Evaluation&lt;/p&gt;&lt;p&gt;   [00:47:15] 3.1 Scaling Laws and Fundamental Limitations&lt;/p&gt;&lt;p&gt;   [00:54:33] 3.2 Connectionism vs Symbolism Debate in Neural Networks&lt;/p&gt;&lt;p&gt;   [00:58:09] 3.3 GSM-Symbolic: Testing Mathematical Reasoning in LLMs&lt;/p&gt;&lt;p&gt;   [01:08:38] 3.4 Benchmark Evaluation and Model Performance Assessment&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:01:00] AlphaZero chess AI system, Silver et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1712.01815&lt;/p&gt;&lt;p&gt;[00:07:10] Game Changer: AlphaZero&amp;#39;s Groundbreaking Chess Strategies, Sadler &amp;amp; Regan&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Game-Changer-AlphaZeros-Groundbreaking-Strategies/dp/9056918184&lt;/p&gt;&lt;p&gt;[00:11:35] Cross-entropy loss in language modeling, Voita&lt;/p&gt;&lt;p&gt;http://lena-voita.github.io/nlp_course/language_modeling.html&lt;/p&gt;&lt;p&gt;[00:17:20] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in LLMs, Mirzadeh et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2410.05229&lt;/p&gt;&lt;p&gt;[00:21:25] Connectionism and Cognitive Architecture: A Critical Analysis, Fodor &amp;amp; Pylyshyn&lt;/p&gt;&lt;p&gt;https://www.sciencedirect.com/science/article/pii/001002779090014B&lt;/p&gt;&lt;p&gt;[00:28:55] Brain-to-body mass ratio scaling laws, Sutskever&lt;/p&gt;&lt;p&gt;https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training&lt;/p&gt;&lt;p&gt;[00:29:40] On the Measure of Intelligence, Chollet&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;&lt;p&gt;[00:33:30] On definition of intelligence, Gignac et al.&lt;/p&gt;&lt;p&gt;https://www.sciencedirect.com/science/article/pii/S0160289624000266&lt;/p&gt;&lt;p&gt;[00:35:30] Defining intelligence, Wang&lt;/p&gt;&lt;p&gt;https://cis.temple.edu/~wangp/papers.html&lt;/p&gt;&lt;p&gt;[00:37:40] How We Learn: Why Brains Learn Better Than Any Machine... for Now, Dehaene&lt;/p&gt;&lt;p&gt;https://www.amazon.com/How-We-Learn-Brains-Machine/dp/0525559884&lt;/p&gt;&lt;p&gt;[00:39:35] Surfaces and Essences: Analogy as the Fuel and Fire of Thinking, Hofstadter and Sander&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Surfaces-Essences-Analogy-Fuel-Thinking/dp/0465018475&lt;/p&gt;&lt;p&gt;[00:43:15] Chain-of-thought prompting, Wei et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2201.11903&lt;/p&gt;&lt;p&gt;[00:47:20] Test-time scaling laws in machine learning, Brown&lt;/p&gt;&lt;p&gt;https://podcasts.apple.com/mv/podcast/openais-noam-brown-ilge-akkaya-and-hunter-lightman-on/id1750736528?i=1000671532058&lt;/p&gt;&lt;p&gt;[00:47:50] Scaling Laws for Neural Language Models, Kaplan et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2001.08361&lt;/p&gt;&lt;p&gt;[00:55:15] Tensor product variable binding, Smolensky&lt;/p&gt;&lt;p&gt;https://www.sciencedirect.com/science/article/abs/pii/000437029090007M&lt;/p&gt;&lt;p&gt;[01:08:45] GSM-8K dataset, OpenAI&lt;/p&gt;&lt;p&gt;https://huggingface.co/datasets/openai/gsm8k&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:11:23</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1742423578962-cd2748f147e88.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Reasoning, Robustness, and Human Feedback in AI - Max Bartolo (Cohere)]]></title>
			<description><![CDATA[<p>Dr. Max Bartolo from Cohere discusses machine learning model development, evaluation, and robustness. Key topics include model reasoning, the DynaBench platform for dynamic benchmarking, data-centric AI development, model training challenges, and the limitations of human feedback mechanisms. The conversation also covers technical aspects like influence functions, model quantization, and the PRISM project.</p><p><br></p><p>Max Bartolo (Cohere):</p><p>https://www.maxbartolo.com/</p><p>https://cohere.com/command</p><p><br></p><p>TRANSCRIPT:</p><p>https://www.dropbox.com/scl/fi/vujxscaffw37pqgb6hpie/MAXB.pdf?rlkey=0oqjxs5u49eqa2m7uaol64lbw&amp;dl=0</p><p><br></p><p>TOC:</p><p>1. Model Reasoning and Verification</p><p> [00:00:00] 1.1 Model Consistency and Reasoning Verification</p><p>   [00:03:25] 1.2 Influence Functions and Distributed Knowledge Analysis</p><p>   [00:10:28] 1.3 AI Application Development and Model Deployment</p><p>   [00:14:24] 1.4 AI Alignment and Human Feedback Limitations</p><p><br></p><p>2. Evaluation and Bias Assessment</p><p>   [00:20:15] 2.1 Human Evaluation Challenges and Factuality Assessment</p><p>   [00:27:15] 2.2 Cultural and Demographic Influences on Model Behavior</p><p>   [00:32:43] 2.3 Adversarial Examples and Model Robustness</p><p><br></p><p>3. Benchmarking Systems and Methods</p><p>   [00:41:54] 3.1 DynaBench and Dynamic Benchmarking Approaches</p><p>   [00:50:02] 3.2 Benchmarking Challenges and Alternative Metrics</p><p>   [00:50:33] 3.3 Evolution of Model Benchmarking Methods</p><p>   [00:51:15] 3.4 Hierarchical Capability Testing Framework</p><p>   [00:52:35] 3.5 Benchmark Platforms and Tools</p><p><br></p><p>4. Model Architecture and Performance</p><p>   [00:55:15] 4.1 Cohere&#39;s Model Development Process</p><p>   [01:00:26] 4.2 Model Quantization and Performance Evaluation</p><p>   [01:05:18] 4.3 Reasoning Capabilities and Benchmark Standards</p><p>   [01:08:27] 4.4 Training Progression and Technical Challenges</p><p><br></p><p>5. Future Directions and Challenges</p><p>   [01:13:48] 5.1 Context Window Evolution and Trade-offs</p><p>   [01:22:47] 5.2 Enterprise Applications and Future Challenges</p><p><br></p><p>REFS:</p><p>[00:03:10] Research at Cohere with Laura Ruis et al., Max Bartolo, Laura Ruis et al.</p><p>https://cohere.com/research/papers/procedural-knowledge-in-pretraining-drives-reasoning-in-large-language-models-2024-11-20</p><p>[00:04:15] Influence functions in machine learning, Koh &amp; Liang</p><p>https://arxiv.org/abs/1703.04730</p><p>[00:08:05] Studying Large Language Model Generalization with Influence Functions, Roger Grosse et al.</p><p>https://storage.prod.researchhub.com/uploads/papers/2023/08/08/2308.03296.pdf</p><p>[00:11:10] The LLM ARChitect: Solving ARC-AGI Is A Matter of Perspective, Daniel Franzen, Jan Disselhoff, and David Hartmann</p><p>https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf</p><p>[00:12:10] Hugging Face model repo for C4AI Command A, Cohere and Cohere For AI</p><p>https://huggingface.co/CohereForAI/c4ai-command-a-03-2025</p><p>[00:13:30] OpenInterpreter</p><p>https://github.com/KillianLucas/open-interpreter</p><p>[00:16:15] Human Feedback is not Gold Standard, Tom Hosking, Max Bartolo, Phil Blunsom</p><p>https://arxiv.org/abs/2309.16349</p><p>[00:27:15] The PRISM Alignment Dataset, Hannah Kirk et al.</p><p>https://arxiv.org/abs/2404.16019</p><p>[00:32:50] How adversarial examples arise, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry</p><p>https://arxiv.org/abs/1905.02175</p><p>[00:43:00] DynaBench platform paper, Douwe Kiela et al.</p><p>https://aclanthology.org/2021.naacl-main.324.pdf</p><p>[00:50:15] Sara Hooker&#39;s work on compute limitations, Sara Hooker</p><p>https://arxiv.org/html/2407.05694v1</p><p>[00:53:25] DataPerf: Community-led benchmark suite, Mazumder et al.</p><p>https://arxiv.org/abs/2207.10062</p><p>[01:04:35] DROP, Dheeru Dua et al.</p><p>https://arxiv.org/abs/1903.00161</p><p>[01:07:05] GSM8k, Cobbe et al.</p><p>https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k</p><p>[01:09:30] ARC, François Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p>[01:15:50] Command A, Cohere</p><p>https://cohere.com/blog/command-a</p><p>[01:22:55] Enterprise search using LLMs, Cohere</p><p>https://cohere.com/blog/commonly-asked-questions-about-search-from-coheres-enterprise-customers</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Reasoning--Robustness--and-Human-Feedback-in-AI---Max-Bartolo-Cohere-e30c1uu</link>
			<guid isPermaLink="false">c3634932-ec71-42b8-ad4f-ac837de15909</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 18 Mar 2025 23:06:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/100058526/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-18%2F936d6a62-ffe6-effb-7f41-e798337ea80c.mp3" length="120222349" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Max Bartolo from Cohere discusses machine learning model development, evaluation, and robustness. Key topics include model reasoning, the DynaBench platform for dynamic benchmarking, data-centric AI development, model training challenges, and the limitations of human feedback mechanisms. The conversation also covers technical aspects like influence functions, model quantization, and the PRISM project.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Max Bartolo (Cohere):&lt;/p&gt;&lt;p&gt;https://www.maxbartolo.com/&lt;/p&gt;&lt;p&gt;https://cohere.com/command&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/vujxscaffw37pqgb6hpie/MAXB.pdf?rlkey=0oqjxs5u49eqa2m7uaol64lbw&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Model Reasoning and Verification&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Model Consistency and Reasoning Verification&lt;/p&gt;&lt;p&gt;   [00:03:25] 1.2 Influence Functions and Distributed Knowledge Analysis&lt;/p&gt;&lt;p&gt;   [00:10:28] 1.3 AI Application Development and Model Deployment&lt;/p&gt;&lt;p&gt;   [00:14:24] 1.4 AI Alignment and Human Feedback Limitations&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Evaluation and Bias Assessment&lt;/p&gt;&lt;p&gt;   [00:20:15] 2.1 Human Evaluation Challenges and Factuality Assessment&lt;/p&gt;&lt;p&gt;   [00:27:15] 2.2 Cultural and Demographic Influences on Model Behavior&lt;/p&gt;&lt;p&gt;   [00:32:43] 2.3 Adversarial Examples and Model Robustness&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Benchmarking Systems and Methods&lt;/p&gt;&lt;p&gt;   [00:41:54] 3.1 DynaBench and Dynamic Benchmarking Approaches&lt;/p&gt;&lt;p&gt;   [00:50:02] 3.2 Benchmarking Challenges and Alternative Metrics&lt;/p&gt;&lt;p&gt;   [00:50:33] 3.3 Evolution of Model Benchmarking Methods&lt;/p&gt;&lt;p&gt;   [00:51:15] 3.4 Hierarchical Capability Testing Framework&lt;/p&gt;&lt;p&gt;   [00:52:35] 3.5 Benchmark Platforms and Tools&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;4. Model Architecture and Performance&lt;/p&gt;&lt;p&gt;   [00:55:15] 4.1 Cohere&amp;#39;s Model Development Process&lt;/p&gt;&lt;p&gt;   [01:00:26] 4.2 Model Quantization and Performance Evaluation&lt;/p&gt;&lt;p&gt;   [01:05:18] 4.3 Reasoning Capabilities and Benchmark Standards&lt;/p&gt;&lt;p&gt;   [01:08:27] 4.4 Training Progression and Technical Challenges&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;5. Future Directions and Challenges&lt;/p&gt;&lt;p&gt;   [01:13:48] 5.1 Context Window Evolution and Trade-offs&lt;/p&gt;&lt;p&gt;   [01:22:47] 5.2 Enterprise Applications and Future Challenges&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:03:10] Research at Cohere with Laura Ruis et al., Max Bartolo, Laura Ruis et al.&lt;/p&gt;&lt;p&gt;https://cohere.com/research/papers/procedural-knowledge-in-pretraining-drives-reasoning-in-large-language-models-2024-11-20&lt;/p&gt;&lt;p&gt;[00:04:15] Influence functions in machine learning, Koh &amp;amp; Liang&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1703.04730&lt;/p&gt;&lt;p&gt;[00:08:05] Studying Large Language Model Generalization with Influence Functions, Roger Grosse et al.&lt;/p&gt;&lt;p&gt;https://storage.prod.researchhub.com/uploads/papers/2023/08/08/2308.03296.pdf&lt;/p&gt;&lt;p&gt;[00:11:10] The LLM ARChitect: Solving ARC-AGI Is A Matter of Perspective, Daniel Franzen, Jan Disselhoff, and David Hartmann&lt;/p&gt;&lt;p&gt;https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf&lt;/p&gt;&lt;p&gt;[00:12:10] Hugging Face model repo for C4AI Command A, Cohere and Cohere For AI&lt;/p&gt;&lt;p&gt;https://huggingface.co/CohereForAI/c4ai-command-a-03-2025&lt;/p&gt;&lt;p&gt;[00:13:30] OpenInterpreter&lt;/p&gt;&lt;p&gt;https://github.com/KillianLucas/open-interpreter&lt;/p&gt;&lt;p&gt;[00:16:15] Human Feedback is not Gold Standard, Tom Hosking, Max Bartolo, Phil Blunsom&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2309.16349&lt;/p&gt;&lt;p&gt;[00:27:15] The PRISM Alignment Dataset, Hannah Kirk et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2404.16019&lt;/p&gt;&lt;p&gt;[00:32:50] How adversarial examples arise, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1905.02175&lt;/p&gt;&lt;p&gt;[00:43:00] DynaBench platform paper, Douwe Kiela et al.&lt;/p&gt;&lt;p&gt;https://aclanthology.org/2021.naacl-main.324.pdf&lt;/p&gt;&lt;p&gt;[00:50:15] Sara Hooker&amp;#39;s work on compute limitations, Sara Hooker&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2407.05694v1&lt;/p&gt;&lt;p&gt;[00:53:25] DataPerf: Community-led benchmark suite, Mazumder et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2207.10062&lt;/p&gt;&lt;p&gt;[01:04:35] DROP, Dheeru Dua et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1903.00161&lt;/p&gt;&lt;p&gt;[01:07:05] GSM8k, Cobbe et al.&lt;/p&gt;&lt;p&gt;https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k&lt;/p&gt;&lt;p&gt;[01:09:30] ARC, François Chollet&lt;/p&gt;&lt;p&gt;https://github.com/fchollet/ARC-AGI&lt;/p&gt;&lt;p&gt;[01:15:50] Command A, Cohere&lt;/p&gt;&lt;p&gt;https://cohere.com/blog/command-a&lt;/p&gt;&lt;p&gt;[01:22:55] Enterprise search using LLMs, Cohere&lt;/p&gt;&lt;p&gt;https://cohere.com/blog/commonly-asked-questions-about-search-from-coheres-enterprise-customers&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:23:11</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1742339116157-18ce47f5db9d7.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Tau Language: The Software Synthesis Future (sponsored)]]></title>
			<description><![CDATA[<p>This sponsored episode features mathematician Ohad Asor discussing logical approaches to AI, focusing on the limitations of machine learning and introducing the Tau language for software development and blockchain tech. Asor argues that machine learning cannot guarantee correctness. Tau allows logical specification of software requirements, automatically creating provably correct implementations with potential to revolutionize distributed systems. The discussion highlights program synthesis, software updates, and applications in finance and governance.SPONSOR MESSAGES:***Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. Goto https://tufalabs.ai/***TRANSCRIPT + RESEARCH:https://www.dropbox.com/scl/fi/t849j6v1juk3gc15g4rsy/TAU.pdf?rlkey=hh11h2mhog3ncdbeapbzpzctc&amp;dl=0Tau:https://tau.net/Tau Language:https://tau.ai/tau-language/Research:https://tau.net/Theories-and-Applications-of-Boolean-Algebras-0.29.pdfTOC:1. Machine Learning Foundations and Limitations [00:00:00] 1.1 Fundamental Limitations of Machine Learning and PAC Learning Theory   [00:04:50] 1.2 Transductive Learning and the Three Curses of Machine Learning   [00:08:57] 1.3 Language, Reality, and AI System Design   [00:12:58] 1.4 Program Synthesis and Formal Verification Approaches2. Logical Programming Architecture   [00:31:55] 2.1 Safe AI Development Requirements   [00:32:05] 2.2 Self-Referential Language Architecture   [00:32:50] 2.3 Boolean Algebra and Logical Foundations   [00:37:52] 2.4 SAT Solvers and Complexity Challenges   [00:44:30] 2.5 Program Synthesis and Specification   [00:47:39] 2.6 Overcoming Tarski&#39;s Undefinability with Boolean Algebra   [00:56:05] 2.7 Tau Language Implementation and User Control3. Blockchain-Based Software Governance   [01:09:10] 3.1 User Control and Software Governance Mechanisms   [01:18:27] 3.2 Tau&#39;s Blockchain Architecture and Meta-Programming Capabilities   [01:21:43] 3.3 Development Status and Token Implementation   [01:24:52] 3.4 Consensus Building and Opinion Mapping System   [01:35:29] 3.5 Automation and Financial ApplicationsCORE REFS (more in pinned comment):[00:03:45] PAC (Probably Approximately Correct) Learning framework, Leslie Valianthttps://en.wikipedia.org/wiki/Probably_approximately_correct_learning[00:06:10] Boolean Satisfiability Problem (SAT), Varioushttps://en.wikipedia.org/wiki/Boolean_satisfiability_problem[00:13:55] Knowledge as Justified True Belief (JTB), Matthias Steuphttps://plato.stanford.edu/entries/epistemology/[00:17:50] Wittgenstein&#39;s concept of the limits of language, Ludwig Wittgensteinhttps://plato.stanford.edu/entries/wittgenstein/[00:21:25] Boolean algebras, Ohad Osorhttps://tau.net/tau-language-research/[00:26:10] The Halting Problemhttps://plato.stanford.edu/entries/turing-machine/#HaltProb[00:30:25] Alfred Tarski (1901-1983), Mario Gómez-Torrentehttps://plato.stanford.edu/entries/tarski/[00:41:50] DPLLhttps://www.cs.princeton.edu/~zkincaid/courses/fall18/readings/SATHandbook-CDCL.pdf[00:49:50] Tarski&#39;s undefinability theorem (1936), Alfred Tarskihttps://plato.stanford.edu/entries/tarski-truth/[00:51:45] Boolean Algebra mathematical foundations, J. Donald Monkhttps://plato.stanford.edu/entries/boolalg-math/[01:02:35] Belief Revision Theory and AGM Postulates, Sven Ove Hanssonhttps://plato.stanford.edu/entries/logic-belief-revision/[01:05:35] Quantifier elimination in atomless boolean algebra, H. Jerome Keislerhttps://people.math.wisc.edu/~hkeisler/random.pdf[01:08:35] Quantifier elimination in Tau language specification, Ohad Asorhttps://tau.ai/Theories-and-Applications-of-Boolean-Algebras-0.29.pdf[01:11:50] Tau Net blockchain platformhttps://tau.net/[01:19:20] Tau blockchain&#39;s innovative approach treating blockchain code itself as a contracthttps://tau.net/Whitepaper.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Tau-Language-The-Software-Synthesis-Future-sponsored-e3038m1</link>
			<guid isPermaLink="false">45e533c7-66eb-4b92-ae84-2eb8369b2ee4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 12 Mar 2025 21:53:03 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/99770497/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-12%2F4ffc9948-b05c-e06b-ad86-719c2d41344d.mp3" length="146407155" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This sponsored episode features mathematician Ohad Asor discussing logical approaches to AI, focusing on the limitations of machine learning and introducing the Tau language for software development and blockchain tech. Asor argues that machine learning cannot guarantee correctness. Tau allows logical specification of software requirements, automatically creating provably correct implementations with potential to revolutionize distributed systems. The discussion highlights program synthesis, software updates, and applications in finance and governance.SPONSOR MESSAGES:***Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. Goto https://tufalabs.ai/***TRANSCRIPT + RESEARCH:https://www.dropbox.com/scl/fi/t849j6v1juk3gc15g4rsy/TAU.pdf?rlkey=hh11h2mhog3ncdbeapbzpzctc&amp;amp;dl=0Tau:https://tau.net/Tau Language:https://tau.ai/tau-language/Research:https://tau.net/Theories-and-Applications-of-Boolean-Algebras-0.29.pdfTOC:1. Machine Learning Foundations and Limitations [00:00:00] 1.1 Fundamental Limitations of Machine Learning and PAC Learning Theory   [00:04:50] 1.2 Transductive Learning and the Three Curses of Machine Learning   [00:08:57] 1.3 Language, Reality, and AI System Design   [00:12:58] 1.4 Program Synthesis and Formal Verification Approaches2. Logical Programming Architecture   [00:31:55] 2.1 Safe AI Development Requirements   [00:32:05] 2.2 Self-Referential Language Architecture   [00:32:50] 2.3 Boolean Algebra and Logical Foundations   [00:37:52] 2.4 SAT Solvers and Complexity Challenges   [00:44:30] 2.5 Program Synthesis and Specification   [00:47:39] 2.6 Overcoming Tarski&amp;#39;s Undefinability with Boolean Algebra   [00:56:05] 2.7 Tau Language Implementation and User Control3. Blockchain-Based Software Governance   [01:09:10] 3.1 User Control and Software Governance Mechanisms   [01:18:27] 3.2 Tau&amp;#39;s Blockchain Architecture and Meta-Programming Capabilities   [01:21:43] 3.3 Development Status and Token Implementation   [01:24:52] 3.4 Consensus Building and Opinion Mapping System   [01:35:29] 3.5 Automation and Financial ApplicationsCORE REFS (more in pinned comment):[00:03:45] PAC (Probably Approximately Correct) Learning framework, Leslie Valianthttps://en.wikipedia.org/wiki/Probably_approximately_correct_learning[00:06:10] Boolean Satisfiability Problem (SAT), Varioushttps://en.wikipedia.org/wiki/Boolean_satisfiability_problem[00:13:55] Knowledge as Justified True Belief (JTB), Matthias Steuphttps://plato.stanford.edu/entries/epistemology/[00:17:50] Wittgenstein&amp;#39;s concept of the limits of language, Ludwig Wittgensteinhttps://plato.stanford.edu/entries/wittgenstein/[00:21:25] Boolean algebras, Ohad Osorhttps://tau.net/tau-language-research/[00:26:10] The Halting Problemhttps://plato.stanford.edu/entries/turing-machine/#HaltProb[00:30:25] Alfred Tarski (1901-1983), Mario Gómez-Torrentehttps://plato.stanford.edu/entries/tarski/[00:41:50] DPLLhttps://www.cs.princeton.edu/~zkincaid/courses/fall18/readings/SATHandbook-CDCL.pdf[00:49:50] Tarski&amp;#39;s undefinability theorem (1936), Alfred Tarskihttps://plato.stanford.edu/entries/tarski-truth/[00:51:45] Boolean Algebra mathematical foundations, J. Donald Monkhttps://plato.stanford.edu/entries/boolalg-math/[01:02:35] Belief Revision Theory and AGM Postulates, Sven Ove Hanssonhttps://plato.stanford.edu/entries/logic-belief-revision/[01:05:35] Quantifier elimination in atomless boolean algebra, H. Jerome Keislerhttps://people.math.wisc.edu/~hkeisler/random.pdf[01:08:35] Quantifier elimination in Tau language specification, Ohad Asorhttps://tau.ai/Theories-and-Applications-of-Boolean-Algebras-0.29.pdf[01:11:50] Tau Net blockchain platformhttps://tau.net/[01:19:20] Tau blockchain&amp;#39;s innovative approach treating blockchain code itself as a contracthttps://tau.net/Whitepaper.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:41:19</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1741816347208-ea7210d7b1172.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[John Palazza - Vice President of Global Sales @ CentML ( sponsored)]]></title>
			<description><![CDATA[<p>John Palazza from CentML joins us in this sponsored interview to discuss the critical importance of infrastructure optimization in the age of Large Language Models and Generative AI. We explore how enterprises can transition from the innovation phase to production and scale, highlighting the significance of efficient GPU utilization and cost management. The conversation covers the open-source versus proprietary model debate, the rise of AI agents, and the need for platform independence to avoid vendor lock-in, as well as emerging trends in AI infrastructure and the pivotal role of strategic partnerships.</p><p><br /></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br /></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br /></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br /></p><p>TRANSCRIPT:</p><p>https://www.dropbox.com/scl/fi/dnjsygrgdgq5ng5fdlfjg/JOHNPALAZZA.pdf?rlkey=hl9wyydi9mj077rbg5acdmo3a&amp;dl=0</p><p><br /></p><p>John Palazza:</p><p>Vice President of Global Sales @ CentML</p><p>https://www.linkedin.com/in/john-p-b34655/</p><p><br /></p><p>TOC:</p><p>1. Enterprise AI Organization and Strategy</p><p> [00:00:00] 1.1 Organizational Structure and ML Ownership</p><p> [00:02:59] 1.2 Infrastructure Efficiency and GPU Utilization</p><p>   [00:07:59] 1.3 Platform Centralization vs Team Autonomy</p><p>   [00:11:32] 1.4 Enterprise AI Adoption Strategy and Leadership</p><p><br /></p><p>2. MLOps Infrastructure and Resource Management</p><p>   [00:15:08] 2.1 Technology Evolution and Enterprise Integration</p><p>   [00:19:10] 2.2 Enterprise MLOps Platform Development</p><p>   [00:22:15] 2.3 AI Interface Evolution and Agent-Based Solutions</p><p>   [00:25:47] 2.4 CentML's Infrastructure Solutions</p><p>   [00:30:00] 2.5 Workload Abstraction and Resource Allocation</p><p><br /></p><p>3. LLM Infrastructure Optimization and Independence</p><p>   [00:33:10] 3.1 GPU Optimization and Cost Efficiency</p><p>   [00:36:47] 3.2 AI Efficiency and Innovation Challenges</p><p>   [00:41:40] 3.3 Cloud Provider Strategy and Infrastructure Control</p><p>   [00:46:52] 3.4 Platform Independence and Vendor Lock-in</p><p>   [00:50:53] 3.5 Technical Innovation and Growth Strategy</p><p><br /></p><p>REFS:</p><p>[00:01:25] Apple Acquires GraphLab, Apple Inc.</p><p>https://techcrunch.com/2016/08/05/apple-acquires-turi-a-machine-learning-company/</p><p>[00:03:50] Bain Tech Report 2024, Gartner</p><p>https://www.bain.com/insights/topics/technology-report/</p><p>[00:04:50] PaaS vs IaaS Efficiency, Gartner</p><p>https://www.gartner.com/en/newsroom/press-releases/2024-11-19-gartner-forecasts-worldwide-public-cloud-end-user-spending-to-total-723-billion-dollars-in-2025</p><p>[00:14:55] Fashion Quote, Oscar Wilde</p><p>https://www.amazon.com/Complete-Works-Oscar-Wilde-Collins/dp/0007144369</p><p>[00:15:30] PointCast Network, PointCast Inc.</p><p>https://en.wikipedia.org/wiki/Push_technology</p><p>[00:18:05] AI Bain Report, Bain &amp; Company</p><p>https://www.bain.com/insights/how-generative-ai-changes-the-game-in-tech-services-tech-report-2024/</p><p>[00:20:40] Uber Michelangelo, Uber Engineering Team</p><p>https://www.uber.com/en-SE/blog/michelangelo-machine-learning-platform/</p><p>[00:20:50] Algorithmia Acquisition, DataRobot</p><p>https://www.datarobot.com/newsroom/press/datarobot-is-acquiring-algorithmia-enhancing-leading-mlops-architecture-for-the-enterprise/</p><p>[00:22:55] Fine Tuning vs RAG, Heydar Soudani, Evangelos Kanoulas &amp; Faegheh Hasibi.</p><p>https://arxiv.org/html/2403.01432v2</p><p>[00:24:40] LLM Agent Survey, Lei Wang et al.</p><p>https://arxiv.org/abs/2308.11432</p><p>[00:26:30] CentML CServe, CentML</p><p>https://docs.centml.ai/apps/llm</p><p>[00:29:15] CentML Snowflake, Snowflake</p><p>https://www.snowflake.com/en/engineering-blog/optimize-llms-with-llama-snowflake-ai-stack/</p><p>[00:30:15] NVIDIA H100 GPU, NVIDIA</p><p>https://www.nvidia.com/en-us/data-center/h100/</p><p>[00:33:25] CentML\'s 60% savings, CentML</p><p>https://centml.ai/platform/</p><p></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/John-Palazza---Vice-President-of-Global-Sales--CentML--sponsored-e3001cd</link>
			<guid isPermaLink="false">90450b9b-9fa1-48ce-b2b1-09909ac041c8</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 10 Mar 2025 22:31:37 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/99664717/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-10%2F883e8222-2dd8-6ae6-fc66-bcb3048f7610.mp3" length="79452374" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;John Palazza from CentML joins us in this sponsored interview to discuss the critical importance of infrastructure optimization in the age of Large Language Models and Generative AI. We explore how enterprises can transition from the innovation phase to production and scale, highlighting the significance of efficient GPU utilization and cost management. The conversation covers the open-source versus proprietary model debate, the rise of AI agents, and the need for platform independence to avoid vendor lock-in, as well as emerging trends in AI infrastructure and the pivotal role of strategic partnerships.&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/dnjsygrgdgq5ng5fdlfjg/JOHNPALAZZA.pdf?rlkey=hl9wyydi9mj077rbg5acdmo3a&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;John Palazza:&lt;/p&gt;&lt;p&gt;Vice President of Global Sales @ CentML&lt;/p&gt;&lt;p&gt;https://www.linkedin.com/in/john-p-b34655/&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Enterprise AI Organization and Strategy&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Organizational Structure and ML Ownership&lt;/p&gt;&lt;p&gt; [00:02:59] 1.2 Infrastructure Efficiency and GPU Utilization&lt;/p&gt;&lt;p&gt;   [00:07:59] 1.3 Platform Centralization vs Team Autonomy&lt;/p&gt;&lt;p&gt;   [00:11:32] 1.4 Enterprise AI Adoption Strategy and Leadership&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;2. MLOps Infrastructure and Resource Management&lt;/p&gt;&lt;p&gt;   [00:15:08] 2.1 Technology Evolution and Enterprise Integration&lt;/p&gt;&lt;p&gt;   [00:19:10] 2.2 Enterprise MLOps Platform Development&lt;/p&gt;&lt;p&gt;   [00:22:15] 2.3 AI Interface Evolution and Agent-Based Solutions&lt;/p&gt;&lt;p&gt;   [00:25:47] 2.4 CentML&apos;s Infrastructure Solutions&lt;/p&gt;&lt;p&gt;   [00:30:00] 2.5 Workload Abstraction and Resource Allocation&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;3. LLM Infrastructure Optimization and Independence&lt;/p&gt;&lt;p&gt;   [00:33:10] 3.1 GPU Optimization and Cost Efficiency&lt;/p&gt;&lt;p&gt;   [00:36:47] 3.2 AI Efficiency and Innovation Challenges&lt;/p&gt;&lt;p&gt;   [00:41:40] 3.3 Cloud Provider Strategy and Infrastructure Control&lt;/p&gt;&lt;p&gt;   [00:46:52] 3.4 Platform Independence and Vendor Lock-in&lt;/p&gt;&lt;p&gt;   [00:50:53] 3.5 Technical Innovation and Growth Strategy&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:01:25] Apple Acquires GraphLab, Apple Inc.&lt;/p&gt;&lt;p&gt;https://techcrunch.com/2016/08/05/apple-acquires-turi-a-machine-learning-company/&lt;/p&gt;&lt;p&gt;[00:03:50] Bain Tech Report 2024, Gartner&lt;/p&gt;&lt;p&gt;https://www.bain.com/insights/topics/technology-report/&lt;/p&gt;&lt;p&gt;[00:04:50] PaaS vs IaaS Efficiency, Gartner&lt;/p&gt;&lt;p&gt;https://www.gartner.com/en/newsroom/press-releases/2024-11-19-gartner-forecasts-worldwide-public-cloud-end-user-spending-to-total-723-billion-dollars-in-2025&lt;/p&gt;&lt;p&gt;[00:14:55] Fashion Quote, Oscar Wilde&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Complete-Works-Oscar-Wilde-Collins/dp/0007144369&lt;/p&gt;&lt;p&gt;[00:15:30] PointCast Network, PointCast Inc.&lt;/p&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Push_technology&lt;/p&gt;&lt;p&gt;[00:18:05] AI Bain Report, Bain &amp;amp; Company&lt;/p&gt;&lt;p&gt;https://www.bain.com/insights/how-generative-ai-changes-the-game-in-tech-services-tech-report-2024/&lt;/p&gt;&lt;p&gt;[00:20:40] Uber Michelangelo, Uber Engineering Team&lt;/p&gt;&lt;p&gt;https://www.uber.com/en-SE/blog/michelangelo-machine-learning-platform/&lt;/p&gt;&lt;p&gt;[00:20:50] Algorithmia Acquisition, DataRobot&lt;/p&gt;&lt;p&gt;https://www.datarobot.com/newsroom/press/datarobot-is-acquiring-algorithmia-enhancing-leading-mlops-architecture-for-the-enterprise/&lt;/p&gt;&lt;p&gt;[00:22:55] Fine Tuning vs RAG, Heydar Soudani, Evangelos Kanoulas &amp;amp; Faegheh Hasibi.&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2403.01432v2&lt;/p&gt;&lt;p&gt;[00:24:40] LLM Agent Survey, Lei Wang et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2308.11432&lt;/p&gt;&lt;p&gt;[00:26:30] CentML CServe, CentML&lt;/p&gt;&lt;p&gt;https://docs.centml.ai/apps/llm&lt;/p&gt;&lt;p&gt;[00:29:15] CentML Snowflake, Snowflake&lt;/p&gt;&lt;p&gt;https://www.snowflake.com/en/engineering-blog/optimize-llms-with-llama-snowflake-ai-stack/&lt;/p&gt;&lt;p&gt;[00:30:15] NVIDIA H100 GPU, NVIDIA&lt;/p&gt;&lt;p&gt;https://www.nvidia.com/en-us/data-center/h100/&lt;/p&gt;&lt;p&gt;[00:33:25] CentML\&apos;s 60% savings, CentML&lt;/p&gt;&lt;p&gt;https://centml.ai/platform/&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:54:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1741645876096-0b684227dfa7f.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Transformers Need Glasses! - Federico Barbero]]></title>
			<description><![CDATA[<p>Federico Barbero (DeepMind/Oxford) is the lead author of &quot;Transformers Need Glasses!&quot;. </p><p><br></p><p>Have you ever wondered why LLMs struggle with seemingly simple tasks like counting or copying long strings of text? We break down the theoretical reasons behind these failures, revealing architectural bottlenecks and the challenges of maintaining information fidelity across extended contexts.</p><p><br></p><p>Federico explains how these issues are rooted in the transformer&#39;s design, drawing parallels to over-squashing in graph neural networks and detailing how the softmax function limits sharp decision-making.</p><p><br></p><p>But it&#39;s not all bad news! Discover practical &quot;glasses&quot; that can help transformers see more clearly, from simple input modifications to architectural tweaks.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>https://federicobarbero.com/</p><p><br></p><p>TRANSCRIPT + RESEARCH:</p><p>https://www.dropbox.com/s/h7ys83ztwktqjje/Federico.pdf?dl=0</p><p><br></p><p>TOC:</p><p>1. Transformer Limitations: Token Detection &amp; Representation</p><p>[00:00:00] 1.1 Transformers fail at single token detection</p><p>[00:02:45] 1.2 Representation collapse in transformers</p><p>[00:03:21] 1.3 Experiment: LLMs fail at copying last tokens</p><p>[00:18:00] 1.4 Attention sharpness limitations in transformers</p><p><br></p><p>2. Transformer Limitations: Information Flow &amp; Quantization</p><p>[00:18:50] 2.1 Unidirectional information mixing</p><p>[00:18:50] 2.2 Unidirectional information flow towards sequence beginning in transformers</p><p>[00:21:50] 2.3 Diagonal attention heads as expensive no-ops in LAMA/Gemma</p><p>[00:27:14] 2.4 Sequence entropy affects transformer model distinguishability</p><p>[00:30:36] 2.5 Quantization limitations lead to information loss &amp; representational collapse</p><p>[00:38:34] 2.6 LLMs use subitizing as opposed to counting algorithms</p><p><br></p><p>3. Transformers and the Nature of Reasoning</p><p>[00:40:30] 3.1 Turing completeness conditions in transformers</p><p>[00:43:23] 3.2 Transformers struggle with sequential tasks</p><p>[00:45:50] 3.3 Windowed attention as solution to information compression</p><p>[00:51:04] 3.4 Chess engines: mechanical computation vs creative reasoning</p><p>[01:00:35] 3.5 Epistemic foraging introduced</p><p><br></p><p>REFS:</p><p>[00:01:05] Transformers Need Glasses!, Barbero et al.</p><p>https://proceedings.neurips.cc/paper_files/paper/2024/file/b1d35561c4a4a0e0b6012b2af531e149-Paper-Conference.pdf</p><p><br></p><p>[00:05:30] Softmax is Not Enough, Veličković et al.</p><p>https://arxiv.org/abs/2410.01104</p><p><br></p><p>[00:11:30] Adv Alg Lecture 15, Chawla</p><p>https://pages.cs.wisc.edu/~shuchi/courses/787-F09/scribe-notes/lec15.pdf</p><p><br></p><p>[00:15:05] Graph Attention Networks, Veličković</p><p>https://arxiv.org/abs/1710.10903</p><p><br></p><p>[00:19:15] Extract Training Data, Carlini et al.</p><p>https://arxiv.org/pdf/2311.17035</p><p><br></p><p>[00:31:30] 1-bit LLMs, Ma et al.</p><p>https://arxiv.org/abs/2402.17764</p><p><br></p><p>[00:38:35] LLMs Solve Math, Nikankin et al.</p><p>https://arxiv.org/html/2410.21272v1</p><p><br></p><p>[00:38:45] Subitizing, Railo</p><p>https://link.springer.com/10.1007/978-1-4419-1428-6_578</p><p><br></p><p>[00:43:25] NN &amp; Chomsky Hierarchy, Delétang et al.</p><p>https://arxiv.org/abs/2207.02098</p><p><br></p><p>[00:51:05] Measure of Intelligence, Chollet</p><p>https://arxiv.org/abs/1911.01547</p><p><br></p><p>[00:52:10] AlphaZero, Silver et al.</p><p>https://pubmed.ncbi.nlm.nih.gov/30523106/</p><p><br></p><p>[00:55:10] Golden Gate Claude, Anthropic</p><p>https://www.anthropic.com/news/golden-gate-claude</p><p><br></p><p>[00:56:40] Chess Positions, Chase &amp; Simon</p><p>https://www.sciencedirect.com/science/article/abs/pii/0010028573900042</p><p><br></p><p>[01:00:35] Epistemic Foraging, Friston</p><p>https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2016.00056/full</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Transformers-Need-Glasses----Federico-Barbero-e2vt2tn</link>
			<guid isPermaLink="false">c7f27f3f-0faf-4316-8403-ec985f36ed79</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 08 Mar 2025 22:49:35 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/99567991/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-8%2F93abdba7-8239-0f46-eb47-df058b0f2033.mp3" length="88254573" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Federico Barbero (DeepMind/Oxford) is the lead author of &amp;quot;Transformers Need Glasses!&amp;quot;. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Have you ever wondered why LLMs struggle with seemingly simple tasks like counting or copying long strings of text? We break down the theoretical reasons behind these failures, revealing architectural bottlenecks and the challenges of maintaining information fidelity across extended contexts.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Federico explains how these issues are rooted in the transformer&amp;#39;s design, drawing parallels to over-squashing in graph neural networks and detailing how the softmax function limits sharp decision-making.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;But it&amp;#39;s not all bad news! Discover practical &amp;quot;glasses&amp;quot; that can help transformers see more clearly, from simple input modifications to architectural tweaks.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;https://federicobarbero.com/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + RESEARCH:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/s/h7ys83ztwktqjje/Federico.pdf?dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. Transformer Limitations: Token Detection &amp;amp; Representation&lt;/p&gt;&lt;p&gt;[00:00:00] 1.1 Transformers fail at single token detection&lt;/p&gt;&lt;p&gt;[00:02:45] 1.2 Representation collapse in transformers&lt;/p&gt;&lt;p&gt;[00:03:21] 1.3 Experiment: LLMs fail at copying last tokens&lt;/p&gt;&lt;p&gt;[00:18:00] 1.4 Attention sharpness limitations in transformers&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Transformer Limitations: Information Flow &amp;amp; Quantization&lt;/p&gt;&lt;p&gt;[00:18:50] 2.1 Unidirectional information mixing&lt;/p&gt;&lt;p&gt;[00:18:50] 2.2 Unidirectional information flow towards sequence beginning in transformers&lt;/p&gt;&lt;p&gt;[00:21:50] 2.3 Diagonal attention heads as expensive no-ops in LAMA/Gemma&lt;/p&gt;&lt;p&gt;[00:27:14] 2.4 Sequence entropy affects transformer model distinguishability&lt;/p&gt;&lt;p&gt;[00:30:36] 2.5 Quantization limitations lead to information loss &amp;amp; representational collapse&lt;/p&gt;&lt;p&gt;[00:38:34] 2.6 LLMs use subitizing as opposed to counting algorithms&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Transformers and the Nature of Reasoning&lt;/p&gt;&lt;p&gt;[00:40:30] 3.1 Turing completeness conditions in transformers&lt;/p&gt;&lt;p&gt;[00:43:23] 3.2 Transformers struggle with sequential tasks&lt;/p&gt;&lt;p&gt;[00:45:50] 3.3 Windowed attention as solution to information compression&lt;/p&gt;&lt;p&gt;[00:51:04] 3.4 Chess engines: mechanical computation vs creative reasoning&lt;/p&gt;&lt;p&gt;[01:00:35] 3.5 Epistemic foraging introduced&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:01:05] Transformers Need Glasses!, Barbero et al.&lt;/p&gt;&lt;p&gt;https://proceedings.neurips.cc/paper_files/paper/2024/file/b1d35561c4a4a0e0b6012b2af531e149-Paper-Conference.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:05:30] Softmax is Not Enough, Veličković et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2410.01104&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:11:30] Adv Alg Lecture 15, Chawla&lt;/p&gt;&lt;p&gt;https://pages.cs.wisc.edu/~shuchi/courses/787-F09/scribe-notes/lec15.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:15:05] Graph Attention Networks, Veličković&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1710.10903&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:19:15] Extract Training Data, Carlini et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/2311.17035&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:31:30] 1-bit LLMs, Ma et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2402.17764&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:38:35] LLMs Solve Math, Nikankin et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2410.21272v1&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:38:45] Subitizing, Railo&lt;/p&gt;&lt;p&gt;https://link.springer.com/10.1007/978-1-4419-1428-6_578&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:43:25] NN &amp;amp; Chomsky Hierarchy, Delétang et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2207.02098&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:51:05] Measure of Intelligence, Chollet&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:52:10] AlphaZero, Silver et al.&lt;/p&gt;&lt;p&gt;https://pubmed.ncbi.nlm.nih.gov/30523106/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:55:10] Golden Gate Claude, Anthropic&lt;/p&gt;&lt;p&gt;https://www.anthropic.com/news/golden-gate-claude&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:56:40] Chess Positions, Chase &amp;amp; Simon&lt;/p&gt;&lt;p&gt;https://www.sciencedirect.com/science/article/abs/pii/0010028573900042&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[01:00:35] Epistemic Foraging, Friston&lt;/p&gt;&lt;p&gt;https://www.frontiersin.org/journals/computational-neuroscience/articles/10.3389/fncom.2016.00056/full&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1741474153118-939712ece7442.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sakana AI - Chris Lu, Robert Tjarko Lange, Cong Lu]]></title>
			<description><![CDATA[<p>We speak with Sakana AI, who are building nature-inspired methods that could fundamentally transform how we develop AI systems.</p><p><br></p><p>The guests include Chris Lu, a researcher who recently completed his DPhil at Oxford University under Prof. Jakob Foerster&#39;s supervision, where he focused on meta-learning and multi-agent systems. Chris is the first author of the DiscoPOP paper, which demonstrates how language models can discover and design better training algorithms. Also joining is Robert Tjarko Lange, a founding member of Sakana AI who specializes in evolutionary algorithms and large language models. Robert leads research at the intersection of evolutionary computation and foundation models, and is completing his PhD at TU Berlin on evolutionary meta-learning. The discussion also features Cong Lu, currently a Research Scientist at Google DeepMind&#39;s Open-Endedness team, who previously helped develop The AI Scientist and Intelligent Go-Explore.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p><br></p><p>* DiscoPOP - A framework where language models discover their own optimization algorithms</p><p>* EvoLLM - Using language models as evolution strategies for optimization</p><p>The AI Scientist - A fully automated system that conducts scientific research end-to-end</p><p>* Neural Attention Memory Models (NAMMs) - Evolved memory systems that make transformers both faster and more accurate</p><p><br></p><p>TRANSCRIPT + REFS:</p><p>https://www.dropbox.com/scl/fi/gflcyvnujp8cl7zlv3v9d/Sakana.pdf?rlkey=woaoo82943170jd4yyi2he71c&amp;dl=0</p><p><br></p><p>Robert Tjarko Lange</p><p>https://roberttlange.com/</p><p>Chris Lu</p><p>https://chrislu.page/</p><p>Cong Lu</p><p>https://www.conglu.co.uk/</p><p>Sakana</p><p>https://sakana.ai/blog/</p><p><br></p><p>TOC:</p><p>1. LLMs for Algorithm Generation and Optimization</p><p> [00:00:00] 1.1 LLMs generating algorithms for training other LLMs</p><p>   [00:04:00] 1.2 Evolutionary black-box optim using neural network loss parameterization</p><p>   [00:11:50] 1.3 DiscoPOP: Non-convex loss function for noisy data</p><p>   [00:20:45] 1.4 External entropy Injection for preventing Model collapse</p><p>   [00:26:25] 1.5 LLMs for black-box optimization using abstract numerical sequences</p><p><br></p><p>2. Model Learning and Generalization</p><p>   [00:31:05] 2.1 Fine-tuning on teacher algorithm trajectories</p><p>   [00:31:30] 2.2 Transformers learning gradient descent</p><p>   [00:33:00] 2.3 LLM tokenization biases towards specific numbers</p><p>   [00:34:50] 2.4 LLMs as evolution strategies for black box optimization</p><p>   [00:38:05] 2.5 DiscoPOP: LLMs discovering novel optimization algorithms</p><p><br></p><p>3. AI Agents and System Architectures</p><p>   [00:51:30] 3.1 ARC challenge: Induction vs. transformer approaches</p><p>   [00:54:35] 3.2 LangChain / modular agent components</p><p>   [00:57:50] 3.3 Debate improves LLM truthfulness</p><p>   [01:00:55] 3.4 Time limits controlling AI agent systems</p><p>   [01:03:00] 3.5 Gemini: Million-token context enables flatter hierarchies</p><p>   [01:04:05] 3.6 Agents follow own interest gradients</p><p>   [01:09:50] 3.7 Go-Explore algorithm: archive-based exploration</p><p>   [01:11:05] 3.8 Foundation models for interesting state discovery</p><p>   [01:13:00] 3.9 LLMs leverage prior game knowledge</p><p><br></p><p>4. AI for Scientific Discovery and Human Alignment</p><p>   [01:17:45] 4.1 Encoding Alignment &amp; Aesthetics via Reward Functions</p><p>   [01:20:00] 4.2 AI Scientist: Automated Open-Ended Scientific Discovery</p><p>   [01:24:15] 4.3 DiscoPOP: LLM for Preference Optimization Algorithms</p><p>   [01:28:30] 4.4 Balancing AI Knowledge with Human Understanding</p><p>   [01:33:55] 4.5 AI-Driven Conferences and Paper Review</p><p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sakana-AI---Chris-Lu--Robert-Tjarko-Lange--Cong-Lu-e2vih1p</link>
			<guid isPermaLink="false">80ad50a4-1703-4c71-b103-5d02704418d0</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 01 Mar 2025 18:40:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/99222009/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-2-1%2F69665050-bc4f-a25b-5de2-11f53c0f4735.mp3" length="141497435" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We speak with Sakana AI, who are building nature-inspired methods that could fundamentally transform how we develop AI systems.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;The guests include Chris Lu, a researcher who recently completed his DPhil at Oxford University under Prof. Jakob Foerster&amp;#39;s supervision, where he focused on meta-learning and multi-agent systems. Chris is the first author of the DiscoPOP paper, which demonstrates how language models can discover and design better training algorithms. Also joining is Robert Tjarko Lange, a founding member of Sakana AI who specializes in evolutionary algorithms and large language models. Robert leads research at the intersection of evolutionary computation and foundation models, and is completing his PhD at TU Berlin on evolutionary meta-learning. The discussion also features Cong Lu, currently a Research Scientist at Google DeepMind&amp;#39;s Open-Endedness team, who previously helped develop The AI Scientist and Intelligent Go-Explore.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;* DiscoPOP - A framework where language models discover their own optimization algorithms&lt;/p&gt;&lt;p&gt;* EvoLLM - Using language models as evolution strategies for optimization&lt;/p&gt;&lt;p&gt;The AI Scientist - A fully automated system that conducts scientific research end-to-end&lt;/p&gt;&lt;p&gt;* Neural Attention Memory Models (NAMMs) - Evolved memory systems that make transformers both faster and more accurate&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + REFS:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/gflcyvnujp8cl7zlv3v9d/Sakana.pdf?rlkey=woaoo82943170jd4yyi2he71c&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Robert Tjarko Lange&lt;/p&gt;&lt;p&gt;https://roberttlange.com/&lt;/p&gt;&lt;p&gt;Chris Lu&lt;/p&gt;&lt;p&gt;https://chrislu.page/&lt;/p&gt;&lt;p&gt;Cong Lu&lt;/p&gt;&lt;p&gt;https://www.conglu.co.uk/&lt;/p&gt;&lt;p&gt;Sakana&lt;/p&gt;&lt;p&gt;https://sakana.ai/blog/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. LLMs for Algorithm Generation and Optimization&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 LLMs generating algorithms for training other LLMs&lt;/p&gt;&lt;p&gt;   [00:04:00] 1.2 Evolutionary black-box optim using neural network loss parameterization&lt;/p&gt;&lt;p&gt;   [00:11:50] 1.3 DiscoPOP: Non-convex loss function for noisy data&lt;/p&gt;&lt;p&gt;   [00:20:45] 1.4 External entropy Injection for preventing Model collapse&lt;/p&gt;&lt;p&gt;   [00:26:25] 1.5 LLMs for black-box optimization using abstract numerical sequences&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Model Learning and Generalization&lt;/p&gt;&lt;p&gt;   [00:31:05] 2.1 Fine-tuning on teacher algorithm trajectories&lt;/p&gt;&lt;p&gt;   [00:31:30] 2.2 Transformers learning gradient descent&lt;/p&gt;&lt;p&gt;   [00:33:00] 2.3 LLM tokenization biases towards specific numbers&lt;/p&gt;&lt;p&gt;   [00:34:50] 2.4 LLMs as evolution strategies for black box optimization&lt;/p&gt;&lt;p&gt;   [00:38:05] 2.5 DiscoPOP: LLMs discovering novel optimization algorithms&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. AI Agents and System Architectures&lt;/p&gt;&lt;p&gt;   [00:51:30] 3.1 ARC challenge: Induction vs. transformer approaches&lt;/p&gt;&lt;p&gt;   [00:54:35] 3.2 LangChain / modular agent components&lt;/p&gt;&lt;p&gt;   [00:57:50] 3.3 Debate improves LLM truthfulness&lt;/p&gt;&lt;p&gt;   [01:00:55] 3.4 Time limits controlling AI agent systems&lt;/p&gt;&lt;p&gt;   [01:03:00] 3.5 Gemini: Million-token context enables flatter hierarchies&lt;/p&gt;&lt;p&gt;   [01:04:05] 3.6 Agents follow own interest gradients&lt;/p&gt;&lt;p&gt;   [01:09:50] 3.7 Go-Explore algorithm: archive-based exploration&lt;/p&gt;&lt;p&gt;   [01:11:05] 3.8 Foundation models for interesting state discovery&lt;/p&gt;&lt;p&gt;   [01:13:00] 3.9 LLMs leverage prior game knowledge&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;4. AI for Scientific Discovery and Human Alignment&lt;/p&gt;&lt;p&gt;   [01:17:45] 4.1 Encoding Alignment &amp;amp; Aesthetics via Reward Functions&lt;/p&gt;&lt;p&gt;   [01:20:00] 4.2 AI Scientist: Automated Open-Ended Scientific Discovery&lt;/p&gt;&lt;p&gt;   [01:24:15] 4.3 DiscoPOP: LLM for Preference Optimization Algorithms&lt;/p&gt;&lt;p&gt;   [01:28:30] 4.4 Balancing AI Knowledge with Human Understanding&lt;/p&gt;&lt;p&gt;   [01:33:55] 4.5 AI-Driven Conferences and Paper Review&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:37:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1740854358856-bcfedf9e1418c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Clement Bonnet - Can Latent Program Networks Solve Abstract Reasoning?]]></title>
			<description><![CDATA[<p>Clement Bonnet discusses his novel approach to the ARC (Abstraction and Reasoning Corpus) challenge. Unlike approaches that rely on fine-tuning LLMs or generating samples at inference time, Clement&#39;s method encodes input-output pairs into a latent space, optimizes this representation with a search algorithm, and decodes outputs for new inputs. This end-to-end architecture uses a VAE loss, including reconstruction and prior losses. </p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT + RESEARCH OVERVIEW:</p><p>https://www.dropbox.com/scl/fi/j7m0gaz1126y594gswtma/CLEMMLST.pdf?rlkey=y5qvwq2er5nchbcibm07rcfpq&amp;dl=0</p><p><br></p><p>Clem and Matthew-</p><p>https://www.linkedin.com/in/clement-bonnet16/</p><p>https://github.com/clement-bonnet</p><p>https://mvmacfarlane.github.io/</p><p><br></p><p>TOC</p><p>1. LPN Fundamentals</p><p> [00:00:00] 1.1 Introduction to ARC Benchmark and LPN Overview</p><p>   [00:05:05] 1.2 Neural Networks&#39; Challenges with ARC and Program Synthesis</p><p>   [00:06:55] 1.3 Induction vs Transduction in Machine Learning</p><p><br></p><p>2. LPN Architecture and Latent Space</p><p>   [00:11:50] 2.1 LPN Architecture and Latent Space Implementation</p><p>   [00:16:25] 2.2 LPN Latent Space Encoding and VAE Architecture</p><p>   [00:20:25] 2.3 Gradient-Based Search Training Strategy</p><p>   [00:23:39] 2.4 LPN Model Architecture and Implementation Details</p><p><br></p><p>3. Implementation and Scaling</p><p>   [00:27:34] 3.1 Training Data Generation and re-ARC Framework</p><p>   [00:31:28] 3.2 Limitations of Latent Space and Multi-Thread Search</p><p>   [00:34:43] 3.3 Program Composition and Computational Graph Architecture</p><p><br></p><p>4. Advanced Concepts and Future Directions</p><p>   [00:45:09] 4.1 AI Creativity and Program Synthesis Approaches</p><p>   [00:49:47] 4.2 Scaling and Interpretability in Latent Space Models</p><p><br></p><p>REFS</p><p>[00:00:05] ARC benchmark, Chollet</p><p>https://arxiv.org/abs/2412.04604</p><p><br></p><p>[00:02:10] Latent Program Spaces, Bonnet, Macfarlane</p><p>https://arxiv.org/abs/2411.08706</p><p><br></p><p>[00:07:45] Kevin Ellis work on program generation</p><p>https://www.cs.cornell.edu/~ellisk/</p><p><br></p><p>[00:08:45] Induction vs transduction in abstract reasoning, Li et al.</p><p>https://arxiv.org/abs/2411.02272</p><p><br></p><p>[00:17:40] VAEs, Kingma, Welling</p><p>https://arxiv.org/abs/1312.6114</p><p><br></p><p>[00:27:50] re-ARC, Hodel</p><p>https://github.com/michaelhodel/re-arc</p><p><br></p><p>[00:29:40] Grid size in ARC tasks, Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p><br></p><p>[00:33:00] Critique of deep learning, Marcus</p><p>https://arxiv.org/vc/arxiv/papers/2002/2002.06177v1.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Clement-Bonnet---Can-Latent-Program-Networks-Solve-Abstract-Reasoning-e2v41og</link>
			<guid isPermaLink="false">f028bec6-c1d5-40e2-9bfa-7118a6ed0311</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 19 Feb 2025 22:05:30 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/98747600/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-19%2F65f76eb2-6635-fdd1-17ed-23a2f70eb94a.mp3" length="74563446" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Clement Bonnet discusses his novel approach to the ARC (Abstraction and Reasoning Corpus) challenge. Unlike approaches that rely on fine-tuning LLMs or generating samples at inference time, Clement&amp;#39;s method encodes input-output pairs into a latent space, optimizes this representation with a search algorithm, and decodes outputs for new inputs. This end-to-end architecture uses a VAE loss, including reconstruction and prior losses. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT + RESEARCH OVERVIEW:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/j7m0gaz1126y594gswtma/CLEMMLST.pdf?rlkey=y5qvwq2er5nchbcibm07rcfpq&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Clem and Matthew-&lt;/p&gt;&lt;p&gt;https://www.linkedin.com/in/clement-bonnet16/&lt;/p&gt;&lt;p&gt;https://github.com/clement-bonnet&lt;/p&gt;&lt;p&gt;https://mvmacfarlane.github.io/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC&lt;/p&gt;&lt;p&gt;1. LPN Fundamentals&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 Introduction to ARC Benchmark and LPN Overview&lt;/p&gt;&lt;p&gt;   [00:05:05] 1.2 Neural Networks&amp;#39; Challenges with ARC and Program Synthesis&lt;/p&gt;&lt;p&gt;   [00:06:55] 1.3 Induction vs Transduction in Machine Learning&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. LPN Architecture and Latent Space&lt;/p&gt;&lt;p&gt;   [00:11:50] 2.1 LPN Architecture and Latent Space Implementation&lt;/p&gt;&lt;p&gt;   [00:16:25] 2.2 LPN Latent Space Encoding and VAE Architecture&lt;/p&gt;&lt;p&gt;   [00:20:25] 2.3 Gradient-Based Search Training Strategy&lt;/p&gt;&lt;p&gt;   [00:23:39] 2.4 LPN Model Architecture and Implementation Details&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Implementation and Scaling&lt;/p&gt;&lt;p&gt;   [00:27:34] 3.1 Training Data Generation and re-ARC Framework&lt;/p&gt;&lt;p&gt;   [00:31:28] 3.2 Limitations of Latent Space and Multi-Thread Search&lt;/p&gt;&lt;p&gt;   [00:34:43] 3.3 Program Composition and Computational Graph Architecture&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;4. Advanced Concepts and Future Directions&lt;/p&gt;&lt;p&gt;   [00:45:09] 4.1 AI Creativity and Program Synthesis Approaches&lt;/p&gt;&lt;p&gt;   [00:49:47] 4.2 Scaling and Interpretability in Latent Space Models&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS&lt;/p&gt;&lt;p&gt;[00:00:05] ARC benchmark, Chollet&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2412.04604&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:02:10] Latent Program Spaces, Bonnet, Macfarlane&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.08706&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:07:45] Kevin Ellis work on program generation&lt;/p&gt;&lt;p&gt;https://www.cs.cornell.edu/~ellisk/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:08:45] Induction vs transduction in abstract reasoning, Li et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.02272&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:17:40] VAEs, Kingma, Welling&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1312.6114&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:27:50] re-ARC, Hodel&lt;/p&gt;&lt;p&gt;https://github.com/michaelhodel/re-arc&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:29:40] Grid size in ARC tasks, Chollet&lt;/p&gt;&lt;p&gt;https://github.com/fchollet/ARC-AGI&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:33:00] Critique of deep learning, Marcus&lt;/p&gt;&lt;p&gt;https://arxiv.org/vc/arxiv/papers/2002/2002.06177v1.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:26</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1740002708517-c82b73895af52.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Jakob Foerster - ImageNet Moment for Reinforcement Learning?]]></title>
			<description><![CDATA[<p>Prof. Jakob Foerster, a leading AI researcher at Oxford University and Meta, and Chris Lu, a researcher at OpenAI -- they explain how AI is moving beyond just mimicking human behaviour to creating truly intelligent agents that can learn and solve problems on their own. Foerster champions open-source AI for responsible, decentralised development. He addresses AI scaling, goal misalignment (Goodhart&#39;s Law), and the need for holistic alignment, offering a quick look at the future of AI and how to guide it.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. </p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT/REFS:</p><p>https://www.dropbox.com/scl/fi/yqjszhntfr00bhjh6t565/JAKOB.pdf?rlkey=scvny4bnwj8th42fjv8zsfu2y&amp;dl=0 </p><p><br></p><p>Prof. Jakob Foerster</p><p>https://x.com/j_foerst</p><p>https://www.jakobfoerster.com/</p><p>University of Oxford Profile: </p><p>https://eng.ox.ac.uk/people/jakob-foerster/</p><p><br></p><p>Chris Lu:</p><p>https://chrislu.page/</p><p><br></p><p>TOC</p><p>1. GPU Acceleration and Training Infrastructure</p><p> [00:00:00] 1.1 ARC Challenge Criticism and FLAIR Lab Overview</p><p>   [00:01:25] 1.2 GPU Acceleration and Hardware Lottery in RL</p><p>   [00:05:50] 1.3 Data Wall Challenges and Simulation-Based Solutions</p><p>   [00:08:40] 1.4 JAX Implementation and Technical Acceleration</p><p><br></p><p>2. Learning Frameworks and Policy Optimization</p><p>   [00:14:18] 2.1 Evolution of RL Algorithms and Mirror Learning Framework</p><p>   [00:15:25] 2.2 Meta-Learning and Policy Optimization Algorithms</p><p>   [00:21:47] 2.3 Language Models and Benchmark Challenges</p><p>   [00:28:15] 2.4 Creativity and Meta-Learning in AI Systems</p><p><br></p><p>3. Multi-Agent Systems and Decentralization</p><p>   [00:31:24] 3.1 Multi-Agent Systems and Emergent Intelligence</p><p>   [00:38:35] 3.2 Swarm Intelligence vs Monolithic AGI Systems</p><p>   [00:42:44] 3.3 Democratic Control and Decentralization of AI Development</p><p>   [00:46:14] 3.4 Open Source AI and Alignment Challenges</p><p>   [00:49:31] 3.5 Collaborative Models for AI Development</p><p><br></p><p>REFS</p><p>[[00:00:05] ARC Benchmark, Chollet</p><p>https://github.com/fchollet/ARC-AGI</p><p><br></p><p>[00:03:05] DRL Doesn&#39;t Work, Irpan</p><p>https://www.alexirpan.com/2018/02/14/rl-hard.html</p><p><br></p><p>[00:05:55] AI Training Data, Data Provenance Initiative</p><p>https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html</p><p><br></p><p>[00:06:10] JaxMARL, Foerster et al.</p><p>https://arxiv.org/html/2311.10090v5</p><p><br></p><p>[00:08:50] M-FOS, Lu et al.</p><p>https://arxiv.org/abs/2205.01447</p><p><br></p><p>[00:09:45] JAX Library, Google Research</p><p>https://github.com/jax-ml/jax</p><p><br></p><p>[00:12:10] Kinetix, Mike and Michael</p><p>https://arxiv.org/abs/2410.23208</p><p><br></p><p>[00:12:45] Genie 2, DeepMind</p><p>https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/</p><p><br></p><p>[00:14:42] Mirror Learning, Grudzien, Kuba et al.</p><p>https://arxiv.org/abs/2208.01682</p><p><br></p><p>[00:16:30] Discovered Policy Optimisation, Lu et al.</p><p>https://arxiv.org/abs/2210.05639</p><p><br></p><p>[00:24:10] Goodhart&#39;s Law, Goodhart</p><p>https://en.wikipedia.org/wiki/Goodhart%27s_law</p><p><br></p><p>[00:25:15] LLM ARChitect, Franzen et al.</p><p>https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf</p><p><br></p><p>[00:28:55] AlphaGo, Silver et al.</p><p>https://arxiv.org/pdf/1712.01815.pdf</p><p><br></p><p>[00:30:10] Meta-learning, Lu, Towers, Foerster</p><p>https://direct.mit.edu/isal/proceedings-pdf/isal2023/35/67/2354943/isal_a_00674.pdf</p><p><br></p><p>[00:31:30] Emergence of Pragmatics, Yuan et al.</p><p>https://arxiv.org/abs/2001.07752</p><p><br></p><p>[00:34:30] AI Safety, Amodei et al.</p><p>https://arxiv.org/abs/1606.06565</p><p><br></p><p>[00:35:45] Intentional Stance, Dennett</p><p>https://plato.stanford.edu/entries/ethics-ai/</p><p><br></p><p>[00:39:25] Multi-Agent RL, Zhou et al.</p><p>https://arxiv.org/pdf/2305.10091</p><p><br></p><p>[00:41:00] Open Source Generative AI, Foerster et al.</p><p>https://arxiv.org/abs/2405.08597</p><p><br></p><p>&lt;trunc, see PDF/YT&gt;</p><p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Jakob-Foerster---ImageNet-Moment-for-Reinforcement-Learning-e2v2cl2</link>
			<guid isPermaLink="false">8eb1de55-6f92-43d6-8bd8-f945ac4cf16c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 18 Feb 2025 20:21:54 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/98693218/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-18%2F52dc0817-4adf-4877-0b7b-e190a4542c3a.mp3" length="77573225" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof. Jakob Foerster, a leading AI researcher at Oxford University and Meta, and Chris Lu, a researcher at OpenAI -- they explain how AI is moving beyond just mimicking human behaviour to creating truly intelligent agents that can learn and solve problems on their own. Foerster champions open-source AI for responsible, decentralised development. He addresses AI scaling, goal misalignment (Goodhart&amp;#39;s Law), and the need for holistic alignment, offering a quick look at the future of AI and how to guide it.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich. &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT/REFS:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/yqjszhntfr00bhjh6t565/JAKOB.pdf?rlkey=scvny4bnwj8th42fjv8zsfu2y&amp;amp;dl=0 &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Prof. Jakob Foerster&lt;/p&gt;&lt;p&gt;https://x.com/j_foerst&lt;/p&gt;&lt;p&gt;https://www.jakobfoerster.com/&lt;/p&gt;&lt;p&gt;University of Oxford Profile: &lt;/p&gt;&lt;p&gt;https://eng.ox.ac.uk/people/jakob-foerster/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Chris Lu:&lt;/p&gt;&lt;p&gt;https://chrislu.page/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC&lt;/p&gt;&lt;p&gt;1. GPU Acceleration and Training Infrastructure&lt;/p&gt;&lt;p&gt; [00:00:00] 1.1 ARC Challenge Criticism and FLAIR Lab Overview&lt;/p&gt;&lt;p&gt;   [00:01:25] 1.2 GPU Acceleration and Hardware Lottery in RL&lt;/p&gt;&lt;p&gt;   [00:05:50] 1.3 Data Wall Challenges and Simulation-Based Solutions&lt;/p&gt;&lt;p&gt;   [00:08:40] 1.4 JAX Implementation and Technical Acceleration&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. Learning Frameworks and Policy Optimization&lt;/p&gt;&lt;p&gt;   [00:14:18] 2.1 Evolution of RL Algorithms and Mirror Learning Framework&lt;/p&gt;&lt;p&gt;   [00:15:25] 2.2 Meta-Learning and Policy Optimization Algorithms&lt;/p&gt;&lt;p&gt;   [00:21:47] 2.3 Language Models and Benchmark Challenges&lt;/p&gt;&lt;p&gt;   [00:28:15] 2.4 Creativity and Meta-Learning in AI Systems&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Multi-Agent Systems and Decentralization&lt;/p&gt;&lt;p&gt;   [00:31:24] 3.1 Multi-Agent Systems and Emergent Intelligence&lt;/p&gt;&lt;p&gt;   [00:38:35] 3.2 Swarm Intelligence vs Monolithic AGI Systems&lt;/p&gt;&lt;p&gt;   [00:42:44] 3.3 Democratic Control and Decentralization of AI Development&lt;/p&gt;&lt;p&gt;   [00:46:14] 3.4 Open Source AI and Alignment Challenges&lt;/p&gt;&lt;p&gt;   [00:49:31] 3.5 Collaborative Models for AI Development&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS&lt;/p&gt;&lt;p&gt;[[00:00:05] ARC Benchmark, Chollet&lt;/p&gt;&lt;p&gt;https://github.com/fchollet/ARC-AGI&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:03:05] DRL Doesn&amp;#39;t Work, Irpan&lt;/p&gt;&lt;p&gt;https://www.alexirpan.com/2018/02/14/rl-hard.html&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:05:55] AI Training Data, Data Provenance Initiative&lt;/p&gt;&lt;p&gt;https://www.nytimes.com/2024/07/19/technology/ai-data-restrictions.html&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:06:10] JaxMARL, Foerster et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2311.10090v5&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:08:50] M-FOS, Lu et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2205.01447&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:09:45] JAX Library, Google Research&lt;/p&gt;&lt;p&gt;https://github.com/jax-ml/jax&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:12:10] Kinetix, Mike and Michael&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2410.23208&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:12:45] Genie 2, DeepMind&lt;/p&gt;&lt;p&gt;https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:14:42] Mirror Learning, Grudzien, Kuba et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2208.01682&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:16:30] Discovered Policy Optimisation, Lu et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2210.05639&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:24:10] Goodhart&amp;#39;s Law, Goodhart&lt;/p&gt;&lt;p&gt;https://en.wikipedia.org/wiki/Goodhart%27s_law&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:25:15] LLM ARChitect, Franzen et al.&lt;/p&gt;&lt;p&gt;https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:28:55] AlphaGo, Silver et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/1712.01815.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:30:10] Meta-learning, Lu, Towers, Foerster&lt;/p&gt;&lt;p&gt;https://direct.mit.edu/isal/proceedings-pdf/isal2023/35/67/2354943/isal_a_00674.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:31:30] Emergence of Pragmatics, Yuan et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2001.07752&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:34:30] AI Safety, Amodei et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1606.06565&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:35:45] Intentional Stance, Dennett&lt;/p&gt;&lt;p&gt;https://plato.stanford.edu/entries/ethics-ai/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:39:25] Multi-Agent RL, Zhou et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/2305.10091&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:41:00] Open Source Generative AI, Foerster et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2405.08597&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&amp;lt;trunc, see PDF/YT&amp;gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1739910089789-4beb6ffe37489.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Daniel Franzen & Jan Disselhoff - ARC Prize 2024 winners]]></title>
			<description><![CDATA[<p>Daniel Franzen and Jan Disselhoff, the &quot;ARChitects&quot; are the official winners of the ARC Prize 2024. Filmed at Tufa Labs in Zurich - they revealed how they achieved a remarkable 53.5% accuracy by creatively utilising large language models (LLMs) in new ways. Discover their innovative techniques, including depth-first search for token selection, test-time training, and a novel augmentation-based validation system. Their results were extremely surprising.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich.</p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>Jan Disselhoff</p><p>https://www.linkedin.com/in/jan-disselhoff-1423a2240/</p><p><br></p><p>Daniel Franzen</p><p>https://github.com/da-fr</p><p><br></p><p>ARC Prize: http://arcprize.org/</p><p><br></p><p>TRANSCRIPT AND BACKGROUND READING:</p><p>https://www.dropbox.com/scl/fi/utkn2i1ma79fn6an4yvjw/ARCHitects.pdf?rlkey=67pe38mtss7oyhjk2ad0d2aza&amp;dl=0</p><p><br></p><p>TOC</p><p>1. Solution Architecture and Strategy Overview</p><p>[00:00:00] 1.1 Initial Solution Overview and Model Architecture</p><p>[00:04:25] 1.2 LLM Capabilities and Dataset Approach</p><p>[00:10:51] 1.3 Test-Time Training and Data Augmentation Strategies</p><p>[00:14:08] 1.4 Sampling Methods and Search Implementation</p><p>[00:17:52] 1.5 ARC vs Language Model Context Comparison</p><p><br></p><p>2. LLM Search and Model Implementation</p><p>[00:21:53] 2.1 LLM-Guided Search Approaches and Solution Validation</p><p>[00:27:04] 2.2 Symmetry Augmentation and Model Architecture</p><p>[00:30:11] 2.3 Model Intelligence Characteristics and Performance</p><p>[00:37:23] 2.4 Tokenization and Numerical Processing Challenges</p><p><br></p><p>3. Advanced Training and Optimization</p><p>[00:45:15] 3.1 DFS Token Selection and Probability Thresholds</p><p>[00:49:41] 3.2 Model Size and Fine-tuning Performance Trade-offs</p><p>[00:53:07] 3.3 LoRA Implementation and Catastrophic Forgetting Prevention</p><p>[00:56:10] 3.4 Training Infrastructure and Optimization Experiments</p><p>[01:02:34] 3.5 Search Tree Analysis and Entropy Distribution Patterns</p><p><br></p><p>REFS</p><p>[00:01:05] Winning ARC 2024 solution using 12B param model, Franzen, Disselhoff, Hartmann</p><p>https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf</p><p><br></p><p>[00:03:40] Robustness of analogical reasoning in LLMs, Melanie Mitchell</p><p>https://arxiv.org/html/2411.14215</p><p><br></p><p>[00:07:50] Re-ARC dataset generator for ARC task variations, Michael Hodel</p><p>https://github.com/michaelhodel/re-arc</p><p><br></p><p>[00:15:00] Analysis of search methods in LLMs (greedy, beam, DFS), Chen et al.</p><p>https://arxiv.org/html/2408.00724v2</p><p><br></p><p>[00:16:55] Language model reachability space exploration, University of Toronto</p><p>https://www.youtube.com/watch?v=Bpgloy1dDn0</p><p><br></p><p>[00:22:30] GPT-4 guided code solutions for ARC tasks, Ryan Greenblatt</p><p>https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt</p><p><br></p><p>[00:41:20] GPT tokenization approach for numbers, OpenAI</p><p>https://platform.openai.com/docs/guides/text-generation/tokenizer-examples</p><p><br></p><p>[00:46:25] DFS in AI search strategies, Russell &amp; Norvig</p><p>https://www.amazon.com/Artificial-Intelligence-Modern-Approach-4th/dp/0134610997</p><p><br></p><p>[00:53:10] Paper on catastrophic forgetting in neural networks, Kirkpatrick et al.</p><p>https://www.pnas.org/doi/10.1073/pnas.1611835114</p><p><br></p><p>[00:54:00] LoRA for efficient fine-tuning of LLMs, Hu et al.</p><p>https://arxiv.org/abs/2106.09685</p><p><br></p><p>[00:57:20] NVIDIA H100 Tensor Core GPU specs, NVIDIA</p><p>https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/</p><p><br></p><p>[01:04:55] Original MCTS in computer Go, Yifan Jin</p><p>https://stanford.edu/~rezab/classes/cme323/S15/projects/montecarlo_search_tree_report.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Daniel-Franzen--Jan-Disselhoff---ARC-Prize-2024-winners-e2upr5k</link>
			<guid isPermaLink="false">ed0ae5f5-90ee-4a8f-8043-05712f5b49e4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 12 Feb 2025 21:05:30 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/98413172/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-12%2Ff8b7b2e7-5a61-710b-2650-1958e0115cec.mp3" length="99951140" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Daniel Franzen and Jan Disselhoff, the &amp;quot;ARChitects&amp;quot; are the official winners of the ARC Prize 2024. Filmed at Tufa Labs in Zurich - they revealed how they achieved a remarkable 53.5% accuracy by creatively utilising large language models (LLMs) in new ways. Discover their innovative techniques, including depth-first search for token selection, test-time training, and a novel augmentation-based validation system. Their results were extremely surprising.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Jan Disselhoff&lt;/p&gt;&lt;p&gt;https://www.linkedin.com/in/jan-disselhoff-1423a2240/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Daniel Franzen&lt;/p&gt;&lt;p&gt;https://github.com/da-fr&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;ARC Prize: http://arcprize.org/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT AND BACKGROUND READING:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/utkn2i1ma79fn6an4yvjw/ARCHitects.pdf?rlkey=67pe38mtss7oyhjk2ad0d2aza&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC&lt;/p&gt;&lt;p&gt;1. Solution Architecture and Strategy Overview&lt;/p&gt;&lt;p&gt;[00:00:00] 1.1 Initial Solution Overview and Model Architecture&lt;/p&gt;&lt;p&gt;[00:04:25] 1.2 LLM Capabilities and Dataset Approach&lt;/p&gt;&lt;p&gt;[00:10:51] 1.3 Test-Time Training and Data Augmentation Strategies&lt;/p&gt;&lt;p&gt;[00:14:08] 1.4 Sampling Methods and Search Implementation&lt;/p&gt;&lt;p&gt;[00:17:52] 1.5 ARC vs Language Model Context Comparison&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. LLM Search and Model Implementation&lt;/p&gt;&lt;p&gt;[00:21:53] 2.1 LLM-Guided Search Approaches and Solution Validation&lt;/p&gt;&lt;p&gt;[00:27:04] 2.2 Symmetry Augmentation and Model Architecture&lt;/p&gt;&lt;p&gt;[00:30:11] 2.3 Model Intelligence Characteristics and Performance&lt;/p&gt;&lt;p&gt;[00:37:23] 2.4 Tokenization and Numerical Processing Challenges&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Advanced Training and Optimization&lt;/p&gt;&lt;p&gt;[00:45:15] 3.1 DFS Token Selection and Probability Thresholds&lt;/p&gt;&lt;p&gt;[00:49:41] 3.2 Model Size and Fine-tuning Performance Trade-offs&lt;/p&gt;&lt;p&gt;[00:53:07] 3.3 LoRA Implementation and Catastrophic Forgetting Prevention&lt;/p&gt;&lt;p&gt;[00:56:10] 3.4 Training Infrastructure and Optimization Experiments&lt;/p&gt;&lt;p&gt;[01:02:34] 3.5 Search Tree Analysis and Entropy Distribution Patterns&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS&lt;/p&gt;&lt;p&gt;[00:01:05] Winning ARC 2024 solution using 12B param model, Franzen, Disselhoff, Hartmann&lt;/p&gt;&lt;p&gt;https://github.com/da-fr/arc-prize-2024/blob/main/the_architects.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:03:40] Robustness of analogical reasoning in LLMs, Melanie Mitchell&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2411.14215&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:07:50] Re-ARC dataset generator for ARC task variations, Michael Hodel&lt;/p&gt;&lt;p&gt;https://github.com/michaelhodel/re-arc&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:15:00] Analysis of search methods in LLMs (greedy, beam, DFS), Chen et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2408.00724v2&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:16:55] Language model reachability space exploration, University of Toronto&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=Bpgloy1dDn0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:22:30] GPT-4 guided code solutions for ARC tasks, Ryan Greenblatt&lt;/p&gt;&lt;p&gt;https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:41:20] GPT tokenization approach for numbers, OpenAI&lt;/p&gt;&lt;p&gt;https://platform.openai.com/docs/guides/text-generation/tokenizer-examples&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:46:25] DFS in AI search strategies, Russell &amp;amp; Norvig&lt;/p&gt;&lt;p&gt;https://www.amazon.com/Artificial-Intelligence-Modern-Approach-4th/dp/0134610997&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:53:10] Paper on catastrophic forgetting in neural networks, Kirkpatrick et al.&lt;/p&gt;&lt;p&gt;https://www.pnas.org/doi/10.1073/pnas.1611835114&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:54:00] LoRA for efficient fine-tuning of LLMs, Hu et al.&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2106.09685&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:57:20] NVIDIA H100 Tensor Core GPU specs, NVIDIA&lt;/p&gt;&lt;p&gt;https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[01:04:55] Original MCTS in computer Go, Yifan Jin&lt;/p&gt;&lt;p&gt;https://stanford.edu/~rezab/classes/cme323/S15/projects/montecarlo_search_tree_report.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:09:04</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1739394289071-f1c2b8ec097df.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sepp Hochreiter - LSTM: The Comeback Story?]]></title>
			<description><![CDATA[<p>Sepp Hochreiter, the inventor of LSTM (Long Short-Term Memory) networks – a foundational technology in AI. Sepp discusses his journey, the origins of LSTM, and why he believes his latest work, XLSTM, could be the next big thing in AI, particularly for applications like robotics and industrial simulation. He also shares his controversial perspective on Large Language Models (LLMs) and why reasoning is a critical missing piece in current AI systems.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich.</p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>TRANSCRIPT AND BACKGROUND READING:</p><p>https://www.dropbox.com/scl/fi/n1vzm79t3uuss8xyinxzo/SEPPH.pdf?rlkey=fp7gwaopjk17uyvgjxekxrh5v&amp;dl=0</p><p><br></p><p>Prof. Sepp Hochreiter</p><p>https://www.nx-ai.com/</p><p>https://x.com/hochreitersepp</p><p>https://scholar.google.at/citations?user=tvUH3WMAAAAJ&amp;hl=en</p><p><br></p><p>TOC:</p><p>1. LLM Evolution and Reasoning Capabilities</p><p>[00:00:00] 1.1 LLM Capabilities and Limitations Debate</p><p>[00:03:16] 1.2 Program Generation and Reasoning in AI Systems</p><p>[00:06:30] 1.3 Human vs AI Reasoning Comparison</p><p>[00:09:59] 1.4 New Research Initiatives and Hybrid Approaches</p><p><br></p><p>2. LSTM Technical Architecture</p><p>[00:13:18] 2.1 LSTM Development History and Technical Background</p><p>[00:20:38] 2.2 LSTM vs RNN Architecture and Computational Complexity</p><p>[00:25:10] 2.3 xLSTM Architecture and Flash Attention Comparison</p><p>[00:30:51] 2.4 Evolution of Gating Mechanisms from Sigmoid to Exponential</p><p><br></p><p>3. Industrial Applications and Neuro-Symbolic AI</p><p>[00:40:35] 3.1 Industrial Applications and Fixed Memory Advantages</p><p>[00:42:31] 3.2 Neuro-Symbolic Integration and Pi AI Project</p><p>[00:46:00] 3.3 Integration of Symbolic and Neural AI Approaches</p><p>[00:51:29] 3.4 Evolution of AI Paradigms and System Thinking</p><p>[00:54:55] 3.5 AI Reasoning and Human Intelligence Comparison</p><p>[00:58:12] 3.6 NXAI Company and Industrial AI Applications</p><p><br></p><p>REFS:</p><p>[00:00:15] Seminal LSTM paper establishing Hochreiter&#39;s expertise (Hochreiter &amp; Schmidhuber)</p><p>https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory</p><p><br></p><p>[00:04:20] Kolmogorov complexity and program composition limitations (Kolmogorov)</p><p>https://link.springer.com/article/10.1007/BF02478259</p><p><br></p><p>[00:07:10] Limitations of LLM mathematical reasoning and symbolic integration (Various Authors)</p><p>https://www.arxiv.org/pdf/2502.03671</p><p><br></p><p>[00:09:05] AlphaGo’s Move 37 demonstrating creative AI (Google DeepMind)</p><p>https://deepmind.google/research/breakthroughs/alphago/</p><p><br></p><p>[00:10:15] New AI research lab in Zurich for fundamental LLM research (Benjamin Crouzier)</p><p>https://tufalabs.ai</p><p><br></p><p>[00:19:40] Introduction of xLSTM with exponential gating (Beck, Hochreiter, et al.)</p><p>https://arxiv.org/abs/2405.04517</p><p><br></p><p>[00:22:55] FlashAttention: fast &amp; memory-efficient attention (Tri Dao et al.)</p><p>https://arxiv.org/abs/2205.14135</p><p><br></p><p>[00:31:00] Historical use of sigmoid/tanh activation in 1990s (James A. McCaffrey)</p><p>https://visualstudiomagazine.com/articles/2015/06/01/alternative-activation-functions.aspx</p><p><br></p><p>[00:36:10] Mamba 2 state space model architecture (Albert Gu et al.)</p><p>https://arxiv.org/abs/2312.00752</p><p><br></p><p>[00:46:00] Austria’s Pi AI project integrating symbolic &amp; neural AI (Hochreiter et al.)</p><p>https://www.jku.at/en/institute-of-machine-learning/research/projects/</p><p><br></p><p>[00:48:10] Neuro-symbolic integration challenges in language models (Diego Calanzone et al.)</p><p>https://openreview.net/forum?id=7PGluppo4k</p><p><br></p><p>[00:49:30] JKU Linz’s historical and neuro-symbolic research (Sepp Hochreiter)</p><p>https://www.jku.at/en/news-events/news/detail/news/bilaterale-ki-projekt-unter-leitung-der-jku-erhaelt-fwf-cluster-of-excellence/</p><p><br></p><p>YT: https://www.youtube.com/watch?v=8u2pW2zZLCs</p><p>&lt;truncated, see show notes/YT&gt;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sepp-Hochreiter---LSTM-The-Comeback-Story-e2uoffb</link>
			<guid isPermaLink="false">6ce02743-772d-47f9-a86d-26a7a396d718</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 12 Feb 2025 00:31:18 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/98368427/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-12%2F528f8181-5bf4-b25d-6c34-e71a7ea674b4.mp3" length="96773630" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Sepp Hochreiter, the inventor of LSTM (Long Short-Term Memory) networks – a foundational technology in AI. Sepp discusses his journey, the origins of LSTM, and why he believes his latest work, XLSTM, could be the next big thing in AI, particularly for applications like robotics and industrial simulation. He also shares his controversial perspective on Large Language Models (LLMs) and why reasoning is a critical missing piece in current AI systems.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Check out their super fast DeepSeek R1 hosting!&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. They are hiring a Chief Engineer and ML engineers. Events in Zurich.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TRANSCRIPT AND BACKGROUND READING:&lt;/p&gt;&lt;p&gt;https://www.dropbox.com/scl/fi/n1vzm79t3uuss8xyinxzo/SEPPH.pdf?rlkey=fp7gwaopjk17uyvgjxekxrh5v&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Prof. Sepp Hochreiter&lt;/p&gt;&lt;p&gt;https://www.nx-ai.com/&lt;/p&gt;&lt;p&gt;https://x.com/hochreitersepp&lt;/p&gt;&lt;p&gt;https://scholar.google.at/citations?user=tvUH3WMAAAAJ&amp;amp;hl=en&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;1. LLM Evolution and Reasoning Capabilities&lt;/p&gt;&lt;p&gt;[00:00:00] 1.1 LLM Capabilities and Limitations Debate&lt;/p&gt;&lt;p&gt;[00:03:16] 1.2 Program Generation and Reasoning in AI Systems&lt;/p&gt;&lt;p&gt;[00:06:30] 1.3 Human vs AI Reasoning Comparison&lt;/p&gt;&lt;p&gt;[00:09:59] 1.4 New Research Initiatives and Hybrid Approaches&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;2. LSTM Technical Architecture&lt;/p&gt;&lt;p&gt;[00:13:18] 2.1 LSTM Development History and Technical Background&lt;/p&gt;&lt;p&gt;[00:20:38] 2.2 LSTM vs RNN Architecture and Computational Complexity&lt;/p&gt;&lt;p&gt;[00:25:10] 2.3 xLSTM Architecture and Flash Attention Comparison&lt;/p&gt;&lt;p&gt;[00:30:51] 2.4 Evolution of Gating Mechanisms from Sigmoid to Exponential&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;3. Industrial Applications and Neuro-Symbolic AI&lt;/p&gt;&lt;p&gt;[00:40:35] 3.1 Industrial Applications and Fixed Memory Advantages&lt;/p&gt;&lt;p&gt;[00:42:31] 3.2 Neuro-Symbolic Integration and Pi AI Project&lt;/p&gt;&lt;p&gt;[00:46:00] 3.3 Integration of Symbolic and Neural AI Approaches&lt;/p&gt;&lt;p&gt;[00:51:29] 3.4 Evolution of AI Paradigms and System Thinking&lt;/p&gt;&lt;p&gt;[00:54:55] 3.5 AI Reasoning and Human Intelligence Comparison&lt;/p&gt;&lt;p&gt;[00:58:12] 3.6 NXAI Company and Industrial AI Applications&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:00:15] Seminal LSTM paper establishing Hochreiter&amp;#39;s expertise (Hochreiter &amp;amp; Schmidhuber)&lt;/p&gt;&lt;p&gt;https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:04:20] Kolmogorov complexity and program composition limitations (Kolmogorov)&lt;/p&gt;&lt;p&gt;https://link.springer.com/article/10.1007/BF02478259&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:07:10] Limitations of LLM mathematical reasoning and symbolic integration (Various Authors)&lt;/p&gt;&lt;p&gt;https://www.arxiv.org/pdf/2502.03671&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:09:05] AlphaGo’s Move 37 demonstrating creative AI (Google DeepMind)&lt;/p&gt;&lt;p&gt;https://deepmind.google/research/breakthroughs/alphago/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:10:15] New AI research lab in Zurich for fundamental LLM research (Benjamin Crouzier)&lt;/p&gt;&lt;p&gt;https://tufalabs.ai&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:19:40] Introduction of xLSTM with exponential gating (Beck, Hochreiter, et al.)&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2405.04517&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:22:55] FlashAttention: fast &amp;amp; memory-efficient attention (Tri Dao et al.)&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2205.14135&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:31:00] Historical use of sigmoid/tanh activation in 1990s (James A. McCaffrey)&lt;/p&gt;&lt;p&gt;https://visualstudiomagazine.com/articles/2015/06/01/alternative-activation-functions.aspx&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:36:10] Mamba 2 state space model architecture (Albert Gu et al.)&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2312.00752&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:46:00] Austria’s Pi AI project integrating symbolic &amp;amp; neural AI (Hochreiter et al.)&lt;/p&gt;&lt;p&gt;https://www.jku.at/en/institute-of-machine-learning/research/projects/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:48:10] Neuro-symbolic integration challenges in language models (Diego Calanzone et al.)&lt;/p&gt;&lt;p&gt;https://openreview.net/forum?id=7PGluppo4k&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:49:30] JKU Linz’s historical and neuro-symbolic research (Sepp Hochreiter)&lt;/p&gt;&lt;p&gt;https://www.jku.at/en/news-events/news/detail/news/bilaterale-ki-projekt-unter-leitung-der-jku-erhaelt-fwf-cluster-of-excellence/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;YT: https://www.youtube.com/watch?v=8u2pW2zZLCs&lt;/p&gt;&lt;p&gt;&amp;lt;truncated, see show notes/YT&amp;gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:01</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1739320215264-0d6d235c630d9.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Want to Understand Neural Networks? Think Elastic Origami! - Prof. Randall Balestriero]]></title>
			<description><![CDATA[<p>Professor Randall Balestriero joins us to discuss neural network geometry, spline theory, and emerging phenomena in deep learning, based on research presented at ICML. Topics include the delayed emergence of adversarial robustness in neural networks (&quot;grokking&quot;), geometric interpretations of neural networks via spline theory, and challenges in reconstruction learning. We also cover geometric analysis of Large Language Models (LLMs) for toxicity detection and the relationship between intrinsic dimensionality and model control in RLHF.</p><p><br></p><p>SPONSOR MESSAGES:</p><p>***</p><p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments.</p><p>https://centml.ai/pricing/</p><p><br></p><p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events?</p><p><br></p><p>Goto https://tufalabs.ai/</p><p>***</p><p><br></p><p>Randall Balestriero</p><p>https://x.com/randall_balestr</p><p>https://randallbalestriero.github.io/</p><p><br></p><p>Show notes and transcript: https://www.dropbox.com/scl/fi/3lufge4upq5gy0ug75j4a/RANDALLSHOW.pdf?rlkey=nbemgpa0jhawt1e86rx7372e4&amp;dl=0</p><p><br></p><p><br></p><p>TOC:</p><p>- Introduction</p><p>    - 00:00:00: Introduction</p><p>- Neural Network Geometry and Spline Theory</p><p>    - 00:01:41: Neural Network Geometry and Spline Theory</p><p>    - 00:07:41: Deep Networks Always Grok</p><p>    - 00:11:39: Grokking and Adversarial Robustness</p><p>    - 00:16:09: Double Descent and Catastrophic Forgetting</p><p>- Reconstruction Learning</p><p>    - 00:18:49: Reconstruction Learning</p><p>    - 00:24:15: Frequency Bias in Neural Networks</p><p>- Geometric Analysis of Neural Networks</p><p>    - 00:29:02: Geometric Analysis of Neural Networks</p><p>    - 00:34:41: Adversarial Examples and Region Concentration</p><p>- LLM Safety and Geometric Analysis</p><p>    - 00:40:05: LLM Safety and Geometric Analysis</p><p>    - 00:46:11: Toxicity Detection in LLMs</p><p>    - 00:52:24: Intrinsic Dimensionality and Model Control</p><p>    - 00:58:07: RLHF and High-Dimensional Spaces</p><p>- Conclusion</p><p>    - 01:02:13: Neural Tangent Kernel</p><p>    - 01:08:07: Conclusion</p><p><br></p><p><br></p><p>REFS:</p><p>[00:01:35] Humayun – Deep network geometry &amp; input space partitioning</p><p>https://arxiv.org/html/2408.04809v1</p><p><br></p><p>[00:03:55] Balestriero &amp; Paris – Linking deep networks to adaptive spline operators</p><p>https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf</p><p><br></p><p>[00:13:55] Song et al. – Gradient-based white-box adversarial attacks</p><p>https://arxiv.org/abs/2012.14965</p><p><br></p><p>[00:16:05] Humayun, Balestriero &amp; Baraniuk – Grokking phenomenon &amp; emergent robustness</p><p>https://arxiv.org/abs/2402.15555</p><p><br></p><p>[00:18:25] Humayun – Training dynamics &amp; double descent via linear region evolution</p><p>https://arxiv.org/abs/2310.12977</p><p><br></p><p>[00:20:15] Balestriero – Power diagram partitions in DNN decision boundaries</p><p>https://arxiv.org/abs/1905.08443</p><p><br></p><p>[00:23:00] Frankle &amp; Carbin – Lottery Ticket Hypothesis for network pruning</p><p>https://arxiv.org/abs/1803.03635</p><p><br></p><p>[00:24:00] Belkin et al. – Double descent phenomenon in modern ML</p><p>https://arxiv.org/abs/1812.11118</p><p><br></p><p>[00:25:55] Balestriero et al. – Batch normalization’s regularization effects</p><p>https://arxiv.org/pdf/2209.14778</p><p><br></p><p>[00:29:35] EU – EU AI Act 2024 with compute restrictions</p><p>https://www.lw.com/admin/upload/SiteAttachments/EU-AI-Act-Navigating-a-Brave-New-World.pdf</p><p><br></p><p>[00:39:30] Humayun, Balestriero &amp; Baraniuk – SplineCam: Visualizing deep network geometry</p><p>https://openaccess.thecvf.com/content/CVPR2023/papers/Humayun_SplineCam_Exact_Visualization_and_Characterization_of_Deep_Network_Geometry_and_CVPR_2023_paper.pdf</p><p><br></p><p>[00:40:40] Carlini – Trade-offs between adversarial robustness and accuracy</p><p>https://arxiv.org/pdf/2407.20099</p><p><br></p><p>[00:44:55] Balestriero &amp; LeCun – Limitations of reconstruction-based learning methods</p><p>https://openreview.net/forum?id=ez7w0Ss4g9</p><p>(truncated, see shownotes PDF)</p><p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Want-to-Understand-Neural-Networks--Think-Elastic-Origami----Prof--Randall-Balestriero-e2ujg9u</link>
			<guid isPermaLink="false">0bb5bc52-2ffd-40f7-ac52-b53787d7a4b8</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 08 Feb 2025 14:18:53 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/98205438/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-1-8%2F0d2541ce-6d8a-f729-83eb-feb798bbbd9b.mp3" length="113085577" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Randall Balestriero joins us to discuss neural network geometry, spline theory, and emerging phenomena in deep learning, based on research presented at ICML. Topics include the delayed emergence of adversarial robustness in neural networks (&amp;quot;grokking&amp;quot;), geometric interpretations of neural networks via spline theory, and challenges in reconstruction learning. We also cover geometric analysis of Large Language Models (LLMs) for toxicity detection and the relationship between intrinsic dimensionality and model control in RLHF.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments.&lt;/p&gt;&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events?&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;&lt;p&gt;***&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Randall Balestriero&lt;/p&gt;&lt;p&gt;https://x.com/randall_balestr&lt;/p&gt;&lt;p&gt;https://randallbalestriero.github.io/&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Show notes and transcript: https://www.dropbox.com/scl/fi/3lufge4upq5gy0ug75j4a/RANDALLSHOW.pdf?rlkey=nbemgpa0jhawt1e86rx7372e4&amp;amp;dl=0&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;TOC:&lt;/p&gt;&lt;p&gt;- Introduction&lt;/p&gt;&lt;p&gt;    - 00:00:00: Introduction&lt;/p&gt;&lt;p&gt;- Neural Network Geometry and Spline Theory&lt;/p&gt;&lt;p&gt;    - 00:01:41: Neural Network Geometry and Spline Theory&lt;/p&gt;&lt;p&gt;    - 00:07:41: Deep Networks Always Grok&lt;/p&gt;&lt;p&gt;    - 00:11:39: Grokking and Adversarial Robustness&lt;/p&gt;&lt;p&gt;    - 00:16:09: Double Descent and Catastrophic Forgetting&lt;/p&gt;&lt;p&gt;- Reconstruction Learning&lt;/p&gt;&lt;p&gt;    - 00:18:49: Reconstruction Learning&lt;/p&gt;&lt;p&gt;    - 00:24:15: Frequency Bias in Neural Networks&lt;/p&gt;&lt;p&gt;- Geometric Analysis of Neural Networks&lt;/p&gt;&lt;p&gt;    - 00:29:02: Geometric Analysis of Neural Networks&lt;/p&gt;&lt;p&gt;    - 00:34:41: Adversarial Examples and Region Concentration&lt;/p&gt;&lt;p&gt;- LLM Safety and Geometric Analysis&lt;/p&gt;&lt;p&gt;    - 00:40:05: LLM Safety and Geometric Analysis&lt;/p&gt;&lt;p&gt;    - 00:46:11: Toxicity Detection in LLMs&lt;/p&gt;&lt;p&gt;    - 00:52:24: Intrinsic Dimensionality and Model Control&lt;/p&gt;&lt;p&gt;    - 00:58:07: RLHF and High-Dimensional Spaces&lt;/p&gt;&lt;p&gt;- Conclusion&lt;/p&gt;&lt;p&gt;    - 01:02:13: Neural Tangent Kernel&lt;/p&gt;&lt;p&gt;    - 01:08:07: Conclusion&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;REFS:&lt;/p&gt;&lt;p&gt;[00:01:35] Humayun – Deep network geometry &amp;amp; input space partitioning&lt;/p&gt;&lt;p&gt;https://arxiv.org/html/2408.04809v1&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:03:55] Balestriero &amp;amp; Paris – Linking deep networks to adaptive spline operators&lt;/p&gt;&lt;p&gt;https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:13:55] Song et al. – Gradient-based white-box adversarial attacks&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2012.14965&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:16:05] Humayun, Balestriero &amp;amp; Baraniuk – Grokking phenomenon &amp;amp; emergent robustness&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2402.15555&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:18:25] Humayun – Training dynamics &amp;amp; double descent via linear region evolution&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2310.12977&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:20:15] Balestriero – Power diagram partitions in DNN decision boundaries&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1905.08443&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:23:00] Frankle &amp;amp; Carbin – Lottery Ticket Hypothesis for network pruning&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1803.03635&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:24:00] Belkin et al. – Double descent phenomenon in modern ML&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/1812.11118&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:25:55] Balestriero et al. – Batch normalization’s regularization effects&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/2209.14778&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:29:35] EU – EU AI Act 2024 with compute restrictions&lt;/p&gt;&lt;p&gt;https://www.lw.com/admin/upload/SiteAttachments/EU-AI-Act-Navigating-a-Brave-New-World.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:39:30] Humayun, Balestriero &amp;amp; Baraniuk – SplineCam: Visualizing deep network geometry&lt;/p&gt;&lt;p&gt;https://openaccess.thecvf.com/content/CVPR2023/papers/Humayun_SplineCam_Exact_Visualization_and_Characterization_of_Deep_Network_Geometry_and_CVPR_2023_paper.pdf&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:40:40] Carlini – Trade-offs between adversarial robustness and accuracy&lt;/p&gt;&lt;p&gt;https://arxiv.org/pdf/2407.20099&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;[00:44:55] Balestriero &amp;amp; LeCun – Limitations of reconstruction-based learning methods&lt;/p&gt;&lt;p&gt;https://openreview.net/forum?id=ez7w0Ss4g9&lt;/p&gt;&lt;p&gt;(truncated, see shownotes PDF)&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:18:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1739024310380-ace0424e10f8c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Nicholas Carlini (Google DeepMind)]]></title>
			<description><![CDATA[<p>Nicholas Carlini from Google DeepMind offers his view of AI security, emergent LLM capabilities, and his groundbreaking model-stealing research. He reveals how LLMs can unexpectedly excel at tasks like chess and discusses the security pitfalls of LLM-generated code. </p>
<p><br></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>
<p><br></p>
<p>Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br></p>
<p>Transcript: https://www.dropbox.com/scl/fi/lat7sfyd4k3g5k9crjpbf/CARLINI.pdf?rlkey=b7kcqbvau17uw6rksbr8ccd8v&amp;dl=0</p>
<p><br></p>
<p>TOC:</p>
<p>1. ML Security Fundamentals </p>
<p>[00:00:00] 1.1 ML Model Reasoning and Security Fundamentals </p>
<p>[00:03:04] 1.2 ML Security Vulnerabilities and System Design </p>
<p>[00:08:22] 1.3 LLM Chess Capabilities and Emergent Behavior  </p>
<p>[00:13:20] 1.4 Model Training, RLHF, and Calibration Effects  </p>
<p><br></p>
<p>2. Model Evaluation and Research Methods  </p>
<p>[00:19:40] 2.1 Model Reasoning and Evaluation Metrics  </p>
<p>[00:24:37] 2.2 Security Research Philosophy and Methodology  </p>
<p>[00:27:50] 2.3 Security Disclosure Norms and Community Differences  </p>
<p><br></p>
<p>3. LLM Applications and Best Practices</p>
<p>[00:44:29] 3.1 Practical LLM Applications and Productivity Gains  </p>
<p>[00:49:51] 3.2 Effective LLM Usage and Prompting Strategies  </p>
<p>[00:53:03] 3.3 Security Vulnerabilities in LLM-Generated Code  </p>
<p><br></p>
<p>4. Advanced LLM Research and Architecture</p>
<p>[00:59:13] 4.1 LLM Code Generation Performance and O(1) Labs Experience  </p>
<p>[01:03:31] 4.2 Adaptation Patterns and Benchmarking Challenges  </p>
<p>[01:10:10] 4.3 Model Stealing Research and Production LLM Architecture Extraction  </p>
<p><br></p>
<p>REFS:</p>
<p>[00:01:15] Nicholas Carlini’s personal website &amp; research profile (Google DeepMind, ML security) - https://nicholas.carlini.com/</p>
<p><br></p>
<p>[00:01:50] CentML AI compute platform for language model workloads - https://centml.ai/</p>
<p><br></p>
<p>[00:04:30] Seminal paper on neural network robustness against adversarial examples (Carlini &amp; Wagner, 2016) - https://arxiv.org/abs/1608.04644</p>
<p><br></p>
<p>[00:05:20] Computer Fraud and Abuse Act (CFAA) – primary U.S. federal law on computer hacking liability - https://www.justice.gov/jm/jm-9-48000-computer-fraud</p>
<p><br></p>
<p>[00:08:30] Blog post: Emergent chess capabilities in GPT-3.5-turbo-instruct (Nicholas Carlini, Sept 2023) - https://nicholas.carlini.com/writing/2023/chess-llm.html</p>
<p><br></p>
<p>[00:16:10] Paper: “Self-Play Preference Optimization for Language Model Alignment” (Yue Wu et al., 2024) - https://arxiv.org/abs/2405.00675</p>
<p><br></p>
<p>[00:18:00] GPT-4 Technical Report: development, capabilities, and calibration analysis - https://arxiv.org/abs/2303.08774</p>
<p><br></p>
<p>[00:22:40] Historical shift from descriptive to algebraic chess notation (FIDE) - https://en.wikipedia.org/wiki/Descriptive_notation</p>
<p><br></p>
<p>[00:23:55] Analysis of distribution shift in ML (Hendrycks et al.) - https://arxiv.org/abs/2006.16241</p>
<p><br></p>
<p>[00:27:40] Nicholas Carlini’s essay “Why I Attack” (June 2024) – motivations for security research - https://nicholas.carlini.com/writing/2024/why-i-attack.html</p>
<p><br></p>
<p>[00:34:05] Google Project Zero’s 90-day vulnerability disclosure policy - https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-policy.html</p>
<p><br></p>
<p>[00:51:15] Evolution of Google search syntax &amp; user behavior (Daniel M. Russell) - https://www.amazon.com/Joy-Search-Google-Master-Information/dp/0262042878</p>
<p><br></p>
<p>[01:04:05] Rust’s ownership &amp; borrowing system for memory safety - https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html</p>
<p><br></p>
<p>[01:10:05] Paper: “Stealing Part of a Production Language Model” (Carlini et al., March 2024) – extraction attacks on ChatGPT, PaLM-2 - https://arxiv.org/abs/2403.06634</p>
<p><br></p>
<p>[01:10:55] First model stealing paper (Tramèr et al., 2016) – attacking ML APIs via prediction - https://arxiv.org/abs/1609.02943</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Nicholas-Carlini-Google-DeepMind-e2tvqch</link>
			<guid isPermaLink="false">56ca916a-6956-4124-a7bc-ddd727bbae94</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 25 Jan 2025 21:22:34 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/97560401/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-25%2Fe2cfa06e-9d17-d934-48a1-c88f969a4f47.mp3" length="117320242" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Nicholas Carlini from Google DeepMind offers his view of AI security, emergent LLM capabilities, and his groundbreaking model-stealing research. He reveals how LLMs can unexpectedly excel at tasks like chess and discusses the security pitfalls of LLM-generated code. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Transcript: https://www.dropbox.com/scl/fi/lat7sfyd4k3g5k9crjpbf/CARLINI.pdf?rlkey=b7kcqbvau17uw6rksbr8ccd8v&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. ML Security Fundamentals &lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 ML Model Reasoning and Security Fundamentals &lt;/p&gt;
&lt;p&gt;[00:03:04] 1.2 ML Security Vulnerabilities and System Design &lt;/p&gt;
&lt;p&gt;[00:08:22] 1.3 LLM Chess Capabilities and Emergent Behavior  &lt;/p&gt;
&lt;p&gt;[00:13:20] 1.4 Model Training, RLHF, and Calibration Effects  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;2. Model Evaluation and Research Methods  &lt;/p&gt;
&lt;p&gt;[00:19:40] 2.1 Model Reasoning and Evaluation Metrics  &lt;/p&gt;
&lt;p&gt;[00:24:37] 2.2 Security Research Philosophy and Methodology  &lt;/p&gt;
&lt;p&gt;[00:27:50] 2.3 Security Disclosure Norms and Community Differences  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3. LLM Applications and Best Practices&lt;/p&gt;
&lt;p&gt;[00:44:29] 3.1 Practical LLM Applications and Productivity Gains  &lt;/p&gt;
&lt;p&gt;[00:49:51] 3.2 Effective LLM Usage and Prompting Strategies  &lt;/p&gt;
&lt;p&gt;[00:53:03] 3.3 Security Vulnerabilities in LLM-Generated Code  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;4. Advanced LLM Research and Architecture&lt;/p&gt;
&lt;p&gt;[00:59:13] 4.1 LLM Code Generation Performance and O(1) Labs Experience  &lt;/p&gt;
&lt;p&gt;[01:03:31] 4.2 Adaptation Patterns and Benchmarking Challenges  &lt;/p&gt;
&lt;p&gt;[01:10:10] 4.3 Model Stealing Research and Production LLM Architecture Extraction  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;[00:01:15] Nicholas Carlini’s personal website &amp;amp; research profile (Google DeepMind, ML security) - https://nicholas.carlini.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:01:50] CentML AI compute platform for language model workloads - https://centml.ai/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:04:30] Seminal paper on neural network robustness against adversarial examples (Carlini &amp;amp; Wagner, 2016) - https://arxiv.org/abs/1608.04644&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:05:20] Computer Fraud and Abuse Act (CFAA) – primary U.S. federal law on computer hacking liability - https://www.justice.gov/jm/jm-9-48000-computer-fraud&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:08:30] Blog post: Emergent chess capabilities in GPT-3.5-turbo-instruct (Nicholas Carlini, Sept 2023) - https://nicholas.carlini.com/writing/2023/chess-llm.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:16:10] Paper: “Self-Play Preference Optimization for Language Model Alignment” (Yue Wu et al., 2024) - https://arxiv.org/abs/2405.00675&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:18:00] GPT-4 Technical Report: development, capabilities, and calibration analysis - https://arxiv.org/abs/2303.08774&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:22:40] Historical shift from descriptive to algebraic chess notation (FIDE) - https://en.wikipedia.org/wiki/Descriptive_notation&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:23:55] Analysis of distribution shift in ML (Hendrycks et al.) - https://arxiv.org/abs/2006.16241&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:27:40] Nicholas Carlini’s essay “Why I Attack” (June 2024) – motivations for security research - https://nicholas.carlini.com/writing/2024/why-i-attack.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:34:05] Google Project Zero’s 90-day vulnerability disclosure policy - https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-policy.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:51:15] Evolution of Google search syntax &amp;amp; user behavior (Daniel M. Russell) - https://www.amazon.com/Joy-Search-Google-Master-Information/dp/0262042878&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[01:04:05] Rust’s ownership &amp;amp; borrowing system for memory safety - https://doc.rust-lang.org/book/ch04-00-understanding-ownership.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[01:10:05] Paper: “Stealing Part of a Production Language Model” (Carlini et al., March 2024) – extraction attacks on ChatGPT, PaLM-2 - https://arxiv.org/abs/2403.06634&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[01:10:55] First model stealing paper (Tramèr et al., 2016) – attacking ML APIs via prediction - https://arxiv.org/abs/1609.02943&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:21:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1737840136317-df98d08139cef.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Subbarao Kambhampati - Do o1 models search?]]></title>
			<description><![CDATA[<p>Join Prof. Subbarao Kambhampati and host Tim Scarfe for a deep dive into OpenAI&#39;s O1 model and the future of AI reasoning systems. </p>
<p><br></p>
<p>* How O1 likely uses reinforcement learning similar to AlphaGo, with hidden reasoning tokens that users pay for but never see</p>
<p>* The evolution from traditional Large Language Models to more sophisticated reasoning systems</p>
<p>* The concept of &quot;fractal intelligence&quot; in AI - where models work brilliantly sometimes but fail unpredictably</p>
<p>* Why O1&#39;s improved performance comes with substantial computational costs</p>
<p>* The ongoing debate between single-model approaches (OpenAI) vs hybrid systems (Google)</p>
<p>* The critical distinction between AI as an intelligence amplifier vs autonomous decision-maker</p>
<p><br></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>
<p><br></p>
<p>Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br></p>
<p>TOC:</p>
<p>1. **O1 Architecture and Reasoning Foundations** </p>
<p>[00:00:00] 1.1 Fractal Intelligence and Reasoning Model Limitations </p>
<p>[00:04:28] 1.2 LLM Evolution: From Simple Prompting to Advanced Reasoning  </p>
<p>[00:14:28] 1.3 O1&#39;s Architecture and AlphaGo-like Reasoning Approach  </p>
<p>[00:23:18] 1.4 Empirical Evaluation of O1&#39;s Planning Capabilities  </p>
<p><br></p>
<p>2. **Monte Carlo Methods and Model Deep-Dive**  </p>
<p>[00:29:30] 2.1 Monte Carlo Methods and MARCO-O1 Implementation  </p>
<p>[00:31:30] 2.2 Reasoning vs. Retrieval in LLM Systems  </p>
<p>[00:40:40] 2.3 Fractal Intelligence Capabilities and Limitations  </p>
<p>[00:45:59] 2.4 Mechanistic Interpretability of Model Behavior  </p>
<p>[00:51:41] 2.5 O1 Response Patterns and Performance Analysis  </p>
<p><br></p>
<p>3. **System Design and Real-World Applications**  </p>
<p>[00:59:30] 3.1 Evolution from LLMs to Language Reasoning Models  </p>
<p>[01:06:48] 3.2 Cost-Efficiency Analysis: LLMs vs O1  </p>
<p>[01:11:28] 3.3 Autonomous vs Human-in-the-Loop Systems  </p>
<p>[01:16:01] 3.4 Program Generation and Fine-Tuning Approaches  </p>
<p>[01:26:08] 3.5 Hybrid Architecture Implementation Strategies  </p>
<p><br></p>
<p>Transcript: https://www.dropbox.com/scl/fi/d0ef4ovnfxi0lknirkvft/Subbarao.pdf?rlkey=l3rp29gs4hkut7he8u04mm1df&amp;dl=0</p>
<p><br></p>
<p>REFS:</p>
<p>[00:02:00] Monty Python (1975)</p>
<p>Witch trial scene: flawed logical reasoning.</p>
<p>https://www.youtube.com/watch?v=zrzMhU_4m-g</p>
<p><br></p>
<p>[00:04:00] Cade Metz (2024)</p>
<p>Microsoft–OpenAI partnership evolution and control dynamics.</p>
<p>https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html</p>
<p><br></p>
<p>[00:07:25] Kojima et al. (2022)</p>
<p>Zero-shot chain-of-thought prompting (&#39;Let&#39;s think step by step&#39;).</p>
<p>https://arxiv.org/pdf/2205.11916</p>
<p><br></p>
<p>[00:12:50] DeepMind Research Team (2023)</p>
<p>Multi-bot game solving with external and internal planning.</p>
<p>https://deepmind.google/research/publications/139455/</p>
<p><br></p>
<p>[00:15:10] Silver et al. (2016)</p>
<p>AlphaGo&#39;s Monte Carlo Tree Search and Q-learning.</p>
<p>https://www.nature.com/articles/nature16961</p>
<p><br></p>
<p>[00:16:30] Kambhampati, S. et al. (2023)</p>
<p>Evaluates O1&#39;s planning in &quot;Strawberry Fields&quot; benchmarks.</p>
<p>https://arxiv.org/pdf/2410.02162</p>
<p><br></p>
<p>[00:29:30] Alibaba AIDC-AI Team (2023)</p>
<p>MARCO-O1: Chain-of-Thought + MCTS for improved reasoning.</p>
<p>https://arxiv.org/html/2411.14405</p>
<p><br></p>
<p>[00:31:30] Kambhampati, S. (2024)</p>
<p>Explores LLM &quot;reasoning vs retrieval&quot; debate.</p>
<p>https://arxiv.org/html/2403.04121v2</p>
<p><br></p>
<p>[00:37:35] Wei, J. et al. (2022)</p>
<p>Chain-of-thought prompting (introduces last-letter concatenation).</p>
<p>https://arxiv.org/pdf/2201.11903</p>
<p><br></p>
<p>[00:42:35] Barbero, F. et al. (2024)</p>
<p>Transformer attention and &quot;information over-squashing.&quot;</p>
<p>https://arxiv.org/html/2406.04267v2</p>
<p><br></p>
<p>[00:46:05] Ruis, L. et al. (2023)</p>
<p>Influence functions to understand procedural knowledge in LLMs.</p>
<p>https://arxiv.org/html/2411.12580v1</p>
<p><br></p>
<p>(truncated - continued in shownotes/transcript doc)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Subbarao-Kambhampati---Do-o1-models-search-e2ts49m</link>
			<guid isPermaLink="false">1ee01beb-0c7d-4e9a-965e-5db8c3a272f7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 23 Jan 2025 01:46:43 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/97439478/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-23%2F9a1ea19a-508f-9cf3-ab02-be2d528461f4.mp3" length="133294109" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Join Prof. Subbarao Kambhampati and host Tim Scarfe for a deep dive into OpenAI&amp;#39;s O1 model and the future of AI reasoning systems. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;* How O1 likely uses reinforcement learning similar to AlphaGo, with hidden reasoning tokens that users pay for but never see&lt;/p&gt;
&lt;p&gt;* The evolution from traditional Large Language Models to more sophisticated reasoning systems&lt;/p&gt;
&lt;p&gt;* The concept of &amp;quot;fractal intelligence&amp;quot; in AI - where models work brilliantly sometimes but fail unpredictably&lt;/p&gt;
&lt;p&gt;* Why O1&amp;#39;s improved performance comes with substantial computational costs&lt;/p&gt;
&lt;p&gt;* The ongoing debate between single-model approaches (OpenAI) vs hybrid systems (Google)&lt;/p&gt;
&lt;p&gt;* The critical distinction between AI as an intelligence amplifier vs autonomous decision-maker&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. **O1 Architecture and Reasoning Foundations** &lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Fractal Intelligence and Reasoning Model Limitations &lt;/p&gt;
&lt;p&gt;[00:04:28] 1.2 LLM Evolution: From Simple Prompting to Advanced Reasoning  &lt;/p&gt;
&lt;p&gt;[00:14:28] 1.3 O1&amp;#39;s Architecture and AlphaGo-like Reasoning Approach  &lt;/p&gt;
&lt;p&gt;[00:23:18] 1.4 Empirical Evaluation of O1&amp;#39;s Planning Capabilities  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;2. **Monte Carlo Methods and Model Deep-Dive**  &lt;/p&gt;
&lt;p&gt;[00:29:30] 2.1 Monte Carlo Methods and MARCO-O1 Implementation  &lt;/p&gt;
&lt;p&gt;[00:31:30] 2.2 Reasoning vs. Retrieval in LLM Systems  &lt;/p&gt;
&lt;p&gt;[00:40:40] 2.3 Fractal Intelligence Capabilities and Limitations  &lt;/p&gt;
&lt;p&gt;[00:45:59] 2.4 Mechanistic Interpretability of Model Behavior  &lt;/p&gt;
&lt;p&gt;[00:51:41] 2.5 O1 Response Patterns and Performance Analysis  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3. **System Design and Real-World Applications**  &lt;/p&gt;
&lt;p&gt;[00:59:30] 3.1 Evolution from LLMs to Language Reasoning Models  &lt;/p&gt;
&lt;p&gt;[01:06:48] 3.2 Cost-Efficiency Analysis: LLMs vs O1  &lt;/p&gt;
&lt;p&gt;[01:11:28] 3.3 Autonomous vs Human-in-the-Loop Systems  &lt;/p&gt;
&lt;p&gt;[01:16:01] 3.4 Program Generation and Fine-Tuning Approaches  &lt;/p&gt;
&lt;p&gt;[01:26:08] 3.5 Hybrid Architecture Implementation Strategies  &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Transcript: https://www.dropbox.com/scl/fi/d0ef4ovnfxi0lknirkvft/Subbarao.pdf?rlkey=l3rp29gs4hkut7he8u04mm1df&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;[00:02:00] Monty Python (1975)&lt;/p&gt;
&lt;p&gt;Witch trial scene: flawed logical reasoning.&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=zrzMhU_4m-g&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:04:00] Cade Metz (2024)&lt;/p&gt;
&lt;p&gt;Microsoft–OpenAI partnership evolution and control dynamics.&lt;/p&gt;
&lt;p&gt;https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:07:25] Kojima et al. (2022)&lt;/p&gt;
&lt;p&gt;Zero-shot chain-of-thought prompting (&amp;#39;Let&amp;#39;s think step by step&amp;#39;).&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2205.11916&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:12:50] DeepMind Research Team (2023)&lt;/p&gt;
&lt;p&gt;Multi-bot game solving with external and internal planning.&lt;/p&gt;
&lt;p&gt;https://deepmind.google/research/publications/139455/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:15:10] Silver et al. (2016)&lt;/p&gt;
&lt;p&gt;AlphaGo&amp;#39;s Monte Carlo Tree Search and Q-learning.&lt;/p&gt;
&lt;p&gt;https://www.nature.com/articles/nature16961&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:16:30] Kambhampati, S. et al. (2023)&lt;/p&gt;
&lt;p&gt;Evaluates O1&amp;#39;s planning in &amp;quot;Strawberry Fields&amp;quot; benchmarks.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2410.02162&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:29:30] Alibaba AIDC-AI Team (2023)&lt;/p&gt;
&lt;p&gt;MARCO-O1: Chain-of-Thought + MCTS for improved reasoning.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/html/2411.14405&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:31:30] Kambhampati, S. (2024)&lt;/p&gt;
&lt;p&gt;Explores LLM &amp;quot;reasoning vs retrieval&amp;quot; debate.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/html/2403.04121v2&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:37:35] Wei, J. et al. (2022)&lt;/p&gt;
&lt;p&gt;Chain-of-thought prompting (introduces last-letter concatenation).&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2201.11903&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:42:35] Barbero, F. et al. (2024)&lt;/p&gt;
&lt;p&gt;Transformer attention and &amp;quot;information over-squashing.&amp;quot;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/html/2406.04267v2&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:46:05] Ruis, L. et al. (2023)&lt;/p&gt;
&lt;p&gt;Influence functions to understand procedural knowledge in LLMs.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/html/2411.12580v1&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(truncated - continued in shownotes/transcript doc)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:32:13</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1737596732073-5b1e64750272e.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[How Do AI Models Actually Think? - Laura Ruis]]></title>
			<description><![CDATA[<p>Laura Ruis, a PhD student at University College London and researcher at Cohere, explains her groundbreaking research into how large language models (LLMs) perform reasoning tasks, the fundamental mechanisms underlying LLM reasoning capabilities, and whether these models primarily rely on retrieval or develop procedural knowledge.</p>
<p><br></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>
<p><br></p>
<p>Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br></p>
<p>TOC</p>
<p><br></p>
<p>1. LLM Foundations and Learning</p>
<p> 1.1 Scale and Learning in Language Models [00:00:00]</p>
<p>   1.2 Procedural Knowledge vs Fact Retrieval [00:03:40]</p>
<p>   1.3 Influence Functions and Model Analysis [00:07:40]</p>
<p>   1.4 Role of Code in LLM Reasoning [00:11:10]</p>
<p>   1.5 Semantic Understanding and Physical Grounding [00:19:30]</p>
<p><br></p>
<p>2. Reasoning Architectures and Measurement</p>
<p>   2.1 Measuring Understanding and Reasoning in Language Models [00:23:10]</p>
<p>   2.2 Formal vs Approximate Reasoning and Model Creativity [00:26:40]</p>
<p>   2.3 Symbolic vs Subsymbolic Computation Debate [00:34:10]</p>
<p>   2.4 Neural Network Architectures and Tensor Product Representations [00:40:50]</p>
<p><br></p>
<p>3. AI Agency and Risk Assessment</p>
<p>   3.1 Agency and Goal-Directed Behavior in Language Models [00:45:10]</p>
<p>   3.2 Defining and Measuring Agency in AI Systems [00:49:50]</p>
<p>   3.3 Core Knowledge Systems and Agency Detection [00:54:40]</p>
<p>   3.4 Language Models as Agent Models and Simulator Theory [01:03:20]</p>
<p>   3.5 AI Safety and Societal Control Mechanisms [01:07:10]</p>
<p>   3.6 Evolution of AI Capabilities and Emergent Risks [01:14:20]</p>
<p><br></p>
<p>REFS:</p>
<p>[00:01:10] Procedural Knowledge in Pretraining &amp; LLM Reasoning</p>
<p>Ruis et al., 2024</p>
<p>https://arxiv.org/abs/2411.12580</p>
<p><br></p>
<p>[00:03:50] EK-FAC Influence Functions in Large LMs</p>
<p>Grosse et al., 2023</p>
<p>https://arxiv.org/abs/2308.03296</p>
<p><br></p>
<p>[00:13:05] Surfaces and Essences: Analogy as the Core of Cognition</p>
<p>Hofstadter &amp; Sander</p>
<p>https://www.amazon.com/Surfaces-Essences-Analogy-Fuel-Thinking/dp/0465018475</p>
<p><br></p>
<p>[00:13:45] Wittgenstein on Language Games</p>
<p>https://plato.stanford.edu/entries/wittgenstein/</p>
<p><br></p>
<p>[00:14:30] Montague Semantics for Natural Language</p>
<p>https://plato.stanford.edu/entries/montague-semantics/</p>
<p><br></p>
<p>[00:19:35] The Chinese Room Argument</p>
<p>David Cole</p>
<p>https://plato.stanford.edu/entries/chinese-room/</p>
<p><br></p>
<p>[00:19:55] ARC: Abstraction and Reasoning Corpus</p>
<p>François Chollet</p>
<p>https://arxiv.org/abs/1911.01547</p>
<p><br></p>
<p>[00:24:20] Systematic Generalization in Neural Nets</p>
<p>Lake &amp; Baroni, 2023</p>
<p>https://www.nature.com/articles/s41586-023-06668-3</p>
<p><br></p>
<p>[00:27:40] Open-Endedness &amp; Creativity in AI</p>
<p>Tim Rocktäschel</p>
<p>https://arxiv.org/html/2406.04268v1</p>
<p><br></p>
<p>[00:30:50] Fodor &amp; Pylyshyn on Connectionism</p>
<p>https://www.sciencedirect.com/science/article/abs/pii/0010027788900315</p>
<p><br></p>
<p>[00:31:30] Tensor Product Representations</p>
<p>Smolensky, 1990</p>
<p>https://www.sciencedirect.com/science/article/abs/pii/000437029090007M</p>
<p><br></p>
<p>[00:35:50] DreamCoder: Wake-Sleep Program Synthesis</p>
<p>Kevin Ellis et al.</p>
<p>https://courses.cs.washington.edu/courses/cse599j1/22sp/papers/dreamcoder.pdf</p>
<p><br></p>
<p>[00:36:30] Compositional Generalization Benchmarks</p>
<p>Ruis, Lake et al., 2022</p>
<p>https://arxiv.org/pdf/2202.10745</p>
<p><br></p>
<p>[00:40:30] RNNs &amp; Tensor Products</p>
<p>McCoy et al., 2018</p>
<p>https://arxiv.org/abs/1812.08718</p>
<p><br></p>
<p>[00:46:10] Formal Causal Definition of Agency</p>
<p>Kenton et al.</p>
<p>https://arxiv.org/pdf/2208.08345v2</p>
<p><br></p>
<p>[00:48:40] Agency in Language Models</p>
<p>Sumers et al.</p>
<p>https://arxiv.org/abs/2309.02427</p>
<p><br></p>
<p>[00:55:20] Heider &amp; Simmel’s Moving Shapes Experiment</p>
<p>https://www.nature.com/articles/s41598-024-65532-0</p>
<p><br></p>
<p>[01:00:40] Language Models as Agent Models</p>
<p>Jacob Andreas, 2022</p>
<p>https://arxiv.org/abs/2212.01681</p>
<p><br></p>
<p>[01:13:35] Pragmatic Understanding in LLMs</p>
<p>Ruis et al.</p>
<p>https://arxiv.org/abs/2210.14986</p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/How-Do-AI-Models-Actually-Think----Laura-Ruis-e2tn9l9</link>
			<guid isPermaLink="false">f1ecadd9-b99b-4a87-815a-476bf42264d6</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 20 Jan 2025 00:28:12 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/97281129/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-20%2F808da284-ab46-0ae7-eb82-bb0885ebbcb1.mp3" length="112639159" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Laura Ruis, a PhD student at University College London and researcher at Cohere, explains her groundbreaking research into how large language models (LLMs) perform reasoning tasks, the fundamental mechanisms underlying LLM reasoning capabilities, and whether these models primarily rely on retrieval or develop procedural knowledge.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;1. LLM Foundations and Learning&lt;/p&gt;
&lt;p&gt; 1.1 Scale and Learning in Language Models [00:00:00]&lt;/p&gt;
&lt;p&gt;   1.2 Procedural Knowledge vs Fact Retrieval [00:03:40]&lt;/p&gt;
&lt;p&gt;   1.3 Influence Functions and Model Analysis [00:07:40]&lt;/p&gt;
&lt;p&gt;   1.4 Role of Code in LLM Reasoning [00:11:10]&lt;/p&gt;
&lt;p&gt;   1.5 Semantic Understanding and Physical Grounding [00:19:30]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;2. Reasoning Architectures and Measurement&lt;/p&gt;
&lt;p&gt;   2.1 Measuring Understanding and Reasoning in Language Models [00:23:10]&lt;/p&gt;
&lt;p&gt;   2.2 Formal vs Approximate Reasoning and Model Creativity [00:26:40]&lt;/p&gt;
&lt;p&gt;   2.3 Symbolic vs Subsymbolic Computation Debate [00:34:10]&lt;/p&gt;
&lt;p&gt;   2.4 Neural Network Architectures and Tensor Product Representations [00:40:50]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3. AI Agency and Risk Assessment&lt;/p&gt;
&lt;p&gt;   3.1 Agency and Goal-Directed Behavior in Language Models [00:45:10]&lt;/p&gt;
&lt;p&gt;   3.2 Defining and Measuring Agency in AI Systems [00:49:50]&lt;/p&gt;
&lt;p&gt;   3.3 Core Knowledge Systems and Agency Detection [00:54:40]&lt;/p&gt;
&lt;p&gt;   3.4 Language Models as Agent Models and Simulator Theory [01:03:20]&lt;/p&gt;
&lt;p&gt;   3.5 AI Safety and Societal Control Mechanisms [01:07:10]&lt;/p&gt;
&lt;p&gt;   3.6 Evolution of AI Capabilities and Emergent Risks [01:14:20]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;[00:01:10] Procedural Knowledge in Pretraining &amp;amp; LLM Reasoning&lt;/p&gt;
&lt;p&gt;Ruis et al., 2024&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2411.12580&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:03:50] EK-FAC Influence Functions in Large LMs&lt;/p&gt;
&lt;p&gt;Grosse et al., 2023&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2308.03296&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:13:05] Surfaces and Essences: Analogy as the Core of Cognition&lt;/p&gt;
&lt;p&gt;Hofstadter &amp;amp; Sander&lt;/p&gt;
&lt;p&gt;https://www.amazon.com/Surfaces-Essences-Analogy-Fuel-Thinking/dp/0465018475&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:13:45] Wittgenstein on Language Games&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/wittgenstein/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:14:30] Montague Semantics for Natural Language&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/montague-semantics/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:19:35] The Chinese Room Argument&lt;/p&gt;
&lt;p&gt;David Cole&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/chinese-room/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:19:55] ARC: Abstraction and Reasoning Corpus&lt;/p&gt;
&lt;p&gt;François Chollet&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:24:20] Systematic Generalization in Neural Nets&lt;/p&gt;
&lt;p&gt;Lake &amp;amp; Baroni, 2023&lt;/p&gt;
&lt;p&gt;https://www.nature.com/articles/s41586-023-06668-3&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:27:40] Open-Endedness &amp;amp; Creativity in AI&lt;/p&gt;
&lt;p&gt;Tim Rocktäschel&lt;/p&gt;
&lt;p&gt;https://arxiv.org/html/2406.04268v1&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:30:50] Fodor &amp;amp; Pylyshyn on Connectionism&lt;/p&gt;
&lt;p&gt;https://www.sciencedirect.com/science/article/abs/pii/0010027788900315&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:31:30] Tensor Product Representations&lt;/p&gt;
&lt;p&gt;Smolensky, 1990&lt;/p&gt;
&lt;p&gt;https://www.sciencedirect.com/science/article/abs/pii/000437029090007M&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:35:50] DreamCoder: Wake-Sleep Program Synthesis&lt;/p&gt;
&lt;p&gt;Kevin Ellis et al.&lt;/p&gt;
&lt;p&gt;https://courses.cs.washington.edu/courses/cse599j1/22sp/papers/dreamcoder.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:36:30] Compositional Generalization Benchmarks&lt;/p&gt;
&lt;p&gt;Ruis, Lake et al., 2022&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2202.10745&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:40:30] RNNs &amp;amp; Tensor Products&lt;/p&gt;
&lt;p&gt;McCoy et al., 2018&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1812.08718&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:46:10] Formal Causal Definition of Agency&lt;/p&gt;
&lt;p&gt;Kenton et al.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2208.08345v2&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:48:40] Agency in Language Models&lt;/p&gt;
&lt;p&gt;Sumers et al.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2309.02427&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:55:20] Heider &amp;amp; Simmel’s Moving Shapes Experiment&lt;/p&gt;
&lt;p&gt;https://www.nature.com/articles/s41598-024-65532-0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[01:00:40] Language Models as Agent Models&lt;/p&gt;
&lt;p&gt;Jacob Andreas, 2022&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2212.01681&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[01:13:35] Pragmatic Understanding in LLMs&lt;/p&gt;
&lt;p&gt;Ruis et al.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.14986&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:18:01</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1737332869806-7fbd78a05b243.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jurgen Schmidhuber on Humans co-existing with AIs]]></title>
			<description><![CDATA[<p>Jürgen Schmidhuber, the father of generative AI, challenges current AI narratives, revealing that early deep learning work is in his opinion misattributed, where it actually originated in Ukraine and Japan. He discusses his early work on linear transformers and artificial curiosity which preceded modern developments, shares his expansive vision of AI colonising space, and explains his groundbreaking 1991 consciousness model. Schmidhuber dismisses fears of human-AI conflict, arguing that superintelligent AI scientists will be fascinated by their own origins and motivated to protect life rather than harm it, while being more interested in other superintelligent AI and in cosmic expansion than earthly matters. He offers unique insights into how humans and AI might coexist.

This was the long-awaited second, unreleased part of our interview we filmed last time. 

SPONSOR MESSAGES:
***
CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. 
https://centml.ai/pricing/

Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? 

Goto https://tufalabs.ai/
***

Interviewer: Tim Scarfe

TOC
[00:00:00] The Nature and Motivations of AI 
[00:02:08] Influential Inventions: 20th vs. 21st Century  
[00:05:28] Transformer and GPT: A Reflection  
The revolutionary impact of modern language models, the 1991 linear transformer, linear vs. quadratic scaling, the fast weight controller, and fast weight matrix memory.

[00:11:03] Pioneering Contributions to AI and Deep Learning  
The invention of the transformer, pre-trained networks, the first GANs, the role of predictive coding, and the emergence of artificial curiosity.

[00:13:58] AI&#39;s Evolution and Achievements  
The role of compute, breakthroughs in handwriting recognition and computer vision, the rise of GPU-based CNNs, achieving superhuman results, and Japanese contributions to CNN development.

[00:15:40] The Hardware Lottery and GPUs  
GPUs as a serendipitous advantage for AI, the gaming-AI parallel, and Nvidia&#39;s strategic shift towards AI.

[00:19:58] AI Applications and Societal Impact  
AI-powered translation breaking communication barriers, AI in medicine for imaging and disease prediction, and AI&#39;s potential for human enhancement and sustainable development.

[00:23:26] The Path to AGI and Current Limitations  
Distinguishing large language models from AGI, challenges in replacing physical world workers, and AI&#39;s difficulty in real-world versus board games.

[00:25:56] AI and Consciousness  
Simulating consciousness through unsupervised learning, chunking and automatizing neural networks, data compression, and self-symbols in predictive world models.

[00:30:50] The Future of AI and Humanity  
Transition from AGIs as tools to AGIs with their own goals, the role of humans in an AGI-dominated world, and the concept of Homo Ludens.

[00:38:05] The AI Race: Europe, China, and the US  
Europe&#39;s historical contributions, current dominance of the US and East Asia, and the role of venture capital and industrial policy.

[00:50:32] Addressing AI Existential Risk  
The obsession with AI existential risk, commercial pressure for friendly AIs, AI vs. hydrogen bombs, and the long-term future of AI.

[00:58:00] The Fermi Paradox and Extraterrestrial Intelligence  
Expanding AI bubbles as an explanation for the Fermi paradox, dark matter and encrypted civilizations, and Earth as the first to spawn an AI bubble.

[01:02:08] The Diversity of AI and AI Ecologies  
The unrealism of a monolithic super intelligence, diverse AIs with varying goals, and intense competition and collaboration in AI ecologies.

[01:12:21] Final Thoughts and Closing Remarks  

REFERENCES:
See pinned comment on YT: <strong>https://youtu.be/fZYUqICYCAk</strong></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jurgen-Schmidhuber-on-Humans-co-existing-with-AIs-e2tjkb6</link>
			<guid isPermaLink="false">6986baf2-ab2d-4a10-8c43-4ae8bfd81e4d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 16 Jan 2025 21:42:55 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/97160998/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-16%2Fdf844efa-ab18-f562-ed39-af86e3f5078d.mp3" length="105386278" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Jürgen Schmidhuber, the father of generative AI, challenges current AI narratives, revealing that early deep learning work is in his opinion misattributed, where it actually originated in Ukraine and Japan. He discusses his early work on linear transformers and artificial curiosity which preceded modern developments, shares his expansive vision of AI colonising space, and explains his groundbreaking 1991 consciousness model. Schmidhuber dismisses fears of human-AI conflict, arguing that superintelligent AI scientists will be fascinated by their own origins and motivated to protect life rather than harm it, while being more interested in other superintelligent AI and in cosmic expansion than earthly matters. He offers unique insights into how humans and AI might coexist.

This was the long-awaited second, unreleased part of our interview we filmed last time. 

SPONSOR MESSAGES:
***
CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. 
https://centml.ai/pricing/

Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? 

Goto https://tufalabs.ai/
***

Interviewer: Tim Scarfe

TOC
[00:00:00] The Nature and Motivations of AI 
[00:02:08] Influential Inventions: 20th vs. 21st Century  
[00:05:28] Transformer and GPT: A Reflection  
The revolutionary impact of modern language models, the 1991 linear transformer, linear vs. quadratic scaling, the fast weight controller, and fast weight matrix memory.

[00:11:03] Pioneering Contributions to AI and Deep Learning  
The invention of the transformer, pre-trained networks, the first GANs, the role of predictive coding, and the emergence of artificial curiosity.

[00:13:58] AI&amp;#39;s Evolution and Achievements  
The role of compute, breakthroughs in handwriting recognition and computer vision, the rise of GPU-based CNNs, achieving superhuman results, and Japanese contributions to CNN development.

[00:15:40] The Hardware Lottery and GPUs  
GPUs as a serendipitous advantage for AI, the gaming-AI parallel, and Nvidia&amp;#39;s strategic shift towards AI.

[00:19:58] AI Applications and Societal Impact  
AI-powered translation breaking communication barriers, AI in medicine for imaging and disease prediction, and AI&amp;#39;s potential for human enhancement and sustainable development.

[00:23:26] The Path to AGI and Current Limitations  
Distinguishing large language models from AGI, challenges in replacing physical world workers, and AI&amp;#39;s difficulty in real-world versus board games.

[00:25:56] AI and Consciousness  
Simulating consciousness through unsupervised learning, chunking and automatizing neural networks, data compression, and self-symbols in predictive world models.

[00:30:50] The Future of AI and Humanity  
Transition from AGIs as tools to AGIs with their own goals, the role of humans in an AGI-dominated world, and the concept of Homo Ludens.

[00:38:05] The AI Race: Europe, China, and the US  
Europe&amp;#39;s historical contributions, current dominance of the US and East Asia, and the role of venture capital and industrial policy.

[00:50:32] Addressing AI Existential Risk  
The obsession with AI existential risk, commercial pressure for friendly AIs, AI vs. hydrogen bombs, and the long-term future of AI.

[00:58:00] The Fermi Paradox and Extraterrestrial Intelligence  
Expanding AI bubbles as an explanation for the Fermi paradox, dark matter and encrypted civilizations, and Earth as the first to spawn an AI bubble.

[01:02:08] The Diversity of AI and AI Ecologies  
The unrealism of a monolithic super intelligence, diverse AIs with varying goals, and intense competition and collaboration in AI ecologies.

[01:12:21] Final Thoughts and Closing Remarks  

REFERENCES:
See pinned comment on YT: &lt;strong&gt;https://youtu.be/fZYUqICYCAk&lt;/strong&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:12:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1737063734146-df13ff28c3b9e.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Yoshua Bengio - Designing out Agency for Safe AI]]></title>
			<description><![CDATA[<p>Professor Yoshua Bengio is a pioneer in deep learning and Turing Award winner. Bengio talks about AI safety, why goal-seeking “agentic” AIs might be dangerous, and his vision for building powerful AI tools without giving them agency. Topics include reward tampering risks, instrumental convergence, global AI governance, and how non-agent AIs could revolutionize science and medicine while reducing existential threats. Perfect for anyone curious about advanced AI risks and how to manage them responsibly.</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>
<p><br /></p>
<p>They are hosting an event in Zurich on January 9th with the ARChitects, join if you can. </p>
<p><br /></p>
<p>Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br /></p>
<p>Interviewer: Tim Scarfe</p>
<p><br /></p>
<p>Yoshua Bengio:</p>
<p>https://x.com/Yoshua_Bengio</p>
<p>https://scholar.google.com/citations?user=kukA0LcAAAAJ&amp;hl=en</p>
<p>https://yoshuabengio.org/</p>
<p>https://en.wikipedia.org/wiki/Yoshua_Bengio</p>
<p><br /></p>
<p>TOC:</p>
<p>1. AI Safety Fundamentals</p>
<p>[00:00:00] 1.1 AI Safety Risks and International Cooperation</p>
<p>[00:03:20] 1.2 Fundamental Principles vs Scaling in AI Development</p>
<p>[00:11:25] 1.3 System 1/2 Thinking and AI Reasoning Capabilities</p>
<p>[00:15:15] 1.4 Reward Tampering and AI Agency Risks</p>
<p>[00:25:17] 1.5 Alignment Challenges and Instrumental Convergence</p>
<p><br /></p>
<p>2. AI Architecture and Safety Design</p>
<p>[00:33:10] 2.1 Instrumental Goals and AI Safety Fundamentals</p>
<p>[00:35:02] 2.2 Separating Intelligence from Goals in AI Systems</p>
<p>[00:40:40] 2.3 Non-Agent AI as Scientific Tools</p>
<p>[00:44:25] 2.4 Oracle AI Systems and Mathematical Safety Frameworks</p>
<p><br /></p>
<p>3. Global Governance and Security</p>
<p>[00:49:50] 3.1 International AI Competition and Hardware Governance</p>
<p>[00:51:58] 3.2 Military and Security Implications of AI Development</p>
<p>[00:56:07] 3.3 Personal Evolution of AI Safety Perspectives</p>
<p>[01:00:25] 3.4 AI Development Scaling and Global Governance Challenges</p>
<p>[01:12:10] 3.5 AI Regulation and Corporate Oversight</p>
<p><br /></p>
<p>4. Technical Innovations</p>
<p>[01:23:00] 4.1 Evolution of Neural Architectures: From RNNs to Transformers</p>
<p>[01:26:02] 4.2 GFlowNets and Symbolic Computation</p>
<p>[01:30:47] 4.3 Neural Dynamics and Consciousness</p>
<p>[01:34:38] 4.4 AI Creativity and Scientific Discovery</p>
<p><br /></p>
<p>SHOWNOTES (Transcript, references, best clips etc):</p>
<p>https://www.dropbox.com/scl/fi/ajucigli8n90fbxv9h94x/BENGIO_SHOW.pdf?rlkey=38hi2m19sylnr8orb76b85wkw&amp;dl=0</p>
<p><br /></p>
<p>CORE REFS (full list in shownotes and pinned comment):</p>
<p>[00:00:15] Bengio et al.: "AI Risk" Statement </p>
<p>https://www.safe.ai/work/statement-on-ai-risk </p>
<p><br /></p>
<p>[00:23:10] Bengio on reward tampering &amp; AI safety (Harvard Data Science Review)  </p>
<p>https://hdsr.mitpress.mit.edu/pub/w974bwb0  </p>
<p><br /></p>
<p>[00:40:45] Munk Debate on AI existential risk, featuring Bengio  </p>
<p>https://munkdebates.com/debates/artificial-intelligence  </p>
<p><br /></p>
<p>[00:44:30] "Can a Bayesian Oracle Prevent Harm from an Agent?" (Bengio et al.) on oracle-to-agent safety  </p>
<p>https://arxiv.org/abs/2408.05284  </p>
<p><br /></p>
<p>[00:51:20] Bengio (2024) memo on hardware-based AI governance verification  </p>
<p>https://yoshuabengio.org/wp-content/uploads/2024/08/FlexHEG-Memo_August-2024.pdf  </p>
<p><br /></p>
<p>[01:12:55] Bengio’s involvement in EU AI Act code of practice  </p>
<p>https://digital-strategy.ec.europa.eu/en/news/meet-chairs-leading-development-first-general-purpose-ai-code-practice  </p>
<p><br /></p>
<p>[01:27:05] Complexity-based compositionality theory (Elmoznino, Jiralerspong, Bengio, Lajoie)  </p>
<p>https://arxiv.org/abs/2410.14817  </p>
<p><br /></p>
<p>[01:29:00] GFlowNet Foundations (Bengio et al.) for probabilistic inference  </p>
<p>https://arxiv.org/pdf/2111.09266  </p>
<p><br /></p>
<p>[01:32:10] Discrete attractor states in neural systems (Nam, Elmoznino, Bengio, Lajoie)  </p>
<p>https://arxiv.org/pdf/2302.06403</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Yoshua-Bengio---Designing-out-Agency-for-Safe-AI-e2thu9p</link>
			<guid isPermaLink="false">e17d0bc4-f4dd-474e-9193-8b92e10ac899</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 15 Jan 2025 19:21:54 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/97105657/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-15%2Fec5f2ba3-9244-22fc-7c18-43ca121cc9e3.mp3" length="147253659" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Yoshua Bengio is a pioneer in deep learning and Turing Award winner. Bengio talks about AI safety, why goal-seeking “agentic” AIs might be dangerous, and his vision for building powerful AI tools without giving them agency. Topics include reward tampering risks, instrumental convergence, global AI governance, and how non-agent AIs could revolutionize science and medicine while reducing existential threats. Perfect for anyone curious about advanced AI risks and how to manage them responsibly.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;They are hosting an event in Zurich on January 9th with the ARChitects, join if you can. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Yoshua Bengio:&lt;/p&gt;
&lt;p&gt;https://x.com/Yoshua_Bengio&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=kukA0LcAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;https://yoshuabengio.org/&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Yoshua_Bengio&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. AI Safety Fundamentals&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 AI Safety Risks and International Cooperation&lt;/p&gt;
&lt;p&gt;[00:03:20] 1.2 Fundamental Principles vs Scaling in AI Development&lt;/p&gt;
&lt;p&gt;[00:11:25] 1.3 System 1/2 Thinking and AI Reasoning Capabilities&lt;/p&gt;
&lt;p&gt;[00:15:15] 1.4 Reward Tampering and AI Agency Risks&lt;/p&gt;
&lt;p&gt;[00:25:17] 1.5 Alignment Challenges and Instrumental Convergence&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. AI Architecture and Safety Design&lt;/p&gt;
&lt;p&gt;[00:33:10] 2.1 Instrumental Goals and AI Safety Fundamentals&lt;/p&gt;
&lt;p&gt;[00:35:02] 2.2 Separating Intelligence from Goals in AI Systems&lt;/p&gt;
&lt;p&gt;[00:40:40] 2.3 Non-Agent AI as Scientific Tools&lt;/p&gt;
&lt;p&gt;[00:44:25] 2.4 Oracle AI Systems and Mathematical Safety Frameworks&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Global Governance and Security&lt;/p&gt;
&lt;p&gt;[00:49:50] 3.1 International AI Competition and Hardware Governance&lt;/p&gt;
&lt;p&gt;[00:51:58] 3.2 Military and Security Implications of AI Development&lt;/p&gt;
&lt;p&gt;[00:56:07] 3.3 Personal Evolution of AI Safety Perspectives&lt;/p&gt;
&lt;p&gt;[01:00:25] 3.4 AI Development Scaling and Global Governance Challenges&lt;/p&gt;
&lt;p&gt;[01:12:10] 3.5 AI Regulation and Corporate Oversight&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Technical Innovations&lt;/p&gt;
&lt;p&gt;[01:23:00] 4.1 Evolution of Neural Architectures: From RNNs to Transformers&lt;/p&gt;
&lt;p&gt;[01:26:02] 4.2 GFlowNets and Symbolic Computation&lt;/p&gt;
&lt;p&gt;[01:30:47] 4.3 Neural Dynamics and Consciousness&lt;/p&gt;
&lt;p&gt;[01:34:38] 4.4 AI Creativity and Scientific Discovery&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SHOWNOTES (Transcript, references, best clips etc):&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/ajucigli8n90fbxv9h94x/BENGIO_SHOW.pdf?rlkey=38hi2m19sylnr8orb76b85wkw&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;CORE REFS (full list in shownotes and pinned comment):&lt;/p&gt;
&lt;p&gt;[00:00:15] Bengio et al.: &quot;AI Risk&quot; Statement &lt;/p&gt;
&lt;p&gt;https://www.safe.ai/work/statement-on-ai-risk &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[00:23:10] Bengio on reward tampering &amp;amp; AI safety (Harvard Data Science Review)  &lt;/p&gt;
&lt;p&gt;https://hdsr.mitpress.mit.edu/pub/w974bwb0  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[00:40:45] Munk Debate on AI existential risk, featuring Bengio  &lt;/p&gt;
&lt;p&gt;https://munkdebates.com/debates/artificial-intelligence  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[00:44:30] &quot;Can a Bayesian Oracle Prevent Harm from an Agent?&quot; (Bengio et al.) on oracle-to-agent safety  &lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2408.05284  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[00:51:20] Bengio (2024) memo on hardware-based AI governance verification  &lt;/p&gt;
&lt;p&gt;https://yoshuabengio.org/wp-content/uploads/2024/08/FlexHEG-Memo_August-2024.pdf  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[01:12:55] Bengio’s involvement in EU AI Act code of practice  &lt;/p&gt;
&lt;p&gt;https://digital-strategy.ec.europa.eu/en/news/meet-chairs-leading-development-first-general-purpose-ai-code-practice  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[01:27:05] Complexity-based compositionality theory (Elmoznino, Jiralerspong, Bengio, Lajoie)  &lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2410.14817  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[01:29:00] GFlowNet Foundations (Bengio et al.) for probabilistic inference  &lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2111.09266  &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[01:32:10] Discrete attractor states in neural systems (Nam, Elmoznino, Bengio, Lajoie)  &lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2302.06403&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:41:53</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1736968886331-327b2f88e647.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Francois Chollet - ARC reflections - NeurIPS 2024]]></title>
			<description><![CDATA[<p>François Chollet discusses the outcomes of the ARC-AGI (Abstraction and Reasoning Corpus) Prize competition in 2024, where accuracy rose from 33% to 55.5% on a private evaluation set.</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>
<p><br /></p>
<p>They are hosting an event in Zurich on January 9th with the ARChitects, join if you can. </p>
<p><br /></p>
<p>Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br /></p>
<p>Read about the recent result on o3 with ARC here (Chollet knew about it at the time of the interview but wasn't allowed to say):</p>
<p>https://arcprize.org/blog/oai-o3-pub-breakthrough</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Introduction and Opening</p>
<p>[00:00:00] 1.1 Deep Learning vs. Symbolic Reasoning: François’s Long-Standing Hybrid View</p>
<p>[00:00:48] 1.2 “Why Do They Call You a Symbolist?” – Addressing Misconceptions</p>
<p>[00:01:31] 1.3 Defining Reasoning </p>
<p><br /></p>
<p>3. ARC Competition 2024 Results and Evolution</p>
<p>[00:07:26] 3.1 ARC Prize 2024: Reflecting on the Narrative Shift Toward System 2</p>
<p>[00:10:29] 3.2 Comparing Private Leaderboard vs. Public Leaderboard Solutions</p>
<p>[00:13:17] 3.3 Two Winning Approaches: Deep Learning–Guided Program Synthesis and Test-Time Training</p>
<p><br /></p>
<p>4. Transduction vs. Induction in ARC</p>
<p>[00:16:04] 4.1 Test-Time Training, Overfitting Concerns, and Developer-Aware Generalization</p>
<p>[00:19:35] 4.2 Gradient Descent Adaptation vs. Discrete Program Search</p>
<p><br /></p>
<p>5. ARC-2 Development and Future Directions</p>
<p>[00:23:51] 5.1 Ensemble Methods, Benchmark Flaws, and the Need for ARC-2</p>
<p>[00:25:35] 5.2 Human-Level Performance Metrics and Private Test Sets</p>
<p>[00:29:44] 5.3 Task Diversity, Redundancy Issues, and Expanded Evaluation Methodology</p>
<p><br /></p>
<p>6. Program Synthesis Approaches</p>
<p>[00:30:18] 6.1 Induction vs. Transduction</p>
<p>[00:32:11] 6.2 Challenges of Writing Algorithms for Perceptual vs. Algorithmic Tasks</p>
<p>[00:34:23] 6.3 Combining Induction and Transduction</p>
<p>[00:37:05] 6.4 Multi-View Insight and Overfitting Regulation</p>
<p><br /></p>
<p>7. Latent Space and Graph-Based Synthesis</p>
<p>[00:38:17] 7.1 Clément Bonnet’s Latent Program Search Approach</p>
<p>[00:40:10] 7.2 Decoding to Symbolic Form and Local Discrete Search</p>
<p>[00:41:15] 7.3 Graph of Operators vs. Token-by-Token Code Generation</p>
<p>[00:45:50] 7.4 Iterative Program Graph Modifications and Reusable Functions</p>
<p><br /></p>
<p>8. Compute Efficiency and Lifelong Learning</p>
<p>[00:48:05] 8.1 Symbolic Process for Architecture Generation</p>
<p>[00:50:33] 8.2 Logarithmic Relationship of Compute and Accuracy</p>
<p>[00:52:20] 8.3 Learning New Building Blocks for Future Tasks</p>
<p><br /></p>
<p>9. AI Reasoning and Future Development</p>
<p>[00:53:15] 9.1 Consciousness as a Self-Consistency Mechanism in Iterative Reasoning</p>
<p>[00:56:30] 9.2 Reconciling Symbolic and Connectionist Views</p>
<p>[01:00:13] 9.3 System 2 Reasoning - Awareness and Consistency</p>
<p>[01:03:05] 9.4 Novel Problem Solving, Abstraction, and Reusability</p>
<p><br /></p>
<p>10. Program Synthesis and Research Lab</p>
<p>[01:05:53] 10.1 François Leaving Google to Focus on Program Synthesis</p>
<p>[01:09:55] 10.2 Democratizing Programming and Natural Language Instruction</p>
<p><br /></p>
<p>11. Frontier Models and O1 Architecture</p>
<p>[01:14:38] 11.1 Search-Based Chain of Thought vs. Standard Forward Pass</p>
<p>[01:16:55] 11.2 o1’s Natural Language Program Generation and Test-Time Compute Scaling</p>
<p>[01:19:35] 11.3 Logarithmic Gains with Deeper Search</p>
<p><br /></p>
<p>12. ARC Evaluation and Human Intelligence</p>
<p>[01:22:55] 12.1 LLMs as Guessing Machines and Agent Reliability Issues</p>
<p>[01:25:02] 12.2 ARC-2 Human Testing and Correlation with g-Factor</p>
<p>[01:26:16] 12.3 Closing Remarks and Future Directions</p>
<p><br /></p>
<p>SHOWNOTES PDF:</p>
<p>https://www.dropbox.com/scl/fi/ujaai0ewpdnsosc5mc30k/CholletNeurips.pdf?rlkey=s68dp432vefpj2z0dp5wmzqz6&amp;st=hazphyx5&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Francois-Chollet---ARC-reflections---NeurIPS-2024-e2t8g92</link>
			<guid isPermaLink="false">74384305-489e-400e-b7aa-c6bbee7bc96e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 09 Jan 2025 02:49:59 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/96796386/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-9%2F4ca3ad40-917d-3330-2912-75d11359b605.mp3" length="125498787" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;François Chollet discusses the outcomes of the ARC-AGI (Abstraction and Reasoning Corpus) Prize competition in 2024, where accuracy rose from 33% to 55.5% on a private evaluation set.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on o-series style reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;They are hosting an event in Zurich on January 9th with the ARChitects, join if you can. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Read about the recent result on o3 with ARC here (Chollet knew about it at the time of the interview but wasn&apos;t allowed to say):&lt;/p&gt;
&lt;p&gt;https://arcprize.org/blog/oai-o3-pub-breakthrough&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Introduction and Opening&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Deep Learning vs. Symbolic Reasoning: François’s Long-Standing Hybrid View&lt;/p&gt;
&lt;p&gt;[00:00:48] 1.2 “Why Do They Call You a Symbolist?” – Addressing Misconceptions&lt;/p&gt;
&lt;p&gt;[00:01:31] 1.3 Defining Reasoning &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. ARC Competition 2024 Results and Evolution&lt;/p&gt;
&lt;p&gt;[00:07:26] 3.1 ARC Prize 2024: Reflecting on the Narrative Shift Toward System 2&lt;/p&gt;
&lt;p&gt;[00:10:29] 3.2 Comparing Private Leaderboard vs. Public Leaderboard Solutions&lt;/p&gt;
&lt;p&gt;[00:13:17] 3.3 Two Winning Approaches: Deep Learning–Guided Program Synthesis and Test-Time Training&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Transduction vs. Induction in ARC&lt;/p&gt;
&lt;p&gt;[00:16:04] 4.1 Test-Time Training, Overfitting Concerns, and Developer-Aware Generalization&lt;/p&gt;
&lt;p&gt;[00:19:35] 4.2 Gradient Descent Adaptation vs. Discrete Program Search&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. ARC-2 Development and Future Directions&lt;/p&gt;
&lt;p&gt;[00:23:51] 5.1 Ensemble Methods, Benchmark Flaws, and the Need for ARC-2&lt;/p&gt;
&lt;p&gt;[00:25:35] 5.2 Human-Level Performance Metrics and Private Test Sets&lt;/p&gt;
&lt;p&gt;[00:29:44] 5.3 Task Diversity, Redundancy Issues, and Expanded Evaluation Methodology&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;6. Program Synthesis Approaches&lt;/p&gt;
&lt;p&gt;[00:30:18] 6.1 Induction vs. Transduction&lt;/p&gt;
&lt;p&gt;[00:32:11] 6.2 Challenges of Writing Algorithms for Perceptual vs. Algorithmic Tasks&lt;/p&gt;
&lt;p&gt;[00:34:23] 6.3 Combining Induction and Transduction&lt;/p&gt;
&lt;p&gt;[00:37:05] 6.4 Multi-View Insight and Overfitting Regulation&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;7. Latent Space and Graph-Based Synthesis&lt;/p&gt;
&lt;p&gt;[00:38:17] 7.1 Clément Bonnet’s Latent Program Search Approach&lt;/p&gt;
&lt;p&gt;[00:40:10] 7.2 Decoding to Symbolic Form and Local Discrete Search&lt;/p&gt;
&lt;p&gt;[00:41:15] 7.3 Graph of Operators vs. Token-by-Token Code Generation&lt;/p&gt;
&lt;p&gt;[00:45:50] 7.4 Iterative Program Graph Modifications and Reusable Functions&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;8. Compute Efficiency and Lifelong Learning&lt;/p&gt;
&lt;p&gt;[00:48:05] 8.1 Symbolic Process for Architecture Generation&lt;/p&gt;
&lt;p&gt;[00:50:33] 8.2 Logarithmic Relationship of Compute and Accuracy&lt;/p&gt;
&lt;p&gt;[00:52:20] 8.3 Learning New Building Blocks for Future Tasks&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;9. AI Reasoning and Future Development&lt;/p&gt;
&lt;p&gt;[00:53:15] 9.1 Consciousness as a Self-Consistency Mechanism in Iterative Reasoning&lt;/p&gt;
&lt;p&gt;[00:56:30] 9.2 Reconciling Symbolic and Connectionist Views&lt;/p&gt;
&lt;p&gt;[01:00:13] 9.3 System 2 Reasoning - Awareness and Consistency&lt;/p&gt;
&lt;p&gt;[01:03:05] 9.4 Novel Problem Solving, Abstraction, and Reusability&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;10. Program Synthesis and Research Lab&lt;/p&gt;
&lt;p&gt;[01:05:53] 10.1 François Leaving Google to Focus on Program Synthesis&lt;/p&gt;
&lt;p&gt;[01:09:55] 10.2 Democratizing Programming and Natural Language Instruction&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;11. Frontier Models and O1 Architecture&lt;/p&gt;
&lt;p&gt;[01:14:38] 11.1 Search-Based Chain of Thought vs. Standard Forward Pass&lt;/p&gt;
&lt;p&gt;[01:16:55] 11.2 o1’s Natural Language Program Generation and Test-Time Compute Scaling&lt;/p&gt;
&lt;p&gt;[01:19:35] 11.3 Logarithmic Gains with Deeper Search&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;12. ARC Evaluation and Human Intelligence&lt;/p&gt;
&lt;p&gt;[01:22:55] 12.1 LLMs as Guessing Machines and Agent Reliability Issues&lt;/p&gt;
&lt;p&gt;[01:25:02] 12.2 ARC-2 Human Testing and Correlation with g-Factor&lt;/p&gt;
&lt;p&gt;[01:26:16] 12.3 Closing Remarks and Future Directions&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SHOWNOTES PDF:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/ujaai0ewpdnsosc5mc30k/CholletNeurips.pdf?rlkey=s68dp432vefpj2z0dp5wmzqz6&amp;amp;st=hazphyx5&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:26:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1736390975411-87fc558baa08.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jeff Clune - Agent AI Needs Darwin]]></title>
			<description><![CDATA[<p>AI professor Jeff Clune ruminates on open-ended evolutionary algorithms—systems designed to generate novel and interesting outcomes forever. Drawing inspiration from nature’s boundless creativity, Clune and his collaborators aim to build “Darwin Complete” search spaces, where any computable environment can be simulated. By harnessing the power of large language models and reinforcement learning, these AI agents continuously develop new skills, explore uncharted domains, and even cooperate with one another in complex tasks.</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? </p>
<p><br /></p>
<p>They are hosting an event in Zurich on January 9th with the ARChitects, join if you can. </p>
<p><br /></p>
<p>Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br /></p>
<p>A central theme throughout Clune’s work is “interestingness”: an elusive quality that nudges AI agents toward genuinely original discoveries. Rather than rely on narrowly defined metrics—which often fail due to Goodhart’s Law—Clune employs language models to serve as proxies for human judgment. In doing so, he ensures that “interesting” always reflects authentic novelty, opening the door to unending innovation. </p>
<p><br /></p>
<p>Yet with these extraordinary possibilities come equally significant risks. Clune says we need AI safety measures—particularly as the technology matures into powerful, open-ended forms. Potential pitfalls include agents inadvertently causing harm or malicious actors subverting AI’s capabilities for destructive ends. To mitigate this, Clune advocates for prudent governance involving democratic coalitions, regulation of cutting-edge models, and global alignment protocols. </p>
<p><br /></p>
<p>Jeff Clune: </p>
<p>https://x.com/jeffclune</p>
<p>http://jeffclune.com/</p>
<p><br /></p>
<p>(Interviewer: Tim Scarfe)</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Introduction</p>
<p>[00:00:00] 1.1 Overview and Opening Thoughts</p>
<p><br /></p>
<p>2. Sponsorship</p>
<p>[00:03:00] 2.1 TufaAI Labs and CentML</p>
<p><br /></p>
<p>3. Evolutionary AI Foundations</p>
<p>[00:04:12] 3.1 Open-Ended Algorithm Development and Abstraction Approaches</p>
<p>[00:07:56] 3.2 Novel Intelligence Forms and Serendipitous Discovery</p>
<p>[00:11:46] 3.3 Frontier Models and the 'Interestingness' Problem</p>
<p>[00:30:36] 3.4 Darwin Complete Systems and Evolutionary Search Spaces</p>
<p><br /></p>
<p>4. System Architecture and Learning</p>
<p>[00:37:35] 4.1 Code Generation vs Neural Networks Comparison</p>
<p>[00:41:04] 4.2 Thought Cloning and Behavioral Learning Systems</p>
<p>[00:47:00] 4.3 Language Emergence in AI Systems</p>
<p>[00:50:23] 4.4 AI Interpretability and Safety Monitoring Techniques</p>
<p><br /></p>
<p>5. AI Safety and Governance</p>
<p>[00:53:56] 5.1 Language Model Consistency and Belief Systems</p>
<p>[00:57:00] 5.2 AI Safety Challenges and Alignment Limitations</p>
<p>[01:02:07] 5.3 Open Source AI Development and Value Alignment</p>
<p>[01:08:19] 5.4 Global AI Governance and Development Control</p>
<p><br /></p>
<p>6. Advanced AI Systems and Evolution</p>
<p>[01:16:55] 6.1 Agent Systems and Performance Evaluation</p>
<p>[01:22:45] 6.2 Continuous Learning Challenges and In-Context Solutions</p>
<p>[01:26:46] 6.3 Evolution Algorithms and Environment Generation</p>
<p>[01:35:36] 6.4 Evolutionary Biology Insights and Experiments</p>
<p>[01:48:08] 6.5 Personal Journey from Philosophy to AI Research</p>
<p><br /></p>
<p>Shownotes:</p>
<p>We craft detailed show notes for each episode with high quality transcript and references and best parts bolded.</p>
<p><br /></p>
<p>https://www.dropbox.com/scl/fi/fz43pdoc5wq5jh7vsnujl/JEFFCLUNE.pdf?rlkey=uu0e70ix9zo6g5xn6amykffpm&amp;st=k2scxteu&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jeff-Clune---Agent-AI-Needs-Darwin-e2t1pjm</link>
			<guid isPermaLink="false">9c71174c-868d-42c1-a059-65ee49666a44</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 04 Jan 2025 02:43:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/96576566/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2025-0-4%2F38a26c33-f494-d09e-9ca1-77ea7ba2cba6.mp3" length="173614052" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;AI professor Jeff Clune ruminates on open-ended evolutionary algorithms—systems designed to generate novel and interesting outcomes forever. Drawing inspiration from nature’s boundless creativity, Clune and his collaborators aim to build “Darwin Complete” search spaces, where any computable environment can be simulated. By harnessing the power of large language models and reinforcement learning, these AI agents continuously develop new skills, explore uncharted domains, and even cooperate with one another in complex tasks.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on reasoning and AGI. Are you interested in working on reasoning, or getting involved in their events? &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;They are hosting an event in Zurich on January 9th with the ARChitects, join if you can. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;A central theme throughout Clune’s work is “interestingness”: an elusive quality that nudges AI agents toward genuinely original discoveries. Rather than rely on narrowly defined metrics—which often fail due to Goodhart’s Law—Clune employs language models to serve as proxies for human judgment. In doing so, he ensures that “interesting” always reflects authentic novelty, opening the door to unending innovation. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Yet with these extraordinary possibilities come equally significant risks. Clune says we need AI safety measures—particularly as the technology matures into powerful, open-ended forms. Potential pitfalls include agents inadvertently causing harm or malicious actors subverting AI’s capabilities for destructive ends. To mitigate this, Clune advocates for prudent governance involving democratic coalitions, regulation of cutting-edge models, and global alignment protocols. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Jeff Clune: &lt;/p&gt;
&lt;p&gt;https://x.com/jeffclune&lt;/p&gt;
&lt;p&gt;http://jeffclune.com/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;(Interviewer: Tim Scarfe)&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Introduction&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Overview and Opening Thoughts&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Sponsorship&lt;/p&gt;
&lt;p&gt;[00:03:00] 2.1 TufaAI Labs and CentML&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Evolutionary AI Foundations&lt;/p&gt;
&lt;p&gt;[00:04:12] 3.1 Open-Ended Algorithm Development and Abstraction Approaches&lt;/p&gt;
&lt;p&gt;[00:07:56] 3.2 Novel Intelligence Forms and Serendipitous Discovery&lt;/p&gt;
&lt;p&gt;[00:11:46] 3.3 Frontier Models and the &apos;Interestingness&apos; Problem&lt;/p&gt;
&lt;p&gt;[00:30:36] 3.4 Darwin Complete Systems and Evolutionary Search Spaces&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. System Architecture and Learning&lt;/p&gt;
&lt;p&gt;[00:37:35] 4.1 Code Generation vs Neural Networks Comparison&lt;/p&gt;
&lt;p&gt;[00:41:04] 4.2 Thought Cloning and Behavioral Learning Systems&lt;/p&gt;
&lt;p&gt;[00:47:00] 4.3 Language Emergence in AI Systems&lt;/p&gt;
&lt;p&gt;[00:50:23] 4.4 AI Interpretability and Safety Monitoring Techniques&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. AI Safety and Governance&lt;/p&gt;
&lt;p&gt;[00:53:56] 5.1 Language Model Consistency and Belief Systems&lt;/p&gt;
&lt;p&gt;[00:57:00] 5.2 AI Safety Challenges and Alignment Limitations&lt;/p&gt;
&lt;p&gt;[01:02:07] 5.3 Open Source AI Development and Value Alignment&lt;/p&gt;
&lt;p&gt;[01:08:19] 5.4 Global AI Governance and Development Control&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;6. Advanced AI Systems and Evolution&lt;/p&gt;
&lt;p&gt;[01:16:55] 6.1 Agent Systems and Performance Evaluation&lt;/p&gt;
&lt;p&gt;[01:22:45] 6.2 Continuous Learning Challenges and In-Context Solutions&lt;/p&gt;
&lt;p&gt;[01:26:46] 6.3 Evolution Algorithms and Environment Generation&lt;/p&gt;
&lt;p&gt;[01:35:36] 6.4 Evolutionary Biology Insights and Experiments&lt;/p&gt;
&lt;p&gt;[01:48:08] 6.5 Personal Journey from Philosophy to AI Research&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes:&lt;/p&gt;
&lt;p&gt;We craft detailed show notes for each episode with high quality transcript and references and best parts bolded.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/fz43pdoc5wq5jh7vsnujl/JEFFCLUNE.pdf?rlkey=uu0e70ix9zo6g5xn6amykffpm&amp;amp;st=k2scxteu&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:00:13</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1735958561346-e64a49a5640d3.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Neel Nanda - Mechanistic Interpretability (Sparse Autoencoders)]]></title>
			<description><![CDATA[<p>Neel Nanda, a senior research scientist at Google DeepMind, leads their mechanistic interpretability team. In this extensive interview, he discusses his work trying to understand how neural networks function internally. At just 25 years old, Nanda has quickly become a prominent voice in AI research after completing his pure mathematics degree at Cambridge in 2020.</p>
<p><br /></p>
<p>Nanda reckons that machine learning is unique because we create neural networks that can perform impressive tasks (like complex reasoning and software engineering) without understanding how they work internally. He compares this to having computer programs that can do things no human programmer knows how to write. His work focuses on "mechanistic interpretability" - attempting to uncover and understand the internal structures and algorithms that emerge within these networks.</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>***</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/</p>
<p>***</p>
<p><br /></p>
<p>SHOWNOTES, TRANSCRIPT, ALL REFERENCES (DONT MISS!):</p>
<p>https://www.dropbox.com/scl/fi/36dvtfl3v3p56hbi30im7/NeelShow.pdf?rlkey=pq8t7lyv2z60knlifyy17jdtx&amp;st=kiutudhc&amp;dl=0</p>
<p><br /></p>
<p>We riff on:</p>
<p>* How neural networks develop meaningful internal representations beyond simple pattern matching</p>
<p>* The effectiveness of chain-of-thought prompting and why it improves model performance</p>
<p>* The importance of hands-on coding over extensive paper reading for new researchers</p>
<p>* His journey from Cambridge to working with Chris Olah at Anthropic and eventually Google DeepMind</p>
<p>* The role of mechanistic interpretability in AI safety</p>
<p><br /></p>
<p>NEEL NANDA:</p>
<p>https://www.neelnanda.io/</p>
<p>https://scholar.google.com/citations?user=GLnX3MkAAAAJ&amp;hl=en</p>
<p>https://x.com/NeelNanda5</p>
<p><br /></p>
<p>Interviewer - Tim Scarfe</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Part 1: Introduction</p>
<p>[00:00:00] 1.1 Introduction and Core Concepts Overview</p>
<p><br /></p>
<p>2. Part 2: Outside Interview</p>
<p>[00:06:45] 2.1 Mechanistic Interpretability Foundations</p>
<p><br /></p>
<p>3. Part 3: Main Interview</p>
<p>[00:32:52] 3.1 Mechanistic Interpretability</p>
<p><br /></p>
<p>4. Neural Architecture and Circuits</p>
<p>[01:00:31] 4.1 Biological Evolution Parallels</p>
<p>[01:04:03] 4.2 Universal Circuit Patterns and Induction Heads</p>
<p>[01:11:07] 4.3 Entity Detection and Knowledge Boundaries</p>
<p>[01:14:26] 4.4 Mechanistic Interpretability and Activation Patching</p>
<p><br /></p>
<p>5. Model Behavior Analysis</p>
<p>[01:30:00] 5.1 Golden Gate Claude Experiment and Feature Amplification</p>
<p>[01:33:27] 5.2 Model Personas and RLHF Behavior Modification</p>
<p>[01:36:28] 5.3 Steering Vectors and Linear Representations</p>
<p>[01:40:00] 5.4 Hallucinations and Model Uncertainty</p>
<p><br /></p>
<p>6. Sparse Autoencoder Architecture</p>
<p>[01:44:54] 6.1 Architecture and Mathematical Foundations</p>
<p>[02:22:03] 6.2 Core Challenges and Solutions</p>
<p>[02:32:04] 6.3 Advanced Activation Functions and Top-k Implementations</p>
<p>[02:34:41] 6.4 Research Applications in Transformer Circuit Analysis</p>
<p><br /></p>
<p>7. Feature Learning and Scaling</p>
<p>[02:48:02] 7.1 Autoencoder Feature Learning and Width Parameters</p>
<p>[03:02:46] 7.2 Scaling Laws and Training Stability</p>
<p>[03:11:00] 7.3 Feature Identification and Bias Correction</p>
<p>[03:19:52] 7.4 Training Dynamics Analysis Methods</p>
<p><br /></p>
<p>8. Engineering Implementation</p>
<p>[03:23:48] 8.1 Scale and Infrastructure Requirements</p>
<p>[03:25:20] 8.2 Computational Requirements and Storage</p>
<p>[03:35:22] 8.3 Chain-of-Thought Reasoning Implementation</p>
<p>[03:37:15] 8.4 Latent Structure Inference in Language Models</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Neel-Nanda---Mechanistic-Interpretability-Sparse-Autoencoders-e2s186i</link>
			<guid isPermaLink="false">0f86c9e2-9e94-4c36-9bb9-072e28332c51</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 07 Dec 2024 21:14:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/95510162/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-11-7%2Fc6f11920-f06a-6c65-f767-1b957d252a38.mp3" length="320950907" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Neel Nanda, a senior research scientist at Google DeepMind, leads their mechanistic interpretability team. In this extensive interview, he discusses his work trying to understand how neural networks function internally. At just 25 years old, Nanda has quickly become a prominent voice in AI research after completing his pure mathematics degree at Cambridge in 2020.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Nanda reckons that machine learning is unique because we create neural networks that can perform impressive tasks (like complex reasoning and software engineering) without understanding how they work internally. He compares this to having computer programs that can do things no human programmer knows how to write. His work focuses on &quot;mechanistic interpretability&quot; - attempting to uncover and understand the internal structures and algorithms that emerge within these networks.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SHOWNOTES, TRANSCRIPT, ALL REFERENCES (DONT MISS!):&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/36dvtfl3v3p56hbi30im7/NeelShow.pdf?rlkey=pq8t7lyv2z60knlifyy17jdtx&amp;amp;st=kiutudhc&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;We riff on:&lt;/p&gt;
&lt;p&gt;* How neural networks develop meaningful internal representations beyond simple pattern matching&lt;/p&gt;
&lt;p&gt;* The effectiveness of chain-of-thought prompting and why it improves model performance&lt;/p&gt;
&lt;p&gt;* The importance of hands-on coding over extensive paper reading for new researchers&lt;/p&gt;
&lt;p&gt;* His journey from Cambridge to working with Chris Olah at Anthropic and eventually Google DeepMind&lt;/p&gt;
&lt;p&gt;* The role of mechanistic interpretability in AI safety&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;NEEL NANDA:&lt;/p&gt;
&lt;p&gt;https://www.neelnanda.io/&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=GLnX3MkAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;https://x.com/NeelNanda5&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Interviewer - Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Part 1: Introduction&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Introduction and Core Concepts Overview&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Part 2: Outside Interview&lt;/p&gt;
&lt;p&gt;[00:06:45] 2.1 Mechanistic Interpretability Foundations&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Part 3: Main Interview&lt;/p&gt;
&lt;p&gt;[00:32:52] 3.1 Mechanistic Interpretability&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Neural Architecture and Circuits&lt;/p&gt;
&lt;p&gt;[01:00:31] 4.1 Biological Evolution Parallels&lt;/p&gt;
&lt;p&gt;[01:04:03] 4.2 Universal Circuit Patterns and Induction Heads&lt;/p&gt;
&lt;p&gt;[01:11:07] 4.3 Entity Detection and Knowledge Boundaries&lt;/p&gt;
&lt;p&gt;[01:14:26] 4.4 Mechanistic Interpretability and Activation Patching&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. Model Behavior Analysis&lt;/p&gt;
&lt;p&gt;[01:30:00] 5.1 Golden Gate Claude Experiment and Feature Amplification&lt;/p&gt;
&lt;p&gt;[01:33:27] 5.2 Model Personas and RLHF Behavior Modification&lt;/p&gt;
&lt;p&gt;[01:36:28] 5.3 Steering Vectors and Linear Representations&lt;/p&gt;
&lt;p&gt;[01:40:00] 5.4 Hallucinations and Model Uncertainty&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;6. Sparse Autoencoder Architecture&lt;/p&gt;
&lt;p&gt;[01:44:54] 6.1 Architecture and Mathematical Foundations&lt;/p&gt;
&lt;p&gt;[02:22:03] 6.2 Core Challenges and Solutions&lt;/p&gt;
&lt;p&gt;[02:32:04] 6.3 Advanced Activation Functions and Top-k Implementations&lt;/p&gt;
&lt;p&gt;[02:34:41] 6.4 Research Applications in Transformer Circuit Analysis&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;7. Feature Learning and Scaling&lt;/p&gt;
&lt;p&gt;[02:48:02] 7.1 Autoencoder Feature Learning and Width Parameters&lt;/p&gt;
&lt;p&gt;[03:02:46] 7.2 Scaling Laws and Training Stability&lt;/p&gt;
&lt;p&gt;[03:11:00] 7.3 Feature Identification and Bias Correction&lt;/p&gt;
&lt;p&gt;[03:19:52] 7.4 Training Dynamics Analysis Methods&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;8. Engineering Implementation&lt;/p&gt;
&lt;p&gt;[03:23:48] 8.1 Scale and Infrastructure Requirements&lt;/p&gt;
&lt;p&gt;[03:25:20] 8.2 Computational Requirements and Storage&lt;/p&gt;
&lt;p&gt;[03:35:22] 8.3 Chain-of-Thought Reasoning Implementation&lt;/p&gt;
&lt;p&gt;[03:37:15] 8.4 Latent Structure Inference in Language Models&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>03:42:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1733606039000-2b398ea8efb61.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jonas Hübotter (ETH) - Test Time Inference]]></title>
			<description><![CDATA[<p>Jonas Hübotter, PhD student at ETH Zurich's Institute for Machine Learning, discusses his groundbreaking research on test-time computation and local learning. He demonstrates how smaller models can outperform larger ones by 30x through strategic test-time computation and introduces a novel paradigm combining inductive and transductive learning approaches.</p>
<p><br /></p>
<p>Using Bayesian linear regression as a surrogate model for uncertainty estimation, Jonas explains how models can efficiently adapt to specific tasks without massive pre-training. He draws an analogy to Google Earth's variable resolution system to illustrate dynamic resource allocation based on task complexity.</p>
<p><br /></p>
<p>The conversation explores the future of AI architecture, envisioning systems that continuously learn and adapt beyond current monolithic models. Jonas concludes by proposing hybrid deployment strategies combining local and cloud computation, suggesting a future where compute resources are allocated based on task complexity rather than fixed model size.</p>
<p><br /></p>
<p>This research represents a significant shift in machine learning, prioritizing intelligent resource allocation and adaptive learning over traditional scaling approaches.</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/</p>
<p><br /></p>
<p>Transcription, references and show notes PDF download:</p>
<p>https://www.dropbox.com/scl/fi/cxg80p388snwt6qbp4m52/JonasFinal.pdf?rlkey=glk9mhpzjvesanlc14rtpvk4r&amp;st=6qwi8n3x&amp;dl=0</p>
<p><br /></p>
<p>Jonas Hübotter</p>
<p>https://jonhue.github.io/</p>
<p>https://scholar.google.com/citations?user=pxi_RkwAAAAJ</p>
<p><br /></p>
<p>Transductive Active Learning: Theory and Applications (NeurIPS 2024)</p>
<p>https://arxiv.org/pdf/2402.15898</p>
<p><br /></p>
<p>EFFICIENTLY LEARNING AT TEST-TIME: ACTIVE FINE-TUNING OF LLMS (SIFT)</p>
<p>https://arxiv.org/pdf/2410.08020</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Test-Time Computation Fundamentals</p>
<p>[00:00:00] Intro</p>
<p>[00:03:10] 1.1 Test-Time Computation and Model Performance Comparison</p>
<p>[00:05:52] 1.2 Retrieval Augmentation and Machine Teaching Strategies</p>
<p>[00:09:40] 1.3 In-Context Learning vs Fine-Tuning Trade-offs</p>
<p><br /></p>
<p>2. System Architecture and Intelligence</p>
<p>[00:15:58] 2.1 System Architecture and Intelligence Emergence</p>
<p>[00:23:22] 2.2 Active Inference and Constrained Agency in AI</p>
<p>[00:29:52] 2.3 Evolution of Local Learning Methods</p>
<p>[00:32:05] 2.4 Vapnik's Contributions to Transductive Learning</p>
<p><br /></p>
<p>3. Resource Optimization and Local Learning</p>
<p>[00:34:35] 3.1 Computational Resource Allocation in ML Models</p>
<p>[00:35:30] 3.2 Historical Context and Traditional ML Optimization</p>
<p>[00:37:55] 3.3 Variable Resolution Processing and Active Inference in ML</p>
<p>[00:43:01] 3.4 Local Learning and Base Model Capacity Trade-offs</p>
<p>[00:48:04] 3.5 Active Learning vs Local Learning Approaches</p>
<p><br /></p>
<p>4. Information Retrieval and Model Interpretability</p>
<p>[00:51:08] 4.1 Information Retrieval and Nearest Neighbor Limitations</p>
<p>[01:03:07] 4.2 Model Interpretability and Surrogate Models</p>
<p>[01:15:03] 4.3 Bayesian Uncertainty Estimation and Surrogate Models</p>
<p><br /></p>
<p>5. Distributed Systems and Deployment</p>
<p>[01:23:56] 5.1 Memory Architecture and Controller Systems</p>
<p>[01:28:14] 5.2 Evolution from Static to Distributed Learning Systems</p>
<p>[01:38:03] 5.3 Transductive Learning and Model Specialization</p>
<p>[01:41:58] 5.4 Hybrid Local-Cloud Deployment Strategies</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jonas-Hbotter-ETH---Test-Time-Inference-e2rnle4</link>
			<guid isPermaLink="false">68e9b1ef-5394-4333-9063-266fc097f13e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 01 Dec 2024 12:25:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/95196036/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-11-1%2Fc5b25fea-927b-5deb-5e11-619380dee886.mp3" length="153123931" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Jonas Hübotter, PhD student at ETH Zurich&apos;s Institute for Machine Learning, discusses his groundbreaking research on test-time computation and local learning. He demonstrates how smaller models can outperform larger ones by 30x through strategic test-time computation and introduces a novel paradigm combining inductive and transductive learning approaches.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Using Bayesian linear regression as a surrogate model for uncertainty estimation, Jonas explains how models can efficiently adapt to specific tasks without massive pre-training. He draws an analogy to Google Earth&apos;s variable resolution system to illustrate dynamic resource allocation based on task complexity.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The conversation explores the future of AI architecture, envisioning systems that continuously learn and adapt beyond current monolithic models. Jonas concludes by proposing hybrid deployment strategies combining local and cloud computation, suggesting a future where compute resources are allocated based on task complexity rather than fixed model size.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;This research represents a significant shift in machine learning, prioritizing intelligent resource allocation and adaptive learning over traditional scaling approaches.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Transcription, references and show notes PDF download:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/cxg80p388snwt6qbp4m52/JonasFinal.pdf?rlkey=glk9mhpzjvesanlc14rtpvk4r&amp;amp;st=6qwi8n3x&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Jonas Hübotter&lt;/p&gt;
&lt;p&gt;https://jonhue.github.io/&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=pxi_RkwAAAAJ&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Transductive Active Learning: Theory and Applications (NeurIPS 2024)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2402.15898&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;EFFICIENTLY LEARNING AT TEST-TIME: ACTIVE FINE-TUNING OF LLMS (SIFT)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2410.08020&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Test-Time Computation Fundamentals&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro&lt;/p&gt;
&lt;p&gt;[00:03:10] 1.1 Test-Time Computation and Model Performance Comparison&lt;/p&gt;
&lt;p&gt;[00:05:52] 1.2 Retrieval Augmentation and Machine Teaching Strategies&lt;/p&gt;
&lt;p&gt;[00:09:40] 1.3 In-Context Learning vs Fine-Tuning Trade-offs&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. System Architecture and Intelligence&lt;/p&gt;
&lt;p&gt;[00:15:58] 2.1 System Architecture and Intelligence Emergence&lt;/p&gt;
&lt;p&gt;[00:23:22] 2.2 Active Inference and Constrained Agency in AI&lt;/p&gt;
&lt;p&gt;[00:29:52] 2.3 Evolution of Local Learning Methods&lt;/p&gt;
&lt;p&gt;[00:32:05] 2.4 Vapnik&apos;s Contributions to Transductive Learning&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Resource Optimization and Local Learning&lt;/p&gt;
&lt;p&gt;[00:34:35] 3.1 Computational Resource Allocation in ML Models&lt;/p&gt;
&lt;p&gt;[00:35:30] 3.2 Historical Context and Traditional ML Optimization&lt;/p&gt;
&lt;p&gt;[00:37:55] 3.3 Variable Resolution Processing and Active Inference in ML&lt;/p&gt;
&lt;p&gt;[00:43:01] 3.4 Local Learning and Base Model Capacity Trade-offs&lt;/p&gt;
&lt;p&gt;[00:48:04] 3.5 Active Learning vs Local Learning Approaches&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Information Retrieval and Model Interpretability&lt;/p&gt;
&lt;p&gt;[00:51:08] 4.1 Information Retrieval and Nearest Neighbor Limitations&lt;/p&gt;
&lt;p&gt;[01:03:07] 4.2 Model Interpretability and Surrogate Models&lt;/p&gt;
&lt;p&gt;[01:15:03] 4.3 Bayesian Uncertainty Estimation and Surrogate Models&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. Distributed Systems and Deployment&lt;/p&gt;
&lt;p&gt;[01:23:56] 5.1 Memory Architecture and Controller Systems&lt;/p&gt;
&lt;p&gt;[01:28:14] 5.2 Evolution from Static to Distributed Learning Systems&lt;/p&gt;
&lt;p&gt;[01:38:03] 5.3 Transductive Learning and Model Specialization&lt;/p&gt;
&lt;p&gt;[01:41:58] 5.4 Hybrid Local-Cloud Deployment Strategies&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:45:56</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1733055872680-6badfa855d9d4.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[How AI Could Be A Mathematician's Co-Pilot by 2026 (Prof. Swarat Chaudhuri)]]></title>
			<description><![CDATA[<p>Professor Swarat Chaudhuri from the University of Texas at Austin and visiting researcher at Google DeepMind discusses breakthroughs in AI reasoning, theorem proving, and mathematical discovery. Chaudhuri explains his groundbreaking work on COPRA (a GPT-based prover agent), shares insights on neurosymbolic approaches to AI.</p>
<p><br /></p>
<p>Professor Swarat Chaudhuri:</p>
<p>https://www.cs.utexas.edu/~swarat/</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/</p>
<p><br /></p>
<p>TOC:</p>
<p>[00:00:00] 0. Introduction / CentML ad, Tufa ad</p>
<p><br /></p>
<p>1. AI Reasoning: From Language Models to Neurosymbolic Approaches</p>
<p>[00:02:27] 1.1 Defining Reasoning in AI</p>
<p>[00:09:51] 1.2 Limitations of Current Language Models</p>
<p>[00:17:22] 1.3 Neuro-symbolic Approaches and Program Synthesis</p>
<p>[00:24:59] 1.4 COPRA and In-Context Learning for Theorem Proving</p>
<p>[00:34:39] 1.5 Symbolic Regression and LLM-Guided Abstraction</p>
<p><br /></p>
<p>2. AI in Mathematics: Theorem Proving and Concept Discovery</p>
<p>[00:43:37] 2.1 AI-Assisted Theorem Proving and Proof Verification</p>
<p>[01:01:37] 2.2 Symbolic Regression and Concept Discovery in Mathematics</p>
<p>[01:11:57] 2.3 Scaling and Modularizing Mathematical Proofs</p>
<p>[01:21:53] 2.4 COPRA: In-Context Learning for Formal Theorem-Proving</p>
<p>[01:28:22] 2.5 AI-driven theorem proving and mathematical discovery</p>
<p><br /></p>
<p>3. Formal Methods and Challenges in AI Mathematics</p>
<p>[01:30:42] 3.1 Formal proofs, empirical predicates, and uncertainty in AI mathematics</p>
<p>[01:34:01] 3.2 Characteristics of good theoretical computer science research</p>
<p>[01:39:16] 3.3 LLMs in theorem generation and proving</p>
<p>[01:42:21] 3.4 Addressing contamination and concept learning in AI systems</p>
<p><br /></p>
<p>REFS:</p>
<p>00:04:58 The Chinese Room Argument, https://plato.stanford.edu/entries/chinese-room/</p>
<p>00:11:42 Software 2.0, https://medium.com/@karpathy/software-2-0-a64152b37c35</p>
<p>00:11:57 Solving Olympiad Geometry Without Human Demonstrations, https://www.nature.com/articles/s41586-023-06747-5</p>
<p>00:13:26 Lean, https://lean-lang.org/</p>
<p>00:15:43 A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play, https://www.science.org/doi/10.1126/science.aar6404</p>
<p>00:19:24 DreamCoder (Ellis et al., PLDI 2021), https://arxiv.org/abs/2006.08381</p>
<p>00:24:37 The Lambda Calculus, https://plato.stanford.edu/entries/lambda-calculus/</p>
<p>00:26:43 Neural Sketch Learning for Conditional Program Generation, https://arxiv.org/pdf/1703.05698</p>
<p>00:28:08 Learning Differentiable Programs With Admissible Neural Heuristics, https://arxiv.org/abs/2007.12101</p>
<p>00:31:03 Symbolic Regression With a Learned Concept Library (Grayeli et al., NeurIPS 2024), https://arxiv.org/abs/2409.09359</p>
<p>00:41:30 Formal Verification of Parallel Programs, https://dl.acm.org/doi/10.1145/360248.360251</p>
<p>01:00:37 Training Compute-Optimal Large Language Models, https://arxiv.org/abs/2203.15556</p>
<p>01:18:19 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, https://arxiv.org/abs/2201.11903</p>
<p>01:18:42 Draft, Sketch, and Prove: Guiding Formal Theorem Provers With Informal Proofs, https://arxiv.org/abs/2210.12283</p>
<p>01:19:49 Learning Formal Mathematics From Intrinsic Motivation, https://arxiv.org/pdf/2407.00695</p>
<p>01:20:19 An In-Context Learning Agent for Formal Theorem-Proving (Thakur et al., CoLM 2024), https://arxiv.org/pdf/2310.04353</p>
<p>01:23:58 Learning to Prove Theorems via Interacting With Proof Assistants, https://arxiv.org/abs/1905.09381</p>
<p>01:39:58 An In-Context Learning Agent for Formal Theorem-Proving (Thakur et al., CoLM 2024), https://arxiv.org/pdf/2310.04353</p>
<p>01:42:24 Programmatically Interpretable Reinforcement Learning (Verma et al., ICML 2018), https://arxiv.org/abs/1804.02477</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/How-AI-Could-Be-A-Mathematicians-Co-Pilot-by-2026-Prof--Swarat-Chaudhuri-e2rf6es</link>
			<guid isPermaLink="false">299a7909-e926-4bda-95dc-1f4c972c1c1e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 25 Nov 2024 08:01:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/94918556/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-25%2F82dd404f-8fb5-b449-b806-05fafc5c01fb.mp3" length="151119311" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Swarat Chaudhuri from the University of Texas at Austin and visiting researcher at Google DeepMind discusses breakthroughs in AI reasoning, theorem proving, and mathematical discovery. Chaudhuri explains his groundbreaking work on COPRA (a GPT-based prover agent), shares insights on neurosymbolic approaches to AI.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Professor Swarat Chaudhuri:&lt;/p&gt;
&lt;p&gt;https://www.cs.utexas.edu/~swarat/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] 0. Introduction / CentML ad, Tufa ad&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;1. AI Reasoning: From Language Models to Neurosymbolic Approaches&lt;/p&gt;
&lt;p&gt;[00:02:27] 1.1 Defining Reasoning in AI&lt;/p&gt;
&lt;p&gt;[00:09:51] 1.2 Limitations of Current Language Models&lt;/p&gt;
&lt;p&gt;[00:17:22] 1.3 Neuro-symbolic Approaches and Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:24:59] 1.4 COPRA and In-Context Learning for Theorem Proving&lt;/p&gt;
&lt;p&gt;[00:34:39] 1.5 Symbolic Regression and LLM-Guided Abstraction&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. AI in Mathematics: Theorem Proving and Concept Discovery&lt;/p&gt;
&lt;p&gt;[00:43:37] 2.1 AI-Assisted Theorem Proving and Proof Verification&lt;/p&gt;
&lt;p&gt;[01:01:37] 2.2 Symbolic Regression and Concept Discovery in Mathematics&lt;/p&gt;
&lt;p&gt;[01:11:57] 2.3 Scaling and Modularizing Mathematical Proofs&lt;/p&gt;
&lt;p&gt;[01:21:53] 2.4 COPRA: In-Context Learning for Formal Theorem-Proving&lt;/p&gt;
&lt;p&gt;[01:28:22] 2.5 AI-driven theorem proving and mathematical discovery&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Formal Methods and Challenges in AI Mathematics&lt;/p&gt;
&lt;p&gt;[01:30:42] 3.1 Formal proofs, empirical predicates, and uncertainty in AI mathematics&lt;/p&gt;
&lt;p&gt;[01:34:01] 3.2 Characteristics of good theoretical computer science research&lt;/p&gt;
&lt;p&gt;[01:39:16] 3.3 LLMs in theorem generation and proving&lt;/p&gt;
&lt;p&gt;[01:42:21] 3.4 Addressing contamination and concept learning in AI systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;00:04:58 The Chinese Room Argument, https://plato.stanford.edu/entries/chinese-room/&lt;/p&gt;
&lt;p&gt;00:11:42 Software 2.0, https://medium.com/@karpathy/software-2-0-a64152b37c35&lt;/p&gt;
&lt;p&gt;00:11:57 Solving Olympiad Geometry Without Human Demonstrations, https://www.nature.com/articles/s41586-023-06747-5&lt;/p&gt;
&lt;p&gt;00:13:26 Lean, https://lean-lang.org/&lt;/p&gt;
&lt;p&gt;00:15:43 A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play, https://www.science.org/doi/10.1126/science.aar6404&lt;/p&gt;
&lt;p&gt;00:19:24 DreamCoder (Ellis et al., PLDI 2021), https://arxiv.org/abs/2006.08381&lt;/p&gt;
&lt;p&gt;00:24:37 The Lambda Calculus, https://plato.stanford.edu/entries/lambda-calculus/&lt;/p&gt;
&lt;p&gt;00:26:43 Neural Sketch Learning for Conditional Program Generation, https://arxiv.org/pdf/1703.05698&lt;/p&gt;
&lt;p&gt;00:28:08 Learning Differentiable Programs With Admissible Neural Heuristics, https://arxiv.org/abs/2007.12101&lt;/p&gt;
&lt;p&gt;00:31:03 Symbolic Regression With a Learned Concept Library (Grayeli et al., NeurIPS 2024), https://arxiv.org/abs/2409.09359&lt;/p&gt;
&lt;p&gt;00:41:30 Formal Verification of Parallel Programs, https://dl.acm.org/doi/10.1145/360248.360251&lt;/p&gt;
&lt;p&gt;01:00:37 Training Compute-Optimal Large Language Models, https://arxiv.org/abs/2203.15556&lt;/p&gt;
&lt;p&gt;01:18:19 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, https://arxiv.org/abs/2201.11903&lt;/p&gt;
&lt;p&gt;01:18:42 Draft, Sketch, and Prove: Guiding Formal Theorem Provers With Informal Proofs, https://arxiv.org/abs/2210.12283&lt;/p&gt;
&lt;p&gt;01:19:49 Learning Formal Mathematics From Intrinsic Motivation, https://arxiv.org/pdf/2407.00695&lt;/p&gt;
&lt;p&gt;01:20:19 An In-Context Learning Agent for Formal Theorem-Proving (Thakur et al., CoLM 2024), https://arxiv.org/pdf/2310.04353&lt;/p&gt;
&lt;p&gt;01:23:58 Learning to Prove Theorems via Interacting With Proof Assistants, https://arxiv.org/abs/1905.09381&lt;/p&gt;
&lt;p&gt;01:39:58 An In-Context Learning Agent for Formal Theorem-Proving (Thakur et al., CoLM 2024), https://arxiv.org/pdf/2310.04353&lt;/p&gt;
&lt;p&gt;01:42:24 Programmatically Interpretable Reinforcement Learning (Verma et al., ICML 2018), https://arxiv.org/abs/1804.02477&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:44:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1732521414254-a0c1989c3db1c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Nora Belrose - AI Development, Safety, and Meaning ]]></title>
			<description><![CDATA[<p>Nora Belrose, Head of Interpretability Research at EleutherAI, discusses critical challenges in AI safety and development. The conversation begins with her technical work on concept erasure in neural networks through LEACE (LEAst-squares Concept Erasure), while highlighting how neural networks' progression from simple to complex learning patterns could have important implications for AI safety.</p>
<p><br /></p>
<p>Many fear that advanced AI will pose an existential threat -- pursuing its own dangerous goals once it's powerful enough. But Belrose challenges this popular doomsday scenario with a fascinating breakdown of why it doesn't add up.</p>
<p><br /></p>
<p>Belrose also provides a detailed critique of current AI alignment approaches, particularly examining "counting arguments" and their limitations when applied to AI safety. She argues that the Principle of Indifference may be insufficient for addressing existential risks from advanced AI systems. The discussion explores how emergent properties in complex AI systems could lead to unpredictable and potentially dangerous behaviors that simple reductionist approaches fail to capture.</p>
<p><br /></p>
<p>The conversation concludes by exploring broader philosophical territory, where Belrose discusses her growing interest in Buddhism's potential relevance to a post-automation future. She connects concepts of moral anti-realism with Buddhist ideas about emptiness and non-attachment, suggesting these frameworks might help humans find meaning in a world where AI handles most practical tasks. Rather than viewing this automated future with alarm, she proposes that Zen Buddhism's emphasis on spontaneity and presence might complement a society freed from traditional labor.</p>
<p><br /></p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/</p>
<p><br /></p>
<p>Nora Belrose:</p>
<p>https://norabelrose.com/</p>
<p>https://scholar.google.com/citations?user=p_oBc64AAAAJ&amp;hl=en</p>
<p>https://x.com/norabelrose</p>
<p><br /></p>
<p>SHOWNOTES:</p>
<p>https://www.dropbox.com/scl/fi/38fhsv2zh8gnubtjaoq4a/NORA_FINAL.pdf?rlkey=0e5r8rd261821g1em4dgv0k70&amp;st=t5c9ckfb&amp;dl=0</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Neural Network Foundations</p>
<p>[00:00:00] 1.1 Philosophical Foundations and Neural Network Simplicity Bias</p>
<p>[00:02:20] 1.2 LEACE and Concept Erasure Fundamentals</p>
<p>[00:13:16] 1.3 LISA Technical Implementation and Applications</p>
<p>[00:18:50] 1.4 Practical Implementation Challenges and Data Requirements</p>
<p>[00:22:13] 1.5 Performance Impact and Limitations of Concept Erasure</p>
<p><br /></p>
<p>2. Machine Learning Theory</p>
<p>[00:32:23] 2.1 Neural Network Learning Progression and Simplicity Bias</p>
<p>[00:37:10] 2.2 Optimal Transport Theory and Image Statistics Manipulation</p>
<p>[00:43:05] 2.3 Grokking Phenomena and Training Dynamics</p>
<p>[00:44:50] 2.4 Texture vs Shape Bias in Computer Vision Models</p>
<p>[00:45:15] 2.5 CNN Architecture and Shape Recognition Limitations</p>
<p><br /></p>
<p>3. AI Systems and Value Learning</p>
<p>[00:47:10] 3.1 Meaning, Value, and Consciousness in AI Systems</p>
<p>[00:53:06] 3.2 Global Connectivity vs Local Culture Preservation</p>
<p>[00:58:18] 3.3 AI Capabilities and Future Development Trajectory</p>
<p><br /></p>
<p>4. Consciousness Theory</p>
<p>[01:03:03] 4.1 4E Cognition and Extended Mind Theory</p>
<p>[01:09:40] 4.2 Thompson's Views on Consciousness and Simulation</p>
<p>[01:12:46] 4.3 Phenomenology and Consciousness Theory</p>
<p>[01:15:43] 4.4 Critique of Illusionism and Embodied Experience</p>
<p>[01:23:16] 4.5 AI Alignment and Counting Arguments Debate</p>
<p><br /></p>
<p>(TRUNCATED, TOC embedded in MP3 file with more information)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Nora-Belrose---AI-Development--Safety--and-Meaning-e2r495k</link>
			<guid isPermaLink="false">0d8c91d2-80cf-491f-9231-bdaf3e3d1ff6</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 17 Nov 2024 21:35:53 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/94560884/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-17%2F32c2c614-a959-6bd8-19d8-8809895158fa.mp3" length="216211082" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Nora Belrose, Head of Interpretability Research at EleutherAI, discusses critical challenges in AI safety and development. The conversation begins with her technical work on concept erasure in neural networks through LEACE (LEAst-squares Concept Erasure), while highlighting how neural networks&apos; progression from simple to complex learning patterns could have important implications for AI safety.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Many fear that advanced AI will pose an existential threat -- pursuing its own dangerous goals once it&apos;s powerful enough. But Belrose challenges this popular doomsday scenario with a fascinating breakdown of why it doesn&apos;t add up.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Belrose also provides a detailed critique of current AI alignment approaches, particularly examining &quot;counting arguments&quot; and their limitations when applied to AI safety. She argues that the Principle of Indifference may be insufficient for addressing existential risks from advanced AI systems. The discussion explores how emergent properties in complex AI systems could lead to unpredictable and potentially dangerous behaviors that simple reductionist approaches fail to capture.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The conversation concludes by exploring broader philosophical territory, where Belrose discusses her growing interest in Buddhism&apos;s potential relevance to a post-automation future. She connects concepts of moral anti-realism with Buddhist ideas about emptiness and non-attachment, suggesting these frameworks might help humans find meaning in a world where AI handles most practical tasks. Rather than viewing this automated future with alarm, she proposes that Zen Buddhism&apos;s emphasis on spontaneity and presence might complement a society freed from traditional labor.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Tufa AI Labs is a brand new research lab in Zurich started by Benjamin Crouzier focussed on ARC and AGI, they just acquired MindsAI - the current winners of the ARC challenge. Are you interested in working on ARC, or getting involved in their events? Goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Nora Belrose:&lt;/p&gt;
&lt;p&gt;https://norabelrose.com/&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=p_oBc64AAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;https://x.com/norabelrose&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SHOWNOTES:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/38fhsv2zh8gnubtjaoq4a/NORA_FINAL.pdf?rlkey=0e5r8rd261821g1em4dgv0k70&amp;amp;st=t5c9ckfb&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Neural Network Foundations&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Philosophical Foundations and Neural Network Simplicity Bias&lt;/p&gt;
&lt;p&gt;[00:02:20] 1.2 LEACE and Concept Erasure Fundamentals&lt;/p&gt;
&lt;p&gt;[00:13:16] 1.3 LISA Technical Implementation and Applications&lt;/p&gt;
&lt;p&gt;[00:18:50] 1.4 Practical Implementation Challenges and Data Requirements&lt;/p&gt;
&lt;p&gt;[00:22:13] 1.5 Performance Impact and Limitations of Concept Erasure&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Machine Learning Theory&lt;/p&gt;
&lt;p&gt;[00:32:23] 2.1 Neural Network Learning Progression and Simplicity Bias&lt;/p&gt;
&lt;p&gt;[00:37:10] 2.2 Optimal Transport Theory and Image Statistics Manipulation&lt;/p&gt;
&lt;p&gt;[00:43:05] 2.3 Grokking Phenomena and Training Dynamics&lt;/p&gt;
&lt;p&gt;[00:44:50] 2.4 Texture vs Shape Bias in Computer Vision Models&lt;/p&gt;
&lt;p&gt;[00:45:15] 2.5 CNN Architecture and Shape Recognition Limitations&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. AI Systems and Value Learning&lt;/p&gt;
&lt;p&gt;[00:47:10] 3.1 Meaning, Value, and Consciousness in AI Systems&lt;/p&gt;
&lt;p&gt;[00:53:06] 3.2 Global Connectivity vs Local Culture Preservation&lt;/p&gt;
&lt;p&gt;[00:58:18] 3.3 AI Capabilities and Future Development Trajectory&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Consciousness Theory&lt;/p&gt;
&lt;p&gt;[01:03:03] 4.1 4E Cognition and Extended Mind Theory&lt;/p&gt;
&lt;p&gt;[01:09:40] 4.2 Thompson&apos;s Views on Consciousness and Simulation&lt;/p&gt;
&lt;p&gt;[01:12:46] 4.3 Phenomenology and Consciousness Theory&lt;/p&gt;
&lt;p&gt;[01:15:43] 4.4 Critique of Illusionism and Embodied Experience&lt;/p&gt;
&lt;p&gt;[01:23:16] 4.5 AI Alignment and Counting Arguments Debate&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;(TRUNCATED, TOC embedded in MP3 file with more information)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:29:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1731879297298-2c9b6ae4fc45c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Why Your GPUs are underutilised for AI - CentML CEO Explains]]></title>
			<description><![CDATA[<p>Prof. Gennady Pekhimenko (CEO of CentML, UofT) joins us in this *sponsored episode* to dive deep into AI system optimization and enterprise implementation. From NVIDIA's technical leadership model to the rise of open-source AI, Pekhimenko shares insights on bridging the gap between academic research and industrial applications. Learn about "dark silicon," GPU utilization challenges in ML workloads, and how modern enterprises can optimize their AI infrastructure. The conversation explores why some companies achieve only 10% GPU efficiency and practical solutions for improving AI system performance. A must-watch for anyone interested in the technical foundations of enterprise AI and hardware optimization.</p>
<p><br /></p>
<p>CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Cheaper, faster, no commitments, pay as you go, scale massively, simple to setup. Check it out! </p>
<p>https://centml.ai/pricing/</p>
<p><br /></p>
<p>SPONSOR MESSAGES:</p>
<p>MLST is also sponsored by Tufa AI Labs - https://tufalabs.ai/</p>
<p>They are hiring cracked ML engineers/researchers to work on ARC and build AGI! </p>
<p><br /></p>
<p>SHOWNOTES (diarised transcript, TOC, references, summary, best quotes etc)</p>
<p>https://www.dropbox.com/scl/fi/w9kbpso7fawtm286kkp6j/Gennady.pdf?rlkey=aqjqmncx3kjnatk2il1gbgknk&amp;st=2a9mccj8&amp;dl=0</p>
<p><br /></p>
<p>TOC:</p>
<p>1. AI Strategy and Leadership</p>
<p>[00:00:00] 1.1 Technical Leadership and Corporate Structure</p>
<p>[00:09:55] 1.2 Open Source vs Proprietary AI Models</p>
<p>[00:16:04] 1.3 Hardware and System Architecture Challenges</p>
<p>[00:23:37] 1.4 Enterprise AI Implementation and Optimization</p>
<p>[00:35:30] 1.5 AI Reasoning Capabilities and Limitations</p>
<p><br /></p>
<p>2. AI System Development</p>
<p>[00:38:45] 2.1 Computational and Cognitive Limitations of AI Systems</p>
<p>[00:42:40] 2.2 Human-LLM Communication Adaptation and Patterns</p>
<p>[00:46:18] 2.3 AI-Assisted Software Development Challenges</p>
<p>[00:47:55] 2.4 Future of Software Engineering Careers in AI Era</p>
<p>[00:49:49] 2.5 Enterprise AI Adoption Challenges and Implementation</p>
<p><br /></p>
<p>3. ML Infrastructure Optimization</p>
<p>[00:54:41] 3.1 MLOps Evolution and Platform Centralization</p>
<p>[00:55:43] 3.2 Hardware Optimization and Performance Constraints</p>
<p>[01:05:24] 3.3 ML Compiler Optimization and Python Performance</p>
<p>[01:15:57] 3.4 Enterprise ML Deployment and Cloud Provider Partnerships</p>
<p><br /></p>
<p>4. Distributed AI Architecture</p>
<p>[01:27:05] 4.1 Multi-Cloud ML Infrastructure and Optimization</p>
<p>[01:29:45] 4.2 AI Agent Systems and Production Readiness</p>
<p>[01:32:00] 4.3 RAG Implementation and Fine-Tuning Considerations</p>
<p>[01:33:45] 4.4 Distributed AI Systems Architecture and Ray Framework</p>
<p><br /></p>
<p>5. AI Industry Standards and Research</p>
<p>[01:37:55] 5.1 Origins and Evolution of MLPerf Benchmarking</p>
<p>[01:43:15] 5.2 MLPerf Methodology and Industry Impact</p>
<p>[01:50:17] 5.3 Academic Research vs Industry Implementation in AI</p>
<p>[01:58:59] 5.4 AI Research History and Safety Concerns</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Why-Your-GPUs-are-underutilised-for-AI---CentML-CEO-Explains-e2qu3ta</link>
			<guid isPermaLink="false">67701f9c-fb9c-4351-853a-cecd1ccbb266</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 13 Nov 2024 15:05:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/94358890/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-13%2F91737282-52b3-b9fd-c84a-d926a6025edc.mp3" length="185774720" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof. Gennady Pekhimenko (CEO of CentML, UofT) joins us in this *sponsored episode* to dive deep into AI system optimization and enterprise implementation. From NVIDIA&apos;s technical leadership model to the rise of open-source AI, Pekhimenko shares insights on bridging the gap between academic research and industrial applications. Learn about &quot;dark silicon,&quot; GPU utilization challenges in ML workloads, and how modern enterprises can optimize their AI infrastructure. The conversation explores why some companies achieve only 10% GPU efficiency and practical solutions for improving AI system performance. A must-watch for anyone interested in the technical foundations of enterprise AI and hardware optimization.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;CentML offers competitive pricing for GenAI model deployment, with flexible options to suit a wide range of models, from small to large-scale deployments. Cheaper, faster, no commitments, pay as you go, scale massively, simple to setup. Check it out! &lt;/p&gt;
&lt;p&gt;https://centml.ai/pricing/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGES:&lt;/p&gt;
&lt;p&gt;MLST is also sponsored by Tufa AI Labs - https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;They are hiring cracked ML engineers/researchers to work on ARC and build AGI! &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SHOWNOTES (diarised transcript, TOC, references, summary, best quotes etc)&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/w9kbpso7fawtm286kkp6j/Gennady.pdf?rlkey=aqjqmncx3kjnatk2il1gbgknk&amp;amp;st=2a9mccj8&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. AI Strategy and Leadership&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Technical Leadership and Corporate Structure&lt;/p&gt;
&lt;p&gt;[00:09:55] 1.2 Open Source vs Proprietary AI Models&lt;/p&gt;
&lt;p&gt;[00:16:04] 1.3 Hardware and System Architecture Challenges&lt;/p&gt;
&lt;p&gt;[00:23:37] 1.4 Enterprise AI Implementation and Optimization&lt;/p&gt;
&lt;p&gt;[00:35:30] 1.5 AI Reasoning Capabilities and Limitations&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. AI System Development&lt;/p&gt;
&lt;p&gt;[00:38:45] 2.1 Computational and Cognitive Limitations of AI Systems&lt;/p&gt;
&lt;p&gt;[00:42:40] 2.2 Human-LLM Communication Adaptation and Patterns&lt;/p&gt;
&lt;p&gt;[00:46:18] 2.3 AI-Assisted Software Development Challenges&lt;/p&gt;
&lt;p&gt;[00:47:55] 2.4 Future of Software Engineering Careers in AI Era&lt;/p&gt;
&lt;p&gt;[00:49:49] 2.5 Enterprise AI Adoption Challenges and Implementation&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. ML Infrastructure Optimization&lt;/p&gt;
&lt;p&gt;[00:54:41] 3.1 MLOps Evolution and Platform Centralization&lt;/p&gt;
&lt;p&gt;[00:55:43] 3.2 Hardware Optimization and Performance Constraints&lt;/p&gt;
&lt;p&gt;[01:05:24] 3.3 ML Compiler Optimization and Python Performance&lt;/p&gt;
&lt;p&gt;[01:15:57] 3.4 Enterprise ML Deployment and Cloud Provider Partnerships&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Distributed AI Architecture&lt;/p&gt;
&lt;p&gt;[01:27:05] 4.1 Multi-Cloud ML Infrastructure and Optimization&lt;/p&gt;
&lt;p&gt;[01:29:45] 4.2 AI Agent Systems and Production Readiness&lt;/p&gt;
&lt;p&gt;[01:32:00] 4.3 RAG Implementation and Fine-Tuning Considerations&lt;/p&gt;
&lt;p&gt;[01:33:45] 4.4 Distributed AI Systems Architecture and Ray Framework&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. AI Industry Standards and Research&lt;/p&gt;
&lt;p&gt;[01:37:55] 5.1 Origins and Evolution of MLPerf Benchmarking&lt;/p&gt;
&lt;p&gt;[01:43:15] 5.2 MLPerf Methodology and Industry Impact&lt;/p&gt;
&lt;p&gt;[01:50:17] 5.3 Academic Research vs Industry Implementation in AI&lt;/p&gt;
&lt;p&gt;[01:58:59] 5.4 AI Research History and Safety Concerns&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:08:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1731510200809-7005279e4a6d9.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Eliezer Yudkowsky and Stephen Wolfram on AI X-risk]]></title>
			<description><![CDATA[<p>Eliezer Yudkowsky and Stephen Wolfram discuss artificial intelligence and its potential existen‑</p>
<p>tial risks. They traversed fundamental questions about AI safety, consciousness, computational irreducibility, and the nature of intelligence.</p>
<p><br /></p>
<p>The discourse centered on Yudkowsky’s argument that advanced AI systems pose an existential threat to humanity, primarily due to the challenge of alignment and the potential for emergent goals that diverge from human values. Wolfram, while acknowledging potential risks, approached the topic from a his signature measured perspective, emphasizing the importance of understanding computational systems’ fundamental nature and questioning whether AI systems would necessarily develop the kind of goal‑directed behavior Yudkowsky fears.</p>
<p><br /></p>
<p>***</p>
<p>MLST IS SPONSORED BY TUFA AI LABS! </p>
<p>The current winners of the ARC challenge, MindsAI are part of Tufa AI Labs. They are hiring ML engineers. Are you interested?! Please goto https://tufalabs.ai/</p>
<p>***</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Foundational AI Concepts and Risks</p>
<p>[00:00:01] 1.1 AI Optimization and System Capabilities Debate</p>
<p>[00:06:46] 1.2 Computational Irreducibility and Intelligence Limitations</p>
<p>[00:20:09] 1.3 Existential Risk and Species Succession</p>
<p>[00:23:28] 1.4 Consciousness and Value Preservation in AI Systems</p>
<p><br /></p>
<p>2. Ethics and Philosophy in AI</p>
<p>[00:33:24] 2.1 Moral Value of Human Consciousness vs. Computation</p>
<p>[00:36:30] 2.2 Ethics and Moral Philosophy Debate</p>
<p>[00:39:58] 2.3 Existential Risks and Digital Immortality</p>
<p>[00:43:30] 2.4 Consciousness and Personal Identity in Brain Emulation</p>
<p><br /></p>
<p>3. Truth and Logic in AI Systems</p>
<p>[00:54:39] 3.1 AI Persuasion Ethics and Truth</p>
<p>[01:01:48] 3.2 Mathematical Truth and Logic in AI Systems</p>
<p>[01:11:29] 3.3 Universal Truth vs Personal Interpretation in Ethics and Mathematics</p>
<p>[01:14:43] 3.4 Quantum Mechanics and Fundamental Reality Debate</p>
<p><br /></p>
<p>4. AI Capabilities and Constraints</p>
<p>[01:21:21] 4.1 AI Perception and Physical Laws</p>
<p>[01:28:33] 4.2 AI Capabilities and Computational Constraints</p>
<p>[01:34:59] 4.3 AI Motivation and Anthropomorphization Debate</p>
<p>[01:38:09] 4.4 Prediction vs Agency in AI Systems</p>
<p><br /></p>
<p>5. AI System Architecture and Behavior</p>
<p>[01:44:47] 5.1 Computational Irreducibility and Probabilistic Prediction</p>
<p>[01:48:10] 5.2 Teleological vs Mechanistic Explanations of AI Behavior</p>
<p>[02:09:41] 5.3 Machine Learning as Assembly of Computational Components</p>
<p>[02:29:52] 5.4 AI Safety and Predictability in Complex Systems</p>
<p><br /></p>
<p>6. Goal Optimization and Alignment</p>
<p>[02:50:30] 6.1 Goal Specification and Optimization Challenges in AI Systems</p>
<p>[02:58:31] 6.2 Intelligence, Computation, and Goal-Directed Behavior</p>
<p>[03:02:18] 6.3 Optimization Goals and Human Existential Risk</p>
<p>[03:08:49] 6.4 Emergent Goals and AI Alignment Challenges</p>
<p><br /></p>
<p>7. AI Evolution and Risk Assessment</p>
<p>[03:19:44] 7.1 Inner Optimization and Mesa-Optimization Theory</p>
<p>[03:34:00] 7.2 Dynamic AI Goals and Extinction Risk Debate</p>
<p>[03:56:05] 7.3 AI Risk and Biological System Analogies</p>
<p>[04:09:37] 7.4 Expert Risk Assessments and Optimism vs Reality</p>
<p><br /></p>
<p>8. Future Implications and Economics</p>
<p>[04:13:01] 8.1 Economic and Proliferation Considerations</p>
<p><br /></p>
<p>SHOWNOTES (transcription, references, summary, best quotes etc):</p>
<p>https://www.dropbox.com/scl/fi/3st8dts2ba7yob161dchd/EliezerWolfram.pdf?rlkey=b6va5j8upgqwl9s2muc924vtt&amp;st=vemwqx7a&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Eliezer-Yudkowsky-and-Stephen-Wolfram-on-AI-X-risk-e2qrfue</link>
			<guid isPermaLink="false">2d640f09-ff05-4f62-b282-9dd4e45019a8</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 11 Nov 2024 19:07:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/94272910/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-11%2F6c16ee13-1ba9-84cc-557d-8894501c5f5e.mp3" length="372874776" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Eliezer Yudkowsky and Stephen Wolfram discuss artificial intelligence and its potential existen‑&lt;/p&gt;
&lt;p&gt;tial risks. They traversed fundamental questions about AI safety, consciousness, computational irreducibility, and the nature of intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The discourse centered on Yudkowsky’s argument that advanced AI systems pose an existential threat to humanity, primarily due to the challenge of alignment and the potential for emergent goals that diverge from human values. Wolfram, while acknowledging potential risks, approached the topic from a his signature measured perspective, emphasizing the importance of understanding computational systems’ fundamental nature and questioning whether AI systems would necessarily develop the kind of goal‑directed behavior Yudkowsky fears.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;MLST IS SPONSORED BY TUFA AI LABS! &lt;/p&gt;
&lt;p&gt;The current winners of the ARC challenge, MindsAI are part of Tufa AI Labs. They are hiring ML engineers. Are you interested?! Please goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Foundational AI Concepts and Risks&lt;/p&gt;
&lt;p&gt;[00:00:01] 1.1 AI Optimization and System Capabilities Debate&lt;/p&gt;
&lt;p&gt;[00:06:46] 1.2 Computational Irreducibility and Intelligence Limitations&lt;/p&gt;
&lt;p&gt;[00:20:09] 1.3 Existential Risk and Species Succession&lt;/p&gt;
&lt;p&gt;[00:23:28] 1.4 Consciousness and Value Preservation in AI Systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Ethics and Philosophy in AI&lt;/p&gt;
&lt;p&gt;[00:33:24] 2.1 Moral Value of Human Consciousness vs. Computation&lt;/p&gt;
&lt;p&gt;[00:36:30] 2.2 Ethics and Moral Philosophy Debate&lt;/p&gt;
&lt;p&gt;[00:39:58] 2.3 Existential Risks and Digital Immortality&lt;/p&gt;
&lt;p&gt;[00:43:30] 2.4 Consciousness and Personal Identity in Brain Emulation&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Truth and Logic in AI Systems&lt;/p&gt;
&lt;p&gt;[00:54:39] 3.1 AI Persuasion Ethics and Truth&lt;/p&gt;
&lt;p&gt;[01:01:48] 3.2 Mathematical Truth and Logic in AI Systems&lt;/p&gt;
&lt;p&gt;[01:11:29] 3.3 Universal Truth vs Personal Interpretation in Ethics and Mathematics&lt;/p&gt;
&lt;p&gt;[01:14:43] 3.4 Quantum Mechanics and Fundamental Reality Debate&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. AI Capabilities and Constraints&lt;/p&gt;
&lt;p&gt;[01:21:21] 4.1 AI Perception and Physical Laws&lt;/p&gt;
&lt;p&gt;[01:28:33] 4.2 AI Capabilities and Computational Constraints&lt;/p&gt;
&lt;p&gt;[01:34:59] 4.3 AI Motivation and Anthropomorphization Debate&lt;/p&gt;
&lt;p&gt;[01:38:09] 4.4 Prediction vs Agency in AI Systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. AI System Architecture and Behavior&lt;/p&gt;
&lt;p&gt;[01:44:47] 5.1 Computational Irreducibility and Probabilistic Prediction&lt;/p&gt;
&lt;p&gt;[01:48:10] 5.2 Teleological vs Mechanistic Explanations of AI Behavior&lt;/p&gt;
&lt;p&gt;[02:09:41] 5.3 Machine Learning as Assembly of Computational Components&lt;/p&gt;
&lt;p&gt;[02:29:52] 5.4 AI Safety and Predictability in Complex Systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;6. Goal Optimization and Alignment&lt;/p&gt;
&lt;p&gt;[02:50:30] 6.1 Goal Specification and Optimization Challenges in AI Systems&lt;/p&gt;
&lt;p&gt;[02:58:31] 6.2 Intelligence, Computation, and Goal-Directed Behavior&lt;/p&gt;
&lt;p&gt;[03:02:18] 6.3 Optimization Goals and Human Existential Risk&lt;/p&gt;
&lt;p&gt;[03:08:49] 6.4 Emergent Goals and AI Alignment Challenges&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;7. AI Evolution and Risk Assessment&lt;/p&gt;
&lt;p&gt;[03:19:44] 7.1 Inner Optimization and Mesa-Optimization Theory&lt;/p&gt;
&lt;p&gt;[03:34:00] 7.2 Dynamic AI Goals and Extinction Risk Debate&lt;/p&gt;
&lt;p&gt;[03:56:05] 7.3 AI Risk and Biological System Analogies&lt;/p&gt;
&lt;p&gt;[04:09:37] 7.4 Expert Risk Assessments and Optimism vs Reality&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;8. Future Implications and Economics&lt;/p&gt;
&lt;p&gt;[04:13:01] 8.1 Economic and Proliferation Considerations&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SHOWNOTES (transcription, references, summary, best quotes etc):&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/3st8dts2ba7yob161dchd/EliezerWolfram.pdf?rlkey=b6va5j8upgqwl9s2muc924vtt&amp;amp;st=vemwqx7a&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>04:18:30</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1731351765309-03e6c8c0c767c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Pattern Recognition vs True Intelligence - Francois Chollet]]></title>
			<description><![CDATA[<p>Francois Chollet, a prominent AI expert and creator of ARC-AGI, discusses intelligence, consciousness, and artificial intelligence.</p>
<p><br /></p>
<p>Chollet explains that real intelligence isn't about memorizing information or having lots of knowledge - it's about being able to handle new situations effectively. This is why he believes current large language models (LLMs) have "near-zero intelligence" despite their impressive abilities. They're more like sophisticated memory and pattern-matching systems than truly intelligent beings.</p>
<p><br /></p>
<p>***</p>
<p>MLST IS SPONSORED BY TUFA AI LABS! </p>
<p>The current winners of the ARC challenge, MindsAI are part of Tufa AI Labs. They are hiring ML engineers. Are you interested?! Please goto https://tufalabs.ai/</p>
<p>***</p>
<p><br /></p>
<p>He introduced his "Kaleidoscope Hypothesis," which suggests that while the world seems infinitely complex, it's actually made up of simpler patterns that repeat and combine in different ways. True intelligence, he argues, involves identifying these basic patterns and using them to understand new situations.</p>
<p><br /></p>
<p>Chollet also talked about consciousness, suggesting it develops gradually in children rather than appearing all at once. He believes consciousness exists in degrees - animals have it to some extent, and even human consciousness varies with age and circumstances (like being more conscious when learning something new versus doing routine tasks).</p>
<p><br /></p>
<p>On AI safety, Chollet takes a notably different stance from many in Silicon Valley. He views AGI development as a scientific challenge rather than a religious quest, and doesn't share the apocalyptic concerns of some AI researchers. He argues that intelligence itself isn't dangerous - it's just a tool for turning information into useful models. What matters is how we choose to use it.</p>
<p><br /></p>
<p>ARC-AGI Prize:</p>
<p>https://arcprize.org/</p>
<p><br /></p>
<p>Francois Chollet:</p>
<p>https://x.com/fchollet</p>
<p><br /></p>
<p>Shownotes: </p>
<p>https://www.dropbox.com/scl/fi/j2068j3hlj8br96pfa7bi/CHOLLET_FINAL.pdf?rlkey=xkbr7tbnrjdl66m246w26uc8k&amp;st=0a4ec4na&amp;dl=0</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Intelligence and Model Building</p>
<p>[00:00:00] 1.1 Intelligence Definition and ARC Benchmark</p>
<p>[00:05:40] 1.2 LLMs as Program Memorization Systems</p>
<p>[00:09:36] 1.3 Kaleidoscope Hypothesis and Abstract Building Blocks</p>
<p>[00:13:39] 1.4 Deep Learning Limitations and System 2 Reasoning</p>
<p>[00:29:38] 1.5 Intelligence vs. Skill in LLMs and Model Building</p>
<p><br /></p>
<p>2. ARC Benchmark and Program Synthesis</p>
<p>[00:37:36] 2.1 Intelligence Definition and LLM Limitations</p>
<p>[00:41:33] 2.2 Meta-Learning System Architecture</p>
<p>[00:56:21] 2.3 Program Search and Occam's Razor</p>
<p>[00:59:42] 2.4 Developer-Aware Generalization</p>
<p>[01:06:49] 2.5 Task Generation and Benchmark Design</p>
<p><br /></p>
<p>3. Cognitive Systems and Program Generation</p>
<p>[01:14:38] 3.1 System 1/2 Thinking Fundamentals</p>
<p>[01:22:17] 3.2 Program Synthesis and Combinatorial Challenges</p>
<p>[01:31:18] 3.3 Test-Time Fine-Tuning Strategies</p>
<p>[01:36:10] 3.4 Evaluation and Leakage Problems</p>
<p>[01:43:22] 3.5 ARC Implementation Approaches</p>
<p><br /></p>
<p>4. Intelligence and Language Systems</p>
<p>[01:50:06] 4.1 Intelligence as Tool vs Agent</p>
<p>[01:53:53] 4.2 Cultural Knowledge Integration</p>
<p>[01:58:42] 4.3 Language and Abstraction Generation</p>
<p>[02:02:41] 4.4 Embodiment in Cognitive Systems</p>
<p>[02:09:02] 4.5 Language as Cognitive Operating System</p>
<p><br /></p>
<p>5. Consciousness and AI Safety</p>
<p>[02:14:05] 5.1 Consciousness and Intelligence Relationship</p>
<p>[02:20:25] 5.2 Development of Machine Consciousness</p>
<p>[02:28:40] 5.3 Consciousness Prerequisites and Indicators</p>
<p>[02:36:36] 5.4 AGI Safety Considerations</p>
<p>[02:40:29] 5.5 AI Regulation Framework</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Pattern-Recognition-vs-True-Intelligence---Francois-Chollet-e2qld8a</link>
			<guid isPermaLink="false">3c5dce2c-db4c-4e98-9a8a-c9cc287707e8</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 06 Nov 2024 23:19:06 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/94073546/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-6%2Fe7018962-36be-57fc-aca2-2b5f5a548a6d.mp3" length="235117962" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Francois Chollet, a prominent AI expert and creator of ARC-AGI, discusses intelligence, consciousness, and artificial intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Chollet explains that real intelligence isn&apos;t about memorizing information or having lots of knowledge - it&apos;s about being able to handle new situations effectively. This is why he believes current large language models (LLMs) have &quot;near-zero intelligence&quot; despite their impressive abilities. They&apos;re more like sophisticated memory and pattern-matching systems than truly intelligent beings.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;MLST IS SPONSORED BY TUFA AI LABS! &lt;/p&gt;
&lt;p&gt;The current winners of the ARC challenge, MindsAI are part of Tufa AI Labs. They are hiring ML engineers. Are you interested?! Please goto https://tufalabs.ai/&lt;/p&gt;
&lt;p&gt;***&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;He introduced his &quot;Kaleidoscope Hypothesis,&quot; which suggests that while the world seems infinitely complex, it&apos;s actually made up of simpler patterns that repeat and combine in different ways. True intelligence, he argues, involves identifying these basic patterns and using them to understand new situations.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Chollet also talked about consciousness, suggesting it develops gradually in children rather than appearing all at once. He believes consciousness exists in degrees - animals have it to some extent, and even human consciousness varies with age and circumstances (like being more conscious when learning something new versus doing routine tasks).&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;On AI safety, Chollet takes a notably different stance from many in Silicon Valley. He views AGI development as a scientific challenge rather than a religious quest, and doesn&apos;t share the apocalyptic concerns of some AI researchers. He argues that intelligence itself isn&apos;t dangerous - it&apos;s just a tool for turning information into useful models. What matters is how we choose to use it.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;ARC-AGI Prize:&lt;/p&gt;
&lt;p&gt;https://arcprize.org/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Francois Chollet:&lt;/p&gt;
&lt;p&gt;https://x.com/fchollet&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes: &lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/j2068j3hlj8br96pfa7bi/CHOLLET_FINAL.pdf?rlkey=xkbr7tbnrjdl66m246w26uc8k&amp;amp;st=0a4ec4na&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Intelligence and Model Building&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Intelligence Definition and ARC Benchmark&lt;/p&gt;
&lt;p&gt;[00:05:40] 1.2 LLMs as Program Memorization Systems&lt;/p&gt;
&lt;p&gt;[00:09:36] 1.3 Kaleidoscope Hypothesis and Abstract Building Blocks&lt;/p&gt;
&lt;p&gt;[00:13:39] 1.4 Deep Learning Limitations and System 2 Reasoning&lt;/p&gt;
&lt;p&gt;[00:29:38] 1.5 Intelligence vs. Skill in LLMs and Model Building&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. ARC Benchmark and Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:37:36] 2.1 Intelligence Definition and LLM Limitations&lt;/p&gt;
&lt;p&gt;[00:41:33] 2.2 Meta-Learning System Architecture&lt;/p&gt;
&lt;p&gt;[00:56:21] 2.3 Program Search and Occam&apos;s Razor&lt;/p&gt;
&lt;p&gt;[00:59:42] 2.4 Developer-Aware Generalization&lt;/p&gt;
&lt;p&gt;[01:06:49] 2.5 Task Generation and Benchmark Design&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Cognitive Systems and Program Generation&lt;/p&gt;
&lt;p&gt;[01:14:38] 3.1 System 1/2 Thinking Fundamentals&lt;/p&gt;
&lt;p&gt;[01:22:17] 3.2 Program Synthesis and Combinatorial Challenges&lt;/p&gt;
&lt;p&gt;[01:31:18] 3.3 Test-Time Fine-Tuning Strategies&lt;/p&gt;
&lt;p&gt;[01:36:10] 3.4 Evaluation and Leakage Problems&lt;/p&gt;
&lt;p&gt;[01:43:22] 3.5 ARC Implementation Approaches&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Intelligence and Language Systems&lt;/p&gt;
&lt;p&gt;[01:50:06] 4.1 Intelligence as Tool vs Agent&lt;/p&gt;
&lt;p&gt;[01:53:53] 4.2 Cultural Knowledge Integration&lt;/p&gt;
&lt;p&gt;[01:58:42] 4.3 Language and Abstraction Generation&lt;/p&gt;
&lt;p&gt;[02:02:41] 4.4 Embodiment in Cognitive Systems&lt;/p&gt;
&lt;p&gt;[02:09:02] 4.5 Language as Cognitive Operating System&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. Consciousness and AI Safety&lt;/p&gt;
&lt;p&gt;[02:14:05] 5.1 Consciousness and Intelligence Relationship&lt;/p&gt;
&lt;p&gt;[02:20:25] 5.2 Development of Machine Consciousness&lt;/p&gt;
&lt;p&gt;[02:28:40] 5.3 Consciousness Prerequisites and Indicators&lt;/p&gt;
&lt;p&gt;[02:36:36] 5.4 AGI Safety Considerations&lt;/p&gt;
&lt;p&gt;[02:40:29] 5.5 AI Regulation Framework&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:42:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1730935010375-63b07791f6e85.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Elegant Math Behind Machine Learning - Anil Ananthaswamy]]></title>
			<description><![CDATA[<p>Anil Ananthaswamy is an award-winning science writer and former staff writer and deputy news editor for the London-based New Scientist magazine. </p>
<p><br /></p>
<p>Machine learning systems are making life-altering decisions for us: approving mortgage loans, determining whether a tumor is cancerous, or deciding if someone gets bail. They now influence developments and discoveries in chemistry, biology, and physics—the study of genomes, extrasolar planets, even the intricacies of quantum systems. And all this before large language models such as ChatGPT came on the scene.</p>
<p><br /></p>
<p>We are living through a revolution in machine learning-powered AI that shows no signs of slowing down. This technology is based on relatively simple mathematical ideas, some of which go back centuries, including linear algebra and calculus, the stuff of seventeenth- and eighteenth-century mathematics. It took the birth and advancement of computer science and the kindling of 1990s computer chips designed for video games to ignite the explosion of AI that we see today. In this enlightening book, Anil Ananthaswamy explains the fundamental math behind machine learning, while suggesting intriguing links between artificial and natural intelligence. Might the same math underpin them both?</p>
<p><br /></p>
<p>As Ananthaswamy resonantly concludes, to make safe and effective use of artificial intelligence, we need to understand its profound capabilities and limitations, the clues to which lie in the math that makes machine learning possible.</p>
<p><br /></p>
<p>Why Machines Learn: The Elegant Math Behind Modern AI:</p>
<p>https://amzn.to/3UAWX3D</p>
<p>https://anilananthaswamy.com/</p>
<p><br /></p>
<p>Sponsor message:</p>
<p>DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?</p>
<p>Interested? Apply for an ML research position: benjamin@tufa.ai</p>
<p><br /></p>
<p>Shownotes: </p>
<p><a href="https://www.youtube.com/redirect?event=comments&amp;redir_token=QUFFLUhqbVJSRUMxdmppbkcxZFNiZldPT3JOX1NUZGxqQXxBQ3Jtc0tsbW5WRExLOFR1RHFHUVhERjBwdUpqWHRZWXR4SU9TMkw0QWQ0aHZWRC1Kd2lNSzRQYW4xTXdsN0p5aVU5NkdVTXpBX0hKemxEVWVtMi1JNlIyR2VfUl81QlpaS2RGdmM1dmMzM2F5bFZtV09KamhFdw&amp;q=https%3A%2F%2Fwww.dropbox.com%2Fscl%2Ffi%2Fwpv22m5jxyiqr6pqfkzwz%2Fanil.pdf%3Frlkey%3D9c233jo5armr548ctwo419n6p%26st%3Dxzhahtje%26dl%3D0" target="_blank" rel="ugc noopener noreferrer">https://www.dropbox.com/scl/fi/wpv22m5jxyiqr6pqfkzwz/anil.pdf?rlkey=9c233jo5armr548ctwo419n6p&amp;st=xzhahtje&amp;dl=0</a></p>
<p><br /></p>
<p>Chapters:</p>
<p>1. ML Fundamentals and Prerequisites</p>
<p>[00:00:00] 1.1 Differences Between Human and Machine Learning</p>
<p>[00:00:35] 1.2 Mathematical Prerequisites and Societal Impact of ML</p>
<p>[00:02:20] 1.3 Author's Journey and Book Background</p>
<p>[00:11:30] 1.4 Mathematical Foundations and Core ML Concepts</p>
<p>[00:21:45] 1.5 Bias-Variance Tradeoff and Modern Deep Learning</p>
<p><br /></p>
<p>2. Deep Learning Architecture</p>
<p>[00:29:05] 2.1 Double Descent and Overparameterization in Deep Learning</p>
<p>[00:32:40] 2.2 Mathematical Foundations and Self-Supervised Learning</p>
<p>[00:40:05] 2.3 High-Dimensional Spaces and Model Architecture</p>
<p>[00:52:55] 2.4 Historical Development of Backpropagation</p>
<p><br /></p>
<p>3. AI Understanding and Limitations</p>
<p>[00:59:13] 3.1 Pattern Matching vs Human Reasoning in ML Models</p>
<p>[01:00:20] 3.2 Mathematical Foundations and Pattern Recognition in AI</p>
<p>[01:04:08] 3.3 LLM Reliability and Machine Understanding Debate</p>
<p>[01:12:50] 3.4 Historical Development of Deep Learning Technologies</p>
<p>[01:15:21] 3.5 Alternative AI Approaches and Bio-inspired Methods</p>
<p><br /></p>
<p>4. Ethical and Neurological Perspectives</p>
<p>[01:24:32] 4.1 Neural Network Scaling and Mathematical Limitations</p>
<p>[01:31:12] 4.2 AI Ethics and Societal Impact</p>
<p>[01:38:30] 4.3 Consciousness and Neurological Conditions</p>
<p>[01:46:17] 4.4 Body Ownership and Agency in Neuroscience</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Elegant-Math-Behind-Machine-Learning---Anil-Ananthaswamy-e2qia6m</link>
			<guid isPermaLink="false">1c8cfe93-17a5-4c6b-a5ca-b5eae39558f7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 04 Nov 2024 21:02:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/93972118/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-10-4%2F864826a6-e0da-0d5d-8777-6420552127e6.mp3" length="163293097" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Anil Ananthaswamy is an award-winning science writer and former staff writer and deputy news editor for the London-based New Scientist magazine. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Machine learning systems are making life-altering decisions for us: approving mortgage loans, determining whether a tumor is cancerous, or deciding if someone gets bail. They now influence developments and discoveries in chemistry, biology, and physics—the study of genomes, extrasolar planets, even the intricacies of quantum systems. And all this before large language models such as ChatGPT came on the scene.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;We are living through a revolution in machine learning-powered AI that shows no signs of slowing down. This technology is based on relatively simple mathematical ideas, some of which go back centuries, including linear algebra and calculus, the stuff of seventeenth- and eighteenth-century mathematics. It took the birth and advancement of computer science and the kindling of 1990s computer chips designed for video games to ignite the explosion of AI that we see today. In this enlightening book, Anil Ananthaswamy explains the fundamental math behind machine learning, while suggesting intriguing links between artificial and natural intelligence. Might the same math underpin them both?&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;As Ananthaswamy resonantly concludes, to make safe and effective use of artificial intelligence, we need to understand its profound capabilities and limitations, the clues to which lie in the math that makes machine learning possible.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Why Machines Learn: The Elegant Math Behind Modern AI:&lt;/p&gt;
&lt;p&gt;https://amzn.to/3UAWX3D&lt;/p&gt;
&lt;p&gt;https://anilananthaswamy.com/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Sponsor message:&lt;/p&gt;
&lt;p&gt;DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?&lt;/p&gt;
&lt;p&gt;Interested? Apply for an ML research position: benjamin@tufa.ai&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes: &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/redirect?event=comments&amp;amp;redir_token=QUFFLUhqbVJSRUMxdmppbkcxZFNiZldPT3JOX1NUZGxqQXxBQ3Jtc0tsbW5WRExLOFR1RHFHUVhERjBwdUpqWHRZWXR4SU9TMkw0QWQ0aHZWRC1Kd2lNSzRQYW4xTXdsN0p5aVU5NkdVTXpBX0hKemxEVWVtMi1JNlIyR2VfUl81QlpaS2RGdmM1dmMzM2F5bFZtV09KamhFdw&amp;amp;q=https%3A%2F%2Fwww.dropbox.com%2Fscl%2Ffi%2Fwpv22m5jxyiqr6pqfkzwz%2Fanil.pdf%3Frlkey%3D9c233jo5armr548ctwo419n6p%26st%3Dxzhahtje%26dl%3D0&quot; target=&quot;_blank&quot; rel=&quot;ugc noopener noreferrer&quot;&gt;https://www.dropbox.com/scl/fi/wpv22m5jxyiqr6pqfkzwz/anil.pdf?rlkey=9c233jo5armr548ctwo419n6p&amp;amp;st=xzhahtje&amp;amp;dl=0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Chapters:&lt;/p&gt;
&lt;p&gt;1. ML Fundamentals and Prerequisites&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Differences Between Human and Machine Learning&lt;/p&gt;
&lt;p&gt;[00:00:35] 1.2 Mathematical Prerequisites and Societal Impact of ML&lt;/p&gt;
&lt;p&gt;[00:02:20] 1.3 Author&apos;s Journey and Book Background&lt;/p&gt;
&lt;p&gt;[00:11:30] 1.4 Mathematical Foundations and Core ML Concepts&lt;/p&gt;
&lt;p&gt;[00:21:45] 1.5 Bias-Variance Tradeoff and Modern Deep Learning&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Deep Learning Architecture&lt;/p&gt;
&lt;p&gt;[00:29:05] 2.1 Double Descent and Overparameterization in Deep Learning&lt;/p&gt;
&lt;p&gt;[00:32:40] 2.2 Mathematical Foundations and Self-Supervised Learning&lt;/p&gt;
&lt;p&gt;[00:40:05] 2.3 High-Dimensional Spaces and Model Architecture&lt;/p&gt;
&lt;p&gt;[00:52:55] 2.4 Historical Development of Backpropagation&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. AI Understanding and Limitations&lt;/p&gt;
&lt;p&gt;[00:59:13] 3.1 Pattern Matching vs Human Reasoning in ML Models&lt;/p&gt;
&lt;p&gt;[01:00:20] 3.2 Mathematical Foundations and Pattern Recognition in AI&lt;/p&gt;
&lt;p&gt;[01:04:08] 3.3 LLM Reliability and Machine Understanding Debate&lt;/p&gt;
&lt;p&gt;[01:12:50] 3.4 Historical Development of Deep Learning Technologies&lt;/p&gt;
&lt;p&gt;[01:15:21] 3.5 Alternative AI Approaches and Bio-inspired Methods&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Ethical and Neurological Perspectives&lt;/p&gt;
&lt;p&gt;[01:24:32] 4.1 Neural Network Scaling and Mathematical Limitations&lt;/p&gt;
&lt;p&gt;[01:31:12] 4.2 AI Ethics and Societal Impact&lt;/p&gt;
&lt;p&gt;[01:38:30] 4.3 Consciousness and Neurological Conditions&lt;/p&gt;
&lt;p&gt;[01:46:17] 4.4 Body Ownership and Agency in Neuroscience&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:53:11</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1730754111316-051076ffb479e.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Michael Levin - Why Intelligence Isn't Limited To Brains.]]></title>
			<description><![CDATA[<p>Professor Michael Levin explores the revolutionary concept of diverse intelligence, demonstrating how cognitive capabilities extend far beyond traditional brain-based intelligence. Drawing from his groundbreaking research, he explains how even simple biological systems like gene regulatory networks exhibit learning, memory, and problem-solving abilities. Levin introduces key concepts like "cognitive light cones" - the scope of goals a system can pursue - and shows how these ideas are transforming our approach to cancer treatment and biological engineering. His insights challenge conventional views of intelligence and agency, with profound implications for both medicine and artificial intelligence development. This deep discussion reveals how understanding intelligence as a spectrum, from molecular networks to human minds, could be crucial for humanity's future technological development. Contains technical discussion of biological systems, cybernetics, and theoretical frameworks for understanding emergent cognition.</p>
<p><br /></p>
<p>Prof. Michael Levin</p>
<p>https://as.tufts.edu/biology/people/faculty/michael-levin</p>
<p>https://x.com/drmichaellevin</p>
<p><br /></p>
<p>Sponsor message:</p>
<p>DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?</p>
<p>Interested? Apply for an ML research position: benjamin@tufa.ai</p>
<p><br /></p>
<p>TOC</p>
<p>1. Intelligence Fundamentals and Evolution</p>
<p>[00:00:00] 1.1 Future Evolution of Human Intelligence and Consciousness</p>
<p>[00:03:00] 1.2 Science Fiction's Role in Exploring Intelligence Possibilities</p>
<p>[00:08:15] 1.3 Essential Characteristics of Human-Level Intelligence and Relationships</p>
<p>[00:14:20] 1.4 Biological Systems Architecture and Intelligence</p>
<p><br /></p>
<p>2. Biological Computing and Cognition</p>
<p>[00:24:00] 2.1 Agency and Intelligence in Biological Systems</p>
<p>[00:30:30] 2.2 Learning Capabilities in Gene Regulatory Networks</p>
<p>[00:35:37] 2.3 Biological Control Systems and Competency Architecture</p>
<p>[00:39:58] 2.4 Scientific Metaphors and Polycomputing Paradigm</p>
<p><br /></p>
<p>3. Systems and Collective Intelligence</p>
<p>[00:43:26] 3.1 Embodiment and Problem-Solving Spaces</p>
<p>[00:44:50] 3.2 Perception-Action Loops and Biological Intelligence</p>
<p>[00:46:55] 3.3 Intelligence, Wisdom and Collective Systems</p>
<p>[00:53:07] 3.4 Cancer and Cognitive Light Cones</p>
<p>[00:57:09] 3.5 Emergent Intelligence and AI Agency</p>
<p><br /></p>
<p>Shownotes:</p>
<p>https://www.dropbox.com/scl/fi/i2vl1vs009thg54lxx5wc/LEVIN.pdf?rlkey=dtk8okhbsejryiu2vrht19qp6&amp;st=uzi0vo45&amp;dl=0</p>
<p><br /></p>
<p>REFS:</p>
<p>[0:05:30] A Fire Upon the Deep - Vernor Vinge sci-fi novel on AI and consciousness</p>
<p><br /></p>
<p>[0:05:35] Maria Chudnovsky - MacArthur Fellow, Princeton mathematician, graph theory expert</p>
<p><br /></p>
<p>[0:14:20] Bow-tie architecture in biological systems - Network structure research by Csete &amp; Doyle</p>
<p><br /></p>
<p>[0:15:40] Richard Watson - Southampton Professor, evolution and learning systems expert</p>
<p><br /></p>
<p>[0:17:00] Levin paper on human issues in AI and evolution</p>
<p><br /></p>
<p>[0:19:00] Bow-tie architecture in Darwin's agential materialism - Levin</p>
<p><br /></p>
<p>[0:22:55] Philip Goff - Work on panpsychism and consciousness in Galileo's Error</p>
<p><br /></p>
<p>[0:23:30] Strange Loop - Hofstadter's work on self-reference and consciousness</p>
<p><br /></p>
<p>[0:25:00] The Hard Problem of Consciousness - Van Gulick</p>
<p><br /></p>
<p>[0:26:15] Daniel Dennett - Theories on consciousness and intentional systems</p>
<p><br /></p>
<p>[0:29:35] Principle of Least Action - Light path selection in physics</p>
<p><br /></p>
<p>[0:29:50] Free Energy Principle - Friston's unified behavioral framework</p>
<p><br /></p>
<p>[0:30:35] Gene regulatory networks - Learning capabilities in biological systems</p>
<p><br /></p>
<p>[0:36:55] Minimal networks with learning capacity - Levin</p>
<p><br /></p>
<p>[0:38:50] Multi-scale competency in biological systems - Levin</p>
<p><br /></p>
<p>[0:41:40] Polycomputing paradigm - Biological computation by Bongard &amp; Levin</p>
<p><br /></p>
<p>[0:45:40] Collective intelligence in biology - Levin et al.</p>
<p><br /></p>
<p>[0:46:55] Niche construction and stigmergy - Torday</p>
<p><br /></p>
<p>[0:53:50] Tasmanian Devil Facial Tumor Disease - Transmissible cancer research</p>
<p><br /></p>
<p>[0:55:05] Cognitive light cone - Computational boundaries of self - Levin</p>
<p><br /></p>
<p>[0:58:05] Cognitive properties in sorting algorithms - Zhang, Goldstein &amp; Levin</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Michael-Levin---Why-Intelligence-Isnt-Limited-To-Brains-e2q3c8p</link>
			<guid isPermaLink="false">e5fbe9ee-6c13-4efe-aaae-ea0549cda83f</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 24 Oct 2024 15:27:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/93482713/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-24%2Fcd5c61f1-9788-f730-6d4b-135bda34ee73.mp3" length="91829337" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Michael Levin explores the revolutionary concept of diverse intelligence, demonstrating how cognitive capabilities extend far beyond traditional brain-based intelligence. Drawing from his groundbreaking research, he explains how even simple biological systems like gene regulatory networks exhibit learning, memory, and problem-solving abilities. Levin introduces key concepts like &quot;cognitive light cones&quot; - the scope of goals a system can pursue - and shows how these ideas are transforming our approach to cancer treatment and biological engineering. His insights challenge conventional views of intelligence and agency, with profound implications for both medicine and artificial intelligence development. This deep discussion reveals how understanding intelligence as a spectrum, from molecular networks to human minds, could be crucial for humanity&apos;s future technological development. Contains technical discussion of biological systems, cybernetics, and theoretical frameworks for understanding emergent cognition.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Prof. Michael Levin&lt;/p&gt;
&lt;p&gt;https://as.tufts.edu/biology/people/faculty/michael-levin&lt;/p&gt;
&lt;p&gt;https://x.com/drmichaellevin&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Sponsor message:&lt;/p&gt;
&lt;p&gt;DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?&lt;/p&gt;
&lt;p&gt;Interested? Apply for an ML research position: benjamin@tufa.ai&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;1. Intelligence Fundamentals and Evolution&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Future Evolution of Human Intelligence and Consciousness&lt;/p&gt;
&lt;p&gt;[00:03:00] 1.2 Science Fiction&apos;s Role in Exploring Intelligence Possibilities&lt;/p&gt;
&lt;p&gt;[00:08:15] 1.3 Essential Characteristics of Human-Level Intelligence and Relationships&lt;/p&gt;
&lt;p&gt;[00:14:20] 1.4 Biological Systems Architecture and Intelligence&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Biological Computing and Cognition&lt;/p&gt;
&lt;p&gt;[00:24:00] 2.1 Agency and Intelligence in Biological Systems&lt;/p&gt;
&lt;p&gt;[00:30:30] 2.2 Learning Capabilities in Gene Regulatory Networks&lt;/p&gt;
&lt;p&gt;[00:35:37] 2.3 Biological Control Systems and Competency Architecture&lt;/p&gt;
&lt;p&gt;[00:39:58] 2.4 Scientific Metaphors and Polycomputing Paradigm&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Systems and Collective Intelligence&lt;/p&gt;
&lt;p&gt;[00:43:26] 3.1 Embodiment and Problem-Solving Spaces&lt;/p&gt;
&lt;p&gt;[00:44:50] 3.2 Perception-Action Loops and Biological Intelligence&lt;/p&gt;
&lt;p&gt;[00:46:55] 3.3 Intelligence, Wisdom and Collective Systems&lt;/p&gt;
&lt;p&gt;[00:53:07] 3.4 Cancer and Cognitive Light Cones&lt;/p&gt;
&lt;p&gt;[00:57:09] 3.5 Emergent Intelligence and AI Agency&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/i2vl1vs009thg54lxx5wc/LEVIN.pdf?rlkey=dtk8okhbsejryiu2vrht19qp6&amp;amp;st=uzi0vo45&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;[0:05:30] A Fire Upon the Deep - Vernor Vinge sci-fi novel on AI and consciousness&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:05:35] Maria Chudnovsky - MacArthur Fellow, Princeton mathematician, graph theory expert&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:14:20] Bow-tie architecture in biological systems - Network structure research by Csete &amp;amp; Doyle&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:15:40] Richard Watson - Southampton Professor, evolution and learning systems expert&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:17:00] Levin paper on human issues in AI and evolution&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:19:00] Bow-tie architecture in Darwin&apos;s agential materialism - Levin&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:22:55] Philip Goff - Work on panpsychism and consciousness in Galileo&apos;s Error&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:23:30] Strange Loop - Hofstadter&apos;s work on self-reference and consciousness&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:25:00] The Hard Problem of Consciousness - Van Gulick&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:26:15] Daniel Dennett - Theories on consciousness and intentional systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:29:35] Principle of Least Action - Light path selection in physics&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:29:50] Free Energy Principle - Friston&apos;s unified behavioral framework&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:30:35] Gene regulatory networks - Learning capabilities in biological systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:36:55] Minimal networks with learning capacity - Levin&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:38:50] Multi-scale competency in biological systems - Levin&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:41:40] Polycomputing paradigm - Biological computation by Bongard &amp;amp; Levin&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:45:40] Collective intelligence in biology - Levin et al.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:46:55] Niche construction and stigmergy - Torday&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:53:50] Tasmanian Devil Facial Tumor Disease - Transmissible cancer research&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:55:05] Cognitive light cone - Computational boundaries of self - Levin&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:58:05] Cognitive properties in sorting algorithms - Zhang, Goldstein &amp;amp; Levin&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:03:35</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1729783636570-ca858f126ca48.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Speechmatics CTO - Next-Generation Speech Recognition]]></title>
			<description><![CDATA[<p>Will Williams is CTO of Speechmatics in Cambridge. In this sponsored episode - he shares deep technical insights into modern speech recognition technology and system architecture. The episode covers several key technical areas:</p>
<p><br /></p>
<p>* Speechmatics' hybrid approach to ASR, which focusses on unsupervised learning methods, achieving comparable results with 100x less data than fully supervised approaches. Williams explains why this is more efficient and generalizable than end-to-end models like Whisper.</p>
<p><br /></p>
<p>* Their production architecture implementing multiple operating points for different latency-accuracy trade-offs, with careful latency padding (up to 1.8 seconds) to ensure consistent user experience. The system uses lattice-based decoding with language model integration for improved accuracy.</p>
<p><br /></p>
<p>* The challenges and solutions in real-time ASR, including their approach to diarization (speaker identification), handling cross-talk, and implicit source separation. Williams explains why these problems remain difficult even with modern deep learning approaches.</p>
<p><br /></p>
<p>* Their testing and deployment infrastructure, including the use of mirrored environments for catching edge cases in production, and their strategy of maintaining global models rather than allowing customer-specific fine-tuning.</p>
<p><br /></p>
<p>* Technical evolution in ASR, from early days of custom CUDA kernels and manual memory management to modern frameworks, with Williams offering interesting critiques of current PyTorch memory management approaches and arguing for more efficient direct memory allocation in production systems.</p>
<p><br /></p>
<p>Get coding with their API! This is their URL:</p>
<p>https://www.speechmatics.com/</p>
<p><br /></p>
<p></p>
<p>DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?</p>
<p>MLST is sponsored by Tufa Labs:</p>
<p>Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.</p>
<p>Interested? Apply for an ML research position: benjamin@tufa.ai</p>
<p></p>
<p><br /></p>
<p>TOC</p>
<p>1. ASR Core Technology &amp; Real-time Architecture</p>
<p>[00:00:00] 1.1 ASR and Diarization Fundamentals</p>
<p>[00:05:25] 1.2 Real-time Conversational AI Architecture</p>
<p>[00:09:21] 1.3 Neural Network Streaming Implementation</p>
<p>[00:12:49] 1.4 Multi-modal System Integration</p>
<p><br /></p>
<p>2. Production System Optimization</p>
<p>[00:29:38] 2.1 Production Deployment and Testing Infrastructure</p>
<p>[00:35:40] 2.2 Model Architecture and Deployment Strategy</p>
<p>[00:37:12] 2.3 Latency-Accuracy Trade-offs</p>
<p>[00:39:15] 2.4 Language Model Integration</p>
<p>[00:40:32] 2.5 Lattice-based Decoding Architecture</p>
<p><br /></p>
<p>3. Performance Evaluation &amp; Ethical Considerations</p>
<p>[00:44:00] 3.1 ASR Performance Metrics and Capabilities</p>
<p>[00:46:35] 3.2 AI Regulation and Evaluation Methods</p>
<p>[00:51:09] 3.3 Benchmark and Testing Challenges</p>
<p>[00:54:30] 3.4 Real-world Implementation Metrics</p>
<p>[01:00:51] 3.5 Ethics and Privacy Considerations</p>
<p><br /></p>
<p>4. ASR Technical Evolution</p>
<p>[01:09:00] 4.1 WER Calculation and Evaluation Methodologies</p>
<p>[01:10:21] 4.2 Supervised vs Self-Supervised Learning Approaches</p>
<p>[01:21:02] 4.3 Temporal Learning and Feature Processing</p>
<p>[01:24:45] 4.4 Feature Engineering to Automated ML</p>
<p><br /></p>
<p>5. Enterprise Implementation &amp; Scale</p>
<p>[01:27:55] 5.1 Future AI Systems and Adaptation</p>
<p>[01:31:52] 5.2 Technical Foundations and History</p>
<p>[01:34:53] 5.3 Infrastructure and Team Scaling</p>
<p>[01:38:05] 5.4 Research and Talent Strategy</p>
<p>[01:41:11] 5.5 Engineering Practice Evolution</p>
<p><br /></p>
<p>Shownotes:</p>
<p>https://www.dropbox.com/scl/fi/d94b1jcgph9o8au8shdym/Speechmatics.pdf?rlkey=bi55wvktzomzx0y5sic6jz99y&amp;st=6qwofv8t&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Speechmatics-CTO---Next-Generation-Speech-Recognition-e2q2g1a</link>
			<guid isPermaLink="false">4f6a7e55-14ce-4a5d-b6a9-204df45471e2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 23 Oct 2024 22:38:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/93453802/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-23%2F28773177-f173-7635-f237-352753d4afbe.mp3" length="153762199" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Will Williams is CTO of Speechmatics in Cambridge. In this sponsored episode - he shares deep technical insights into modern speech recognition technology and system architecture. The episode covers several key technical areas:&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;* Speechmatics&apos; hybrid approach to ASR, which focusses on unsupervised learning methods, achieving comparable results with 100x less data than fully supervised approaches. Williams explains why this is more efficient and generalizable than end-to-end models like Whisper.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;* Their production architecture implementing multiple operating points for different latency-accuracy trade-offs, with careful latency padding (up to 1.8 seconds) to ensure consistent user experience. The system uses lattice-based decoding with language model integration for improved accuracy.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;* The challenges and solutions in real-time ASR, including their approach to diarization (speaker identification), handling cross-talk, and implicit source separation. Williams explains why these problems remain difficult even with modern deep learning approaches.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;* Their testing and deployment infrastructure, including the use of mirrored environments for catching edge cases in production, and their strategy of maintaining global models rather than allowing customer-specific fine-tuning.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;* Technical evolution in ASR, from early days of custom CUDA kernels and manual memory management to modern frameworks, with Williams offering interesting critiques of current PyTorch memory management approaches and arguing for more efficient direct memory allocation in production systems.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Get coding with their API! This is their URL:&lt;/p&gt;
&lt;p&gt;https://www.speechmatics.com/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Tufa Labs:&lt;/p&gt;
&lt;p&gt;Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.&lt;/p&gt;
&lt;p&gt;Interested? Apply for an ML research position: benjamin@tufa.ai&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;1. ASR Core Technology &amp;amp; Real-time Architecture&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 ASR and Diarization Fundamentals&lt;/p&gt;
&lt;p&gt;[00:05:25] 1.2 Real-time Conversational AI Architecture&lt;/p&gt;
&lt;p&gt;[00:09:21] 1.3 Neural Network Streaming Implementation&lt;/p&gt;
&lt;p&gt;[00:12:49] 1.4 Multi-modal System Integration&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Production System Optimization&lt;/p&gt;
&lt;p&gt;[00:29:38] 2.1 Production Deployment and Testing Infrastructure&lt;/p&gt;
&lt;p&gt;[00:35:40] 2.2 Model Architecture and Deployment Strategy&lt;/p&gt;
&lt;p&gt;[00:37:12] 2.3 Latency-Accuracy Trade-offs&lt;/p&gt;
&lt;p&gt;[00:39:15] 2.4 Language Model Integration&lt;/p&gt;
&lt;p&gt;[00:40:32] 2.5 Lattice-based Decoding Architecture&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Performance Evaluation &amp;amp; Ethical Considerations&lt;/p&gt;
&lt;p&gt;[00:44:00] 3.1 ASR Performance Metrics and Capabilities&lt;/p&gt;
&lt;p&gt;[00:46:35] 3.2 AI Regulation and Evaluation Methods&lt;/p&gt;
&lt;p&gt;[00:51:09] 3.3 Benchmark and Testing Challenges&lt;/p&gt;
&lt;p&gt;[00:54:30] 3.4 Real-world Implementation Metrics&lt;/p&gt;
&lt;p&gt;[01:00:51] 3.5 Ethics and Privacy Considerations&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. ASR Technical Evolution&lt;/p&gt;
&lt;p&gt;[01:09:00] 4.1 WER Calculation and Evaluation Methodologies&lt;/p&gt;
&lt;p&gt;[01:10:21] 4.2 Supervised vs Self-Supervised Learning Approaches&lt;/p&gt;
&lt;p&gt;[01:21:02] 4.3 Temporal Learning and Feature Processing&lt;/p&gt;
&lt;p&gt;[01:24:45] 4.4 Feature Engineering to Automated ML&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. Enterprise Implementation &amp;amp; Scale&lt;/p&gt;
&lt;p&gt;[01:27:55] 5.1 Future AI Systems and Adaptation&lt;/p&gt;
&lt;p&gt;[01:31:52] 5.2 Technical Foundations and History&lt;/p&gt;
&lt;p&gt;[01:34:53] 5.3 Infrastructure and Team Scaling&lt;/p&gt;
&lt;p&gt;[01:38:05] 5.4 Research and Talent Strategy&lt;/p&gt;
&lt;p&gt;[01:41:11] 5.5 Engineering Practice Evolution&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/d94b1jcgph9o8au8shdym/Speechmatics.pdf?rlkey=bi55wvktzomzx0y5sic6jz99y&amp;amp;st=6qwofv8t&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:46:23</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1729723087076-c98b66a1a9aea.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dr. Sanjeev Namjoshi - Active Inference]]></title>
			<description><![CDATA[<p>Dr. Sanjeev Namjoshi, a machine learning engineer who recently submitted a book on Active Inference to MIT Press, discusses the theoretical foundations and practical applications of Active Inference, the Free Energy Principle (FEP), and Bayesian mechanics. He explains how these frameworks describe how biological and artificial systems maintain stability by minimizing uncertainty about their environment.</p>
<p><br /></p>
<p></p>
<p>DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?</p>
<p>MLST is sponsored by Tufa Labs:</p>
<p>Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.</p>
<p>Future plans: Expanding to complex environments like Warcraft 2 and Starcraft 2.</p>
<p>Interested? Apply for an ML research position: benjamin@tufa.ai</p>
<p></p>
<p><br /></p>
<p>Namjoshi traces the evolution of these fields from early 2000s neuroscience research to current developments, highlighting how Active Inference provides a unified framework for perception and action through variational free energy minimization. He contrasts this with traditional machine learning approaches, emphasizing Active Inference's natural capacity for exploration and curiosity through epistemic value.</p>
<p><br /></p>
<p>He sees Active Inference as being at a similar stage to deep learning in the early 2000s - poised for significant breakthroughs but requiring better tools and wider adoption. While acknowledging current computational challenges, he emphasizes Active Inference's potential advantages over reinforcement learning, particularly its principled approach to exploration and planning.</p>
<p><br /></p>
<p>Dr. Sanjeev Namjoshi</p>
<p>https://snamjoshi.github.io/</p>
<p><br /></p>
<p>TOC:</p>
<p>1. Theoretical Foundations: AI Agency and Sentience</p>
<p>[00:00:00] 1.1 Intro</p>
<p>[00:02:45] 1.2 Free Energy Principle and Active Inference Theory</p>
<p>[00:11:16] 1.3 Emergence and Self-Organization in Complex Systems</p>
<p>[00:19:11] 1.4 Agency and Representation in AI Systems</p>
<p>[00:29:59] 1.5 Bayesian Mechanics and Systems Modeling</p>
<p><br /></p>
<p>2. Technical Framework: Active Inference and Free Energy</p>
<p>[00:38:37] 2.1 Generative Processes and Agent-Environment Modeling</p>
<p>[00:42:27] 2.2 Markov Blankets and System Boundaries</p>
<p>[00:44:30] 2.3 Bayesian Inference and Prior Distributions</p>
<p>[00:52:41] 2.4 Variational Free Energy Minimization Framework</p>
<p>[00:55:07] 2.5 VFE Optimization Techniques: Generalized Filtering vs DEM</p>
<p><br /></p>
<p>3. Implementation and Optimization Methods</p>
<p>[00:58:25] 3.1 Information Theory and Free Energy Concepts</p>
<p>[01:05:25] 3.2 Surprise Minimization and Action in Active Inference</p>
<p>[01:15:58] 3.3 Evolution of Active Inference Models: Continuous to Discrete Approaches</p>
<p>[01:26:00] 3.4 Uncertainty Reduction and Control Systems in Active Inference</p>
<p><br /></p>
<p>4. Safety and Regulatory Frameworks</p>
<p>[01:32:40] 4.1 Historical Evolution of Risk Management and Predictive Systems</p>
<p>[01:36:12] 4.2 Agency and Reality: Philosophical Perspectives on Models</p>
<p>[01:39:20] 4.3 Limitations of Symbolic AI and Current System Design</p>
<p>[01:46:40] 4.4 AI Safety Regulation and Corporate Governance</p>
<p><br /></p>
<p>5. Socioeconomic Integration and Modeling</p>
<p>[01:52:55] 5.1 Economic Policy and Public Sentiment Modeling</p>
<p>[01:55:21] 5.2 Free Energy Principle: Libertarian vs Collectivist Perspectives</p>
<p>[01:58:53] 5.3 Regulation of Complex Socio-Technical Systems</p>
<p>[02:03:04] 5.4 Evolution and Current State of Active Inference Research</p>
<p><br /></p>
<p>6. Future Directions and Applications</p>
<p>[02:14:26] 6.1 Active Inference Applications and Future Development</p>
<p>[02:22:58] 6.2 Cultural Learning and Active Inference</p>
<p>[02:29:19] 6.3 Hierarchical Relationship Between FEP, Active Inference, and Bayesian Mechanics</p>
<p>[02:33:22] 6.4 Historical Evolution of Free Energy Principle</p>
<p>[02:38:52] 6.5 Active Inference vs Traditional Machine Learning Approaches</p>
<p><br /></p>
<p>Transcript and shownotes with refs and URLs:</p>
<p>https://www.dropbox.com/scl/fi/qj22a660cob1795ej0gbw/SanjeevShow.pdf?rlkey=w323r3e8zfsnve22caayzb17k&amp;st=el1fdgfr&amp;dl=0</p>
<p><br /></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Dr--Sanjeev-Namjoshi---Active-Inference-e2q0uk6</link>
			<guid isPermaLink="false">0bb5d8d3-de5a-4d88-8edb-4350cf8ee350</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 22 Oct 2024 21:35:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/93403206/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-22%2F101db96c-3f18-2959-af87-3fe9099bc526.mp3" length="238930849" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Sanjeev Namjoshi, a machine learning engineer who recently submitted a book on Active Inference to MIT Press, discusses the theoretical foundations and practical applications of Active Inference, the Free Energy Principle (FEP), and Bayesian mechanics. He explains how these frameworks describe how biological and artificial systems maintain stability by minimizing uncertainty about their environment.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Tufa Labs:&lt;/p&gt;
&lt;p&gt;Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.&lt;/p&gt;
&lt;p&gt;Future plans: Expanding to complex environments like Warcraft 2 and Starcraft 2.&lt;/p&gt;
&lt;p&gt;Interested? Apply for an ML research position: benjamin@tufa.ai&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Namjoshi traces the evolution of these fields from early 2000s neuroscience research to current developments, highlighting how Active Inference provides a unified framework for perception and action through variational free energy minimization. He contrasts this with traditional machine learning approaches, emphasizing Active Inference&apos;s natural capacity for exploration and curiosity through epistemic value.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;He sees Active Inference as being at a similar stage to deep learning in the early 2000s - poised for significant breakthroughs but requiring better tools and wider adoption. While acknowledging current computational challenges, he emphasizes Active Inference&apos;s potential advantages over reinforcement learning, particularly its principled approach to exploration and planning.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Dr. Sanjeev Namjoshi&lt;/p&gt;
&lt;p&gt;https://snamjoshi.github.io/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Theoretical Foundations: AI Agency and Sentience&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Intro&lt;/p&gt;
&lt;p&gt;[00:02:45] 1.2 Free Energy Principle and Active Inference Theory&lt;/p&gt;
&lt;p&gt;[00:11:16] 1.3 Emergence and Self-Organization in Complex Systems&lt;/p&gt;
&lt;p&gt;[00:19:11] 1.4 Agency and Representation in AI Systems&lt;/p&gt;
&lt;p&gt;[00:29:59] 1.5 Bayesian Mechanics and Systems Modeling&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Technical Framework: Active Inference and Free Energy&lt;/p&gt;
&lt;p&gt;[00:38:37] 2.1 Generative Processes and Agent-Environment Modeling&lt;/p&gt;
&lt;p&gt;[00:42:27] 2.2 Markov Blankets and System Boundaries&lt;/p&gt;
&lt;p&gt;[00:44:30] 2.3 Bayesian Inference and Prior Distributions&lt;/p&gt;
&lt;p&gt;[00:52:41] 2.4 Variational Free Energy Minimization Framework&lt;/p&gt;
&lt;p&gt;[00:55:07] 2.5 VFE Optimization Techniques: Generalized Filtering vs DEM&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Implementation and Optimization Methods&lt;/p&gt;
&lt;p&gt;[00:58:25] 3.1 Information Theory and Free Energy Concepts&lt;/p&gt;
&lt;p&gt;[01:05:25] 3.2 Surprise Minimization and Action in Active Inference&lt;/p&gt;
&lt;p&gt;[01:15:58] 3.3 Evolution of Active Inference Models: Continuous to Discrete Approaches&lt;/p&gt;
&lt;p&gt;[01:26:00] 3.4 Uncertainty Reduction and Control Systems in Active Inference&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Safety and Regulatory Frameworks&lt;/p&gt;
&lt;p&gt;[01:32:40] 4.1 Historical Evolution of Risk Management and Predictive Systems&lt;/p&gt;
&lt;p&gt;[01:36:12] 4.2 Agency and Reality: Philosophical Perspectives on Models&lt;/p&gt;
&lt;p&gt;[01:39:20] 4.3 Limitations of Symbolic AI and Current System Design&lt;/p&gt;
&lt;p&gt;[01:46:40] 4.4 AI Safety Regulation and Corporate Governance&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;5. Socioeconomic Integration and Modeling&lt;/p&gt;
&lt;p&gt;[01:52:55] 5.1 Economic Policy and Public Sentiment Modeling&lt;/p&gt;
&lt;p&gt;[01:55:21] 5.2 Free Energy Principle: Libertarian vs Collectivist Perspectives&lt;/p&gt;
&lt;p&gt;[01:58:53] 5.3 Regulation of Complex Socio-Technical Systems&lt;/p&gt;
&lt;p&gt;[02:03:04] 5.4 Evolution and Current State of Active Inference Research&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;6. Future Directions and Applications&lt;/p&gt;
&lt;p&gt;[02:14:26] 6.1 Active Inference Applications and Future Development&lt;/p&gt;
&lt;p&gt;[02:22:58] 6.2 Cultural Learning and Active Inference&lt;/p&gt;
&lt;p&gt;[02:29:19] 6.3 Hierarchical Relationship Between FEP, Active Inference, and Bayesian Mechanics&lt;/p&gt;
&lt;p&gt;[02:33:22] 6.4 Historical Evolution of Free Energy Principle&lt;/p&gt;
&lt;p&gt;[02:38:52] 6.5 Active Inference vs Traditional Machine Learning Approaches&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Transcript and shownotes with refs and URLs:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/qj22a660cob1795ej0gbw/SanjeevShow.pdf?rlkey=w323r3e8zfsnve22caayzb17k&amp;amp;st=el1fdgfr&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:45:32</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1729632911524-2fce3d344a40d.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Joscha Bach - Why Your Thoughts Aren't Yours.]]></title>
			<description><![CDATA[<p>Dr. Joscha Bach discusses advanced AI, consciousness, and cognitive modeling. He presents consciousness as a virtual property emerging from self-organizing software patterns, challenging panpsychism and materialism. Bach introduces "Cyberanima," reinterpreting animism through information processing, viewing spirits as self-organizing software agents.</p>
<p>He addresses limitations of current large language models and advocates for smaller, more efficient AI models capable of reasoning from first principles. Bach describes his work with Liquid AI on novel neural network architectures for improved expressiveness and efficiency.</p>
<p>The interview covers AI's societal implications, including regulation challenges and impact on innovation. Bach argues for balancing oversight with technological progress, warning against overly restrictive regulations.</p>
<p>Throughout, Bach frames consciousness, intelligence, and agency as emergent properties of complex information processing systems, proposing a computational framework for cognitive phenomena and reality.</p>
<p><br /></p>
<p>SPONSOR MESSAGE:</p>
<p>DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?
MLST is sponsored by Tufa Labs:
Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.
Future plans: Expanding to complex environments like Warcraft 2 and Starcraft 2.
Interested? Apply for an ML research position: benjamin@<a href="https://www.youtube.com/redirect?event=comments&amp;redir_token=QUFFLUhqbmNOdVNwS3pHRm9INk1FVEptS1lLNkRtS0I1d3xBQ3Jtc0ttQ0ZYMmRwQWFmUk9Ud2NGNnlsbDRLa0dud0xVekp5aGpsZEd5UEtnUFRPQnhKSlNiV0tDNGRpN1pZeUYwYmpUNlQ5bFpyRkN0QUs0eVl1TkVFdHRCaE9JZjZYcWcyMGxGSHAxV2NPMDRGMlctc1Z3MA&amp;q=http%3A%2F%2Ftufa.ai%2F" target="_blank" rel="ugc noopener noreferrer">tufa.ai</a></p>
<p><br /></p>
<p>TOC</p>
<p>[00:00:00] 1.1 Consciousness and Intelligence in AI Development</p>
<p>[00:07:44] 1.2 Agency, Intelligence, and Their Relationship to Physical Reality</p>
<p>[00:13:36] 1.3 Virtual Patterns and Causal Structures in Consciousness</p>
<p>[00:25:49] 1.4 Reinterpreting Concepts of God and Animism in Information Processing Terms</p>
<p>[00:32:50] 1.5 Animism and Evolution as Competition Between Software Agents</p>
<p><br /></p>
<p>2. Self-Organizing Systems and Cognitive Models in AI</p>
<p>[00:37:59] 2.1 Consciousness as self-organizing software</p>
<p>[00:45:49] 2.2 Critique of panpsychism and alternative views on consciousness</p>
<p>[00:50:48] 2.3 Emergence of consciousness in complex systems</p>
<p>[00:52:50] 2.4 Neuronal motivation and the origins of consciousness</p>
<p>[00:56:47] 2.5 Coherence and Self-Organization in AI Systems</p>
<p><br /></p>
<p>3. Advanced AI Architectures and Cognitive Processes</p>
<p>[00:57:50] 3.1 Second-Order Software and Complex Mental Processes</p>
<p>[01:01:05] 3.2 Collective Agency and Shared Values in AI</p>
<p>[01:05:40] 3.3 Limitations of Current AI Agents and LLMs</p>
<p>[01:06:40] 3.4 Liquid AI and Novel Neural Network Architectures</p>
<p>[01:10:06] 3.5 AI Model Efficiency and Future Directions</p>
<p>[01:19:00] 3.6 LLM Limitations and Internal State Representation</p>
<p><br /></p>
<p>4. AI Regulation and Societal Impact</p>
<p>[01:31:23] 4.1 AI Regulation and Societal Impact</p>
<p>[01:49:50] 4.2 Open-Source AI and Industry Challenges</p>
<p><br /></p>
<p>Refs in shownotes and MP3 metadata</p>
<p><br /></p>
<p>Shownotes:</p>
<p>https://www.dropbox.com/scl/fi/g28dosz19bzcfs5imrvbu/JoschaInterview.pdf?rlkey=s3y18jy192ktz6ogd7qtvry3d&amp;st=10z7q7w9&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Joscha-Bach---Why-Your-Thoughts-Arent-Yours-e2pti01</link>
			<guid isPermaLink="false">7e0c338f-8d54-4f44-a18d-c34e26704c5a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 20 Oct 2024 15:09:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/93291969/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-20%2F78260547-211e-1163-ae4e-6f576a708b01.mp3" length="162757536" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Joscha Bach discusses advanced AI, consciousness, and cognitive modeling. He presents consciousness as a virtual property emerging from self-organizing software patterns, challenging panpsychism and materialism. Bach introduces &quot;Cyberanima,&quot; reinterpreting animism through information processing, viewing spirits as self-organizing software agents.&lt;/p&gt;
&lt;p&gt;He addresses limitations of current large language models and advocates for smaller, more efficient AI models capable of reasoning from first principles. Bach describes his work with Liquid AI on novel neural network architectures for improved expressiveness and efficiency.&lt;/p&gt;
&lt;p&gt;The interview covers AI&apos;s societal implications, including regulation challenges and impact on innovation. Bach argues for balancing oversight with technological progress, warning against overly restrictive regulations.&lt;/p&gt;
&lt;p&gt;Throughout, Bach frames consciousness, intelligence, and agency as emergent properties of complex information processing systems, proposing a computational framework for cognitive phenomena and reality.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;SPONSOR MESSAGE:&lt;/p&gt;
&lt;p&gt;DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?
MLST is sponsored by Tufa Labs:
Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.
Future plans: Expanding to complex environments like Warcraft 2 and Starcraft 2.
Interested? Apply for an ML research position: benjamin@&lt;a href=&quot;https://www.youtube.com/redirect?event=comments&amp;amp;redir_token=QUFFLUhqbmNOdVNwS3pHRm9INk1FVEptS1lLNkRtS0I1d3xBQ3Jtc0ttQ0ZYMmRwQWFmUk9Ud2NGNnlsbDRLa0dud0xVekp5aGpsZEd5UEtnUFRPQnhKSlNiV0tDNGRpN1pZeUYwYmpUNlQ5bFpyRkN0QUs0eVl1TkVFdHRCaE9JZjZYcWcyMGxGSHAxV2NPMDRGMlctc1Z3MA&amp;amp;q=http%3A%2F%2Ftufa.ai%2F&quot; target=&quot;_blank&quot; rel=&quot;ugc noopener noreferrer&quot;&gt;tufa.ai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Consciousness and Intelligence in AI Development&lt;/p&gt;
&lt;p&gt;[00:07:44] 1.2 Agency, Intelligence, and Their Relationship to Physical Reality&lt;/p&gt;
&lt;p&gt;[00:13:36] 1.3 Virtual Patterns and Causal Structures in Consciousness&lt;/p&gt;
&lt;p&gt;[00:25:49] 1.4 Reinterpreting Concepts of God and Animism in Information Processing Terms&lt;/p&gt;
&lt;p&gt;[00:32:50] 1.5 Animism and Evolution as Competition Between Software Agents&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Self-Organizing Systems and Cognitive Models in AI&lt;/p&gt;
&lt;p&gt;[00:37:59] 2.1 Consciousness as self-organizing software&lt;/p&gt;
&lt;p&gt;[00:45:49] 2.2 Critique of panpsychism and alternative views on consciousness&lt;/p&gt;
&lt;p&gt;[00:50:48] 2.3 Emergence of consciousness in complex systems&lt;/p&gt;
&lt;p&gt;[00:52:50] 2.4 Neuronal motivation and the origins of consciousness&lt;/p&gt;
&lt;p&gt;[00:56:47] 2.5 Coherence and Self-Organization in AI Systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Advanced AI Architectures and Cognitive Processes&lt;/p&gt;
&lt;p&gt;[00:57:50] 3.1 Second-Order Software and Complex Mental Processes&lt;/p&gt;
&lt;p&gt;[01:01:05] 3.2 Collective Agency and Shared Values in AI&lt;/p&gt;
&lt;p&gt;[01:05:40] 3.3 Limitations of Current AI Agents and LLMs&lt;/p&gt;
&lt;p&gt;[01:06:40] 3.4 Liquid AI and Novel Neural Network Architectures&lt;/p&gt;
&lt;p&gt;[01:10:06] 3.5 AI Model Efficiency and Future Directions&lt;/p&gt;
&lt;p&gt;[01:19:00] 3.6 LLM Limitations and Internal State Representation&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. AI Regulation and Societal Impact&lt;/p&gt;
&lt;p&gt;[01:31:23] 4.1 AI Regulation and Societal Impact&lt;/p&gt;
&lt;p&gt;[01:49:50] 4.2 Open-Source AI and Industry Challenges&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Refs in shownotes and MP3 metadata&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes:&lt;/p&gt;
&lt;p&gt;https://www.dropbox.com/scl/fi/g28dosz19bzcfs5imrvbu/JoschaInterview.pdf?rlkey=s3y18jy192ktz6ogd7qtvry3d&amp;amp;st=10z7q7w9&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:52:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1729436950432-c715225597a62.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Decompiling Dreams: A New Approach to ARC? - Alessandro Palmarini]]></title>
			<description><![CDATA[<p>Alessandro Palmarini is a post-baccalaureate researcher at the Santa Fe Institute working under the supervision of Melanie Mitchell. He completed his undergraduate degree in Artificial Intelligence and Computer Science at the University of Edinburgh. Palmarini's current research focuses on developing AI systems that can efficiently acquire new skills from limited data, inspired by François Chollet's work on measuring intelligence. His work builds upon the DreamCoder program synthesis system, introducing a novel approach called "dream decompiling" to improve library learning in inductive program synthesis. Palmarini is particularly interested in addressing the Abstraction and Reasoning Corpus (ARC) challenge, aiming to create AI systems that can perform abstract reasoning tasks more efficiently than current approaches. His research explores the balance between computational efficiency and data efficiency in AI learning processes.</p>
<p><br /></p>
<p>DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?
MLST is sponsored by Tufa Labs:
Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.
Future plans: Expanding to complex environments like Warcraft 2 and Starcraft 2.
Interested? Apply for an ML research position: benjamin@<a href="https://www.youtube.com/redirect?event=comments&amp;redir_token=QUFFLUhqbEkwM05JYlJZM0FYdWdBVjFFRTdKbFc3ZEpLd3xBQ3Jtc0tta3NUdlhZWjBaVS1XV0NTbFZrWkd4d3U0QVg1cS1BTml1NnVzZHhrWVpDdWgyYmJOamY4dXlnSDg2VUUyclNQZ1h1aTlRYnl3WGdRenNjbm1ZTDI4NUN0eUl0bzVfWi1GcHU3dHZzUDVhMl93T0tBQQ&amp;q=http%3A%2F%2Ftufa.ai%2F" target="_blank" rel="ugc noopener noreferrer">tufa.ai</a></p>
<p><br /></p>
<p>TOC:</p>
<p>1. Intelligence Measurement in AI Systems</p>
<p>[00:00:00] 1.1 Defining Intelligence in AI Systems</p>
<p>[00:02:00] 1.2 Research at Santa Fe Institute</p>
<p>[00:04:35] 1.3 Impact of Gaming on AI Development</p>
<p>[00:05:10] 1.4 Comparing AI and Human Learning Efficiency</p>
<p><br /></p>
<p>2. Efficient Skill Acquisition in AI</p>
<p>[00:06:40] 2.1 Intelligence as Skill Acquisition Efficiency</p>
<p>[00:08:25] 2.2 Limitations of Current AI Systems in Generalization</p>
<p>[00:09:45] 2.3 Human vs. AI Cognitive Processes</p>
<p>[00:10:40] 2.4 Measuring AI Intelligence: Chollet's ARC Challenge</p>
<p><br /></p>
<p>3. Program Synthesis and ARC Challenge</p>
<p>[00:12:55] 3.1 Philosophical Foundations of Program Synthesis</p>
<p>[00:17:14] 3.2 Introduction to Program Induction and ARC Tasks</p>
<p>[00:18:49] 3.3 DreamCoder: Principles and Techniques</p>
<p>[00:27:55] 3.4 Trade-offs in Program Synthesis Search Strategies</p>
<p>[00:31:52] 3.5 Neural Networks and Bayesian Program Learning</p>
<p><br /></p>
<p>4. Advanced Program Synthesis Techniques</p>
<p>[00:32:30] 4.1 DreamCoder and Dream Decompiling Approach</p>
<p>[00:39:00] 4.2 Beta Distribution and Caching in Program Synthesis</p>
<p>[00:45:10] 4.3 Performance and Limitations of Dream Decompiling</p>
<p>[00:47:45] 4.4 Alessandro's Approach to ARC Challenge</p>
<p>[00:51:12] 4.5 Conclusion and Future Discussions</p>
<p><br /></p>
<p>Refs:</p>
<p>Full reflist on YT VD, Show Notes and MP3 metadata</p>
<p><br /></p>
<p>Show Notes: https://www.dropbox.com/scl/fi/x50201tgqucj5ba2q4typ/Ale.pdf?rlkey=0ubvk7p5gtyx1gpownpdadim8&amp;st=5pniu3nq&amp;dl=0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Decompiling-Dreams-A-New-Approach-to-ARC----Alessandro-Palmarini-e2psmmi</link>
			<guid isPermaLink="false">2a782174-80e8-4e37-b80d-82fb54bf9c57</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 19 Oct 2024 16:12:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/93264018/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-19%2Ff7911b01-908c-b0ee-a56f-0a887128d747.mp3" length="74595925" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Alessandro Palmarini is a post-baccalaureate researcher at the Santa Fe Institute working under the supervision of Melanie Mitchell. He completed his undergraduate degree in Artificial Intelligence and Computer Science at the University of Edinburgh. Palmarini&apos;s current research focuses on developing AI systems that can efficiently acquire new skills from limited data, inspired by François Chollet&apos;s work on measuring intelligence. His work builds upon the DreamCoder program synthesis system, introducing a novel approach called &quot;dream decompiling&quot; to improve library learning in inductive program synthesis. Palmarini is particularly interested in addressing the Abstraction and Reasoning Corpus (ARC) challenge, aiming to create AI systems that can perform abstract reasoning tasks more efficiently than current approaches. His research explores the balance between computational efficiency and data efficiency in AI learning processes.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;DO YOU WANT WORK ON ARC with the MindsAI team (current ARC winners)?
MLST is sponsored by Tufa Labs:
Focus: ARC, LLMs, test-time-compute, active inference, system2 reasoning, and more.
Future plans: Expanding to complex environments like Warcraft 2 and Starcraft 2.
Interested? Apply for an ML research position: benjamin@&lt;a href=&quot;https://www.youtube.com/redirect?event=comments&amp;amp;redir_token=QUFFLUhqbEkwM05JYlJZM0FYdWdBVjFFRTdKbFc3ZEpLd3xBQ3Jtc0tta3NUdlhZWjBaVS1XV0NTbFZrWkd4d3U0QVg1cS1BTml1NnVzZHhrWVpDdWgyYmJOamY4dXlnSDg2VUUyclNQZ1h1aTlRYnl3WGdRenNjbm1ZTDI4NUN0eUl0bzVfWi1GcHU3dHZzUDVhMl93T0tBQQ&amp;amp;q=http%3A%2F%2Ftufa.ai%2F&quot; target=&quot;_blank&quot; rel=&quot;ugc noopener noreferrer&quot;&gt;tufa.ai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;1. Intelligence Measurement in AI Systems&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 Defining Intelligence in AI Systems&lt;/p&gt;
&lt;p&gt;[00:02:00] 1.2 Research at Santa Fe Institute&lt;/p&gt;
&lt;p&gt;[00:04:35] 1.3 Impact of Gaming on AI Development&lt;/p&gt;
&lt;p&gt;[00:05:10] 1.4 Comparing AI and Human Learning Efficiency&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. Efficient Skill Acquisition in AI&lt;/p&gt;
&lt;p&gt;[00:06:40] 2.1 Intelligence as Skill Acquisition Efficiency&lt;/p&gt;
&lt;p&gt;[00:08:25] 2.2 Limitations of Current AI Systems in Generalization&lt;/p&gt;
&lt;p&gt;[00:09:45] 2.3 Human vs. AI Cognitive Processes&lt;/p&gt;
&lt;p&gt;[00:10:40] 2.4 Measuring AI Intelligence: Chollet&apos;s ARC Challenge&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Program Synthesis and ARC Challenge&lt;/p&gt;
&lt;p&gt;[00:12:55] 3.1 Philosophical Foundations of Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:17:14] 3.2 Introduction to Program Induction and ARC Tasks&lt;/p&gt;
&lt;p&gt;[00:18:49] 3.3 DreamCoder: Principles and Techniques&lt;/p&gt;
&lt;p&gt;[00:27:55] 3.4 Trade-offs in Program Synthesis Search Strategies&lt;/p&gt;
&lt;p&gt;[00:31:52] 3.5 Neural Networks and Bayesian Program Learning&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Advanced Program Synthesis Techniques&lt;/p&gt;
&lt;p&gt;[00:32:30] 4.1 DreamCoder and Dream Decompiling Approach&lt;/p&gt;
&lt;p&gt;[00:39:00] 4.2 Beta Distribution and Caching in Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:45:10] 4.3 Performance and Limitations of Dream Decompiling&lt;/p&gt;
&lt;p&gt;[00:47:45] 4.4 Alessandro&apos;s Approach to ARC Challenge&lt;/p&gt;
&lt;p&gt;[00:51:12] 4.5 Conclusion and Future Discussions&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;Full reflist on YT VD, Show Notes and MP3 metadata&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Show Notes: https://www.dropbox.com/scl/fi/x50201tgqucj5ba2q4typ/Ale.pdf?rlkey=0ubvk7p5gtyx1gpownpdadim8&amp;amp;st=5pniu3nq&amp;amp;dl=0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:34</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1729354328206-43fb1e854e964.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[It's Not About Scale, It's About Abstraction - Francois Chollet]]></title>
			<description><![CDATA[<p>François Chollet discusses the limitations of Large Language Models (LLMs) and proposes a new approach to advancing artificial intelligence. He argues that current AI systems excel at pattern recognition but struggle with logical reasoning and true generalization. </p>
<p><br /></p>
<p>This was Chollet's keynote talk at AGI-24, filmed in high-quality. We will be releasing a full interview with him shortly. A teaser clip from that is played in the intro!</p>
<p><br /></p>
<p>Chollet introduces the Abstraction and Reasoning Corpus (ARC) as a benchmark for measuring AI progress towards human-like intelligence. He explains the concept of abstraction in AI systems and proposes combining deep learning with program synthesis to overcome current limitations. Chollet suggests that breakthroughs in AI might come from outside major tech labs and encourages researchers to explore new ideas in the pursuit of artificial general intelligence.</p>
<p><br /></p>
<p>TOC</p>
<p>1. LLM Limitations and Intelligence Concepts</p>
<p>[00:00:00] 1.1 LLM Limitations and Composition</p>
<p>[00:12:05] 1.2 Intelligence as Process vs. Skill</p>
<p>[00:17:15] 1.3 Generalization as Key to AI Progress</p>
<p><br /></p>
<p>2. ARC-AGI Benchmark and LLM Performance</p>
<p>[00:19:59] 2.1 Introduction to ARC-AGI Benchmark</p>
<p>[00:20:05] 2.2 Introduction to ARC-AGI and the ARC Prize</p>
<p>[00:23:35] 2.3 Performance of LLMs and Humans on ARC-AGI</p>
<p><br /></p>
<p>3. Abstraction in AI Systems</p>
<p>[00:26:10] 3.1 The Kaleidoscope Hypothesis and Abstraction Spectrum</p>
<p>[00:30:05] 3.2 LLM Capabilities and Limitations in Abstraction</p>
<p>[00:32:10] 3.3 Value-Centric vs Program-Centric Abstraction</p>
<p>[00:33:25] 3.4 Types of Abstraction in AI Systems</p>
<p><br /></p>
<p>4. Advancing AI: Combining Deep Learning and Program Synthesis</p>
<p>[00:34:05] 4.1 Limitations of Transformers and Need for Program Synthesis</p>
<p>[00:36:45] 4.2 Combining Deep Learning and Program Synthesis</p>
<p>[00:39:59] 4.3 Applying Combined Approaches to ARC Tasks</p>
<p>[00:44:20] 4.4 State-of-the-Art Solutions for ARC</p>
<p><br /></p>
<p>Shownotes (new!): https://www.dropbox.com/scl/fi/i7nsyoahuei6np95lbjxw/CholletKeynote.pdf?rlkey=t3502kbov5exsdxhderq70b9i&amp;st=1ca91ewz&amp;dl=0</p>
<p><br /></p>
<p>[0:01:15] Abstraction and Reasoning Corpus (ARC): AI benchmark (François Chollet)</p>
<p>https://arxiv.org/abs/1911.01547</p>
<p><br /></p>
<p>[0:05:30] Monty Hall problem: Probability puzzle (Steve Selvin)</p>
<p>https://www.tandfonline.com/doi/abs/10.1080/00031305.1975.10479121</p>
<p><br /></p>
<p>[0:06:20] LLM training dynamics analysis (Tirumala et al.)</p>
<p>https://arxiv.org/abs/2205.10770</p>
<p><br /></p>
<p>[0:10:20] Transformer limitations on compositionality (Dziri et al.)</p>
<p>https://arxiv.org/abs/2305.18654</p>
<p><br /></p>
<p>[0:10:25] Reversal Curse in LLMs (Berglund et al.)</p>
<p>https://arxiv.org/abs/2309.12288</p>
<p><br /></p>
<p>[0:19:25] Measure of intelligence using algorithmic information theory (François Chollet)</p>
<p>https://arxiv.org/abs/1911.01547</p>
<p><br /></p>
<p>[0:20:10] ARC-AGI: GitHub repository (François Chollet)</p>
<p>https://github.com/fchollet/ARC-AGI</p>
<p><br /></p>
<p>[0:22:15] ARC Prize: $1,000,000+ competition (François Chollet)</p>
<p>https://arcprize.org/</p>
<p><br /></p>
<p>[0:33:30] System 1 and System 2 thinking (Daniel Kahneman)</p>
<p>https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555</p>
<p><br /></p>
<p>[0:34:00] Core knowledge in infants (Elizabeth Spelke)</p>
<p>https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf</p>
<p><br /></p>
<p>[0:34:30] Embedding interpretive spaces in ML (Tennenholtz et al.)</p>
<p>https://arxiv.org/abs/2310.04475</p>
<p><br /></p>
<p>[0:44:20] Hypothesis Search with LLMs for ARC (Wang et al.)</p>
<p>https://arxiv.org/abs/2309.05660</p>
<p><br /></p>
<p>[0:44:50] Ryan Greenblatt's high score on ARC public leaderboard</p>
<p>https://arcprize.org/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Its-Not-About-Scale--Its-About-Abstraction---Francois-Chollet-e2pj27h</link>
			<guid isPermaLink="false">9618ef53-29de-4be6-b005-db97e761b4bc</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 12 Oct 2024 21:21:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/92948145/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-12%2Fab6fafeb-0931-545a-05f7-a60b471139a2.mp3" length="67260965" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;François Chollet discusses the limitations of Large Language Models (LLMs) and proposes a new approach to advancing artificial intelligence. He argues that current AI systems excel at pattern recognition but struggle with logical reasoning and true generalization. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;This was Chollet&apos;s keynote talk at AGI-24, filmed in high-quality. We will be releasing a full interview with him shortly. A teaser clip from that is played in the intro!&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Chollet introduces the Abstraction and Reasoning Corpus (ARC) as a benchmark for measuring AI progress towards human-like intelligence. He explains the concept of abstraction in AI systems and proposes combining deep learning with program synthesis to overcome current limitations. Chollet suggests that breakthroughs in AI might come from outside major tech labs and encourages researchers to explore new ideas in the pursuit of artificial general intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;1. LLM Limitations and Intelligence Concepts&lt;/p&gt;
&lt;p&gt;[00:00:00] 1.1 LLM Limitations and Composition&lt;/p&gt;
&lt;p&gt;[00:12:05] 1.2 Intelligence as Process vs. Skill&lt;/p&gt;
&lt;p&gt;[00:17:15] 1.3 Generalization as Key to AI Progress&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2. ARC-AGI Benchmark and LLM Performance&lt;/p&gt;
&lt;p&gt;[00:19:59] 2.1 Introduction to ARC-AGI Benchmark&lt;/p&gt;
&lt;p&gt;[00:20:05] 2.2 Introduction to ARC-AGI and the ARC Prize&lt;/p&gt;
&lt;p&gt;[00:23:35] 2.3 Performance of LLMs and Humans on ARC-AGI&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3. Abstraction in AI Systems&lt;/p&gt;
&lt;p&gt;[00:26:10] 3.1 The Kaleidoscope Hypothesis and Abstraction Spectrum&lt;/p&gt;
&lt;p&gt;[00:30:05] 3.2 LLM Capabilities and Limitations in Abstraction&lt;/p&gt;
&lt;p&gt;[00:32:10] 3.3 Value-Centric vs Program-Centric Abstraction&lt;/p&gt;
&lt;p&gt;[00:33:25] 3.4 Types of Abstraction in AI Systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;4. Advancing AI: Combining Deep Learning and Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:34:05] 4.1 Limitations of Transformers and Need for Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:36:45] 4.2 Combining Deep Learning and Program Synthesis&lt;/p&gt;
&lt;p&gt;[00:39:59] 4.3 Applying Combined Approaches to ARC Tasks&lt;/p&gt;
&lt;p&gt;[00:44:20] 4.4 State-of-the-Art Solutions for ARC&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Shownotes (new!): https://www.dropbox.com/scl/fi/i7nsyoahuei6np95lbjxw/CholletKeynote.pdf?rlkey=t3502kbov5exsdxhderq70b9i&amp;amp;st=1ca91ewz&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:01:15] Abstraction and Reasoning Corpus (ARC): AI benchmark (François Chollet)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:05:30] Monty Hall problem: Probability puzzle (Steve Selvin)&lt;/p&gt;
&lt;p&gt;https://www.tandfonline.com/doi/abs/10.1080/00031305.1975.10479121&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:06:20] LLM training dynamics analysis (Tirumala et al.)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2205.10770&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:10:20] Transformer limitations on compositionality (Dziri et al.)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2305.18654&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:10:25] Reversal Curse in LLMs (Berglund et al.)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2309.12288&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:19:25] Measure of intelligence using algorithmic information theory (François Chollet)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:20:10] ARC-AGI: GitHub repository (François Chollet)&lt;/p&gt;
&lt;p&gt;https://github.com/fchollet/ARC-AGI&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:22:15] ARC Prize: $1,000,000+ competition (François Chollet)&lt;/p&gt;
&lt;p&gt;https://arcprize.org/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:33:30] System 1 and System 2 thinking (Daniel Kahneman)&lt;/p&gt;
&lt;p&gt;https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:34:00] Core knowledge in infants (Elizabeth Spelke)&lt;/p&gt;
&lt;p&gt;https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:34:30] Embedding interpretive spaces in ML (Tennenholtz et al.)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2310.04475&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:44:20] Hypothesis Search with LLMs for ARC (Wang et al.)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2309.05660&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;[0:44:50] Ryan Greenblatt&apos;s high score on ARC public leaderboard&lt;/p&gt;
&lt;p&gt;https://arcprize.org/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:46:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1728768060569-7850a1195467c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Bold AI Predictions From Cohere Co-founder]]></title>
			<description><![CDATA[<p>Ivan Zhang, co-founder of Cohere, discusses the company's enterprise-focused AI solutions. He explains Cohere's early emphasis on embedding technology and training models for secure environments. </p>
<p><br /></p>
<p>Zhang highlights their implementation of Retrieval-Augmented Generation in healthcare, significantly reducing doctor preparation time. He explores the shift from monolithic AI models to heterogeneous systems and the importance of improving various AI system components. Zhang shares insights on using synthetic data to teach models reasoning, the democratization of software development through AI, and how his gaming skills transfer to running an AI company. </p>
<p><br /></p>
<p>He advises young developers to fully embrace AI technologies and offers perspectives on AI reliability, potential risks, and future model architectures.</p>
<p><br /></p>
<p>https://cohere.com/</p>
<p>https://ivanzhang.ca/</p>
<p>https://x.com/1vnzh</p>
<p><br /></p>
<p>TOC:</p>
<p>00:00:00 Intro</p>
<p>00:03:20 AI &amp; Language Model Evolution</p>
<p>00:06:09 Future AI Apps &amp; Development</p>
<p>00:09:29 Impact on Software Dev Practices</p>
<p>00:13:03 Philosophical &amp; Societal Implications</p>
<p>00:16:30 Compute Efficiency &amp; RAG</p>
<p>00:20:39 Adoption Challenges &amp; Solutions</p>
<p>00:22:30 GPU Optimization &amp; Kubernetes Limits</p>
<p>00:24:16 Cohere's Implementation Approach</p>
<p>00:28:13 Gaming's Professional Influence</p>
<p>00:34:45 Transformer Optimizations</p>
<p>00:36:45 Future Models &amp; System-Level Focus</p>
<p>00:39:20 Inference-Time Computation &amp; Reasoning</p>
<p>00:42:05 Capturing Human Thought in AI</p>
<p>00:43:15 Research, Hiring &amp; Developer Advice</p>
<p><br /></p>
<p>REFS:</p>
<p>00:02:31 Cohere, https://cohere.com/</p>
<p>00:02:40 The Transformer architecture, https://arxiv.org/abs/1706.03762</p>
<p>00:03:22 The Innovator's Dilemma, https://www.amazon.com/Innovators-Dilemma-Technologies-Management-Innovation/dp/1633691780</p>
<p>00:09:15 The actor model, https://en.wikipedia.org/wiki/Actor_model</p>
<p>00:14:35 John Searle's Chinese Room Argument, https://plato.stanford.edu/entries/chinese-room/</p>
<p>00:18:00 Retrieval-Augmented Generation, https://arxiv.org/abs/2005.11401</p>
<p>00:18:40 Retrieval-Augmented Generation, https://docs.cohere.com/v2/docs/retrieval-augmented-generation-rag</p>
<p>00:35:39 Let’s Verify Step by Step, https://arxiv.org/pdf/2305.20050</p>
<p>00:39:20 Adaptive Inference-Time Compute, https://arxiv.org/abs/2410.02725</p>
<p>00:43:20 Ryan Greenblatt ARC entry, https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt</p>
<p><br /></p>
<p>Disclaimer: This show is part of our Cohere partnership series</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Bold-AI-Predictions-From-Cohere-Co-founder-e2pflom</link>
			<guid isPermaLink="false">5de03151-2c1d-4ce3-abf2-cf5d8ade4451</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 10 Oct 2024 13:07:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/92837078/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-10%2F088bb22b-caaa-75a8-cd18-6843ddfadf5a.mp3" length="68397149" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Ivan Zhang, co-founder of Cohere, discusses the company&apos;s enterprise-focused AI solutions. He explains Cohere&apos;s early emphasis on embedding technology and training models for secure environments. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Zhang highlights their implementation of Retrieval-Augmented Generation in healthcare, significantly reducing doctor preparation time. He explores the shift from monolithic AI models to heterogeneous systems and the importance of improving various AI system components. Zhang shares insights on using synthetic data to teach models reasoning, the democratization of software development through AI, and how his gaming skills transfer to running an AI company. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;He advises young developers to fully embrace AI technologies and offers perspectives on AI reliability, potential risks, and future model architectures.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;https://cohere.com/&lt;/p&gt;
&lt;p&gt;https://ivanzhang.ca/&lt;/p&gt;
&lt;p&gt;https://x.com/1vnzh&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:03:20 AI &amp;amp; Language Model Evolution&lt;/p&gt;
&lt;p&gt;00:06:09 Future AI Apps &amp;amp; Development&lt;/p&gt;
&lt;p&gt;00:09:29 Impact on Software Dev Practices&lt;/p&gt;
&lt;p&gt;00:13:03 Philosophical &amp;amp; Societal Implications&lt;/p&gt;
&lt;p&gt;00:16:30 Compute Efficiency &amp;amp; RAG&lt;/p&gt;
&lt;p&gt;00:20:39 Adoption Challenges &amp;amp; Solutions&lt;/p&gt;
&lt;p&gt;00:22:30 GPU Optimization &amp;amp; Kubernetes Limits&lt;/p&gt;
&lt;p&gt;00:24:16 Cohere&apos;s Implementation Approach&lt;/p&gt;
&lt;p&gt;00:28:13 Gaming&apos;s Professional Influence&lt;/p&gt;
&lt;p&gt;00:34:45 Transformer Optimizations&lt;/p&gt;
&lt;p&gt;00:36:45 Future Models &amp;amp; System-Level Focus&lt;/p&gt;
&lt;p&gt;00:39:20 Inference-Time Computation &amp;amp; Reasoning&lt;/p&gt;
&lt;p&gt;00:42:05 Capturing Human Thought in AI&lt;/p&gt;
&lt;p&gt;00:43:15 Research, Hiring &amp;amp; Developer Advice&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;00:02:31 Cohere, https://cohere.com/&lt;/p&gt;
&lt;p&gt;00:02:40 The Transformer architecture, https://arxiv.org/abs/1706.03762&lt;/p&gt;
&lt;p&gt;00:03:22 The Innovator&apos;s Dilemma, https://www.amazon.com/Innovators-Dilemma-Technologies-Management-Innovation/dp/1633691780&lt;/p&gt;
&lt;p&gt;00:09:15 The actor model, https://en.wikipedia.org/wiki/Actor_model&lt;/p&gt;
&lt;p&gt;00:14:35 John Searle&apos;s Chinese Room Argument, https://plato.stanford.edu/entries/chinese-room/&lt;/p&gt;
&lt;p&gt;00:18:00 Retrieval-Augmented Generation, https://arxiv.org/abs/2005.11401&lt;/p&gt;
&lt;p&gt;00:18:40 Retrieval-Augmented Generation, https://docs.cohere.com/v2/docs/retrieval-augmented-generation-rag&lt;/p&gt;
&lt;p&gt;00:35:39 Let’s Verify Step by Step, https://arxiv.org/pdf/2305.20050&lt;/p&gt;
&lt;p&gt;00:39:20 Adaptive Inference-Time Compute, https://arxiv.org/abs/2410.02725&lt;/p&gt;
&lt;p&gt;00:43:20 Ryan Greenblatt ARC entry, https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This show is part of our Cohere partnership series&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:47:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1728565625852-f966426753512.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Open-Ended AI: The Key to Superhuman Intelligence? - Prof. Tim Rocktäschel]]></title>
			<description><![CDATA[<p>Prof. Tim Rocktäschel, AI researcher at UCL and Google DeepMind, talks about open-ended AI systems. These systems aim to keep learning and improving on their own, like evolution does in nature. </p>
<p><br /></p>
<p>Ad: Are you a hardcore ML engineer who wants to work for Daniel Cahn at SlingshotAI building AI for mental health? Give him an email! - danielc@<a href="https://www.youtube.com/redirect?event=comments&amp;redir_token=QUFFLUhqbF83NzZXUlExeHBOamd0NVdPTEQ2ajJyQjBhZ3xBQ3Jtc0ttSE9rdEdnZkNOWGx5MzV3QjNRZnRhNnJ6ZjRWVzhQVWg0U1RoYUcwUG9mZ25ITVk3eThZVnl0NHN6cjFHT19lSFpLZktHMnVxRmdjVTZjT3dTRVZBbWFPYXhXMWJHRWsxNjNsMFJXNnJ2T3YtNTJuZw&amp;q=http%3A%2F%2Fslingshot.xyz%2F" target="_blank" rel="ugc noopener noreferrer">slingshot.xyz</a></p>
<p><br /></p>
<p>TOC:</p>
<p>00:00:00 Introduction to Open-Ended AI and Key Concepts</p>
<p>00:01:37 Tim Rocktäschel's Background and Research Focus</p>
<p>00:06:25 Defining Open-Endedness in AI Systems</p>
<p>00:10:39 Subjective Nature of Interestingness and Learnability</p>
<p>00:16:22 Open-Endedness in Practice: Examples and Limitations</p>
<p>00:17:50 Assessing Novelty in Open-ended AI Systems</p>
<p>00:20:05 Adversarial Attacks and AI Robustness</p>
<p>00:24:05 Rainbow Teaming and LLM Safety</p>
<p>00:25:48 Open-ended Research Approaches in AI</p>
<p>00:29:05 Balancing Long-term Vision and Exploration in AI Research</p>
<p>00:37:25 LLMs in Program Synthesis and Open-Ended Learning</p>
<p>00:37:55 Transition from Human-Based to Novel AI Strategies</p>
<p>00:39:00 Expanding Context Windows and Prompt Evolution</p>
<p>00:40:17 AI Intelligibility and Human-AI Interfaces</p>
<p>00:46:04 Self-Improvement and Evolution in AI Systems</p>
<p><br /></p>
<p>Show notes (New!) https://www.dropbox.com/scl/fi/5avpsyz8jbn4j1az7kevs/TimR.pdf?rlkey=pqjlcqbtm3undp4udtgfmie8n&amp;st=x50u1d1m&amp;dl=0</p>
<p><br /></p>
<p>REFS:</p>
<p>00:01:47 - UCL DARK Lab (Rocktäschel) - AI research lab focusing on RL and open-ended learning - https://ucldark.com/</p>
<p><br /></p>
<p>00:02:31 - GENIE (Bruce) - Generative interactive environment from unlabelled videos - https://arxiv.org/abs/2402.15391</p>
<p><br /></p>
<p>00:02:42 - Promptbreeder (Fernando) - Self-referential LLM prompt evolution - https://arxiv.org/abs/2309.16797</p>
<p><br /></p>
<p>00:03:05 - Picbreeder (Secretan) - Collaborative online image evolution - https://dl.acm.org/doi/10.1145/1357054.1357328</p>
<p><br /></p>
<p>00:03:14 - Why Greatness Cannot Be Planned (Stanley) - Book on open-ended exploration - https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237</p>
<p><br /></p>
<p>00:04:36 - NetHack Learning Environment (Küttler) - RL research in procedurally generated game - https://arxiv.org/abs/2006.13760</p>
<p><br /></p>
<p>00:07:35 - Open-ended learning (Clune) - AI systems for continual learning and adaptation - https://arxiv.org/abs/1905.10985</p>
<p><br /></p>
<p>00:07:35 - OMNI (Zhang) - LLMs modeling human interestingness for exploration - https://arxiv.org/abs/2306.01711</p>
<p><br /></p>
<p>00:10:42 - Observer theory (Wolfram) - Computationally bounded observers in complex systems - https://writings.stephenwolfram.com/2023/12/observer-theory/</p>
<p><br /></p>
<p>00:15:25 - Human-Timescale Adaptation (Rocktäschel) - RL agent adapting to novel 3D tasks - https://arxiv.org/abs/2301.07608</p>
<p><br /></p>
<p>00:16:15 - Open-Endedness for AGI (Hughes) - Importance of open-ended learning for AGI - https://arxiv.org/abs/2406.04268</p>
<p><br /></p>
<p>00:16:35 - POET algorithm (Wang) - Open-ended approach to generate and solve challenges - https://arxiv.org/abs/1901.01753</p>
<p><br /></p>
<p>00:17:20 - AlphaGo (Silver) - AI mastering the game of Go - https://deepmind.google/technologies/alphago/</p>
<p><br /></p>
<p>00:20:35 - Adversarial Go attacks (Dennis) - Exploiting weaknesses in Go AI systems - https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1630.pdf</p>
<p><br /></p>
<p>00:22:00 - Levels of AGI (Morris) - Framework for categorizing AGI progress - https://arxiv.org/abs/2311.02462</p>
<p><br /></p>
<p>00:24:30 - Rainbow Teaming (Samvelyan) - LLM-based adversarial prompt generation - https://arxiv.org/abs/2402.16822</p>
<p><br /></p>
<p>00:25:50 - Why Greatness Cannot Be Planned (Stanley) - 'False compass' and 'stepping stone collection' concepts - https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237</p>
<p><br /></p>
<p>00:27:45 - AI Debate (Khan) - Improving LLM truthfulness through debate - https://proceedings.mlr.press/v235/khan24a.html</p>
<p><br /></p>
<p>00:29:40 - Gemini (Google DeepMind) - Advanced multimodal AI model - https://deepmind.google/technologies/gemini/</p>
<p><br /></p>
<p>00:30:15 - How to Take Smart Notes (Ahrens) - Effective note-taking methodology - https://www.amazon.com/How-Take-Smart-Notes-Nonfiction/dp/1542866502</p>
<p><br /></p>
<p>(truncated, see shownotes)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Open-Ended-AI-The-Key-to-Superhuman-Intelligence----Prof--Tim-Rocktschel-e2p8drk</link>
			<guid isPermaLink="false">9e708e15-c57a-4ed8-89e2-430e4d2d3182</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 04 Oct 2024 22:46:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/92599604/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-4%2F56819f16-9f8f-20db-3672-b2a59ad48b08.mp3" length="79734108" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof. Tim Rocktäschel, AI researcher at UCL and Google DeepMind, talks about open-ended AI systems. These systems aim to keep learning and improving on their own, like evolution does in nature. &lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Ad: Are you a hardcore ML engineer who wants to work for Daniel Cahn at SlingshotAI building AI for mental health? Give him an email! - danielc@&lt;a href=&quot;https://www.youtube.com/redirect?event=comments&amp;amp;redir_token=QUFFLUhqbF83NzZXUlExeHBOamd0NVdPTEQ2ajJyQjBhZ3xBQ3Jtc0ttSE9rdEdnZkNOWGx5MzV3QjNRZnRhNnJ6ZjRWVzhQVWg0U1RoYUcwUG9mZ25ITVk3eThZVnl0NHN6cjFHT19lSFpLZktHMnVxRmdjVTZjT3dTRVZBbWFPYXhXMWJHRWsxNjNsMFJXNnJ2T3YtNTJuZw&amp;amp;q=http%3A%2F%2Fslingshot.xyz%2F&quot; target=&quot;_blank&quot; rel=&quot;ugc noopener noreferrer&quot;&gt;slingshot.xyz&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction to Open-Ended AI and Key Concepts&lt;/p&gt;
&lt;p&gt;00:01:37 Tim Rocktäschel&apos;s Background and Research Focus&lt;/p&gt;
&lt;p&gt;00:06:25 Defining Open-Endedness in AI Systems&lt;/p&gt;
&lt;p&gt;00:10:39 Subjective Nature of Interestingness and Learnability&lt;/p&gt;
&lt;p&gt;00:16:22 Open-Endedness in Practice: Examples and Limitations&lt;/p&gt;
&lt;p&gt;00:17:50 Assessing Novelty in Open-ended AI Systems&lt;/p&gt;
&lt;p&gt;00:20:05 Adversarial Attacks and AI Robustness&lt;/p&gt;
&lt;p&gt;00:24:05 Rainbow Teaming and LLM Safety&lt;/p&gt;
&lt;p&gt;00:25:48 Open-ended Research Approaches in AI&lt;/p&gt;
&lt;p&gt;00:29:05 Balancing Long-term Vision and Exploration in AI Research&lt;/p&gt;
&lt;p&gt;00:37:25 LLMs in Program Synthesis and Open-Ended Learning&lt;/p&gt;
&lt;p&gt;00:37:55 Transition from Human-Based to Novel AI Strategies&lt;/p&gt;
&lt;p&gt;00:39:00 Expanding Context Windows and Prompt Evolution&lt;/p&gt;
&lt;p&gt;00:40:17 AI Intelligibility and Human-AI Interfaces&lt;/p&gt;
&lt;p&gt;00:46:04 Self-Improvement and Evolution in AI Systems&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Show notes (New!) https://www.dropbox.com/scl/fi/5avpsyz8jbn4j1az7kevs/TimR.pdf?rlkey=pqjlcqbtm3undp4udtgfmie8n&amp;amp;st=x50u1d1m&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;00:01:47 - UCL DARK Lab (Rocktäschel) - AI research lab focusing on RL and open-ended learning - https://ucldark.com/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:02:31 - GENIE (Bruce) - Generative interactive environment from unlabelled videos - https://arxiv.org/abs/2402.15391&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:02:42 - Promptbreeder (Fernando) - Self-referential LLM prompt evolution - https://arxiv.org/abs/2309.16797&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:03:05 - Picbreeder (Secretan) - Collaborative online image evolution - https://dl.acm.org/doi/10.1145/1357054.1357328&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:03:14 - Why Greatness Cannot Be Planned (Stanley) - Book on open-ended exploration - https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:04:36 - NetHack Learning Environment (Küttler) - RL research in procedurally generated game - https://arxiv.org/abs/2006.13760&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:07:35 - Open-ended learning (Clune) - AI systems for continual learning and adaptation - https://arxiv.org/abs/1905.10985&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:07:35 - OMNI (Zhang) - LLMs modeling human interestingness for exploration - https://arxiv.org/abs/2306.01711&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:10:42 - Observer theory (Wolfram) - Computationally bounded observers in complex systems - https://writings.stephenwolfram.com/2023/12/observer-theory/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:15:25 - Human-Timescale Adaptation (Rocktäschel) - RL agent adapting to novel 3D tasks - https://arxiv.org/abs/2301.07608&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:16:15 - Open-Endedness for AGI (Hughes) - Importance of open-ended learning for AGI - https://arxiv.org/abs/2406.04268&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:16:35 - POET algorithm (Wang) - Open-ended approach to generate and solve challenges - https://arxiv.org/abs/1901.01753&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:17:20 - AlphaGo (Silver) - AI mastering the game of Go - https://deepmind.google/technologies/alphago/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:20:35 - Adversarial Go attacks (Dennis) - Exploiting weaknesses in Go AI systems - https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p1630.pdf&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:22:00 - Levels of AGI (Morris) - Framework for categorizing AGI progress - https://arxiv.org/abs/2311.02462&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:24:30 - Rainbow Teaming (Samvelyan) - LLM-based adversarial prompt generation - https://arxiv.org/abs/2402.16822&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:25:50 - Why Greatness Cannot Be Planned (Stanley) - &apos;False compass&apos; and &apos;stepping stone collection&apos; concepts - https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:27:45 - AI Debate (Khan) - Improving LLM truthfulness through debate - https://proceedings.mlr.press/v235/khan24a.html&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:29:40 - Gemini (Google DeepMind) - Advanced multimodal AI model - https://deepmind.google/technologies/gemini/&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;00:30:15 - How to Take Smart Notes (Ahrens) - Effective note-taking methodology - https://www.amazon.com/How-Take-Smart-Notes-Nonfiction/dp/1542866502&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;(truncated, see shownotes)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1728081883606-32af806e2719b.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Ben Goertzel on "Superintelligence"]]></title>
			<description><![CDATA[<p>Ben Goertzel discusses AGI development, transhumanism, and the potential societal impacts of superintelligent AI. He predicts human-level AGI by 2029 and argues that the transition to superintelligence could happen within a few years after. Goertzel explores the challenges of AI regulation, the limitations of current language models, and the need for neuro-symbolic approaches in AGI research. He also addresses concerns about resource allocation and cultural perspectives on transhumanism.</p>
<p><br /></p>
<p>TOC:</p>
<p>[00:00:00] AGI Timeline Predictions and Development Speed</p>
<p>[00:00:45] Limitations of Language Models in AGI Development</p>
<p>[00:02:18] Current State and Trends in AI Research and Development</p>
<p>[00:09:02] Emergent Reasoning Capabilities and Limitations of LLMs</p>
<p>[00:18:15] Neuro-Symbolic Approaches and the Future of AI Systems</p>
<p>[00:20:00] Evolutionary Algorithms and LLMs in Creative Tasks</p>
<p>[00:21:25] Symbolic vs. Sub-Symbolic Approaches in AI</p>
<p>[00:28:05] Language as Internal Thought and External Communication</p>
<p>[00:30:20] AGI Development and Goal-Directed Behavior</p>
<p>[00:35:51] Consciousness and AI: Expanding States of Experience</p>
<p>[00:48:50] AI Regulation: Challenges and Approaches</p>
<p>[00:55:35] Challenges in AI Regulation</p>
<p>[00:59:20] AI Alignment and Ethical Considerations</p>
<p>[01:09:15] AGI Development Timeline Predictions</p>
<p>[01:12:40] OpenCog Hyperon and AGI Progress</p>
<p>[01:17:48] Transhumanism and Resource Allocation Debate</p>
<p>[01:20:12] Cultural Perspectives on Transhumanism</p>
<p>[01:23:54] AGI and Post-Scarcity Society</p>
<p>[01:31:35] Challenges and Implications of AGI Development</p>
<p><br /></p>
<p>New! PDF Show notes: https://www.dropbox.com/scl/fi/fyetzwgoaf70gpovyfc4x/BenGoertzel.pdf?rlkey=pze5dt9vgf01tf2wip32p5hk5&amp;st=svbcofm3&amp;dl=0</p>
<p><br /></p>
<p>Refs:</p>
<p>00:00:15 Ray Kurzweil's AGI timeline prediction, Ray Kurzweil, https://en.wikipedia.org/wiki/Technological_singularity</p>
<p>00:01:45 Ben Goertzel: SingularityNET founder, Ben Goertzel, https://singularitynet.io/</p>
<p>00:02:35 AGI Conference series, AGI Conference Organizers, https://agi-conf.org/2024/</p>
<p>00:03:55 Ben Goertzel's contributions to AGI, Wikipedia contributors, https://en.wikipedia.org/wiki/Ben_Goertzel</p>
<p>00:11:05 Chain-of-Thought prompting, Subbarao Kambhampati, https://arxiv.org/abs/2405.04776</p>
<p>00:11:35 Algorithmic information content, Pieter Adriaans, https://plato.stanford.edu/entries/information-entropy/</p>
<p>00:12:10 Turing completeness in neural networks, Various contributors, https://plato.stanford.edu/entries/turing-machine/</p>
<p>00:16:15 AlphaGeometry: AI for geometry problems, Trieu, Li, et al., https://www.nature.com/articles/s41586-023-06747-5</p>
<p>00:18:25 Shane Legg and Ben Goertzel's collaboration, Shane Legg, https://en.wikipedia.org/wiki/Shane_Legg</p>
<p>00:20:00 Evolutionary algorithms in music generation, Yanxu Chen, https://arxiv.org/html/2409.03715v1</p>
<p>00:22:00 Peirce's theory of semiotics, Charles Sanders Peirce, https://plato.stanford.edu/entries/peirce-semiotics/</p>
<p>00:28:10 Chomsky's view on language, Noam Chomsky, https://chomsky.info/1983____/</p>
<p>00:34:05 Greg Egan's 'Diaspora', Greg Egan, https://www.amazon.co.uk/Diaspora-post-apocalyptic-thriller-perfect-MIRROR/dp/0575082097</p>
<p>00:40:35 'The Consciousness Explosion', Ben Goertzel &amp; Gabriel Axel Montes, https://www.amazon.com/Consciousness-Explosion-Technological-Experiential-Singularity/dp/B0D8C7QYZD</p>
<p>00:41:55 Ray Kurzweil's books on singularity, Ray Kurzweil, https://www.amazon.com/Singularity-Near-Humans-Transcend-Biology/dp/0143037889</p>
<p>00:50:50 California AI regulation bills, California State Senate, https://sd18.senate.ca.gov/news/senate-unanimously-approves-senator-padillas-artificial-intelligence-package</p>
<p>00:56:40 Limitations of Compute Thresholds, Sara Hooker, https://arxiv.org/abs/2407.05694</p>
<p>00:56:55 'Taming Silicon Valley', Gary F. Marcus, https://www.penguinrandomhouse.com/books/768076/taming-silicon-valley-by-gary-f-marcus/</p>
<p>01:09:15 Kurzweil's AGI prediction update, Ray Kurzweil, https://www.theguardian.com/technology/article/2024/jun/29/ray-kurzweil-google-ai-the-singularity-is-nearer</p>
<p></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Ben-Goertzel-on-Superintelligence-e2p490r</link>
			<guid isPermaLink="false">827f78b7-e597-44dd-b503-057ddb46e264</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 01 Oct 2024 23:32:57 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/92463579/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-9-1%2Fdf396abc-ceb7-a419-f796-86b772be29b2.mp3" length="140373212" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Ben Goertzel discusses AGI development, transhumanism, and the potential societal impacts of superintelligent AI. He predicts human-level AGI by 2029 and argues that the transition to superintelligence could happen within a few years after. Goertzel explores the challenges of AI regulation, the limitations of current language models, and the need for neuro-symbolic approaches in AGI research. He also addresses concerns about resource allocation and cultural perspectives on transhumanism.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] AGI Timeline Predictions and Development Speed&lt;/p&gt;
&lt;p&gt;[00:00:45] Limitations of Language Models in AGI Development&lt;/p&gt;
&lt;p&gt;[00:02:18] Current State and Trends in AI Research and Development&lt;/p&gt;
&lt;p&gt;[00:09:02] Emergent Reasoning Capabilities and Limitations of LLMs&lt;/p&gt;
&lt;p&gt;[00:18:15] Neuro-Symbolic Approaches and the Future of AI Systems&lt;/p&gt;
&lt;p&gt;[00:20:00] Evolutionary Algorithms and LLMs in Creative Tasks&lt;/p&gt;
&lt;p&gt;[00:21:25] Symbolic vs. Sub-Symbolic Approaches in AI&lt;/p&gt;
&lt;p&gt;[00:28:05] Language as Internal Thought and External Communication&lt;/p&gt;
&lt;p&gt;[00:30:20] AGI Development and Goal-Directed Behavior&lt;/p&gt;
&lt;p&gt;[00:35:51] Consciousness and AI: Expanding States of Experience&lt;/p&gt;
&lt;p&gt;[00:48:50] AI Regulation: Challenges and Approaches&lt;/p&gt;
&lt;p&gt;[00:55:35] Challenges in AI Regulation&lt;/p&gt;
&lt;p&gt;[00:59:20] AI Alignment and Ethical Considerations&lt;/p&gt;
&lt;p&gt;[01:09:15] AGI Development Timeline Predictions&lt;/p&gt;
&lt;p&gt;[01:12:40] OpenCog Hyperon and AGI Progress&lt;/p&gt;
&lt;p&gt;[01:17:48] Transhumanism and Resource Allocation Debate&lt;/p&gt;
&lt;p&gt;[01:20:12] Cultural Perspectives on Transhumanism&lt;/p&gt;
&lt;p&gt;[01:23:54] AGI and Post-Scarcity Society&lt;/p&gt;
&lt;p&gt;[01:31:35] Challenges and Implications of AGI Development&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;New! PDF Show notes: https://www.dropbox.com/scl/fi/fyetzwgoaf70gpovyfc4x/BenGoertzel.pdf?rlkey=pze5dt9vgf01tf2wip32p5hk5&amp;amp;st=svbcofm3&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;00:00:15 Ray Kurzweil&apos;s AGI timeline prediction, Ray Kurzweil, https://en.wikipedia.org/wiki/Technological_singularity&lt;/p&gt;
&lt;p&gt;00:01:45 Ben Goertzel: SingularityNET founder, Ben Goertzel, https://singularitynet.io/&lt;/p&gt;
&lt;p&gt;00:02:35 AGI Conference series, AGI Conference Organizers, https://agi-conf.org/2024/&lt;/p&gt;
&lt;p&gt;00:03:55 Ben Goertzel&apos;s contributions to AGI, Wikipedia contributors, https://en.wikipedia.org/wiki/Ben_Goertzel&lt;/p&gt;
&lt;p&gt;00:11:05 Chain-of-Thought prompting, Subbarao Kambhampati, https://arxiv.org/abs/2405.04776&lt;/p&gt;
&lt;p&gt;00:11:35 Algorithmic information content, Pieter Adriaans, https://plato.stanford.edu/entries/information-entropy/&lt;/p&gt;
&lt;p&gt;00:12:10 Turing completeness in neural networks, Various contributors, https://plato.stanford.edu/entries/turing-machine/&lt;/p&gt;
&lt;p&gt;00:16:15 AlphaGeometry: AI for geometry problems, Trieu, Li, et al., https://www.nature.com/articles/s41586-023-06747-5&lt;/p&gt;
&lt;p&gt;00:18:25 Shane Legg and Ben Goertzel&apos;s collaboration, Shane Legg, https://en.wikipedia.org/wiki/Shane_Legg&lt;/p&gt;
&lt;p&gt;00:20:00 Evolutionary algorithms in music generation, Yanxu Chen, https://arxiv.org/html/2409.03715v1&lt;/p&gt;
&lt;p&gt;00:22:00 Peirce&apos;s theory of semiotics, Charles Sanders Peirce, https://plato.stanford.edu/entries/peirce-semiotics/&lt;/p&gt;
&lt;p&gt;00:28:10 Chomsky&apos;s view on language, Noam Chomsky, https://chomsky.info/1983____/&lt;/p&gt;
&lt;p&gt;00:34:05 Greg Egan&apos;s &apos;Diaspora&apos;, Greg Egan, https://www.amazon.co.uk/Diaspora-post-apocalyptic-thriller-perfect-MIRROR/dp/0575082097&lt;/p&gt;
&lt;p&gt;00:40:35 &apos;The Consciousness Explosion&apos;, Ben Goertzel &amp;amp; Gabriel Axel Montes, https://www.amazon.com/Consciousness-Explosion-Technological-Experiential-Singularity/dp/B0D8C7QYZD&lt;/p&gt;
&lt;p&gt;00:41:55 Ray Kurzweil&apos;s books on singularity, Ray Kurzweil, https://www.amazon.com/Singularity-Near-Humans-Transcend-Biology/dp/0143037889&lt;/p&gt;
&lt;p&gt;00:50:50 California AI regulation bills, California State Senate, https://sd18.senate.ca.gov/news/senate-unanimously-approves-senator-padillas-artificial-intelligence-package&lt;/p&gt;
&lt;p&gt;00:56:40 Limitations of Compute Thresholds, Sara Hooker, https://arxiv.org/abs/2407.05694&lt;/p&gt;
&lt;p&gt;00:56:55 &apos;Taming Silicon Valley&apos;, Gary F. Marcus, https://www.penguinrandomhouse.com/books/768076/taming-silicon-valley-by-gary-f-marcus/&lt;/p&gt;
&lt;p&gt;01:09:15 Kurzweil&apos;s AGI prediction update, Ray Kurzweil, https://www.theguardian.com/technology/article/2024/jun/29/ray-kurzweil-google-ai-the-singularity-is-nearer&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:37:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1727825558247-02f9631873721.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Taming Silicon Valley - Prof. Gary Marcus]]></title>
			<description><![CDATA[<p>AI expert Prof. Gary Marcus doesn&#39;t mince words about today&#39;s artificial intelligence. He argues that despite the buzz, chatbots like ChatGPT aren&#39;t as smart as they seem and could cause real problems if we&#39;re not careful.</p>
<p><br></p>
<p>Marcus is worried about tech companies putting profits before people. He thinks AI could make fake news and privacy issues even worse. He&#39;s also concerned that a few big tech companies have too much power. Looking ahead, Marcus believes the AI hype will die down as reality sets in. He wants to see AI developed in smarter, more responsible ways. His message to the public? We need to speak up and demand better AI before it&#39;s too late.</p>
<p><br></p>
<p>Buy Taming Silicon Valley:</p>
<p>https://amzn.to/3XTlC5s</p>
<p><br></p>
<p>Gary Marcus:</p>
<p>https://garymarcus.substack.com/</p>
<p>https://x.com/GaryMarcus</p>
<p><br></p>
<p>Interviewer: </p>
<p>Dr. Tim Scarfe</p>
<p><br></p>
<p>(Refs in top comment)</p>
<p><br></p>
<p>TOC</p>
<p>[00:00:00] AI Flaws, Improvements &amp; Industry Critique</p>
<p>[00:16:29] AI Safety Theater &amp; Image Generation Issues</p>
<p>[00:23:49] AI&#39;s Lack of World Models &amp; Human-like Understanding</p>
<p>[00:31:09] LLMs: Superficial Intelligence vs. True Reasoning</p>
<p>[00:34:45] AI in Specialized Domains: Chess, Coding &amp; Limitations</p>
<p>[00:42:10] AI-Generated Code: Capabilities &amp; Human-AI Interaction</p>
<p>[00:48:10] AI Regulation: Industry Resistance &amp; Oversight Challenges</p>
<p>[00:54:55] Copyright Issues in AI &amp; Tech Business Models</p>
<p>[00:57:26] AI&#39;s Societal Impact: Risks, Misinformation &amp; Ethics</p>
<p>[01:23:14] AI X-risk, Alignment &amp; Moral Principles Implementation</p>
<p>[01:37:10] Persistent AI Flaws: System Limitations &amp; Architecture Challenges</p>
<p>[01:44:33] AI Future: Surveillance Concerns, Economic Challenges &amp; Neuro-Symbolic AI</p>
<p><br></p>
<p>YT version with refs: https://youtu.be/o9MfuUoGlSw</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Taming-Silicon-Valley---Prof--Gary-Marcus-e2oqqli</link>
			<guid isPermaLink="false">4c66fe9a-7cb0-46ab-9151-9ae7f280e4c3</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 24 Sep 2024 20:45:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/92153970/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-24%2F07fc532f-22b6-af88-999b-daff9175d82d.mp3" length="168720628" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;AI expert Prof. Gary Marcus doesn&amp;#39;t mince words about today&amp;#39;s artificial intelligence. He argues that despite the buzz, chatbots like ChatGPT aren&amp;#39;t as smart as they seem and could cause real problems if we&amp;#39;re not careful.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Marcus is worried about tech companies putting profits before people. He thinks AI could make fake news and privacy issues even worse. He&amp;#39;s also concerned that a few big tech companies have too much power. Looking ahead, Marcus believes the AI hype will die down as reality sets in. He wants to see AI developed in smarter, more responsible ways. His message to the public? We need to speak up and demand better AI before it&amp;#39;s too late.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy Taming Silicon Valley:&lt;/p&gt;
&lt;p&gt;https://amzn.to/3XTlC5s&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gary Marcus:&lt;/p&gt;
&lt;p&gt;https://garymarcus.substack.com/&lt;/p&gt;
&lt;p&gt;https://x.com/GaryMarcus&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: &lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;(Refs in top comment)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;[00:00:00] AI Flaws, Improvements &amp;amp; Industry Critique&lt;/p&gt;
&lt;p&gt;[00:16:29] AI Safety Theater &amp;amp; Image Generation Issues&lt;/p&gt;
&lt;p&gt;[00:23:49] AI&amp;#39;s Lack of World Models &amp;amp; Human-like Understanding&lt;/p&gt;
&lt;p&gt;[00:31:09] LLMs: Superficial Intelligence vs. True Reasoning&lt;/p&gt;
&lt;p&gt;[00:34:45] AI in Specialized Domains: Chess, Coding &amp;amp; Limitations&lt;/p&gt;
&lt;p&gt;[00:42:10] AI-Generated Code: Capabilities &amp;amp; Human-AI Interaction&lt;/p&gt;
&lt;p&gt;[00:48:10] AI Regulation: Industry Resistance &amp;amp; Oversight Challenges&lt;/p&gt;
&lt;p&gt;[00:54:55] Copyright Issues in AI &amp;amp; Tech Business Models&lt;/p&gt;
&lt;p&gt;[00:57:26] AI&amp;#39;s Societal Impact: Risks, Misinformation &amp;amp; Ethics&lt;/p&gt;
&lt;p&gt;[01:23:14] AI X-risk, Alignment &amp;amp; Moral Principles Implementation&lt;/p&gt;
&lt;p&gt;[01:37:10] Persistent AI Flaws: System Limitations &amp;amp; Architecture Challenges&lt;/p&gt;
&lt;p&gt;[01:44:33] AI Future: Surveillance Concerns, Economic Challenges &amp;amp; Neuro-Symbolic AI&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version with refs: https://youtu.be/o9MfuUoGlSw&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:56:55</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1727210187631-4f1814ecae69d.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Mark Solms - The Hidden Spring]]></title>
			<description><![CDATA[<p>Prof. Mark Solms, a neuroscientist and psychoanalyst, discusses his groundbreaking work on consciousness, challenging conventional cortex-centric views and emphasizing the role of brainstem structures in generating consciousness and affect.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Key points discussed:</p>
<p>The limitations of vision-centric approaches to consciousness studies.</p>
<p>Evidence from decorticated animals and hydranencephalic children supporting the brainstem&#39;s role in consciousness.</p>
<p>The relationship between homeostasis, the free energy principle, and consciousness.</p>
<p>Critiques of behaviorism and modern theories of consciousness.</p>
<p>The importance of subjective experience in understanding brain function.</p>
<p><br></p>
<p>The discussion also explored broader topics:</p>
<p>The potential impact of affect-based theories on AI development.</p>
<p>The role of the SEEKING system in exploration and learning.</p>
<p>Connections between neuroscience, psychoanalysis, and philosophy of mind.</p>
<p>Challenges in studying consciousness and the limitations of current theories.</p>
<p><br></p>
<p>Mark Solms: </p>
<p>https://neuroscience.uct.ac.za/contacts/mark-solms</p>
<p><br></p>
<p>Show notes and transcript: https://www.dropbox.com/scl/fo/roipwmnlfmwk2e7kivzms/ACjZF-VIGC2-Suo30KcwVV0?rlkey=53y8v2cajfcgrf17p1h7v3suz&amp;st=z8vu81hn&amp;dl=0</p>
<p><br></p>
<p>TOC (*) are best bits</p>
<p>00:00:00 1. Intro: Challenging vision-centric approaches to consciousness *</p>
<p>00:02:20 2. Evidence from decorticated animals and hydranencephalic children *</p>
<p>00:07:40 3. Emotional responses in hydranencephalic children</p>
<p>00:10:40 4. Brainstem stimulation and affective states</p>
<p>00:15:00 5. Brainstem&#39;s role in generating affective consciousness *</p>
<p>00:21:50 6. Dual-aspect monism and the mind-brain relationship</p>
<p>00:29:37 7. Information, affect, and the hard problem of consciousness *</p>
<p>00:37:25 8. Wheeler&#39;s participatory universe and Chalmers&#39; theories</p>
<p>00:48:51 9. Homeostasis, free energy principle, and consciousness *</p>
<p>00:59:25 10. Affect, voluntary behavior, and decision-making</p>
<p>01:05:45 11. Psychoactive substances, REM sleep, and consciousness research</p>
<p>01:12:14 12. Critiquing behaviorism and modern consciousness theories *</p>
<p>01:24:25 13. The SEEKING system and exploration in neuroscience</p>
<p><br></p>
<p>Refs:</p>
<p>1. Mark Solms&#39; book &quot;The Hidden Spring&quot; [00:20:34] (MUST READ!)</p>
<p> https://amzn.to/3XyETb3</p>
<p><br></p>
<p>2. Karl Friston&#39;s free energy principle [00:03:50]</p>
<p>   https://www.nature.com/articles/nrn2787</p>
<p><br></p>
<p>3. Hydranencephaly condition [00:07:10]</p>
<p>   https://en.wikipedia.org/wiki/Hydranencephaly</p>
<p><br></p>
<p>4. Periaqueductal gray (PAG) [00:08:57]</p>
<p>   https://en.wikipedia.org/wiki/Periaqueductal_gray</p>
<p><br></p>
<p>5. Positron Emission Tomography (PET) [00:13:52]</p>
<p>   https://en.wikipedia.org/wiki/Positron_emission_tomography</p>
<p><br></p>
<p>6. Paul MacLean&#39;s triune brain theory [00:03:30]</p>
<p>   https://en.wikipedia.org/wiki/Triune_brain</p>
<p><br></p>
<p>7. Baruch Spinoza&#39;s philosophy of mind [00:23:48]</p>
<p>   https://plato.stanford.edu/entries/spinoza-epistemology-mind</p>
<p><br></p>
<p>8. Claude Shannon&#39;s &quot;A Mathematical Theory of Communication&quot; [00:32:15]</p>
<p>   https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</p>
<p><br></p>
<p>9. Francis Crick&#39;s &quot;The Astonishing Hypothesis&quot; [00:39:57]</p>
<p>   https://en.wikipedia.org/wiki/The_Astonishing_Hypothesis</p>
<p><br></p>
<p>10. Frank Jackson&#39;s Knowledge Argument [00:40:54]</p>
<p>    https://plato.stanford.edu/entries/qualia-knowledge/</p>
<p><br></p>
<p>11. Mesolimbic dopamine system [01:11:51]</p>
<p>    https://en.wikipedia.org/wiki/Mesolimbic_pathway</p>
<p><br></p>
<p>12. Jaak Panksepp&#39;s SEEKING system [01:25:23]</p>
<p>    https://en.wikipedia.org/wiki/Jaak_Panksepp#Affective_neuroscience</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Mark-Solms---The-Hidden-Spring-e2oinpp</link>
			<guid isPermaLink="false">0f2620e4-40c1-4ab0-8d63-58703f610633</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 18 Sep 2024 20:14:31 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91888889/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-18%2Fb0c23e11-aa25-87e7-f2a3-268b7e8dae67.mp3" length="125271150" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof. Mark Solms, a neuroscientist and psychoanalyst, discusses his groundbreaking work on consciousness, challenging conventional cortex-centric views and emphasizing the role of brainstem structures in generating consciousness and affect.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Key points discussed:&lt;/p&gt;
&lt;p&gt;The limitations of vision-centric approaches to consciousness studies.&lt;/p&gt;
&lt;p&gt;Evidence from decorticated animals and hydranencephalic children supporting the brainstem&amp;#39;s role in consciousness.&lt;/p&gt;
&lt;p&gt;The relationship between homeostasis, the free energy principle, and consciousness.&lt;/p&gt;
&lt;p&gt;Critiques of behaviorism and modern theories of consciousness.&lt;/p&gt;
&lt;p&gt;The importance of subjective experience in understanding brain function.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The discussion also explored broader topics:&lt;/p&gt;
&lt;p&gt;The potential impact of affect-based theories on AI development.&lt;/p&gt;
&lt;p&gt;The role of the SEEKING system in exploration and learning.&lt;/p&gt;
&lt;p&gt;Connections between neuroscience, psychoanalysis, and philosophy of mind.&lt;/p&gt;
&lt;p&gt;Challenges in studying consciousness and the limitations of current theories.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Mark Solms: &lt;/p&gt;
&lt;p&gt;https://neuroscience.uct.ac.za/contacts/mark-solms&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Show notes and transcript: https://www.dropbox.com/scl/fo/roipwmnlfmwk2e7kivzms/ACjZF-VIGC2-Suo30KcwVV0?rlkey=53y8v2cajfcgrf17p1h7v3suz&amp;amp;st=z8vu81hn&amp;amp;dl=0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC (*) are best bits&lt;/p&gt;
&lt;p&gt;00:00:00 1. Intro: Challenging vision-centric approaches to consciousness *&lt;/p&gt;
&lt;p&gt;00:02:20 2. Evidence from decorticated animals and hydranencephalic children *&lt;/p&gt;
&lt;p&gt;00:07:40 3. Emotional responses in hydranencephalic children&lt;/p&gt;
&lt;p&gt;00:10:40 4. Brainstem stimulation and affective states&lt;/p&gt;
&lt;p&gt;00:15:00 5. Brainstem&amp;#39;s role in generating affective consciousness *&lt;/p&gt;
&lt;p&gt;00:21:50 6. Dual-aspect monism and the mind-brain relationship&lt;/p&gt;
&lt;p&gt;00:29:37 7. Information, affect, and the hard problem of consciousness *&lt;/p&gt;
&lt;p&gt;00:37:25 8. Wheeler&amp;#39;s participatory universe and Chalmers&amp;#39; theories&lt;/p&gt;
&lt;p&gt;00:48:51 9. Homeostasis, free energy principle, and consciousness *&lt;/p&gt;
&lt;p&gt;00:59:25 10. Affect, voluntary behavior, and decision-making&lt;/p&gt;
&lt;p&gt;01:05:45 11. Psychoactive substances, REM sleep, and consciousness research&lt;/p&gt;
&lt;p&gt;01:12:14 12. Critiquing behaviorism and modern consciousness theories *&lt;/p&gt;
&lt;p&gt;01:24:25 13. The SEEKING system and exploration in neuroscience&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;1. Mark Solms&amp;#39; book &amp;quot;The Hidden Spring&amp;quot; [00:20:34] (MUST READ!)&lt;/p&gt;
&lt;p&gt; https://amzn.to/3XyETb3&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;2. Karl Friston&amp;#39;s free energy principle [00:03:50]&lt;/p&gt;
&lt;p&gt;   https://www.nature.com/articles/nrn2787&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3. Hydranencephaly condition [00:07:10]&lt;/p&gt;
&lt;p&gt;   https://en.wikipedia.org/wiki/Hydranencephaly&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;4. Periaqueductal gray (PAG) [00:08:57]&lt;/p&gt;
&lt;p&gt;   https://en.wikipedia.org/wiki/Periaqueductal_gray&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;5. Positron Emission Tomography (PET) [00:13:52]&lt;/p&gt;
&lt;p&gt;   https://en.wikipedia.org/wiki/Positron_emission_tomography&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;6. Paul MacLean&amp;#39;s triune brain theory [00:03:30]&lt;/p&gt;
&lt;p&gt;   https://en.wikipedia.org/wiki/Triune_brain&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;7. Baruch Spinoza&amp;#39;s philosophy of mind [00:23:48]&lt;/p&gt;
&lt;p&gt;   https://plato.stanford.edu/entries/spinoza-epistemology-mind&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;8. Claude Shannon&amp;#39;s &amp;quot;A Mathematical Theory of Communication&amp;quot; [00:32:15]&lt;/p&gt;
&lt;p&gt;   https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;9. Francis Crick&amp;#39;s &amp;quot;The Astonishing Hypothesis&amp;quot; [00:39:57]&lt;/p&gt;
&lt;p&gt;   https://en.wikipedia.org/wiki/The_Astonishing_Hypothesis&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;10. Frank Jackson&amp;#39;s Knowledge Argument [00:40:54]&lt;/p&gt;
&lt;p&gt;    https://plato.stanford.edu/entries/qualia-knowledge/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;11. Mesolimbic dopamine system [01:11:51]&lt;/p&gt;
&lt;p&gt;    https://en.wikipedia.org/wiki/Mesolimbic_pathway&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;12. Jaak Panksepp&amp;#39;s SEEKING system [01:25:23]&lt;/p&gt;
&lt;p&gt;    https://en.wikipedia.org/wiki/Jaak_Panksepp#Affective_neuroscience&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:26:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1726690445303-4fcd31b07ccd5.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Patrick Lewis (Cohere) - Retrieval Augmented Generation]]></title>
			<description><![CDATA[<p>Dr. Patrick Lewis, who coined the term RAG (Retrieval Augmented Generation) and now works at Cohere, discusses the evolution of language models, RAG systems, and challenges in AI evaluation. </p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmented generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Key topics covered:</p>
<p>- Origins and evolution of Retrieval Augmented Generation (RAG)</p>
<p>- Challenges in evaluating RAG systems and language models</p>
<p>- Human-AI collaboration in research and knowledge work</p>
<p>- Word embeddings and the progression to modern language models</p>
<p>- Dense vs sparse retrieval methods in information retrieval</p>
<p><br></p>
<p>The discussion also explored broader implications and applications:</p>
<p>- Balancing faithfulness and fluency in RAG systems</p>
<p>- User interface design for AI-augmented research tools</p>
<p>- The journey from chemistry to AI research</p>
<p>- Challenges in enterprise search compared to web search</p>
<p>- The importance of data quality in training AI models</p>
<p><br></p>
<p>Patrick Lewis: https://www.patricklewis.io/</p>
<p><br></p>
<p>Cohere Command Models, check them out - they are amazing for RAG!</p>
<p>https://cohere.com/command</p>
<p><br></p>
<p>TOC</p>
<p>00:00:00 1. Intro to RAG</p>
<p>00:05:30 2. RAG Evaluation: Poll framework &amp; model performance</p>
<p>00:12:55 3. Data Quality: Cleanliness vs scale in AI training</p>
<p>00:15:13 4. Human-AI Collaboration: Research agents &amp; UI design</p>
<p>00:22:57 5. RAG Origins: Open-domain QA to generative models</p>
<p>00:30:18 6. RAG Challenges: Info retrieval, tool use, faithfulness</p>
<p>00:42:01 7. Dense vs Sparse Retrieval: Techniques &amp; trade-offs</p>
<p>00:47:02 8. RAG Applications: Grounding, attribution, hallucination prevention</p>
<p>00:54:04 9. UI for RAG: Human-computer interaction &amp; model optimization</p>
<p>00:59:01 10. Word Embeddings: Word2Vec, GloVe, and semantic spaces</p>
<p>01:06:43 11. Language Model Evolution: BERT, GPT, and beyond</p>
<p>01:11:38 12. AI &amp; Human Cognition: Sequential processing &amp; chain-of-thought</p>
<p><br></p>
<p>Refs:</p>
<p>1. Retrieval Augmented Generation (RAG) paper / Patrick Lewis et al. [00:27:45]</p>
<p> https://arxiv.org/abs/2005.11401</p>
<p>2. LAMA (LAnguage Model Analysis) probe / Petroni et al. [00:26:35]</p>
<p>   https://arxiv.org/abs/1909.01066</p>
<p>3. KILT (Knowledge Intensive Language Tasks) benchmark / Petroni et al. [00:27:05]</p>
<p>   https://arxiv.org/abs/2009.02252</p>
<p>4. Word2Vec algorithm / Tomas Mikolov et al. [01:00:25]</p>
<p>   https://arxiv.org/abs/1301.3781</p>
<p>5. GloVe (Global Vectors for Word Representation) / Pennington et al. [01:04:35]</p>
<p>   https://nlp.stanford.edu/projects/glove/</p>
<p>6. BERT (Bidirectional Encoder Representations from Transformers) / Devlin et al. [01:08:00]</p>
<p>   https://arxiv.org/abs/1810.04805</p>
<p>7. &#39;The Language Game&#39; book / Nick Chater and Morten H. Christiansen [01:11:40]</p>
<p>   https://amzn.to/4grEUpG</p>
<p><br></p>
<p>Disclaimer: This is the sixth video from our Cohere partnership. We were not told what to say in the interview. Filmed in Seattle in June 2024.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Patrick-Lewis-Cohere---Retrieval-Augmented-Generation-e2ofomu</link>
			<guid isPermaLink="false">fd41eb07-5c2b-44d8-b29d-0e4167052ae3</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 16 Sep 2024 18:36:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91791518/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-16%2Fe5358a0e-d3e0-3c43-d400-f12ce212ea4c.mp3" length="106520038" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Patrick Lewis, who coined the term RAG (Retrieval Augmented Generation) and now works at Cohere, discusses the evolution of language models, RAG systems, and challenges in AI evaluation. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmented generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Key topics covered:&lt;/p&gt;
&lt;p&gt;- Origins and evolution of Retrieval Augmented Generation (RAG)&lt;/p&gt;
&lt;p&gt;- Challenges in evaluating RAG systems and language models&lt;/p&gt;
&lt;p&gt;- Human-AI collaboration in research and knowledge work&lt;/p&gt;
&lt;p&gt;- Word embeddings and the progression to modern language models&lt;/p&gt;
&lt;p&gt;- Dense vs sparse retrieval methods in information retrieval&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The discussion also explored broader implications and applications:&lt;/p&gt;
&lt;p&gt;- Balancing faithfulness and fluency in RAG systems&lt;/p&gt;
&lt;p&gt;- User interface design for AI-augmented research tools&lt;/p&gt;
&lt;p&gt;- The journey from chemistry to AI research&lt;/p&gt;
&lt;p&gt;- Challenges in enterprise search compared to web search&lt;/p&gt;
&lt;p&gt;- The importance of data quality in training AI models&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Patrick Lewis: https://www.patricklewis.io/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cohere Command Models, check them out - they are amazing for RAG!&lt;/p&gt;
&lt;p&gt;https://cohere.com/command&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;00:00:00 1. Intro to RAG&lt;/p&gt;
&lt;p&gt;00:05:30 2. RAG Evaluation: Poll framework &amp;amp; model performance&lt;/p&gt;
&lt;p&gt;00:12:55 3. Data Quality: Cleanliness vs scale in AI training&lt;/p&gt;
&lt;p&gt;00:15:13 4. Human-AI Collaboration: Research agents &amp;amp; UI design&lt;/p&gt;
&lt;p&gt;00:22:57 5. RAG Origins: Open-domain QA to generative models&lt;/p&gt;
&lt;p&gt;00:30:18 6. RAG Challenges: Info retrieval, tool use, faithfulness&lt;/p&gt;
&lt;p&gt;00:42:01 7. Dense vs Sparse Retrieval: Techniques &amp;amp; trade-offs&lt;/p&gt;
&lt;p&gt;00:47:02 8. RAG Applications: Grounding, attribution, hallucination prevention&lt;/p&gt;
&lt;p&gt;00:54:04 9. UI for RAG: Human-computer interaction &amp;amp; model optimization&lt;/p&gt;
&lt;p&gt;00:59:01 10. Word Embeddings: Word2Vec, GloVe, and semantic spaces&lt;/p&gt;
&lt;p&gt;01:06:43 11. Language Model Evolution: BERT, GPT, and beyond&lt;/p&gt;
&lt;p&gt;01:11:38 12. AI &amp;amp; Human Cognition: Sequential processing &amp;amp; chain-of-thought&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;1. Retrieval Augmented Generation (RAG) paper / Patrick Lewis et al. [00:27:45]&lt;/p&gt;
&lt;p&gt; https://arxiv.org/abs/2005.11401&lt;/p&gt;
&lt;p&gt;2. LAMA (LAnguage Model Analysis) probe / Petroni et al. [00:26:35]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/1909.01066&lt;/p&gt;
&lt;p&gt;3. KILT (Knowledge Intensive Language Tasks) benchmark / Petroni et al. [00:27:05]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/2009.02252&lt;/p&gt;
&lt;p&gt;4. Word2Vec algorithm / Tomas Mikolov et al. [01:00:25]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/1301.3781&lt;/p&gt;
&lt;p&gt;5. GloVe (Global Vectors for Word Representation) / Pennington et al. [01:04:35]&lt;/p&gt;
&lt;p&gt;   https://nlp.stanford.edu/projects/glove/&lt;/p&gt;
&lt;p&gt;6. BERT (Bidirectional Encoder Representations from Transformers) / Devlin et al. [01:08:00]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/1810.04805&lt;/p&gt;
&lt;p&gt;7. &amp;#39;The Language Game&amp;#39; book / Nick Chater and Morten H. Christiansen [01:11:40]&lt;/p&gt;
&lt;p&gt;   https://amzn.to/4grEUpG&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This is the sixth video from our Cohere partnership. We were not told what to say in the interview. Filmed in Seattle in June 2024.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:13:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1726511731117-7ff23719f8c23.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Ashley Edwards - Genie Paper (DeepMind/Runway)]]></title>
			<description><![CDATA[<p>Ashley Edwards, who was working at DeepMind when she co-authored the Genie paper and is now at Runway, covered several key aspects of the Genie AI system and its applications in video generation, robotics, and game creation. </p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Genie&#39;s approach to learning interactive environments, balancing compression and fidelity.</p>
<p>The use of latent action models and VQE models for video processing and tokenization.</p>
<p>Challenges in maintaining action consistency across frames and integrating text-to-image models.</p>
<p>Evaluation metrics for AI-generated content, such as FID and PS&amp;R diff metrics.</p>
<p><br></p>
<p>The discussion also explored broader implications and applications:</p>
<p><br></p>
<p>The potential impact of AI video generation on content creation jobs.</p>
<p>Applications of Genie in game generation and robotics.</p>
<p>The use of foundation models in robotics and the differences between internet video data and specialized robotics data.</p>
<p>Challenges in mapping AI-generated actions to real-world robotic actions.</p>
<p><br></p>
<p>Ashley Edwards: https://ashedwards.github.io/</p>
<p><br></p>
<p>TOC (*) are best bits</p>
<p>00:00:00 1. Intro to Genie &amp; Brave Search API: Trade-offs &amp; limitations *</p>
<p>00:02:26 2. Genie&#39;s Architecture: Latent action, VQE, video processing *</p>
<p>00:05:06 3. Genie&#39;s Constraints: Frame consistency &amp; image model integration</p>
<p>00:07:26 4. Evaluation: FID, PS&amp;R diff metrics &amp; latent induction methods</p>
<p>00:09:44 5. AI Video Gen: Content creation impact, depth &amp; parallax effects</p>
<p>00:11:39 6. Model Scaling: Training data impact &amp; computational trade-offs</p>
<p>00:13:50 7. Game &amp; Robotics Apps: Gamification &amp; action mapping challenges *</p>
<p>00:16:16 8. Robotics Foundation Models: Action space &amp; data considerations *</p>
<p>00:19:18 9. Mask-GPT &amp; Video Frames: Real-time optimization, RL from videos</p>
<p>00:20:34 10. Research Challenges: AI value, efficiency vs. quality, safety</p>
<p>00:24:20 11. Future Dev: Efficiency improvements &amp; fine-tuning strategies</p>
<p><br></p>
<p>Refs:</p>
<p>1. Genie (learning interactive environments from videos) / Ashley and DM collegues [00:01]</p>
<p> https://arxiv.org/abs/2402.15391</p>
<p><br></p>
<p>2. VQ-VAE (Vector Quantized Variational Autoencoder) / Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu [02:43]</p>
<p> https://arxiv.org/abs/1711.00937</p>
<p><br></p>
<p>3. FID (Fréchet Inception Distance) metric / Martin Heusel et al. [07:37]</p>
<p>   https://arxiv.org/abs/1706.08500</p>
<p><br></p>
<p>4. PS&amp;R (Precision and Recall) metric / Mehdi S. M. Sajjadi et al. [08:02]</p>
<p>   https://arxiv.org/abs/1806.00035</p>
<p><br></p>
<p>5. Vision Transformer (ViT) architecture / Alexey Dosovitskiy et al. [12:14]</p>
<p>   https://arxiv.org/abs/2010.11929</p>
<p><br></p>
<p>6. Genie (robotics foundation models) / Google DeepMind [17:34]</p>
<p>   https://deepmind.google/research/publications/60474/</p>
<p><br></p>
<p>7. Chelsea Finn&#39;s lab work on robotics datasets / Chelsea Finn [17:38]</p>
<p>   https://ai.stanford.edu/~cbfinn/</p>
<p><br></p>
<p>8. Imitation from observation in reinforcement learning / YuXuan Liu [20:58]</p>
<p>   https://arxiv.org/abs/1707.03374</p>
<p><br></p>
<p>9. Waymo&#39;s autonomous driving technology / Waymo [22:38]</p>
<p>   https://waymo.com/</p>
<p><br></p>
<p>10. Gen3 model release by Runway / Runway [23:48]</p>
<p>    https://runwayml.com/</p>
<p><br></p>
<p>11. Classifier-free guidance technique / Jonathan Ho and Tim Salimans [24:43]</p>
<p>    https://arxiv.org/abs/2207.12598</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Ashley-Edwards---Genie-Paper-DeepMindRunway-e2oc9m5</link>
			<guid isPermaLink="false">6e9fc284-add6-458f-a5ce-5d0188220efc</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 13 Sep 2024 18:59:35 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91677829/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-13%2F43b709ac-1c14-74ab-673a-ab9bb09beac6.mp3" length="36284601" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Ashley Edwards, who was working at DeepMind when she co-authored the Genie paper and is now at Runway, covered several key aspects of the Genie AI system and its applications in video generation, robotics, and game creation. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Genie&amp;#39;s approach to learning interactive environments, balancing compression and fidelity.&lt;/p&gt;
&lt;p&gt;The use of latent action models and VQE models for video processing and tokenization.&lt;/p&gt;
&lt;p&gt;Challenges in maintaining action consistency across frames and integrating text-to-image models.&lt;/p&gt;
&lt;p&gt;Evaluation metrics for AI-generated content, such as FID and PS&amp;amp;R diff metrics.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The discussion also explored broader implications and applications:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The potential impact of AI video generation on content creation jobs.&lt;/p&gt;
&lt;p&gt;Applications of Genie in game generation and robotics.&lt;/p&gt;
&lt;p&gt;The use of foundation models in robotics and the differences between internet video data and specialized robotics data.&lt;/p&gt;
&lt;p&gt;Challenges in mapping AI-generated actions to real-world robotic actions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ashley Edwards: https://ashedwards.github.io/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC (*) are best bits&lt;/p&gt;
&lt;p&gt;00:00:00 1. Intro to Genie &amp;amp; Brave Search API: Trade-offs &amp;amp; limitations *&lt;/p&gt;
&lt;p&gt;00:02:26 2. Genie&amp;#39;s Architecture: Latent action, VQE, video processing *&lt;/p&gt;
&lt;p&gt;00:05:06 3. Genie&amp;#39;s Constraints: Frame consistency &amp;amp; image model integration&lt;/p&gt;
&lt;p&gt;00:07:26 4. Evaluation: FID, PS&amp;amp;R diff metrics &amp;amp; latent induction methods&lt;/p&gt;
&lt;p&gt;00:09:44 5. AI Video Gen: Content creation impact, depth &amp;amp; parallax effects&lt;/p&gt;
&lt;p&gt;00:11:39 6. Model Scaling: Training data impact &amp;amp; computational trade-offs&lt;/p&gt;
&lt;p&gt;00:13:50 7. Game &amp;amp; Robotics Apps: Gamification &amp;amp; action mapping challenges *&lt;/p&gt;
&lt;p&gt;00:16:16 8. Robotics Foundation Models: Action space &amp;amp; data considerations *&lt;/p&gt;
&lt;p&gt;00:19:18 9. Mask-GPT &amp;amp; Video Frames: Real-time optimization, RL from videos&lt;/p&gt;
&lt;p&gt;00:20:34 10. Research Challenges: AI value, efficiency vs. quality, safety&lt;/p&gt;
&lt;p&gt;00:24:20 11. Future Dev: Efficiency improvements &amp;amp; fine-tuning strategies&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;1. Genie (learning interactive environments from videos) / Ashley and DM collegues [00:01]&lt;/p&gt;
&lt;p&gt; https://arxiv.org/abs/2402.15391&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;2. VQ-VAE (Vector Quantized Variational Autoencoder) / Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu [02:43]&lt;/p&gt;
&lt;p&gt; https://arxiv.org/abs/1711.00937&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;3. FID (Fréchet Inception Distance) metric / Martin Heusel et al. [07:37]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/1706.08500&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;4. PS&amp;amp;R (Precision and Recall) metric / Mehdi S. M. Sajjadi et al. [08:02]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/1806.00035&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;5. Vision Transformer (ViT) architecture / Alexey Dosovitskiy et al. [12:14]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/2010.11929&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;6. Genie (robotics foundation models) / Google DeepMind [17:34]&lt;/p&gt;
&lt;p&gt;   https://deepmind.google/research/publications/60474/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;7. Chelsea Finn&amp;#39;s lab work on robotics datasets / Chelsea Finn [17:38]&lt;/p&gt;
&lt;p&gt;   https://ai.stanford.edu/~cbfinn/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;8. Imitation from observation in reinforcement learning / YuXuan Liu [20:58]&lt;/p&gt;
&lt;p&gt;   https://arxiv.org/abs/1707.03374&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;9. Waymo&amp;#39;s autonomous driving technology / Waymo [22:38]&lt;/p&gt;
&lt;p&gt;   https://waymo.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;10. Gen3 model release by Runway / Runway [23:48]&lt;/p&gt;
&lt;p&gt;    https://runwayml.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;11. Classifier-free guidance technique / Jonathan Ho and Tim Salimans [24:43]&lt;/p&gt;
&lt;p&gt;    https://arxiv.org/abs/2207.12598&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:25:04</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1726253903859-df81db79abe4c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Cohere's SVP Technology - Saurabh Baji]]></title>
			<description><![CDATA[<p>Saurabh Baji discusses Cohere&#39;s approach to developing and deploying large language models (LLMs) for enterprise use. </p>
<p><br></p>
<p>* Cohere focuses on pragmatic, efficient models tailored for business applications rather than pursuing the largest possible models.</p>
<p>* They offer flexible deployment options, from cloud services to on-premises installations, to meet diverse enterprise needs.</p>
<p>* Retrieval-augmented generation (RAG) is highlighted as a critical capability, allowing models to leverage enterprise data securely.</p>
<p>* Cohere emphasizes model customization, fine-tuning, and tools like reranking to optimize performance for specific use cases.</p>
<p>* The company has seen significant growth, transitioning from developer-focused to enterprise-oriented services.</p>
<p>* Major customers like Oracle, Fujitsu, and TD Bank are using Cohere&#39;s models across various applications, from HR to finance.</p>
<p>* Baji predicts a surge in enterprise AI adoption over the next 12-18 months as more companies move from experimentation to production.</p>
<p>* He emphasizes the importance of trust, security, and verifiability in enterprise AI applications.</p>
<p><br></p>
<p>The interview provides insights into Cohere&#39;s strategy, technology, and vision for the future of enterprise AI adoption.</p>
<p><br></p>
<p>https://www.linkedin.com/in/saurabhbaji/</p>
<p>https://x.com/sbaji</p>
<p>https://cohere.com/</p>
<p>https://cohere.com/business</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>TOC (*) are best bits</p>
<p>00:00:00 1. Introduction and Background</p>
<p>00:04:24 2. Cloud Infrastructure and LLM Optimization</p>
<p>00:06:43 2.1 Model deployment and fine-tuning strategies *</p>
<p>00:09:37 3. Enterprise AI Deployment Strategies</p>
<p>00:11:10 3.1 Retrieval-augmented generation in enterprise environments *</p>
<p>00:13:40 3.2 Standardization vs. customization in cloud services *</p>
<p>00:18:20 4. AI Model Evaluation and Deployment</p>
<p>00:18:20 4.1 Comprehensive evaluation frameworks *</p>
<p>00:21:20   4.2 Key components of AI model stacks *</p>
<p>00:25:50 5. Retrieval Augmented Generation (RAG) in Enterprise</p>
<p>00:32:10   5.1 Pragmatic approach to RAG implementation *</p>
<p>00:33:45 6. AI Agents and Tool Integration</p>
<p>00:33:45   6.1 Leveraging tools for AI insights *</p>
<p>00:35:30   6.2 Agent-based AI systems and diagnostics *</p>
<p>00:42:55 7. AI Transparency and Reasoning Capabilities</p>
<p>00:49:10 8. AI Model Training and Customization</p>
<p>00:57:10 9. Enterprise AI Model Management</p>
<p>01:02:10   9.1 Managing AI model versions for enterprise customers *</p>
<p>01:04:30   9.2 Future of language model programming *</p>
<p>01:06:10 10. AI-Driven Software Development</p>
<p>01:06:10   10.1 AI bridging human expression and task achievement *</p>
<p>01:08:00   10.2 AI-driven virtual app fabrics in enterprise *</p>
<p>01:13:33 11. Future of AI and Enterprise Applications</p>
<p>01:21:55 12. Cohere&#39;s Customers and Use Cases</p>
<p>01:21:55   12.1 Cohere&#39;s growth and enterprise partnerships *</p>
<p>01:27:14   12.2 Diverse customers using generative AI *</p>
<p>01:27:50   12.3 Industry adaptation to generative AI *</p>
<p>01:29:00 13. Technical Advantages of Cohere Models</p>
<p>01:29:00   13.1 Handling large context windows *</p>
<p>01:29:40   13.2 Low latency impact on developer productivity *</p>
<p><br></p>
<p>Disclaimer: This is the fifth video from our Cohere partnership. We were not told what to say in the interview, and didn&#39;t edit anything out from the interview. Filmed in Seattle in Aug 2024.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Coheres-SVP-Technology---Saurabh-Baji-e2oaip5</link>
			<guid isPermaLink="false">9932f8d1-987b-4649-9358-bac6f6dd7bfd</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 12 Sep 2024 14:38:11 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91621605/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-12%2Fe009712b-d928-a548-ac9c-0deefab73e3b.mp3" length="130555448" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Saurabh Baji discusses Cohere&amp;#39;s approach to developing and deploying large language models (LLMs) for enterprise use. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;* Cohere focuses on pragmatic, efficient models tailored for business applications rather than pursuing the largest possible models.&lt;/p&gt;
&lt;p&gt;* They offer flexible deployment options, from cloud services to on-premises installations, to meet diverse enterprise needs.&lt;/p&gt;
&lt;p&gt;* Retrieval-augmented generation (RAG) is highlighted as a critical capability, allowing models to leverage enterprise data securely.&lt;/p&gt;
&lt;p&gt;* Cohere emphasizes model customization, fine-tuning, and tools like reranking to optimize performance for specific use cases.&lt;/p&gt;
&lt;p&gt;* The company has seen significant growth, transitioning from developer-focused to enterprise-oriented services.&lt;/p&gt;
&lt;p&gt;* Major customers like Oracle, Fujitsu, and TD Bank are using Cohere&amp;#39;s models across various applications, from HR to finance.&lt;/p&gt;
&lt;p&gt;* Baji predicts a surge in enterprise AI adoption over the next 12-18 months as more companies move from experimentation to production.&lt;/p&gt;
&lt;p&gt;* He emphasizes the importance of trust, security, and verifiability in enterprise AI applications.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The interview provides insights into Cohere&amp;#39;s strategy, technology, and vision for the future of enterprise AI adoption.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/saurabhbaji/&lt;/p&gt;
&lt;p&gt;https://x.com/sbaji&lt;/p&gt;
&lt;p&gt;https://cohere.com/&lt;/p&gt;
&lt;p&gt;https://cohere.com/business&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC (*) are best bits&lt;/p&gt;
&lt;p&gt;00:00:00 1. Introduction and Background&lt;/p&gt;
&lt;p&gt;00:04:24 2. Cloud Infrastructure and LLM Optimization&lt;/p&gt;
&lt;p&gt;00:06:43 2.1 Model deployment and fine-tuning strategies *&lt;/p&gt;
&lt;p&gt;00:09:37 3. Enterprise AI Deployment Strategies&lt;/p&gt;
&lt;p&gt;00:11:10 3.1 Retrieval-augmented generation in enterprise environments *&lt;/p&gt;
&lt;p&gt;00:13:40 3.2 Standardization vs. customization in cloud services *&lt;/p&gt;
&lt;p&gt;00:18:20 4. AI Model Evaluation and Deployment&lt;/p&gt;
&lt;p&gt;00:18:20 4.1 Comprehensive evaluation frameworks *&lt;/p&gt;
&lt;p&gt;00:21:20   4.2 Key components of AI model stacks *&lt;/p&gt;
&lt;p&gt;00:25:50 5. Retrieval Augmented Generation (RAG) in Enterprise&lt;/p&gt;
&lt;p&gt;00:32:10   5.1 Pragmatic approach to RAG implementation *&lt;/p&gt;
&lt;p&gt;00:33:45 6. AI Agents and Tool Integration&lt;/p&gt;
&lt;p&gt;00:33:45   6.1 Leveraging tools for AI insights *&lt;/p&gt;
&lt;p&gt;00:35:30   6.2 Agent-based AI systems and diagnostics *&lt;/p&gt;
&lt;p&gt;00:42:55 7. AI Transparency and Reasoning Capabilities&lt;/p&gt;
&lt;p&gt;00:49:10 8. AI Model Training and Customization&lt;/p&gt;
&lt;p&gt;00:57:10 9. Enterprise AI Model Management&lt;/p&gt;
&lt;p&gt;01:02:10   9.1 Managing AI model versions for enterprise customers *&lt;/p&gt;
&lt;p&gt;01:04:30   9.2 Future of language model programming *&lt;/p&gt;
&lt;p&gt;01:06:10 10. AI-Driven Software Development&lt;/p&gt;
&lt;p&gt;01:06:10   10.1 AI bridging human expression and task achievement *&lt;/p&gt;
&lt;p&gt;01:08:00   10.2 AI-driven virtual app fabrics in enterprise *&lt;/p&gt;
&lt;p&gt;01:13:33 11. Future of AI and Enterprise Applications&lt;/p&gt;
&lt;p&gt;01:21:55 12. Cohere&amp;#39;s Customers and Use Cases&lt;/p&gt;
&lt;p&gt;01:21:55   12.1 Cohere&amp;#39;s growth and enterprise partnerships *&lt;/p&gt;
&lt;p&gt;01:27:14   12.2 Diverse customers using generative AI *&lt;/p&gt;
&lt;p&gt;01:27:50   12.3 Industry adaptation to generative AI *&lt;/p&gt;
&lt;p&gt;01:29:00 13. Technical Advantages of Cohere Models&lt;/p&gt;
&lt;p&gt;01:29:00   13.1 Handling large context windows *&lt;/p&gt;
&lt;p&gt;01:29:40   13.2 Low latency impact on developer productivity *&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This is the fifth video from our Cohere partnership. We were not told what to say in the interview, and didn&amp;#39;t edit anything out from the interview. Filmed in Seattle in Aug 2024.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:30:25</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1726151726110-b5b762267e0b7.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[David Hanson's Vision for Sentient Robots]]></title>
			<description><![CDATA[<p>David Hanson, CEO of Hanson Robotics and creator of the humanoid robot Sofia, explores the intersection of artificial intelligence, ethics, and human potential. In this thought-provoking interview, Hanson discusses his vision for developing AI systems that embody the best aspects of humanity while pushing beyond our current limitations, aiming to achieve what he calls &quot;super wisdom.&quot;</p>
<p><br></p>
<p>YT version: https://youtu.be/LFCIEhlsozU</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>The interview with David Hanson covers:</p>
<p><br></p>
<p>The importance of incorporating biological drives and compassion into AI systems</p>
<p>Hanson&#39;s concept of &quot;existential pattern ethics&quot; as a basis for AI morality</p>
<p>The potential for AI to enhance human intelligence and wisdom</p>
<p>Challenges in developing artificial general intelligence (AGI)</p>
<p>The need to democratize AI technologies globally</p>
<p>Potential future advancements in human-AI integration and their societal impacts</p>
<p>Concerns about technological augmentation exacerbating inequality</p>
<p>The role of ethics in guiding AI development and deployment</p>
<p><br></p>
<p>Hanson advocates for creating AI systems that embody the best aspects of humanity while surpassing current human limitations, aiming for &quot;super wisdom&quot; rather than just artificial super intelligence.</p>
<p><br></p>
<p>David Hanson:</p>
<p>https://www.hansonrobotics.com/david-hanson/</p>
<p>https://www.youtube.com/watch?v=9u1O954cMmE</p>
<p><br></p>
<p>TOC</p>
<p>1. Introduction and Background [00:00:00]</p>
<p> 1.1. David Hanson&#39;s interdisciplinary background [0:01:49]</p>
<p> 1.2. Introduction to Sofia, the realistic robot [0:03:27]</p>
<p>2. Human Cognition and AI [0:03:50]</p>
<p>  2.1. Importance of social interaction in cognition [0:03:50]</p>
<p>  2.2. Compassion as distinguishing factor [0:05:55]</p>
<p>  2.3. AI augmenting human intelligence [0:09:54]</p>
<p>3. Developing Human-like AI [0:13:17]</p>
<p>  3.1. Incorporating biological drives in AI [0:13:17]</p>
<p>  3.2. Creating AI with agency [0:20:34]</p>
<p>  3.3. Implementing flexible desires in AI [0:23:23]</p>
<p>4. Ethics and Morality in AI [0:27:53]</p>
<p>  4.1. Enhancing humanity through AI [0:27:53]</p>
<p>  4.2. Existential pattern ethics [0:30:14]</p>
<p>  4.3. Expanding morality beyond restrictions [0:35:35]</p>
<p>5. Societal Impact of AI [0:38:07]</p>
<p>  5.1. AI adoption and integration [0:38:07]</p>
<p>  5.2. Democratizing AI technologies [0:38:32]</p>
<p>  5.3. Human-AI integration and identity [0:43:37]</p>
<p>6. Future Considerations [0:50:03]</p>
<p>  6.1. Technological augmentation and inequality [0:50:03]</p>
<p>  6.2. Emerging technologies for mental health [0:50:32]</p>
<p>  6.3. Corporate ethics in AI development [0:52:26]</p>
<p><br></p>
<p>This was filmed at AGI-24</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/David-Hansons-Vision-for-Sentient-Robots-e2o7m8l</link>
			<guid isPermaLink="false">7afd0bd5-e003-49c9-ab99-64b4305a903a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 10 Sep 2024 12:23:51 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91526869/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-10%2F46ba67e2-af77-8c48-67a4-b887c16babe8.mp3" length="76954995" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;David Hanson, CEO of Hanson Robotics and creator of the humanoid robot Sofia, explores the intersection of artificial intelligence, ethics, and human potential. In this thought-provoking interview, Hanson discusses his vision for developing AI systems that embody the best aspects of humanity while pushing beyond our current limitations, aiming to achieve what he calls &amp;quot;super wisdom.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/LFCIEhlsozU&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The interview with David Hanson covers:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The importance of incorporating biological drives and compassion into AI systems&lt;/p&gt;
&lt;p&gt;Hanson&amp;#39;s concept of &amp;quot;existential pattern ethics&amp;quot; as a basis for AI morality&lt;/p&gt;
&lt;p&gt;The potential for AI to enhance human intelligence and wisdom&lt;/p&gt;
&lt;p&gt;Challenges in developing artificial general intelligence (AGI)&lt;/p&gt;
&lt;p&gt;The need to democratize AI technologies globally&lt;/p&gt;
&lt;p&gt;Potential future advancements in human-AI integration and their societal impacts&lt;/p&gt;
&lt;p&gt;Concerns about technological augmentation exacerbating inequality&lt;/p&gt;
&lt;p&gt;The role of ethics in guiding AI development and deployment&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Hanson advocates for creating AI systems that embody the best aspects of humanity while surpassing current human limitations, aiming for &amp;quot;super wisdom&amp;quot; rather than just artificial super intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;David Hanson:&lt;/p&gt;
&lt;p&gt;https://www.hansonrobotics.com/david-hanson/&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=9u1O954cMmE&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;1. Introduction and Background [00:00:00]&lt;/p&gt;
&lt;p&gt; 1.1. David Hanson&amp;#39;s interdisciplinary background [0:01:49]&lt;/p&gt;
&lt;p&gt; 1.2. Introduction to Sofia, the realistic robot [0:03:27]&lt;/p&gt;
&lt;p&gt;2. Human Cognition and AI [0:03:50]&lt;/p&gt;
&lt;p&gt;  2.1. Importance of social interaction in cognition [0:03:50]&lt;/p&gt;
&lt;p&gt;  2.2. Compassion as distinguishing factor [0:05:55]&lt;/p&gt;
&lt;p&gt;  2.3. AI augmenting human intelligence [0:09:54]&lt;/p&gt;
&lt;p&gt;3. Developing Human-like AI [0:13:17]&lt;/p&gt;
&lt;p&gt;  3.1. Incorporating biological drives in AI [0:13:17]&lt;/p&gt;
&lt;p&gt;  3.2. Creating AI with agency [0:20:34]&lt;/p&gt;
&lt;p&gt;  3.3. Implementing flexible desires in AI [0:23:23]&lt;/p&gt;
&lt;p&gt;4. Ethics and Morality in AI [0:27:53]&lt;/p&gt;
&lt;p&gt;  4.1. Enhancing humanity through AI [0:27:53]&lt;/p&gt;
&lt;p&gt;  4.2. Existential pattern ethics [0:30:14]&lt;/p&gt;
&lt;p&gt;  4.3. Expanding morality beyond restrictions [0:35:35]&lt;/p&gt;
&lt;p&gt;5. Societal Impact of AI [0:38:07]&lt;/p&gt;
&lt;p&gt;  5.1. AI adoption and integration [0:38:07]&lt;/p&gt;
&lt;p&gt;  5.2. Democratizing AI technologies [0:38:32]&lt;/p&gt;
&lt;p&gt;  5.3. Human-AI integration and identity [0:43:37]&lt;/p&gt;
&lt;p&gt;6. Future Considerations [0:50:03]&lt;/p&gt;
&lt;p&gt;  6.1. Technological augmentation and inequality [0:50:03]&lt;/p&gt;
&lt;p&gt;  6.2. Emerging technologies for mental health [0:50:32]&lt;/p&gt;
&lt;p&gt;  6.3. Corporate ethics in AI development [0:52:26]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This was filmed at AGI-24&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1725971001041-aecbbb0e7c347.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Fabric of Knowledge - David Spivak]]></title>
			<description><![CDATA[<p>David Spivak, a mathematician known for his work in category theory, discusses a wide range of topics related to intelligence, creativity, and the nature of knowledge. He explains category theory in simple terms and explores how it relates to understanding complex systems and relationships.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>We discuss abstract concepts like collective intelligence, the importance of embodiment in understanding the world, and how we acquire and process knowledge. Spivak shares his thoughts on creativity, discussing where it comes from and how it might be modeled mathematically.</p>
<p><br></p>
<p>A significant portion of the discussion focuses on the impact of artificial intelligence on human thinking and its potential role in the evolution of intelligence. Spivak also touches on the importance of language, particularly written language, in transmitting knowledge and shaping our understanding of the world.</p>
<p><br></p>
<p>David Spivak</p>
<p>http://www.dspivak.net/</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Introduction to category theory and functors</p>
<p>00:04:40 Collective intelligence and sense-making</p>
<p>00:09:54 Embodiment and physical concepts in knowledge acquisition</p>
<p>00:16:23 Creativity, open-endedness, and AI&#39;s impact on thinking</p>
<p>00:25:46 Modeling creativity and the evolution of intelligence</p>
<p>00:36:04 Evolution, optimization, and the significance of AI</p>
<p>00:44:14 Written language and its impact on knowledge transmission</p>
<p><br></p>
<p>REFS:</p>
<p>Mike Levin&#39;s work</p>
<p>https://scholar.google.com/citations?user=luouyakAAAAJ&amp;hl=en</p>
<p>Eric Smith&#39;s videos on complexity and early life</p>
<p>https://www.youtube.com/watch?v=SpJZw-68QyE</p>
<p>Richard Dawkins&#39; book &quot;The Selfish Gene&quot;</p>
<p>https://amzn.to/3X73X8w</p>
<p>Carl Sagan&#39;s statement about the cosmos knowing itself</p>
<p>https://amzn.to/3XhPruK</p>
<p>Herbert Simon&#39;s concept of &quot;satisficing&quot;</p>
<p>https://plato.stanford.edu/entries/bounded-rationality/</p>
<p>DeepMind paper on open-ended systems</p>
<p>https://arxiv.org/abs/2406.04268</p>
<p>Karl Friston&#39;s work on active inference</p>
<p>https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind</p>
<p>MIT category theory lectures by David Spivak (available on the Topos Institute channel)</p>
<p>https://www.youtube.com/watch?v=UusLtx9fIjs</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Fabric-of-Knowledge---David-Spivak-e2o220h</link>
			<guid isPermaLink="false">95bcd85a-4f4c-4063-8bb2-1203e3dbd47d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 05 Sep 2024 17:56:26 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91342289/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-8-5%2F36a47360-f8fa-68aa-a676-7adfd1c3d0c8.mp3" length="67254254" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;David Spivak, a mathematician known for his work in category theory, discusses a wide range of topics related to intelligence, creativity, and the nature of knowledge. He explains category theory in simple terms and explores how it relates to understanding complex systems and relationships.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We discuss abstract concepts like collective intelligence, the importance of embodiment in understanding the world, and how we acquire and process knowledge. Spivak shares his thoughts on creativity, discussing where it comes from and how it might be modeled mathematically.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A significant portion of the discussion focuses on the impact of artificial intelligence on human thinking and its potential role in the evolution of intelligence. Spivak also touches on the importance of language, particularly written language, in transmitting knowledge and shaping our understanding of the world.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;David Spivak&lt;/p&gt;
&lt;p&gt;http://www.dspivak.net/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction to category theory and functors&lt;/p&gt;
&lt;p&gt;00:04:40 Collective intelligence and sense-making&lt;/p&gt;
&lt;p&gt;00:09:54 Embodiment and physical concepts in knowledge acquisition&lt;/p&gt;
&lt;p&gt;00:16:23 Creativity, open-endedness, and AI&amp;#39;s impact on thinking&lt;/p&gt;
&lt;p&gt;00:25:46 Modeling creativity and the evolution of intelligence&lt;/p&gt;
&lt;p&gt;00:36:04 Evolution, optimization, and the significance of AI&lt;/p&gt;
&lt;p&gt;00:44:14 Written language and its impact on knowledge transmission&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;REFS:&lt;/p&gt;
&lt;p&gt;Mike Levin&amp;#39;s work&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=luouyakAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;Eric Smith&amp;#39;s videos on complexity and early life&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=SpJZw-68QyE&lt;/p&gt;
&lt;p&gt;Richard Dawkins&amp;#39; book &amp;quot;The Selfish Gene&amp;quot;&lt;/p&gt;
&lt;p&gt;https://amzn.to/3X73X8w&lt;/p&gt;
&lt;p&gt;Carl Sagan&amp;#39;s statement about the cosmos knowing itself&lt;/p&gt;
&lt;p&gt;https://amzn.to/3XhPruK&lt;/p&gt;
&lt;p&gt;Herbert Simon&amp;#39;s concept of &amp;quot;satisficing&amp;quot;&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/bounded-rationality/&lt;/p&gt;
&lt;p&gt;DeepMind paper on open-ended systems&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2406.04268&lt;/p&gt;
&lt;p&gt;Karl Friston&amp;#39;s work on active inference&lt;/p&gt;
&lt;p&gt;https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind&lt;/p&gt;
&lt;p&gt;MIT category theory lectures by David Spivak (available on the Topos Institute channel)&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=UusLtx9fIjs&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:46:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1725558957564-4cc4fa362186a.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jürgen Schmidhuber - Neural and Non-Neural AI, Reasoning, Transformers, and LSTMs]]></title>
			<description><![CDATA[<p>Jürgen Schmidhuber, the father of generative AI shares his groundbreaking work in deep learning and artificial intelligence. In this exclusive interview, he discusses the history of AI, some of his contributions to the field, and his vision for the future of intelligent machines. Schmidhuber offers unique insights into the exponential growth of technology and the potential impact of AI on humanity and the universe.</p>
<p><br></p>
<p>YT version: https://youtu.be/DP454c1K_vQ</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>TOC</p>
<p>00:00:00 Intro</p>
<p>00:03:38 Reasoning</p>
<p>00:13:09 Potential AI Breakthroughs Reducing Computation Needs</p>
<p>00:20:39 Memorization vs. Generalization in AI</p>
<p>00:25:19 Approach to the ARC Challenge</p>
<p>00:29:10 Perceptions of Chat GPT and AGI</p>
<p>00:58:45 Abstract Principles of Jurgen&#39;s Approach</p>
<p>01:04:17 Analogical Reasoning and Compression</p>
<p>01:05:48 Breakthroughs in 1991: the P, the G, and the T in ChatGPT and Generative AI</p>
<p>01:15:50 Use of LSTM in Language Models by Tech Giants</p>
<p>01:21:08 Neural Network Aspect Ratio Theory</p>
<p>01:26:53 Reinforcement Learning Without Explicit Teachers</p>
<p><br></p>
<p>Refs:</p>
<p>★ &quot;Annotated History of Modern AI and Deep Learning&quot; (2022 survey by Schmidhuber):</p>
<p>★ Chain Rule For Backward Credit Assignment (Leibniz, 1676)</p>
<p>★ First Neural Net / Linear Regression / Shallow Learning (Gauss &amp; Legendre, circa 1800)</p>
<p>★ First 20th Century Pioneer of Practical AI (Quevedo, 1914)</p>
<p>★ First Recurrent NN (RNN) Architecture (Lenz, Ising, 1920-1925)</p>
<p>★ AI Theory: Fundamental Limitations of Computation and Computation-Based AI (Gödel, 1931-34)</p>
<p>★ Unpublished ideas about evolving RNNs (Turing, 1948)</p>
<p>★ Multilayer Feedforward NN Without Deep Learning (Rosenblatt, 1958)</p>
<p>★ First Published Learning RNNs (Amari and others, ~1972)</p>
<p>★ First Deep Learning (Ivakhnenko &amp; Lapa, 1965)</p>
<p>★ Deep Learning by Stochastic Gradient Descent (Amari, 1967-68)</p>
<p>★ ReLUs (Fukushima, 1969)</p>
<p>★ Backpropagation (Linnainmaa, 1970); precursor (Kelley, 1960)</p>
<p>★ Backpropagation for NNs (Werbos, 1982)</p>
<p>★ First Deep Convolutional NN (Fukushima, 1979); later combined with Backprop (Waibel 1987, Zhang 1988).</p>
<p>★ Metalearning or Learning to Learn (Schmidhuber, 1987)</p>
<p>★ Generative Adversarial Networks / Artificial Curiosity / NN Online Planners (Schmidhuber, Feb 1990; see the G in Generative AI and ChatGPT)</p>
<p>★ NNs Learn to Generate Subgoals and Work on Command (Schmidhuber, April 1990)</p>
<p>★ NNs Learn to Program NNs: Unnormalized Linear Transformer (Schmidhuber, March 1991; see the T in ChatGPT)</p>
<p>★ Deep Learning by Self-Supervised Pre-Training. Distilling NNs (Schmidhuber, April 1991; see the P in ChatGPT)</p>
<p>★ Experiments with Pre-Training; Analysis of Vanishing/Exploding Gradients, Roots of Long Short-Term Memory / Highway Nets / ResNets (Hochreiter, June 1991, further developed 1999-2015 with other students of Schmidhuber)</p>
<p>★ LSTM journal paper (1997, most cited AI paper of the 20th century)</p>
<p>★ xLSTM (Hochreiter, 2024)</p>
<p>★ Reinforcement Learning Prompt Engineer for Abstract Reasoning and Planning (Schmidhuber 2015)</p>
<p>★ Mindstorms in Natural Language-Based Societies of Mind (2023 paper by Schmidhuber&#39;s team)</p>
<p> https://arxiv.org/abs/2305.17066</p>
<p>★ Bremermann&#39;s physical limit of computation (1982)</p>
<p><br></p>
<p>EXTERNAL LINKS</p>
<p>CogX 2018 - Professor Juergen Schmidhuber</p>
<p>https://www.youtube.com/watch?v=17shdT9-wuA</p>
<p>Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability (Neural Networks, 1997)</p>
<p>https://sferics.idsia.ch/pub/juergen/loconet.pdf</p>
<p>The paradox at the heart of mathematics: Gödel&#39;s Incompleteness Theorem - Marcus du Sautoy</p>
<p>https://www.youtube.com/watch?v=I4pQbo5MQOs</p>
<p>(Refs truncated, full version on YT VD)</p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jrgen-Schmidhuber---Neural-and-Non-Neural-AI--Reasoning--Transformers--and-LSTMs-e2nnmjl</link>
			<guid isPermaLink="false">ad883c65-b25b-4723-8840-829b0a98ca5f</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 28 Aug 2024 17:44:55 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/91002933/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-28%2Fcf47b479-3c88-d4c3-62f1-009ef82e37c6.mp3" length="143970725" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Jürgen Schmidhuber, the father of generative AI shares his groundbreaking work in deep learning and artificial intelligence. In this exclusive interview, he discusses the history of AI, some of his contributions to the field, and his vision for the future of intelligent machines. Schmidhuber offers unique insights into the exponential growth of technology and the potential impact of AI on humanity and the universe.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/DP454c1K_vQ&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:03:38 Reasoning&lt;/p&gt;
&lt;p&gt;00:13:09 Potential AI Breakthroughs Reducing Computation Needs&lt;/p&gt;
&lt;p&gt;00:20:39 Memorization vs. Generalization in AI&lt;/p&gt;
&lt;p&gt;00:25:19 Approach to the ARC Challenge&lt;/p&gt;
&lt;p&gt;00:29:10 Perceptions of Chat GPT and AGI&lt;/p&gt;
&lt;p&gt;00:58:45 Abstract Principles of Jurgen&amp;#39;s Approach&lt;/p&gt;
&lt;p&gt;01:04:17 Analogical Reasoning and Compression&lt;/p&gt;
&lt;p&gt;01:05:48 Breakthroughs in 1991: the P, the G, and the T in ChatGPT and Generative AI&lt;/p&gt;
&lt;p&gt;01:15:50 Use of LSTM in Language Models by Tech Giants&lt;/p&gt;
&lt;p&gt;01:21:08 Neural Network Aspect Ratio Theory&lt;/p&gt;
&lt;p&gt;01:26:53 Reinforcement Learning Without Explicit Teachers&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;★ &amp;quot;Annotated History of Modern AI and Deep Learning&amp;quot; (2022 survey by Schmidhuber):&lt;/p&gt;
&lt;p&gt;★ Chain Rule For Backward Credit Assignment (Leibniz, 1676)&lt;/p&gt;
&lt;p&gt;★ First Neural Net / Linear Regression / Shallow Learning (Gauss &amp;amp; Legendre, circa 1800)&lt;/p&gt;
&lt;p&gt;★ First 20th Century Pioneer of Practical AI (Quevedo, 1914)&lt;/p&gt;
&lt;p&gt;★ First Recurrent NN (RNN) Architecture (Lenz, Ising, 1920-1925)&lt;/p&gt;
&lt;p&gt;★ AI Theory: Fundamental Limitations of Computation and Computation-Based AI (Gödel, 1931-34)&lt;/p&gt;
&lt;p&gt;★ Unpublished ideas about evolving RNNs (Turing, 1948)&lt;/p&gt;
&lt;p&gt;★ Multilayer Feedforward NN Without Deep Learning (Rosenblatt, 1958)&lt;/p&gt;
&lt;p&gt;★ First Published Learning RNNs (Amari and others, ~1972)&lt;/p&gt;
&lt;p&gt;★ First Deep Learning (Ivakhnenko &amp;amp; Lapa, 1965)&lt;/p&gt;
&lt;p&gt;★ Deep Learning by Stochastic Gradient Descent (Amari, 1967-68)&lt;/p&gt;
&lt;p&gt;★ ReLUs (Fukushima, 1969)&lt;/p&gt;
&lt;p&gt;★ Backpropagation (Linnainmaa, 1970); precursor (Kelley, 1960)&lt;/p&gt;
&lt;p&gt;★ Backpropagation for NNs (Werbos, 1982)&lt;/p&gt;
&lt;p&gt;★ First Deep Convolutional NN (Fukushima, 1979); later combined with Backprop (Waibel 1987, Zhang 1988).&lt;/p&gt;
&lt;p&gt;★ Metalearning or Learning to Learn (Schmidhuber, 1987)&lt;/p&gt;
&lt;p&gt;★ Generative Adversarial Networks / Artificial Curiosity / NN Online Planners (Schmidhuber, Feb 1990; see the G in Generative AI and ChatGPT)&lt;/p&gt;
&lt;p&gt;★ NNs Learn to Generate Subgoals and Work on Command (Schmidhuber, April 1990)&lt;/p&gt;
&lt;p&gt;★ NNs Learn to Program NNs: Unnormalized Linear Transformer (Schmidhuber, March 1991; see the T in ChatGPT)&lt;/p&gt;
&lt;p&gt;★ Deep Learning by Self-Supervised Pre-Training. Distilling NNs (Schmidhuber, April 1991; see the P in ChatGPT)&lt;/p&gt;
&lt;p&gt;★ Experiments with Pre-Training; Analysis of Vanishing/Exploding Gradients, Roots of Long Short-Term Memory / Highway Nets / ResNets (Hochreiter, June 1991, further developed 1999-2015 with other students of Schmidhuber)&lt;/p&gt;
&lt;p&gt;★ LSTM journal paper (1997, most cited AI paper of the 20th century)&lt;/p&gt;
&lt;p&gt;★ xLSTM (Hochreiter, 2024)&lt;/p&gt;
&lt;p&gt;★ Reinforcement Learning Prompt Engineer for Abstract Reasoning and Planning (Schmidhuber 2015)&lt;/p&gt;
&lt;p&gt;★ Mindstorms in Natural Language-Based Societies of Mind (2023 paper by Schmidhuber&amp;#39;s team)&lt;/p&gt;
&lt;p&gt; https://arxiv.org/abs/2305.17066&lt;/p&gt;
&lt;p&gt;★ Bremermann&amp;#39;s physical limit of computation (1982)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;EXTERNAL LINKS&lt;/p&gt;
&lt;p&gt;CogX 2018 - Professor Juergen Schmidhuber&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=17shdT9-wuA&lt;/p&gt;
&lt;p&gt;Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability (Neural Networks, 1997)&lt;/p&gt;
&lt;p&gt;https://sferics.idsia.ch/pub/juergen/loconet.pdf&lt;/p&gt;
&lt;p&gt;The paradox at the heart of mathematics: Gödel&amp;#39;s Incompleteness Theorem - Marcus du Sautoy&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=I4pQbo5MQOs&lt;/p&gt;
&lt;p&gt;(Refs truncated, full version on YT VD)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:39:39</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1724867069935-b9e722e99d531.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA["AI should NOT be regulated at all!" - Prof. Pedro Domingos]]></title>
			<description><![CDATA[<p>Professor Pedro Domingos, is an AI researcher and professor of computer science. He expresses skepticism about current AI regulation efforts and argues for faster AI development rather than slowing it down. He also discusses the need for new innovations to fulfil the promises of current AI techniques.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmented generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Show notes:</p>
<p>* Domingos&#39; views on AI regulation and why he believes it&#39;s misguided</p>
<p>* His thoughts on the current state of AI technology and its limitations</p>
<p>* Discussion of his novel &quot;2040&quot;, a satirical take on AI and tech culture</p>
<p>* Explanation of his work on &quot;tensor logic&quot;, which aims to unify neural networks and symbolic AI</p>
<p>* Critiques of other approaches in AI, including those of OpenAI and Gary Marcus</p>
<p>* Thoughts on the AI &quot;bubble&quot; and potential future developments in the field</p>
<p><br></p>
<p>Prof. Pedro Domingos:</p>
<p>https://x.com/pmddomingos</p>
<p><br></p>
<p>2040: A Silicon Valley Satire [Pedro&#39;s new book]</p>
<p>https://amzn.to/3T51ISd</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Intro</p>
<p>00:06:31 Bio</p>
<p>00:08:40 Filmmaking skit</p>
<p>00:10:35 AI and the wisdom of crowds</p>
<p>00:19:49 Social Media</p>
<p>00:27:48 Master algorithm</p>
<p>00:30:48 Neurosymbolic AI / abstraction</p>
<p>00:39:01 Language</p>
<p>00:45:38 Chomsky</p>
<p>01:00:49 2040 Book</p>
<p>01:18:03 Satire as a shield for criticism?</p>
<p>01:29:12 AI Regulation</p>
<p>01:35:15 Gary Marcus</p>
<p>01:52:37 Copyright</p>
<p>01:56:11 Stochastic parrots come home to roost</p>
<p>02:00:03 Privacy</p>
<p>02:01:55 LLM ecosystem</p>
<p>02:05:06 Tensor logic</p>
<p><br></p>
<p>Refs:</p>
<p>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World [Pedro Domingos]</p>
<p>https://amzn.to/3MiWs9B</p>
<p><br></p>
<p>Rebooting AI: Building Artificial Intelligence We Can Trust [Gary Marcus]</p>
<p>https://amzn.to/3AAywvL</p>
<p><br></p>
<p>Flash Boys [Michael Lewis]</p>
<p>https://amzn.to/4dUGm1M</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/AI-should-NOT-be-regulated-at-all----Prof--Pedro-Domingos-e2njada</link>
			<guid isPermaLink="false">173f5e18-4bb9-44e9-9c40-2fe3de0aef94</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 25 Aug 2024 14:05:24 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90859370/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-25%2F75faedd3-d828-e304-6b74-ef99b82e9929.mp3" length="190906507" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Pedro Domingos, is an AI researcher and professor of computer science. He expresses skepticism about current AI regulation efforts and argues for faster AI development rather than slowing it down. He also discusses the need for new innovations to fulfil the promises of current AI techniques.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmented generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Show notes:&lt;/p&gt;
&lt;p&gt;* Domingos&amp;#39; views on AI regulation and why he believes it&amp;#39;s misguided&lt;/p&gt;
&lt;p&gt;* His thoughts on the current state of AI technology and its limitations&lt;/p&gt;
&lt;p&gt;* Discussion of his novel &amp;quot;2040&amp;quot;, a satirical take on AI and tech culture&lt;/p&gt;
&lt;p&gt;* Explanation of his work on &amp;quot;tensor logic&amp;quot;, which aims to unify neural networks and symbolic AI&lt;/p&gt;
&lt;p&gt;* Critiques of other approaches in AI, including those of OpenAI and Gary Marcus&lt;/p&gt;
&lt;p&gt;* Thoughts on the AI &amp;quot;bubble&amp;quot; and potential future developments in the field&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. Pedro Domingos:&lt;/p&gt;
&lt;p&gt;https://x.com/pmddomingos&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;2040: A Silicon Valley Satire [Pedro&amp;#39;s new book]&lt;/p&gt;
&lt;p&gt;https://amzn.to/3T51ISd&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:06:31 Bio&lt;/p&gt;
&lt;p&gt;00:08:40 Filmmaking skit&lt;/p&gt;
&lt;p&gt;00:10:35 AI and the wisdom of crowds&lt;/p&gt;
&lt;p&gt;00:19:49 Social Media&lt;/p&gt;
&lt;p&gt;00:27:48 Master algorithm&lt;/p&gt;
&lt;p&gt;00:30:48 Neurosymbolic AI / abstraction&lt;/p&gt;
&lt;p&gt;00:39:01 Language&lt;/p&gt;
&lt;p&gt;00:45:38 Chomsky&lt;/p&gt;
&lt;p&gt;01:00:49 2040 Book&lt;/p&gt;
&lt;p&gt;01:18:03 Satire as a shield for criticism?&lt;/p&gt;
&lt;p&gt;01:29:12 AI Regulation&lt;/p&gt;
&lt;p&gt;01:35:15 Gary Marcus&lt;/p&gt;
&lt;p&gt;01:52:37 Copyright&lt;/p&gt;
&lt;p&gt;01:56:11 Stochastic parrots come home to roost&lt;/p&gt;
&lt;p&gt;02:00:03 Privacy&lt;/p&gt;
&lt;p&gt;02:01:55 LLM ecosystem&lt;/p&gt;
&lt;p&gt;02:05:06 Tensor logic&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World [Pedro Domingos]&lt;/p&gt;
&lt;p&gt;https://amzn.to/3MiWs9B&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Rebooting AI: Building Artificial Intelligence We Can Trust [Gary Marcus]&lt;/p&gt;
&lt;p&gt;https://amzn.to/3AAywvL&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Flash Boys [Michael Lewis]&lt;/p&gt;
&lt;p&gt;https://amzn.to/4dUGm1M&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:12:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1724594635630-c5e734f9fd018.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Adversarial Examples and Data Modelling - Andrew Ilyas (MIT)]]></title>
			<description><![CDATA[<p>Andrew Ilyas, a PhD student at MIT who is about to start as a professor at CMU. We discuss Data modeling and understanding how datasets influence model predictions, Adversarial examples in machine learning and why they occur, Robustness in machine learning models, Black box attacks on machine learning systems, Biases in data collection and dataset creation, particularly in ImageNet and Self-selection bias in data and methods to address it.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api </p>
<p><br></p>
<p>Andrew&#39;s site:</p>
<p>https://andrewilyas.com/</p>
<p>https://x.com/andrew_ilyas</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 - Introduction and Andrew&#39;s background</p>
<p>00:03:52 - Overview of the machine learning pipeline</p>
<p>00:06:31 - Data modeling paper discussion</p>
<p>00:26:28 - TRAK: Evolution of data modeling work</p>
<p>00:43:58 - Discussion on abstraction, reasoning, and neural networks</p>
<p>00:53:16 - &quot;Adversarial Examples Are Not Bugs, They Are Features&quot; paper</p>
<p>01:03:24 - Types of features learned by neural networks</p>
<p>01:10:51 - Black box attacks paper</p>
<p>01:15:39 - Work on data collection and bias</p>
<p>01:25:48 - Future research plans and closing thoughts</p>
<p><br></p>
<p>References:</p>
<p>Adversarial Examples Are Not Bugs, They Are Features</p>
<p>https://arxiv.org/pdf/1905.02175</p>
<p><br></p>
<p>TRAK: Attributing Model Behavior at Scale</p>
<p>https://arxiv.org/pdf/2303.14186</p>
<p><br></p>
<p>Datamodels: Predicting Predictions from Training Data</p>
<p>https://arxiv.org/pdf/2202.00622</p>
<p><br></p>
<p>Adversarial Examples Are Not Bugs, They Are Features</p>
<p>https://arxiv.org/pdf/1905.02175</p>
<p><br></p>
<p>IMAGENET-TRAINED CNNS</p>
<p>https://arxiv.org/pdf/1811.12231</p>
<p><br></p>
<p>ZOO: Zeroth Order Optimization Based Black-box</p>
<p>https://arxiv.org/pdf/1708.03999</p>
<p><br></p>
<p>A Spline Theory of Deep Networks</p>
<p>https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf</p>
<p><br></p>
<p>Scaling Monosemanticity</p>
<p>https://transformer-circuits.pub/2024/scaling-monosemanticity/</p>
<p><br></p>
<p>Adversarial Examples Are Not Bugs, They Are Features</p>
<p>https://gradientscience.org/adv/</p>
<p><br></p>
<p>Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies</p>
<p>https://proceedings.mlr.press/v235/bartoldson24a.html</p>
<p><br></p>
<p>Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors</p>
<p>https://arxiv.org/abs/1807.07978</p>
<p><br></p>
<p>Estimation of Standard Auction Models</p>
<p>https://arxiv.org/abs/2205.02060</p>
<p><br></p>
<p>From ImageNet to Image Classification: Contextualizing Progress on Benchmarks</p>
<p>https://arxiv.org/abs/2005.11295</p>
<p><br></p>
<p>Estimation of Standard Auction Models</p>
<p>https://arxiv.org/abs/2205.02060</p>
<p><br></p>
<p>What Makes A Good Fisherman? Linear Regression under Self-Selection Bias</p>
<p>https://arxiv.org/abs/2205.03246</p>
<p><br></p>
<p>Towards Tracing Factual Knowledge in Language Models Back to the</p>
<p>Training Data [Akyürek]</p>
<p>https://arxiv.org/pdf/2205.11482</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Adversarial-Examples-and-Data-Modelling---Andrew-Ilyas-MIT-e2nfov9</link>
			<guid isPermaLink="false">20355b07-60ae-4bb0-98ae-887fa03eed88</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 22 Aug 2024 07:02:03 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90743209/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-22%2F55ac46ce-094a-59bf-3bca-0792f898e1e2.mp3" length="127011728" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Andrew Ilyas, a PhD student at MIT who is about to start as a professor at CMU. We discuss Data modeling and understanding how datasets influence model predictions, Adversarial examples in machine learning and why they occur, Robustness in machine learning models, Black box attacks on machine learning systems, Biases in data collection and dataset creation, particularly in ImageNet and Self-selection bias in data and methods to address it.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Andrew&amp;#39;s site:&lt;/p&gt;
&lt;p&gt;https://andrewilyas.com/&lt;/p&gt;
&lt;p&gt;https://x.com/andrew_ilyas&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 - Introduction and Andrew&amp;#39;s background&lt;/p&gt;
&lt;p&gt;00:03:52 - Overview of the machine learning pipeline&lt;/p&gt;
&lt;p&gt;00:06:31 - Data modeling paper discussion&lt;/p&gt;
&lt;p&gt;00:26:28 - TRAK: Evolution of data modeling work&lt;/p&gt;
&lt;p&gt;00:43:58 - Discussion on abstraction, reasoning, and neural networks&lt;/p&gt;
&lt;p&gt;00:53:16 - &amp;quot;Adversarial Examples Are Not Bugs, They Are Features&amp;quot; paper&lt;/p&gt;
&lt;p&gt;01:03:24 - Types of features learned by neural networks&lt;/p&gt;
&lt;p&gt;01:10:51 - Black box attacks paper&lt;/p&gt;
&lt;p&gt;01:15:39 - Work on data collection and bias&lt;/p&gt;
&lt;p&gt;01:25:48 - Future research plans and closing thoughts&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Adversarial Examples Are Not Bugs, They Are Features&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1905.02175&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TRAK: Attributing Model Behavior at Scale&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2303.14186&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Datamodels: Predicting Predictions from Training Data&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2202.00622&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Adversarial Examples Are Not Bugs, They Are Features&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1905.02175&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;IMAGENET-TRAINED CNNS&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1811.12231&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;ZOO: Zeroth Order Optimization Based Black-box&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1708.03999&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Spline Theory of Deep Networks&lt;/p&gt;
&lt;p&gt;https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Scaling Monosemanticity&lt;/p&gt;
&lt;p&gt;https://transformer-circuits.pub/2024/scaling-monosemanticity/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Adversarial Examples Are Not Bugs, They Are Features&lt;/p&gt;
&lt;p&gt;https://gradientscience.org/adv/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies&lt;/p&gt;
&lt;p&gt;https://proceedings.mlr.press/v235/bartoldson24a.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1807.07978&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Estimation of Standard Auction Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2205.02060&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;From ImageNet to Image Classification: Contextualizing Progress on Benchmarks&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2005.11295&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Estimation of Standard Auction Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2205.02060&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What Makes A Good Fisherman? Linear Regression under Self-Selection Bias&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2205.03246&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Towards Tracing Factual Knowledge in Language Models Back to the&lt;/p&gt;
&lt;p&gt;Training Data [Akyürek]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2205.11482&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:28:00</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1724310071382-c6230fc6a99a3.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Joscha Bach - AGI24 Keynote (Cyberanimism)]]></title>
			<description><![CDATA[<p>Dr. Joscha Bach introduces a surprising idea called &quot;cyber animism&quot; in his AGI-24 talk - the notion that nature might be full of self-organizing software agents, similar to the spirits in ancient belief systems. Bach suggests that consciousness could be a kind of software running on our brains, and wonders if similar &quot;programs&quot; might exist in plants or even entire ecosystems. </p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Joscha takes us on a tour de force through history, philosophy, and cutting-edge computer science, teasing us to rethink what we know about minds, machines, and the world around us. Joscha believes we should blur the lines between human, artificial, and natural intelligence, and argues that consciousness might be more widespread and interconnected than we ever thought possible.</p>
<p><br></p>
<p>Dr. Joscha Bach</p>
<p>https://x.com/Plinz</p>
<p><br></p>
<p>This is video 2/9 from our coverage of AGI-24 in Seattle https://agi-conf.org/2024/ </p>
<p>Watch the official MLST interview with Joscha which we did right after this talk on our Patreon now on early access - https://www.patreon.com/posts/joscha-bach-110199676 (you also get access to our private discord and biweekly calls)</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Introduction: AGI and Cyberanimism</p>
<p>00:03:57 The Nature of Consciousness</p>
<p>00:08:46 Aristotle&#39;s Concepts of Mind and Consciousness</p>
<p>00:13:23 The Hard Problem of Consciousness</p>
<p>00:16:17 Functional Definition of Consciousness</p>
<p>00:20:24 Comparing LLMs and Human Consciousness</p>
<p>00:26:52 Testing for Consciousness in AI Systems</p>
<p>00:30:00 Animism and Software Agents in Nature</p>
<p>00:37:02 Plant Consciousness and Ecosystem Intelligence</p>
<p>00:40:36 The California Institute for Machine Consciousness</p>
<p>00:44:52 Ethics of Conscious AI and Suffering</p>
<p>00:46:29 Philosophical Perspectives on Consciousness</p>
<p>00:49:55 Q&amp;A: Formalisms for Conscious Systems</p>
<p>00:53:27 Coherence, Self-Organization, and Compute Resources</p>
<p><br></p>
<p>YT version (very high quality, filmed by us live)</p>
<p>https://youtu.be/34VOI_oo-qM</p>
<p><br></p>
<p>Refs:</p>
<p>Aristotle&#39;s work on the soul and consciousness</p>
<p>Richard Dawkins&#39; work on genes and evolution</p>
<p>Gerald Edelman&#39;s concept of Neural Darwinism</p>
<p>Thomas Metzinger&#39;s book &quot;Being No One&quot;</p>
<p>Yoshua Bengio&#39;s concept of the &quot;consciousness prior&quot;</p>
<p>Stuart Hameroff&#39;s theories on microtubules and consciousness</p>
<p>Christof Koch&#39;s work on consciousness</p>
<p>Daniel Dennett&#39;s &quot;Cartesian Theater&quot; concept</p>
<p>Giulio Tononi&#39;s Integrated Information Theory</p>
<p>Mike Levin&#39;s work on organismal intelligence</p>
<p>The concept of animism in various cultures</p>
<p>Freud&#39;s model of the mind</p>
<p>Buddhist perspectives on consciousness and meditation</p>
<p>The Genesis creation narrative (for its metaphorical interpretation)</p>
<p>California Institute for Machine Consciousness</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Joscha-Bach---AGI24-Keynote-Cyberanimism-e2neeui</link>
			<guid isPermaLink="false">7028322c-f965-4768-9f8c-478be64f8f3a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 21 Aug 2024 06:23:08 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90700178/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-21%2F90b41e5b-7b95-22bd-11d1-6ef4e7b11782.mp3" length="83023062" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Joscha Bach introduces a surprising idea called &amp;quot;cyber animism&amp;quot; in his AGI-24 talk - the notion that nature might be full of self-organizing software agents, similar to the spirits in ancient belief systems. Bach suggests that consciousness could be a kind of software running on our brains, and wonders if similar &amp;quot;programs&amp;quot; might exist in plants or even entire ecosystems. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Joscha takes us on a tour de force through history, philosophy, and cutting-edge computer science, teasing us to rethink what we know about minds, machines, and the world around us. Joscha believes we should blur the lines between human, artificial, and natural intelligence, and argues that consciousness might be more widespread and interconnected than we ever thought possible.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Joscha Bach&lt;/p&gt;
&lt;p&gt;https://x.com/Plinz&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This is video 2/9 from our coverage of AGI-24 in Seattle https://agi-conf.org/2024/ &lt;/p&gt;
&lt;p&gt;Watch the official MLST interview with Joscha which we did right after this talk on our Patreon now on early access - https://www.patreon.com/posts/joscha-bach-110199676 (you also get access to our private discord and biweekly calls)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction: AGI and Cyberanimism&lt;/p&gt;
&lt;p&gt;00:03:57 The Nature of Consciousness&lt;/p&gt;
&lt;p&gt;00:08:46 Aristotle&amp;#39;s Concepts of Mind and Consciousness&lt;/p&gt;
&lt;p&gt;00:13:23 The Hard Problem of Consciousness&lt;/p&gt;
&lt;p&gt;00:16:17 Functional Definition of Consciousness&lt;/p&gt;
&lt;p&gt;00:20:24 Comparing LLMs and Human Consciousness&lt;/p&gt;
&lt;p&gt;00:26:52 Testing for Consciousness in AI Systems&lt;/p&gt;
&lt;p&gt;00:30:00 Animism and Software Agents in Nature&lt;/p&gt;
&lt;p&gt;00:37:02 Plant Consciousness and Ecosystem Intelligence&lt;/p&gt;
&lt;p&gt;00:40:36 The California Institute for Machine Consciousness&lt;/p&gt;
&lt;p&gt;00:44:52 Ethics of Conscious AI and Suffering&lt;/p&gt;
&lt;p&gt;00:46:29 Philosophical Perspectives on Consciousness&lt;/p&gt;
&lt;p&gt;00:49:55 Q&amp;amp;A: Formalisms for Conscious Systems&lt;/p&gt;
&lt;p&gt;00:53:27 Coherence, Self-Organization, and Compute Resources&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version (very high quality, filmed by us live)&lt;/p&gt;
&lt;p&gt;https://youtu.be/34VOI_oo-qM&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;Aristotle&amp;#39;s work on the soul and consciousness&lt;/p&gt;
&lt;p&gt;Richard Dawkins&amp;#39; work on genes and evolution&lt;/p&gt;
&lt;p&gt;Gerald Edelman&amp;#39;s concept of Neural Darwinism&lt;/p&gt;
&lt;p&gt;Thomas Metzinger&amp;#39;s book &amp;quot;Being No One&amp;quot;&lt;/p&gt;
&lt;p&gt;Yoshua Bengio&amp;#39;s concept of the &amp;quot;consciousness prior&amp;quot;&lt;/p&gt;
&lt;p&gt;Stuart Hameroff&amp;#39;s theories on microtubules and consciousness&lt;/p&gt;
&lt;p&gt;Christof Koch&amp;#39;s work on consciousness&lt;/p&gt;
&lt;p&gt;Daniel Dennett&amp;#39;s &amp;quot;Cartesian Theater&amp;quot; concept&lt;/p&gt;
&lt;p&gt;Giulio Tononi&amp;#39;s Integrated Information Theory&lt;/p&gt;
&lt;p&gt;Mike Levin&amp;#39;s work on organismal intelligence&lt;/p&gt;
&lt;p&gt;The concept of animism in various cultures&lt;/p&gt;
&lt;p&gt;Freud&amp;#39;s model of the mind&lt;/p&gt;
&lt;p&gt;Buddhist perspectives on consciousness and meditation&lt;/p&gt;
&lt;p&gt;The Genesis creation narrative (for its metaphorical interpretation)&lt;/p&gt;
&lt;p&gt;California Institute for Machine Consciousness&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1724221270671-4c74498673a14.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Gary Marcus' keynote at AGI-24]]></title>
			<description><![CDATA[<p>Prof Gary Marcus revisited his keynote from AGI-21, noting that many of the issues he highlighted then are still relevant today despite significant advances in AI.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Gary Marcus criticized current large language models (LLMs) and generative AI for their unreliability, tendency to hallucinate, and inability to truly understand concepts.</p>
<p>Marcus argued that the AI field is experiencing diminishing returns with current approaches, particularly the &quot;scaling hypothesis&quot; that simply adding more data and compute will lead to AGI.</p>
<p>He advocated for a hybrid approach to AI that combines deep learning with symbolic AI, emphasizing the need for systems with deeper conceptual understanding.</p>
<p>Marcus highlighted the importance of developing AI with innate understanding of concepts like space, time, and causality.</p>
<p>He expressed concern about the moral decline in Silicon Valley and the rush to deploy potentially harmful AI technologies without adequate safeguards.</p>
<p>Marcus predicted a possible upcoming &quot;AI winter&quot; due to inflated valuations, lack of profitability, and overhyped promises in the industry.</p>
<p>He stressed the need for better regulation of AI, including transparency in training data, full disclosure of testing, and independent auditing of AI systems.</p>
<p>Marcus proposed the creation of national and global AI agencies to oversee the development and deployment of AI technologies.</p>
<p>He concluded by emphasizing the importance of interdisciplinary collaboration, focusing on robust AI with deep understanding, and implementing smart, agile governance for AI and AGI.</p>
<p><br></p>
<p>YT Version (very high quality filmed)</p>
<p>https://youtu.be/91SK90SahHc</p>
<p><br></p>
<p>Pre-order Gary&#39;s new book here:</p>
<p>Taming Silicon Valley: How We Can Ensure That AI Works for Us</p>
<p>https://amzn.to/4fO46pY</p>
<p><br></p>
<p>Filmed at the AGI-24 conference: </p>
<p>https://agi-conf.org/2024/</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Introduction</p>
<p>00:02:34 Introduction by Ben G</p>
<p>00:05:17 Gary Marcus begins talk</p>
<p>00:07:38 Critiquing current state of AI</p>
<p>00:12:21 Lack of progress on key AI challenges</p>
<p>00:16:05 Continued reliability issues with AI</p>
<p>00:19:54 Economic challenges for AI industry</p>
<p>00:25:11 Need for hybrid AI approaches</p>
<p>00:29:58 Moral decline in Silicon Valley</p>
<p>00:34:59 Risks of current generative AI</p>
<p>00:40:43 Need for AI regulation and governance</p>
<p>00:49:21 Concluding thoughts</p>
<p>00:54:38 Q&amp;A: Cycles of AI hype and winters</p>
<p>01:00:10 Predicting a potential AI winter</p>
<p>01:02:46 Discussion on interdisciplinary approach</p>
<p>01:05:46 Question on regulating AI</p>
<p>01:07:27 Ben G&#39;s perspective on AI winter</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Gary-Marcus-keynote-at-AGI-24-e2na1t7</link>
			<guid isPermaLink="false">e9dfd74a-de24-463f-9fd9-146ce0626d5c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 17 Aug 2024 20:35:43 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90555751/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-17%2F3488c017-c039-6cf1-7cb9-980321040c02.mp3" length="87069015" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof Gary Marcus revisited his keynote from AGI-21, noting that many of the issues he highlighted then are still relevant today despite significant advances in AI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gary Marcus criticized current large language models (LLMs) and generative AI for their unreliability, tendency to hallucinate, and inability to truly understand concepts.&lt;/p&gt;
&lt;p&gt;Marcus argued that the AI field is experiencing diminishing returns with current approaches, particularly the &amp;quot;scaling hypothesis&amp;quot; that simply adding more data and compute will lead to AGI.&lt;/p&gt;
&lt;p&gt;He advocated for a hybrid approach to AI that combines deep learning with symbolic AI, emphasizing the need for systems with deeper conceptual understanding.&lt;/p&gt;
&lt;p&gt;Marcus highlighted the importance of developing AI with innate understanding of concepts like space, time, and causality.&lt;/p&gt;
&lt;p&gt;He expressed concern about the moral decline in Silicon Valley and the rush to deploy potentially harmful AI technologies without adequate safeguards.&lt;/p&gt;
&lt;p&gt;Marcus predicted a possible upcoming &amp;quot;AI winter&amp;quot; due to inflated valuations, lack of profitability, and overhyped promises in the industry.&lt;/p&gt;
&lt;p&gt;He stressed the need for better regulation of AI, including transparency in training data, full disclosure of testing, and independent auditing of AI systems.&lt;/p&gt;
&lt;p&gt;Marcus proposed the creation of national and global AI agencies to oversee the development and deployment of AI technologies.&lt;/p&gt;
&lt;p&gt;He concluded by emphasizing the importance of interdisciplinary collaboration, focusing on robust AI with deep understanding, and implementing smart, agile governance for AI and AGI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT Version (very high quality filmed)&lt;/p&gt;
&lt;p&gt;https://youtu.be/91SK90SahHc&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Pre-order Gary&amp;#39;s new book here:&lt;/p&gt;
&lt;p&gt;Taming Silicon Valley: How We Can Ensure That AI Works for Us&lt;/p&gt;
&lt;p&gt;https://amzn.to/4fO46pY&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Filmed at the AGI-24 conference: &lt;/p&gt;
&lt;p&gt;https://agi-conf.org/2024/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&lt;/p&gt;
&lt;p&gt;00:02:34 Introduction by Ben G&lt;/p&gt;
&lt;p&gt;00:05:17 Gary Marcus begins talk&lt;/p&gt;
&lt;p&gt;00:07:38 Critiquing current state of AI&lt;/p&gt;
&lt;p&gt;00:12:21 Lack of progress on key AI challenges&lt;/p&gt;
&lt;p&gt;00:16:05 Continued reliability issues with AI&lt;/p&gt;
&lt;p&gt;00:19:54 Economic challenges for AI industry&lt;/p&gt;
&lt;p&gt;00:25:11 Need for hybrid AI approaches&lt;/p&gt;
&lt;p&gt;00:29:58 Moral decline in Silicon Valley&lt;/p&gt;
&lt;p&gt;00:34:59 Risks of current generative AI&lt;/p&gt;
&lt;p&gt;00:40:43 Need for AI regulation and governance&lt;/p&gt;
&lt;p&gt;00:49:21 Concluding thoughts&lt;/p&gt;
&lt;p&gt;00:54:38 Q&amp;amp;A: Cycles of AI hype and winters&lt;/p&gt;
&lt;p&gt;01:00:10 Predicting a potential AI winter&lt;/p&gt;
&lt;p&gt;01:02:46 Discussion on interdisciplinary approach&lt;/p&gt;
&lt;p&gt;01:05:46 Question on regulating AI&lt;/p&gt;
&lt;p&gt;01:07:27 Ben G&amp;#39;s perspective on AI winter&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:12:16</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1723926822877-878923c4f64cd.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Is ChatGPT an N-gram model on steroids?]]></title>
			<description><![CDATA[<p>DeepMind Research Scientist / MIT scholar Dr. Timothy Nguyen discusses his recent paper on understanding transformers through n-gram statistics. Nguyen explains his approach to analyzing transformer behavior using a kind of &quot;template matching&quot; (N-grams), providing insights into how these models process and predict language.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Key points covered include:</p>
<p>A method for describing transformer predictions using n-gram statistics without relying on internal mechanisms.</p>
<p>The discovery of a technique to detect overfitting in large language models without using holdout sets.</p>
<p>Observations on curriculum learning, showing how transformers progress from simpler to more complex rules during training.</p>
<p>Discussion of distance measures used in the analysis, particularly the variational distance.</p>
<p>Exploration of model sizes, training dynamics, and their impact on the results.</p>
<p><br></p>
<p>We also touch on philosophical aspects of describing versus explaining AI behavior, and the challenges in understanding the abstractions formed by neural networks. Nguyen concludes by discussing potential future research directions, including attempts to convert descriptions of transformer behavior into explanations of internal mechanisms.</p>
<p><br></p>
<p>Timothy Nguyen&#39;s earned his B.S. and Ph.D. in mathematics from Caltech and MIT, respectively. He held positions as Research Assistant Professor at the Simons Center for Geometry and Physics (2011-2014) and Visiting Assistant Professor at Michigan State University (2014-2017). During this time, his research expanded into high-energy physics, focusing on mathematical problems in quantum field theory. His work notably provided a simplified and corrected formulation of perturbative path integrals.</p>
<p><br></p>
<p>Since 2017, Nguyen has been working in industry, applying his expertise to machine learning. He is currently at DeepMind, where he contributes to both fundamental research and practical applications of deep learning to solve real-world problems.</p>
<p><br></p>
<p>Refs:</p>
<p>The Cartesian Cafe</p>
<p>https://www.youtube.com/@TimothyNguyen</p>
<p><br></p>
<p>Understanding Transformers via N-Gram Statistics</p>
<p>https://www.researchgate.net/publication/382204056_Understanding_Transformers_via_N-Gram_Statistics</p>
<p><br></p>
<p>TOC</p>
<p>00:00:00 Timothy Nguyen&#39;s background</p>
<p>00:02:50 Paper overview: transformers and n-gram statistics</p>
<p>00:04:55 Template matching and hash table approach</p>
<p>00:08:55 Comparing templates to transformer predictions</p>
<p>00:12:01 Describing vs explaining transformer behavior</p>
<p>00:15:36 Detecting overfitting without holdout sets</p>
<p>00:22:47 Curriculum learning in training</p>
<p>00:26:32 Distance measures in analysis</p>
<p>00:28:58 Model sizes and training dynamics</p>
<p>00:30:39 Future research directions</p>
<p>00:32:06 Conclusion and future topics</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Is-ChatGPT-an-N-gram-model-on-steroids-e2n74rr</link>
			<guid isPermaLink="false">ea872137-e817-4472-b744-b39d7a2c83c9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 15 Aug 2024 05:42:06 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90460475/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-15%2F8357f15a-4c0a-9942-6abe-41ce610b8b07.mp3" length="47794363" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;DeepMind Research Scientist / MIT scholar Dr. Timothy Nguyen discusses his recent paper on understanding transformers through n-gram statistics. Nguyen explains his approach to analyzing transformer behavior using a kind of &amp;quot;template matching&amp;quot; (N-grams), providing insights into how these models process and predict language.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Key points covered include:&lt;/p&gt;
&lt;p&gt;A method for describing transformer predictions using n-gram statistics without relying on internal mechanisms.&lt;/p&gt;
&lt;p&gt;The discovery of a technique to detect overfitting in large language models without using holdout sets.&lt;/p&gt;
&lt;p&gt;Observations on curriculum learning, showing how transformers progress from simpler to more complex rules during training.&lt;/p&gt;
&lt;p&gt;Discussion of distance measures used in the analysis, particularly the variational distance.&lt;/p&gt;
&lt;p&gt;Exploration of model sizes, training dynamics, and their impact on the results.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We also touch on philosophical aspects of describing versus explaining AI behavior, and the challenges in understanding the abstractions formed by neural networks. Nguyen concludes by discussing potential future research directions, including attempts to convert descriptions of transformer behavior into explanations of internal mechanisms.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Timothy Nguyen&amp;#39;s earned his B.S. and Ph.D. in mathematics from Caltech and MIT, respectively. He held positions as Research Assistant Professor at the Simons Center for Geometry and Physics (2011-2014) and Visiting Assistant Professor at Michigan State University (2014-2017). During this time, his research expanded into high-energy physics, focusing on mathematical problems in quantum field theory. His work notably provided a simplified and corrected formulation of perturbative path integrals.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Since 2017, Nguyen has been working in industry, applying his expertise to machine learning. He is currently at DeepMind, where he contributes to both fundamental research and practical applications of deep learning to solve real-world problems.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;The Cartesian Cafe&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/@TimothyNguyen&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Understanding Transformers via N-Gram Statistics&lt;/p&gt;
&lt;p&gt;https://www.researchgate.net/publication/382204056_Understanding_Transformers_via_N-Gram_Statistics&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;00:00:00 Timothy Nguyen&amp;#39;s background&lt;/p&gt;
&lt;p&gt;00:02:50 Paper overview: transformers and n-gram statistics&lt;/p&gt;
&lt;p&gt;00:04:55 Template matching and hash table approach&lt;/p&gt;
&lt;p&gt;00:08:55 Comparing templates to transformer predictions&lt;/p&gt;
&lt;p&gt;00:12:01 Describing vs explaining transformer behavior&lt;/p&gt;
&lt;p&gt;00:15:36 Detecting overfitting without holdout sets&lt;/p&gt;
&lt;p&gt;00:22:47 Curriculum learning in training&lt;/p&gt;
&lt;p&gt;00:26:32 Distance measures in analysis&lt;/p&gt;
&lt;p&gt;00:28:58 Model sizes and training dynamics&lt;/p&gt;
&lt;p&gt;00:30:39 Future research directions&lt;/p&gt;
&lt;p&gt;00:32:06 Conclusion and future topics&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:32:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1723700472413-f97b5084294ac.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jay Alammar on LLMs, RAG, and AI Engineering]]></title>
			<description><![CDATA[<p>Jay Alammar, renowned AI educator and researcher at Cohere, discusses the latest developments in large language models (LLMs) and their applications in industry. Jay shares his expertise on retrieval augmented generation (RAG), semantic search, and the future of AI architectures.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Cohere Command R model series: https://cohere.com/command</p>
<p><br></p>
<p>Jay Alamaar:</p>
<p>https://x.com/jayalammar</p>
<p><br></p>
<p>Buy Jay&#39;s new book here!</p>
<p>Hands-On Large Language Models: Language Understanding and Generation</p>
<p>https://amzn.to/4fzOUgh</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Introduction to Jay Alammar and AI Education</p>
<p>00:01:47 Cohere&#39;s Approach to RAG and AI Re-ranking</p>
<p>00:07:15 Implementing AI in Enterprise: Challenges and Solutions</p>
<p>00:09:26 Jay&#39;s Role at Cohere and the Importance of Learning in Public</p>
<p>00:15:16 The Evolution of AI in Industry: From Deep Learning to LLMs</p>
<p>00:26:12 Expert Advice for Newcomers in Machine Learning</p>
<p>00:32:39 The Power of Semantic Search and Embeddings in AI Systems</p>
<p>00:37:59 Jay Alammar&#39;s Journey as an AI Educator and Visualizer</p>
<p>00:43:36 Visual Learning in AI: Making Complex Concepts Accessible</p>
<p>00:47:38 Strategies for Keeping Up with Rapid AI Advancements</p>
<p>00:49:12 The Future of Transformer Models and AI Architectures</p>
<p>00:51:40 Evolution of the Transformer: From 2017 to Present</p>
<p>00:54:19 Preview of Jay&#39;s Upcoming Book on Large Language Models</p>
<p><br></p>
<p>Disclaimer: This is the fourth video from our Cohere partnership. We were not told what to say in the interview, and didn&#39;t edit anything out from the interview. Note also that this combines several previously unpublished interviews from Jay into one, the earlier one at Tim&#39;s house was shot in Aug 2023, and the more recent one in Toronto in May 2024. </p>
<p><br></p>
<p>Refs:</p>
<p>The Illustrated Transformer</p>
<p>https://jalammar.github.io/illustrated-transformer/</p>
<p><br></p>
<p>Attention Is All You Need</p>
<p>https://arxiv.org/abs/1706.03762</p>
<p><br></p>
<p>The Unreasonable Effectiveness of Recurrent Neural Networks</p>
<p>http://karpathy.github.io/2015/05/21/rnn-effectiveness/</p>
<p><br></p>
<p>Neural Networks in 11 Lines of Code</p>
<p>https://iamtrask.github.io/2015/07/12/basic-python-network/</p>
<p><br></p>
<p>Understanding LSTM Networks (Chris Olah&#39;s blog post)</p>
<p>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</p>
<p><br></p>
<p>Luis Serrano&#39;s YouTube Channel</p>
<p>https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ</p>
<p><br></p>
<p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</p>
<p>https://arxiv.org/abs/1908.10084</p>
<p><br></p>
<p>GPT (Generative Pre-trained Transformer) models</p>
<p>https://jalammar.github.io/illustrated-gpt2/</p>
<p>https://openai.com/research/gpt-4</p>
<p><br></p>
<p>BERT (Bidirectional Encoder Representations from Transformers)</p>
<p>https://jalammar.github.io/illustrated-bert/</p>
<p>https://arxiv.org/abs/1810.04805</p>
<p><br></p>
<p>RoPE (Rotary Positional Encoding)</p>
<p>https://arxiv.org/abs/2104.09864 (Linked paper discussing rotary embeddings)</p>
<p><br></p>
<p>Grouped Query Attention</p>
<p>https://arxiv.org/pdf/2305.13245</p>
<p><br></p>
<p>RLHF (Reinforcement Learning from Human Feedback)</p>
<p>https://openai.com/research/learning-from-human-preferences</p>
<p>https://arxiv.org/abs/1706.03741</p>
<p><br></p>
<p>DPO (Direct Preference Optimization)</p>
<p>https://arxiv.org/abs/2305.18290</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jay-Alammar-on-LLMs--RAG--and-AI-Engineering-e2n2koj</link>
			<guid isPermaLink="false">20dd8077-a65c-40a9-aae3-99c365e8e40a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 11 Aug 2024 20:16:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90312915/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-11%2F9be9c216-1f7a-6045-b79c-2dfedc30d327.mp3" length="83062074" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Jay Alammar, renowned AI educator and researcher at Cohere, discusses the latest developments in large language models (LLMs) and their applications in industry. Jay shares his expertise on retrieval augmented generation (RAG), semantic search, and the future of AI architectures.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cohere Command R model series: https://cohere.com/command&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Jay Alamaar:&lt;/p&gt;
&lt;p&gt;https://x.com/jayalammar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy Jay&amp;#39;s new book here!&lt;/p&gt;
&lt;p&gt;Hands-On Large Language Models: Language Understanding and Generation&lt;/p&gt;
&lt;p&gt;https://amzn.to/4fzOUgh&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction to Jay Alammar and AI Education&lt;/p&gt;
&lt;p&gt;00:01:47 Cohere&amp;#39;s Approach to RAG and AI Re-ranking&lt;/p&gt;
&lt;p&gt;00:07:15 Implementing AI in Enterprise: Challenges and Solutions&lt;/p&gt;
&lt;p&gt;00:09:26 Jay&amp;#39;s Role at Cohere and the Importance of Learning in Public&lt;/p&gt;
&lt;p&gt;00:15:16 The Evolution of AI in Industry: From Deep Learning to LLMs&lt;/p&gt;
&lt;p&gt;00:26:12 Expert Advice for Newcomers in Machine Learning&lt;/p&gt;
&lt;p&gt;00:32:39 The Power of Semantic Search and Embeddings in AI Systems&lt;/p&gt;
&lt;p&gt;00:37:59 Jay Alammar&amp;#39;s Journey as an AI Educator and Visualizer&lt;/p&gt;
&lt;p&gt;00:43:36 Visual Learning in AI: Making Complex Concepts Accessible&lt;/p&gt;
&lt;p&gt;00:47:38 Strategies for Keeping Up with Rapid AI Advancements&lt;/p&gt;
&lt;p&gt;00:49:12 The Future of Transformer Models and AI Architectures&lt;/p&gt;
&lt;p&gt;00:51:40 Evolution of the Transformer: From 2017 to Present&lt;/p&gt;
&lt;p&gt;00:54:19 Preview of Jay&amp;#39;s Upcoming Book on Large Language Models&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This is the fourth video from our Cohere partnership. We were not told what to say in the interview, and didn&amp;#39;t edit anything out from the interview. Note also that this combines several previously unpublished interviews from Jay into one, the earlier one at Tim&amp;#39;s house was shot in Aug 2023, and the more recent one in Toronto in May 2024. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;The Illustrated Transformer&lt;/p&gt;
&lt;p&gt;https://jalammar.github.io/illustrated-transformer/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Attention Is All You Need&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1706.03762&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/p&gt;
&lt;p&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Neural Networks in 11 Lines of Code&lt;/p&gt;
&lt;p&gt;https://iamtrask.github.io/2015/07/12/basic-python-network/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Understanding LSTM Networks (Chris Olah&amp;#39;s blog post)&lt;/p&gt;
&lt;p&gt;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Luis Serrano&amp;#39;s YouTube Channel&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1908.10084&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;GPT (Generative Pre-trained Transformer) models&lt;/p&gt;
&lt;p&gt;https://jalammar.github.io/illustrated-gpt2/&lt;/p&gt;
&lt;p&gt;https://openai.com/research/gpt-4&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;BERT (Bidirectional Encoder Representations from Transformers)&lt;/p&gt;
&lt;p&gt;https://jalammar.github.io/illustrated-bert/&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1810.04805&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;RoPE (Rotary Positional Encoding)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2104.09864 (Linked paper discussing rotary embeddings)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Grouped Query Attention&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2305.13245&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;RLHF (Reinforcement Learning from Human Feedback)&lt;/p&gt;
&lt;p&gt;https://openai.com/research/learning-from-human-preferences&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1706.03741&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;DPO (Direct Preference Optimization)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2305.18290&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1723407343266-bc1ebd90bd291.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Can AI therapy be more effective than drugs?]]></title>
			<description><![CDATA[<p>Daniel Cahn, co-founder of Slingshot AI, on the potential of AI in therapy. Why is anxiety and depression affecting a large population? To what extent are these real categories? Why is the mental health getting worse? How often do you want an AI to agree with you? What are the ethics of persuasive AI? You will discover all in this conversation. </p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>Daniel Cahn (who is also hiring ML engineers by the way!)</p>
<p>https://x.com/thecahnartist?lang=en</p>
<p> / cahnd </p>
<p>https://thinkingmachinespodcast.com/</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Intro</p>
<p>00:01:56 Therapy effectiveness vs drugs and societal implications</p>
<p>00:04:02 Mental health categories: Iatrogenesis and social constructs</p>
<p>00:10:19 Psychiatric treatment models and cognitive perspectives</p>
<p>00:13:30 AI design and human-like interactions: Intentionality debates</p>
<p>00:20:04 AI in therapy: Ethics, anthropomorphism, and loneliness mitigation</p>
<p>00:28:13 Therapy efficacy: Neuroplasticity, suffering, and AI placebos</p>
<p>00:33:29 AI&#39;s impact on human agency and cognitive modeling</p>
<p>00:41:17 Social media&#39;s effects on brain structure and behavior</p>
<p>00:50:46 AI ethics: Altering values and free will considerations</p>
<p>01:00:00 Work value perception and personal identity formation</p>
<p>01:13:37 Free will, agency, and mutable personal identity in therapy</p>
<p>01:24:27 AI in healthcare: Challenges, ethics, and therapy improvements</p>
<p>01:53:25 AI development: Societal impacts and cultural implications</p>
<p><br></p>
<p>Full references on YT VD: https://www.youtube.com/watch?v=7hwX6OZyNC0 (and baked into mp3 metadata)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Can-AI-therapy-be-more-effective-than-drugs-e2mvhd0</link>
			<guid isPermaLink="false">a0e209fc-f6c2-4b07-a62e-b9a4208513f9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 08 Aug 2024 18:30:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/90211168/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-7-8%2F7ab263b2-ff32-f2de-204b-b7bf4edb486b.mp3" length="193303413" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Daniel Cahn, co-founder of Slingshot AI, on the potential of AI in therapy. Why is anxiety and depression affecting a large population? To what extent are these real categories? Why is the mental health getting worse? How often do you want an AI to agree with you? What are the ethics of persuasive AI? You will discover all in this conversation. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Daniel Cahn (who is also hiring ML engineers by the way!)&lt;/p&gt;
&lt;p&gt;https://x.com/thecahnartist?lang=en&lt;/p&gt;
&lt;p&gt; / cahnd &lt;/p&gt;
&lt;p&gt;https://thinkingmachinespodcast.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:01:56 Therapy effectiveness vs drugs and societal implications&lt;/p&gt;
&lt;p&gt;00:04:02 Mental health categories: Iatrogenesis and social constructs&lt;/p&gt;
&lt;p&gt;00:10:19 Psychiatric treatment models and cognitive perspectives&lt;/p&gt;
&lt;p&gt;00:13:30 AI design and human-like interactions: Intentionality debates&lt;/p&gt;
&lt;p&gt;00:20:04 AI in therapy: Ethics, anthropomorphism, and loneliness mitigation&lt;/p&gt;
&lt;p&gt;00:28:13 Therapy efficacy: Neuroplasticity, suffering, and AI placebos&lt;/p&gt;
&lt;p&gt;00:33:29 AI&amp;#39;s impact on human agency and cognitive modeling&lt;/p&gt;
&lt;p&gt;00:41:17 Social media&amp;#39;s effects on brain structure and behavior&lt;/p&gt;
&lt;p&gt;00:50:46 AI ethics: Altering values and free will considerations&lt;/p&gt;
&lt;p&gt;01:00:00 Work value perception and personal identity formation&lt;/p&gt;
&lt;p&gt;01:13:37 Free will, agency, and mutable personal identity in therapy&lt;/p&gt;
&lt;p&gt;01:24:27 AI in healthcare: Challenges, ethics, and therapy improvements&lt;/p&gt;
&lt;p&gt;01:53:25 AI development: Societal impacts and cultural implications&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Full references on YT VD: https://www.youtube.com/watch?v=7hwX6OZyNC0 (and baked into mp3 metadata)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:14:07</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1723141816807-614c80a1573c7.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Subbarao Kambhampati - LLMs don't reason, they memorize (ICML2024 2/13)]]></title>
			<description><![CDATA[<p>Prof. Subbarao Kambhampati argues that while LLMs are impressive and useful tools, especially for creative tasks, they have fundamental limitations in logical reasoning and cannot provide guarantees about the correctness of their outputs. He advocates for hybrid approaches that combine LLMs with external verification systems.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. </p>
<p><br></p>
<p>TOC (sorry the ones baked into the MP3 were wrong apropos due to LLM hallucination!)</p>
<p>[00:00:00] Intro</p>
<p>[00:02:06] Bio</p>
<p>[00:03:02] LLMs are n-gram models on steroids</p>
<p>[00:07:26] Is natural language a formal language?</p>
<p>[00:08:34] Natural language is formal?</p>
<p>[00:11:01] Do LLMs reason?</p>
<p>[00:19:13] Definition of reasoning</p>
<p>[00:31:40] Creativity in reasoning</p>
<p>[00:50:27] Chollet&#39;s ARC challenge</p>
<p>[01:01:31] Can we reason without verification?</p>
<p>[01:10:00] LLMs cant solve some tasks</p>
<p>[01:19:07] LLM Modulo framework</p>
<p>[01:29:26] Future trends of architecture</p>
<p>[01:34:48] Future research directions</p>
<p><br></p>
<p>Youtube version: https://www.youtube.com/watch?v=y1WnHpedi2A</p>
<p><br></p>
<p>Refs: (we didn&#39;t have space for URLs here, check YT video description instead)</p>
<ul>
 <li>Can LLMs Really Reason and Plan?</li>
 <li>On the Planning Abilities of Large Language Models : A Critical Investigation</li>
  <li>Chain of Thoughtlessness? An Analysis of CoT in Planning</li>
  <li>On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks</li>
  <li>LLMs Can&#39;t Plan, But Can Help Planning in LLM-Modulo Frameworks</li>
  <li>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve</li>
  <li>&quot;Task Success&quot; is not Enough</li>
  <li>Partition function (number theory) (Srinivasa Ramanujan and G.H. Hardy&#39;s work)</li>
  <li>Poincaré conjecture</li>
  <li>Gödel&#39;s incompleteness theorems</li>
  <li>ROT13 (Rotate13, &quot;rotate by 13 places&quot;)</li>
  <li>A Mathematical Theory of Communication (C. E. SHANNON)</li>
  <li>Sparks of AGI</li>
  <li>Kambhampati thesis on speech recognition (1983)</li>
  <li>PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change</li>
  <li>Explainable human-AI interaction</li>
  <li>Tree of Thoughts</li>
  <li>On the Measure of Intelligence (ARC Challenge)</li>
  <li>Getting 50% (SoTA) on ARC-AGI with GPT-4o (Ryan Greenblatt ARC solution)</li>
  <li>PROGRAMS WITH COMMON SENSE (John McCarthy) - &quot;AI should be an advice taker program&quot;</li>
  <li>Original chain of thought paper</li>
  <li>ICAPS 2024 Keynote: Dale Schuurmans on &quot;Computing and Planning with Large Generative Models&quot; (COT)</li>
  <li>The Hardware Lottery (Hooker)</li>
  <li>A Path Towards Autonomous Machine Intelligence (JEPA/LeCun)</li>
  <li>AlphaGeometry</li>
  <li>FunSearch</li>
  <li>Emergent Abilities of Large Language Models</li>
  <li>Language models are not naysayers (Negation in LLMs)</li>
  <li>The Reversal Curse: LLMs trained on &quot;A is B&quot; fail to learn &quot;B is A&quot;</li>
  <li>Embracing negative results</li>
</ul>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Subbarao-Kambhampati---LLMs-dont-reason--they-memorize-ICML2024-213-e2mjcse</link>
			<guid isPermaLink="false">ed66eb64-43f7-4a8e-9891-38b34723880b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 29 Jul 2024 18:32:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/89813326/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-29%2F24b87b76-4192-b8fd-013e-4d817810331f.mp3" length="147853000" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Prof. Subbarao Kambhampati argues that while LLMs are impressive and useful tools, especially for creative tasks, they have fundamental limitations in logical reasoning and cannot provide guarantees about the correctness of their outputs. He advocates for hybrid approaches that combine LLMs with external verification systems.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at http://brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC (sorry the ones baked into the MP3 were wrong apropos due to LLM hallucination!)&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro&lt;/p&gt;
&lt;p&gt;[00:02:06] Bio&lt;/p&gt;
&lt;p&gt;[00:03:02] LLMs are n-gram models on steroids&lt;/p&gt;
&lt;p&gt;[00:07:26] Is natural language a formal language?&lt;/p&gt;
&lt;p&gt;[00:08:34] Natural language is formal?&lt;/p&gt;
&lt;p&gt;[00:11:01] Do LLMs reason?&lt;/p&gt;
&lt;p&gt;[00:19:13] Definition of reasoning&lt;/p&gt;
&lt;p&gt;[00:31:40] Creativity in reasoning&lt;/p&gt;
&lt;p&gt;[00:50:27] Chollet&amp;#39;s ARC challenge&lt;/p&gt;
&lt;p&gt;[01:01:31] Can we reason without verification?&lt;/p&gt;
&lt;p&gt;[01:10:00] LLMs cant solve some tasks&lt;/p&gt;
&lt;p&gt;[01:19:07] LLM Modulo framework&lt;/p&gt;
&lt;p&gt;[01:29:26] Future trends of architecture&lt;/p&gt;
&lt;p&gt;[01:34:48] Future research directions&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Youtube version: https://www.youtube.com/watch?v=y1WnHpedi2A&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs: (we didn&amp;#39;t have space for URLs here, check YT video description instead)&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Can LLMs Really Reason and Plan?&lt;/li&gt;
 &lt;li&gt;On the Planning Abilities of Large Language Models : A Critical Investigation&lt;/li&gt;
  &lt;li&gt;Chain of Thoughtlessness? An Analysis of CoT in Planning&lt;/li&gt;
  &lt;li&gt;On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks&lt;/li&gt;
  &lt;li&gt;LLMs Can&amp;#39;t Plan, But Can Help Planning in LLM-Modulo Frameworks&lt;/li&gt;
  &lt;li&gt;Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve&lt;/li&gt;
  &lt;li&gt;&amp;quot;Task Success&amp;quot; is not Enough&lt;/li&gt;
  &lt;li&gt;Partition function (number theory) (Srinivasa Ramanujan and G.H. Hardy&amp;#39;s work)&lt;/li&gt;
  &lt;li&gt;Poincaré conjecture&lt;/li&gt;
  &lt;li&gt;Gödel&amp;#39;s incompleteness theorems&lt;/li&gt;
  &lt;li&gt;ROT13 (Rotate13, &amp;quot;rotate by 13 places&amp;quot;)&lt;/li&gt;
  &lt;li&gt;A Mathematical Theory of Communication (C. E. SHANNON)&lt;/li&gt;
  &lt;li&gt;Sparks of AGI&lt;/li&gt;
  &lt;li&gt;Kambhampati thesis on speech recognition (1983)&lt;/li&gt;
  &lt;li&gt;PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change&lt;/li&gt;
  &lt;li&gt;Explainable human-AI interaction&lt;/li&gt;
  &lt;li&gt;Tree of Thoughts&lt;/li&gt;
  &lt;li&gt;On the Measure of Intelligence (ARC Challenge)&lt;/li&gt;
  &lt;li&gt;Getting 50% (SoTA) on ARC-AGI with GPT-4o (Ryan Greenblatt ARC solution)&lt;/li&gt;
  &lt;li&gt;PROGRAMS WITH COMMON SENSE (John McCarthy) - &amp;quot;AI should be an advice taker program&amp;quot;&lt;/li&gt;
  &lt;li&gt;Original chain of thought paper&lt;/li&gt;
  &lt;li&gt;ICAPS 2024 Keynote: Dale Schuurmans on &amp;quot;Computing and Planning with Large Generative Models&amp;quot; (COT)&lt;/li&gt;
  &lt;li&gt;The Hardware Lottery (Hooker)&lt;/li&gt;
  &lt;li&gt;A Path Towards Autonomous Machine Intelligence (JEPA/LeCun)&lt;/li&gt;
  &lt;li&gt;AlphaGeometry&lt;/li&gt;
  &lt;li&gt;FunSearch&lt;/li&gt;
  &lt;li&gt;Emergent Abilities of Large Language Models&lt;/li&gt;
  &lt;li&gt;Language models are not naysayers (Negation in LLMs)&lt;/li&gt;
  &lt;li&gt;The Reversal Curse: LLMs trained on &amp;quot;A is B&amp;quot; fail to learn &amp;quot;B is A&amp;quot;&lt;/li&gt;
  &lt;li&gt;Embracing negative results&lt;/li&gt;
&lt;/ul&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:42:27</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1722277732102-9938a2c0ff343.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sayash Kapoor - How seriously should we take AI X-risk? (ICML 1/13)]]></title>
			<description><![CDATA[<p>How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions.</p>
<p><br></p>
<p>MLST is sponsored by Brave:</p>
<p>The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at brave.com/api. </p>
<p><br></p>
<p>Sayash Kapoor is a computer science Ph.D. candidate at Princeton University&#39;s Center for Information Technology Policy. His research focuses on the societal impact of AI. Kapoor has previously worked on AI in both industry and academia, with experience at Facebook, Columbia University, and EPFL Switzerland. He is a recipient of a best paper award at ACM FAccT and an impact recognition award at ACM CSCW. Notably, Kapoor was included in TIME&#39;s inaugural list of the 100 most influential people in AI.</p>
<p><br></p>
<p>Sayash Kapoor</p>
<p>https://x.com/sayashk</p>
<p>https://www.cs.princeton.edu/~sayashk/</p>
<p><br></p>
<p>Arvind Narayanan (other half of the AI Snake Oil duo)</p>
<p>https://x.com/random_walker</p>
<p><br></p>
<p>AI existential risk probabilities are too unreliable to inform policy</p>
<p>https://www.aisnakeoil.com/p/ai-existential-risk-probabilities</p>
<p><br></p>
<p>Pre-order AI Snake Oil Book</p>
<p>https://amzn.to/4fq2HGb</p>
<p><br></p>
<p>AI Snake Oil blog</p>
<p>https://www.aisnakeoil.com/</p>
<p><br></p>
<p>AI Agents That Matter</p>
<p>https://arxiv.org/abs/2407.01502</p>
<p><br></p>
<p>Shortcut learning in deep neural networks</p>
<p>https://www.semanticscholar.org/paper/Shortcut-learning-in-deep-neural-networks-Geirhos-Jacobsen/1b04936c2599e59b120f743fbb30df2eed3fd782</p>
<p><br></p>
<p>77% Of Employees Report AI Has Increased Workloads And Hampered Productivity, Study Finds</p>
<p>https://www.forbes.com/sites/bryanrobinson/2024/07/23/employees-report-ai-increased-workload/</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Intro</p>
<p>00:01:57 How seriously should we take Xrisk threat?</p>
<p>00:02:55 Risk too unrealiable to inform policy</p>
<p>00:10:20 Overinflated risks</p>
<p>00:12:05 Perils of utility maximisation</p>
<p>00:13:55 Scaling vs airplane speeds</p>
<p>00:17:31 Shift to smaller models?</p>
<p>00:19:08 Commercial LLM ecosystem</p>
<p>00:22:10 Synthetic data</p>
<p>00:24:09 Is AI complexifying our jobs?</p>
<p>00:25:50 Does ChatGPT make us dumber or smarter?</p>
<p>00:26:55 Are AI Agents overhyped?</p>
<p>00:28:12 Simple vs complex baselines</p>
<p>00:30:00 Cost tradeoff in agent design</p>
<p>00:32:30 Model eval vs downastream perf</p>
<p>00:36:49 Shortcuts in metrics</p>
<p>00:40:09 Standardisation of agent evals</p>
<p>00:41:21 Humans in the loop</p>
<p>00:43:54 Levels of agent generality</p>
<p>00:47:25 ARC challenge</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sayash-Kapoor---How-seriously-should-we-take-AI-X-risk--ICML-113-e2mhuoc</link>
			<guid isPermaLink="false">797c903b-9639-4222-93e8-35b331cf409d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 28 Jul 2024 16:14:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/89766092/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-28%2Fc7d57c66-93af-1ac3-0a5c-b62ccdcdd61b.mp3" length="71890388" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;How seriously should governments take the threat of existential risk from AI, given the lack of consensus among researchers? On the one hand, existential risks (x-risks) are necessarily somewhat speculative: by the time there is concrete evidence, it may be too late. On the other hand, governments must prioritize — after all, they don’t worry too much about x-risk from alien invasions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST is sponsored by Brave:&lt;/p&gt;
&lt;p&gt;The Brave Search API covers over 20 billion webpages, built from scratch without Big Tech biases or the recent extortionate price hikes on search API access. Perfect for AI model training and retrieval augmentated generation. Try it now - get 2,000 free queries monthly at brave.com/api. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sayash Kapoor is a computer science Ph.D. candidate at Princeton University&amp;#39;s Center for Information Technology Policy. His research focuses on the societal impact of AI. Kapoor has previously worked on AI in both industry and academia, with experience at Facebook, Columbia University, and EPFL Switzerland. He is a recipient of a best paper award at ACM FAccT and an impact recognition award at ACM CSCW. Notably, Kapoor was included in TIME&amp;#39;s inaugural list of the 100 most influential people in AI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sayash Kapoor&lt;/p&gt;
&lt;p&gt;https://x.com/sayashk&lt;/p&gt;
&lt;p&gt;https://www.cs.princeton.edu/~sayashk/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Arvind Narayanan (other half of the AI Snake Oil duo)&lt;/p&gt;
&lt;p&gt;https://x.com/random_walker&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;AI existential risk probabilities are too unreliable to inform policy&lt;/p&gt;
&lt;p&gt;https://www.aisnakeoil.com/p/ai-existential-risk-probabilities&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Pre-order AI Snake Oil Book&lt;/p&gt;
&lt;p&gt;https://amzn.to/4fq2HGb&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;AI Snake Oil blog&lt;/p&gt;
&lt;p&gt;https://www.aisnakeoil.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;AI Agents That Matter&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2407.01502&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Shortcut learning in deep neural networks&lt;/p&gt;
&lt;p&gt;https://www.semanticscholar.org/paper/Shortcut-learning-in-deep-neural-networks-Geirhos-Jacobsen/1b04936c2599e59b120f743fbb30df2eed3fd782&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;77% Of Employees Report AI Has Increased Workloads And Hampered Productivity, Study Finds&lt;/p&gt;
&lt;p&gt;https://www.forbes.com/sites/bryanrobinson/2024/07/23/employees-report-ai-increased-workload/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:01:57 How seriously should we take Xrisk threat?&lt;/p&gt;
&lt;p&gt;00:02:55 Risk too unrealiable to inform policy&lt;/p&gt;
&lt;p&gt;00:10:20 Overinflated risks&lt;/p&gt;
&lt;p&gt;00:12:05 Perils of utility maximisation&lt;/p&gt;
&lt;p&gt;00:13:55 Scaling vs airplane speeds&lt;/p&gt;
&lt;p&gt;00:17:31 Shift to smaller models?&lt;/p&gt;
&lt;p&gt;00:19:08 Commercial LLM ecosystem&lt;/p&gt;
&lt;p&gt;00:22:10 Synthetic data&lt;/p&gt;
&lt;p&gt;00:24:09 Is AI complexifying our jobs?&lt;/p&gt;
&lt;p&gt;00:25:50 Does ChatGPT make us dumber or smarter?&lt;/p&gt;
&lt;p&gt;00:26:55 Are AI Agents overhyped?&lt;/p&gt;
&lt;p&gt;00:28:12 Simple vs complex baselines&lt;/p&gt;
&lt;p&gt;00:30:00 Cost tradeoff in agent design&lt;/p&gt;
&lt;p&gt;00:32:30 Model eval vs downastream perf&lt;/p&gt;
&lt;p&gt;00:36:49 Shortcuts in metrics&lt;/p&gt;
&lt;p&gt;00:40:09 Standardisation of agent evals&lt;/p&gt;
&lt;p&gt;00:41:21 Humans in the loop&lt;/p&gt;
&lt;p&gt;00:43:54 Levels of agent generality&lt;/p&gt;
&lt;p&gt;00:47:25 ARC challenge&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1722183224988-303275fe5a9c3.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sara Hooker - Why US AI Act Compute Thresholds Are Misguided]]></title>
			<description><![CDATA[<p>Sara Hooker is VP of Research at Cohere and leader of Cohere for AI. We discuss her recent paper critiquing the use of compute thresholds, measured in FLOPs (floating point operations), as an AI governance strategy. </p>
<p><br></p>
<p>We explore why this approach, recently adopted in both US and EU AI policies, may be problematic and oversimplified. Sara explains the limitations of using raw computational power as a measure of AI capability or risk, and discusses the complex relationship between compute, data, and model architecture.</p>
<p><br></p>
<p>Equally important, we go into Sara&#39;s work on &quot;The AI Language Gap.&quot; This research highlights the challenges and inequalities in developing AI systems that work across multiple languages. Sara discusses how current AI models, predominantly trained on English and a handful of high-resource languages, fail to serve the linguistic diversity of our global population. We explore the technical, ethical, and societal implications of this gap, and discuss potential solutions for creating more inclusive and representative AI systems.</p>
<p><br></p>
<p>We broadly discuss the relationship between language, culture, and AI capabilities, as well as the ethical considerations in AI development and deployment.</p>
<p><br></p>
<p>YT Version: https://youtu.be/dBZp47999Ko</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Intro</p>
<p>[00:02:12] FLOPS paper</p>
<p>[00:26:42] Hardware lottery</p>
<p>[00:30:22] The Language gap</p>
<p>[00:33:25] Safety</p>
<p>[00:38:31] Emergent</p>
<p>[00:41:23] Creativity</p>
<p>[00:43:40] Long tail</p>
<p>[00:44:26] LLMs and society</p>
<p>[00:45:36] Model bias</p>
<p>[00:48:51] Language and capabilities</p>
<p>[00:52:27] Ethical frameworks and RLHF</p>
<p><br></p>
<p><br></p>
<p>Sara Hooker</p>
<p>https://www.sarahooker.me/</p>
<p>https://www.linkedin.com/in/sararosehooker/</p>
<p>https://scholar.google.com/citations?user=2xy6h3sAAAAJ&amp;hl=en</p>
<p>https://x.com/sarahookr</p>
<p><br></p>
<p>Interviewer: Tim Scarfe</p>
<p><br></p>
<p>Refs</p>
<p><br></p>
<p>The AI Language gap</p>
<p>https://cohere.com/research/papers/the-AI-language-gap.pdf</p>
<p><br></p>
<p>On the Limitations of Compute Thresholds as a Governance Strategy.</p>
<p>https://arxiv.org/pdf/2407.05694v1</p>
<p><br></p>
<p>The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm</p>
<p>https://arxiv.org/pdf/2406.18682</p>
<p><br></p>
<p>Cohere Aya</p>
<p>https://cohere.com/research/aya</p>
<p><br></p>
<p>RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs</p>
<p>https://arxiv.org/pdf/2407.02552</p>
<p><br></p>
<p>Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs</p>
<p>https://arxiv.org/pdf/2402.14740</p>
<p><br></p>
<p>Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence</p>
<p>https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/</p>
<p><br></p>
<p>EU AI Act</p>
<p>https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf</p>
<p><br></p>
<p>The bitter lesson</p>
<p>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</p>
<p><br></p>
<p>Neel Nanda interview</p>
<p>https://www.youtube.com/watch?v=_Ygf0GnlwmY</p>
<p><br></p>
<p>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</p>
<p>https://transformer-circuits.pub/2024/scaling-monosemanticity/</p>
<p><br></p>
<p>Chollet&#39;s ARC challenge</p>
<p>https://github.com/fchollet/ARC-AGI</p>
<p><br></p>
<p>Ryan Greenblatt on ARC</p>
<p>https://www.youtube.com/watch?v=z9j3wB1RRGA</p>
<p><br></p>
<p>Disclaimer: This is the third video from our Cohere partnership. We were not told what to say in the interview, and didn&#39;t edit anything out from the interview.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sara-Hooker---Why-US-AI-Act-Compute-Thresholds-Are-Misguided-e2m6qm4</link>
			<guid isPermaLink="false">111a9825-6ebf-4474-8501-7195a56100f8</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 18 Jul 2024 22:51:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/89401476/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-18%2F91087ab7-05de-3101-b203-620838e6e030.mp3" length="95015061" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Sara Hooker is VP of Research at Cohere and leader of Cohere for AI. We discuss her recent paper critiquing the use of compute thresholds, measured in FLOPs (floating point operations), as an AI governance strategy. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We explore why this approach, recently adopted in both US and EU AI policies, may be problematic and oversimplified. Sara explains the limitations of using raw computational power as a measure of AI capability or risk, and discusses the complex relationship between compute, data, and model architecture.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Equally important, we go into Sara&amp;#39;s work on &amp;quot;The AI Language Gap.&amp;quot; This research highlights the challenges and inequalities in developing AI systems that work across multiple languages. Sara discusses how current AI models, predominantly trained on English and a handful of high-resource languages, fail to serve the linguistic diversity of our global population. We explore the technical, ethical, and societal implications of this gap, and discuss potential solutions for creating more inclusive and representative AI systems.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We broadly discuss the relationship between language, culture, and AI capabilities, as well as the ethical considerations in AI development and deployment.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT Version: https://youtu.be/dBZp47999Ko&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro&lt;/p&gt;
&lt;p&gt;[00:02:12] FLOPS paper&lt;/p&gt;
&lt;p&gt;[00:26:42] Hardware lottery&lt;/p&gt;
&lt;p&gt;[00:30:22] The Language gap&lt;/p&gt;
&lt;p&gt;[00:33:25] Safety&lt;/p&gt;
&lt;p&gt;[00:38:31] Emergent&lt;/p&gt;
&lt;p&gt;[00:41:23] Creativity&lt;/p&gt;
&lt;p&gt;[00:43:40] Long tail&lt;/p&gt;
&lt;p&gt;[00:44:26] LLMs and society&lt;/p&gt;
&lt;p&gt;[00:45:36] Model bias&lt;/p&gt;
&lt;p&gt;[00:48:51] Language and capabilities&lt;/p&gt;
&lt;p&gt;[00:52:27] Ethical frameworks and RLHF&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sara Hooker&lt;/p&gt;
&lt;p&gt;https://www.sarahooker.me/&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/sararosehooker/&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=2xy6h3sAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;https://x.com/sarahookr&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The AI Language gap&lt;/p&gt;
&lt;p&gt;https://cohere.com/research/papers/the-AI-language-gap.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;On the Limitations of Compute Thresholds as a Governance Strategy.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2407.05694v1&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2406.18682&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cohere Aya&lt;/p&gt;
&lt;p&gt;https://cohere.com/research/aya&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;RLHF Can Speak Many Languages: Unlocking Multilingual Preference Optimization for LLMs&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2407.02552&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2402.14740&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence&lt;/p&gt;
&lt;p&gt;https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;EU AI Act&lt;/p&gt;
&lt;p&gt;https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The bitter lesson&lt;/p&gt;
&lt;p&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Neel Nanda interview&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=_Ygf0GnlwmY&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet&lt;/p&gt;
&lt;p&gt;https://transformer-circuits.pub/2024/scaling-monosemanticity/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chollet&amp;#39;s ARC challenge&lt;/p&gt;
&lt;p&gt;https://github.com/fchollet/ARC-AGI&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ryan Greenblatt on ARC&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=z9j3wB1RRGA&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This is the third video from our Cohere partnership. We were not told what to say in the interview, and didn&amp;#39;t edit anything out from the interview.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:05:41</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1721343031115-7bc49ca2feb1a.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Murray Shanahan - Machines Don't Think Like Us]]></title>
			<description><![CDATA[<p>Murray Shanahan is a professor of Cognitive Robotics at Imperial College London and a senior research scientist at DeepMind. He challenges our assumptions about AI consciousness and urges us to rethink how we talk about machine intelligence. </p>
<p><br></p>
<p>We explore the dangers of anthropomorphizing AI, the limitations of current language in describing AI capabilities, and the fascinating intersection of philosophy and artificial intelligence.</p>
<p><br></p>
<p>Show notes and full references: https://docs.google.com/document/d/1ICtBI574W-xGi8Z2ZtUNeKWiOiGZ_DRsp9EnyYAISws/edit?usp=sharing </p>
<p><br></p>
<p>Prof Murray Shanahan:</p>
<p>https://www.doc.ic.ac.uk/~mpsha/ (look at his selected publications)</p>
<p>https://scholar.google.co.uk/citations?user=00bnGpAAAAAJ&amp;hl=en</p>
<p>https://en.wikipedia.org/wiki/Murray_Shanahan </p>
<p>https://x.com/mpshanahan </p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>Refs (links in the Google doc linked above):</p>
<p>Role play with large language models</p>
<p>Waluigi effect</p>
<p>&quot;Conscious Exotica&quot; - Paper by Murray Shanahan (2016)</p>
<p>&quot;Simulators&quot; - Article by Janis from LessWrong</p>
<p>&quot;Embodiment and the Inner Life&quot; - Book by Murray Shanahan (2010)</p>
<p>&quot;The Technological Singularity&quot; - Book by Murray Shanahan (2015)</p>
<p>&quot;Simulacra as Conscious Exotica&quot; - Paper by Murray Shanahan (newer paper of the original focussed on LLMs)</p>
<p>A recent paper by Anthropic on using autoencoders to find features in language models (referring to the &quot;Scaling Monosemanticity&quot; paper)</p>
<p>Work by Peter Godfrey-Smith on octopus consciousness</p>
<p>&quot;Metaphors We Live By&quot; - Book by George Lakoff (1980s)</p>
<p>Work by Aaron Sloman on the concept of &quot;space of possible minds&quot; (1984 article mentioned)</p>
<p>Wittgenstein&#39;s &quot;Philosophical Investigations&quot; (posthumously published)</p>
<p>Daniel Dennett&#39;s work on the &quot;intentional stance&quot;</p>
<p>Alan Turing&#39;s original paper on the Turing Test (1950)</p>
<p>Thomas Nagel&#39;s paper &quot;What is it like to be a bat?&quot; (1974)</p>
<p>John Searle&#39;s Chinese Room Argument (mentioned but not detailed)</p>
<p>Work by Richard Evans on tackling reasoning problems</p>
<p>Claude Shannon&#39;s quote on knowledge and control</p>
<p>&quot;Are We Bodies or Souls?&quot; - Book by Richard Swinburne</p>
<p>Reference to work by Ethan Perez and others at Anthropic on potential deceptive behavior in language models</p>
<p>Reference to a paper by Murray Shanahan and Antonia Creswell on the &quot;selection inference framework&quot;</p>
<p>Mention of work by Francois Chollet, particularly the ARC (Abstraction and Reasoning Corpus) challenge</p>
<p>Reference to Elizabeth Spelke&#39;s work on core knowledge in infants</p>
<p>Mention of Karl Friston&#39;s work on planning as inference (active inference)</p>
<p>The film &quot;Ex Machina&quot; - Murray Shanahan was the scientific advisor</p>
<p>&quot;The Waluigi Effect&quot;</p>
<p>Anthropic&#39;s constitutional AI approach</p>
<p>Loom system by Lara Reynolds and Kyle McDonald for visualizing conversation trees</p>
<p>DeepMind&#39;s AlphaGo (mentioned multiple times as an example)</p>
<p>Mention of the &quot;Golden Gate Claude&quot; experiment</p>
<p>Reference to an interview Tim Scarfe conducted with University of Toronto students about self-attention controllability theorem</p>
<p>Mention of an interview with Irina Rish</p>
<p>Reference to an interview Tim Scarfe conducted with Daniel Dennett</p>
<p>Reference to an interview with Maria Santa Caterina</p>
<p>Mention of an interview with Philip Goff</p>
<p>Nick Chater and Martin Christianson&#39;s book (&quot;The Language Game: How Improvisation Created Language and Changed the World&quot;)</p>
<p>Peter Singer&#39;s work from 1975 on ascribing moral status to conscious beings</p>
<p>Demis Hassabis&#39; discussion on the &quot;ladder of creativity&quot;</p>
<p>Reference to B.F. Skinner and behaviorism</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Murray-Shanahan---Machines-Dont-Think-Like-Us-e2m1499</link>
			<guid isPermaLink="false">440edf42-54c6-4273-a226-457dd4baf2de</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 14 Jul 2024 19:58:21 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/89214697/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-14%2F45f81b8b-673a-f931-4cb5-8955ca84aaf6.mp3" length="195292252" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Murray Shanahan is a professor of Cognitive Robotics at Imperial College London and a senior research scientist at DeepMind. He challenges our assumptions about AI consciousness and urges us to rethink how we talk about machine intelligence. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We explore the dangers of anthropomorphizing AI, the limitations of current language in describing AI capabilities, and the fascinating intersection of philosophy and artificial intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Show notes and full references: https://docs.google.com/document/d/1ICtBI574W-xGi8Z2ZtUNeKWiOiGZ_DRsp9EnyYAISws/edit?usp=sharing &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof Murray Shanahan:&lt;/p&gt;
&lt;p&gt;https://www.doc.ic.ac.uk/~mpsha/ (look at his selected publications)&lt;/p&gt;
&lt;p&gt;https://scholar.google.co.uk/citations?user=00bnGpAAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Murray_Shanahan &lt;/p&gt;
&lt;p&gt;https://x.com/mpshanahan &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs (links in the Google doc linked above):&lt;/p&gt;
&lt;p&gt;Role play with large language models&lt;/p&gt;
&lt;p&gt;Waluigi effect&lt;/p&gt;
&lt;p&gt;&amp;quot;Conscious Exotica&amp;quot; - Paper by Murray Shanahan (2016)&lt;/p&gt;
&lt;p&gt;&amp;quot;Simulators&amp;quot; - Article by Janis from LessWrong&lt;/p&gt;
&lt;p&gt;&amp;quot;Embodiment and the Inner Life&amp;quot; - Book by Murray Shanahan (2010)&lt;/p&gt;
&lt;p&gt;&amp;quot;The Technological Singularity&amp;quot; - Book by Murray Shanahan (2015)&lt;/p&gt;
&lt;p&gt;&amp;quot;Simulacra as Conscious Exotica&amp;quot; - Paper by Murray Shanahan (newer paper of the original focussed on LLMs)&lt;/p&gt;
&lt;p&gt;A recent paper by Anthropic on using autoencoders to find features in language models (referring to the &amp;quot;Scaling Monosemanticity&amp;quot; paper)&lt;/p&gt;
&lt;p&gt;Work by Peter Godfrey-Smith on octopus consciousness&lt;/p&gt;
&lt;p&gt;&amp;quot;Metaphors We Live By&amp;quot; - Book by George Lakoff (1980s)&lt;/p&gt;
&lt;p&gt;Work by Aaron Sloman on the concept of &amp;quot;space of possible minds&amp;quot; (1984 article mentioned)&lt;/p&gt;
&lt;p&gt;Wittgenstein&amp;#39;s &amp;quot;Philosophical Investigations&amp;quot; (posthumously published)&lt;/p&gt;
&lt;p&gt;Daniel Dennett&amp;#39;s work on the &amp;quot;intentional stance&amp;quot;&lt;/p&gt;
&lt;p&gt;Alan Turing&amp;#39;s original paper on the Turing Test (1950)&lt;/p&gt;
&lt;p&gt;Thomas Nagel&amp;#39;s paper &amp;quot;What is it like to be a bat?&amp;quot; (1974)&lt;/p&gt;
&lt;p&gt;John Searle&amp;#39;s Chinese Room Argument (mentioned but not detailed)&lt;/p&gt;
&lt;p&gt;Work by Richard Evans on tackling reasoning problems&lt;/p&gt;
&lt;p&gt;Claude Shannon&amp;#39;s quote on knowledge and control&lt;/p&gt;
&lt;p&gt;&amp;quot;Are We Bodies or Souls?&amp;quot; - Book by Richard Swinburne&lt;/p&gt;
&lt;p&gt;Reference to work by Ethan Perez and others at Anthropic on potential deceptive behavior in language models&lt;/p&gt;
&lt;p&gt;Reference to a paper by Murray Shanahan and Antonia Creswell on the &amp;quot;selection inference framework&amp;quot;&lt;/p&gt;
&lt;p&gt;Mention of work by Francois Chollet, particularly the ARC (Abstraction and Reasoning Corpus) challenge&lt;/p&gt;
&lt;p&gt;Reference to Elizabeth Spelke&amp;#39;s work on core knowledge in infants&lt;/p&gt;
&lt;p&gt;Mention of Karl Friston&amp;#39;s work on planning as inference (active inference)&lt;/p&gt;
&lt;p&gt;The film &amp;quot;Ex Machina&amp;quot; - Murray Shanahan was the scientific advisor&lt;/p&gt;
&lt;p&gt;&amp;quot;The Waluigi Effect&amp;quot;&lt;/p&gt;
&lt;p&gt;Anthropic&amp;#39;s constitutional AI approach&lt;/p&gt;
&lt;p&gt;Loom system by Lara Reynolds and Kyle McDonald for visualizing conversation trees&lt;/p&gt;
&lt;p&gt;DeepMind&amp;#39;s AlphaGo (mentioned multiple times as an example)&lt;/p&gt;
&lt;p&gt;Mention of the &amp;quot;Golden Gate Claude&amp;quot; experiment&lt;/p&gt;
&lt;p&gt;Reference to an interview Tim Scarfe conducted with University of Toronto students about self-attention controllability theorem&lt;/p&gt;
&lt;p&gt;Mention of an interview with Irina Rish&lt;/p&gt;
&lt;p&gt;Reference to an interview Tim Scarfe conducted with Daniel Dennett&lt;/p&gt;
&lt;p&gt;Reference to an interview with Maria Santa Caterina&lt;/p&gt;
&lt;p&gt;Mention of an interview with Philip Goff&lt;/p&gt;
&lt;p&gt;Nick Chater and Martin Christianson&amp;#39;s book (&amp;quot;The Language Game: How Improvisation Created Language and Changed the World&amp;quot;)&lt;/p&gt;
&lt;p&gt;Peter Singer&amp;#39;s work from 1975 on ascribing moral status to conscious beings&lt;/p&gt;
&lt;p&gt;Demis Hassabis&amp;#39; discussion on the &amp;quot;ladder of creativity&amp;quot;&lt;/p&gt;
&lt;p&gt;Reference to B.F. Skinner and behaviorism&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:15:22</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1720987044858-65d8ad0483066.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[David Chalmers - Reality+]]></title>
			<description><![CDATA[<p>In the coming decades, the technology that enables virtual and augmented reality will improve beyond recognition. Within a century, world-renowned philosopher David J. Chalmers predicts, we will have virtual worlds that are impossible to distinguish from non-virtual worlds. But is virtual reality just escapism?</p>
<p><br></p>
<p>In a highly original work of &#39;technophilosophy&#39;, Chalmers argues categorically, no: virtual reality is genuine reality. Virtual worlds are not second-class worlds. We can live a meaningful life in virtual reality - and increasingly, we will.</p>
<p><br></p>
<p>What is reality, anyway? How can we lead a good life? Is there a god? How do we know there&#39;s an external world - and how do we know we&#39;re not living in a computer simulation? In Reality+, Chalmers conducts a grand tour of philosophy, using cutting-edge technology to provide invigorating new answers to age-old questions.</p>
<p><br></p>
<p>David J. Chalmers is an Australian philosopher and cognitive scientist specializing in the areas of philosophy of mind and philosophy of language. He is Professor of Philosophy and Neural Science at New York University, as well as co-director of NYU&#39;s Center for Mind, Brain, and Consciousness. Chalmers is best known for his work on consciousness, including his formulation of the &quot;hard problem of consciousness.&quot;</p>
<p><br></p>
<p>Reality+: Virtual Worlds and the Problems of Philosophy</p>
<p>https://amzn.to/3RYyGD2</p>
<p><br></p>
<p>https://consc.net/</p>
<p>https://x.com/davidchalmers42</p>
<p><br></p>
<p>00:00:00 Reality+ Intro</p>
<p>00:12:02 GPT conscious? 10/10</p>
<p>00:14:19 The consciousness processor thought experiment (11/10)</p>
<p>00:20:34 Intelligence and Consciousness entangled? 10/10</p>
<p>00:22:44 Karl Friston / Meta Problem 10/10</p>
<p>00:29:05 Knowledge argument / subjective experience (6/10)</p>
<p>00:32:34 Emergence 11/10 (best chapter)</p>
<p>00:42:45 Working with Douglas Hofstadter 10/10</p>
<p>00:46:14 Intelligence is analogy making? 10/10</p>
<p>00:50:47 Intelligence explosion 8/10</p>
<p>00:58:44 Hypercomputation 10/10</p>
<p>01:09:44 Who designed the designer? (7/10)</p>
<p>01:13:57 Experience machine (7/10)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/David-Chalmers---Reality-e2lq0f1</link>
			<guid isPermaLink="false">72818461-e3ba-40ac-ba4f-6a86fe3e193e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 08 Jul 2024 18:54:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/88981409/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-8%2Fcf89bea4-5bf7-5d7d-4292-d8cf67b3de65.mp3" length="112563254" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In the coming decades, the technology that enables virtual and augmented reality will improve beyond recognition. Within a century, world-renowned philosopher David J. Chalmers predicts, we will have virtual worlds that are impossible to distinguish from non-virtual worlds. But is virtual reality just escapism?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In a highly original work of &amp;#39;technophilosophy&amp;#39;, Chalmers argues categorically, no: virtual reality is genuine reality. Virtual worlds are not second-class worlds. We can live a meaningful life in virtual reality - and increasingly, we will.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What is reality, anyway? How can we lead a good life? Is there a god? How do we know there&amp;#39;s an external world - and how do we know we&amp;#39;re not living in a computer simulation? In Reality+, Chalmers conducts a grand tour of philosophy, using cutting-edge technology to provide invigorating new answers to age-old questions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;David J. Chalmers is an Australian philosopher and cognitive scientist specializing in the areas of philosophy of mind and philosophy of language. He is Professor of Philosophy and Neural Science at New York University, as well as co-director of NYU&amp;#39;s Center for Mind, Brain, and Consciousness. Chalmers is best known for his work on consciousness, including his formulation of the &amp;quot;hard problem of consciousness.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Reality+: Virtual Worlds and the Problems of Philosophy&lt;/p&gt;
&lt;p&gt;https://amzn.to/3RYyGD2&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://consc.net/&lt;/p&gt;
&lt;p&gt;https://x.com/davidchalmers42&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Reality+ Intro&lt;/p&gt;
&lt;p&gt;00:12:02 GPT conscious? 10/10&lt;/p&gt;
&lt;p&gt;00:14:19 The consciousness processor thought experiment (11/10)&lt;/p&gt;
&lt;p&gt;00:20:34 Intelligence and Consciousness entangled? 10/10&lt;/p&gt;
&lt;p&gt;00:22:44 Karl Friston / Meta Problem 10/10&lt;/p&gt;
&lt;p&gt;00:29:05 Knowledge argument / subjective experience (6/10)&lt;/p&gt;
&lt;p&gt;00:32:34 Emergence 11/10 (best chapter)&lt;/p&gt;
&lt;p&gt;00:42:45 Working with Douglas Hofstadter 10/10&lt;/p&gt;
&lt;p&gt;00:46:14 Intelligence is analogy making? 10/10&lt;/p&gt;
&lt;p&gt;00:50:47 Intelligence explosion 8/10&lt;/p&gt;
&lt;p&gt;00:58:44 Hypercomputation 10/10&lt;/p&gt;
&lt;p&gt;01:09:44 Who designed the designer? (7/10)&lt;/p&gt;
&lt;p&gt;01:13:57 Experience machine (7/10)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:17:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1720464802150-2568943de2188.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Ryan Greenblatt - Solving ARC with GPT4o]]></title>
			<description><![CDATA[<p>Ryan Greenblatt from Redwood Research recently published &quot;Getting 50% on ARC-AGI with GPT-4.0,&quot; where he used GPT4o to reach a state-of-the-art accuracy on Francois Chollet&#39;s ARC Challenge by generating many Python programs. </p>
<p><br></p>
<p>Sponsor:</p>
<p>Sign up to Kalshi here https://kalshi.onelink.me/1r91/mlst -- the first 500 traders who deposit $100 will get a free $20 credit! Important disclaimer - In case it&#39;s not obvious - this is basically gambling and a *high risk* activity - only trade what you can afford to lose.￼</p>
<p><br></p>
<p>We discuss:</p>
<p>- Ryan&#39;s unique approach to solving the ARC Challenge and achieving impressive results.</p>
<p>- The strengths and weaknesses of current AI models.</p>
<p>- How AI and humans differ in learning and reasoning.</p>
<p>- Combining various techniques to create smarter AI systems.</p>
<p>- The potential risks and future advancements in AI, including the idea of agentic AI.</p>
<p><br></p>
<p>https://x.com/RyanPGreenblatt</p>
<p>https://www.redwoodresearch.org/</p>
<p><br></p>
<p><br></p>
<p>Refs:</p>
<p>Getting 50% (SoTA) on ARC-AGI with GPT-4o [Ryan Greenblatt]</p>
<p>https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt</p>
<p><br></p>
<p>On the Measure of Intelligence [Chollet]</p>
<p>https://arxiv.org/abs/1911.01547</p>
<p><br></p>
<p>Connectionism and Cognitive Architecture: A Critical Analysis [Jerry A. Fodor and Zenon W. Pylyshyn]</p>
<p>https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf</p>
<p><br></p>
<p>Software 2.0 [Andrej Karpathy]</p>
<p>https://karpathy.medium.com/software-2-0-a64152b37c35</p>
<p><br></p>
<p>Why Greatness Cannot Be Planned: The Myth of the Objective [Kenneth Stanley]</p>
<p>https://amzn.to/3Wfy2E0</p>
<p><br></p>
<p>Biographical account of Terence Tao’s mathematical development. [M.A.(KEN) CLEMENTS]</p>
<p>https://gwern.net/doc/iq/high/smpy/1984-clements.pdf</p>
<p><br></p>
<p>Model Evaluation and Threat Research (METR)</p>
<p>https://metr.org/</p>
<p><br></p>
<p>Why Tool AIs Want to Be Agent AIs</p>
<p>https://gwern.net/tool-ai</p>
<p><br></p>
<p>Simulators - Janus</p>
<p>https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators</p>
<p><br></p>
<p>AI Control: Improving Safety Despite Intentional Subversion</p>
<p>https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion</p>
<p>https://arxiv.org/abs/2312.06942</p>
<p><br></p>
<p>What a Compute-Centric Framework Says About Takeoff Speeds</p>
<p>https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/</p>
<p><br></p>
<p>Global GDP over the long run</p>
<p>https://ourworldindata.org/grapher/global-gdp-over-the-long-run?yScale=log</p>
<p><br></p>
<p>Safety Cases: How to Justify the Safety of Advanced AI Systems</p>
<p>https://arxiv.org/abs/2403.10462</p>
<p><br></p>
<p>The Danger of a “Safety Case&quot;</p>
<p>http://sunnyday.mit.edu/The-Danger-of-a-Safety-Case.pdf</p>
<p><br></p>
<p>The Future Of Work Looks Like A UPS Truck (~02:15:50)</p>
<p>https://www.npr.org/sections/money/2014/05/02/308640135/episode-536-the-future-of-work-looks-like-a-ups-truck</p>
<p><br></p>
<p>SWE-bench</p>
<p>https://www.swebench.com/</p>
<p><br></p>
<p>Using DeepSpeed and Megatron to Train Megatron-Turing NLG</p>
<p>530B, A Large-Scale Generative Language Model</p>
<p>https://arxiv.org/pdf/2201.11990 </p>
<p><br></p>
<p>Algorithmic Progress in Language Models </p>
<p>https://epochai.org/blog/algorithmic-progress-in-language-models</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Ryan-Greenblatt---Solving-ARC-with-GPT4o-e2lnplq</link>
			<guid isPermaLink="false">c8ecccaa-9fdc-47c2-9ca9-e6ea5f09233d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 06 Jul 2024 19:14:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/88908922/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-6-6%2F17966cac-d485-a913-5ade-6644c3feacfc.mp3" length="198911948" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Ryan Greenblatt from Redwood Research recently published &amp;quot;Getting 50% on ARC-AGI with GPT-4.0,&amp;quot; where he used GPT4o to reach a state-of-the-art accuracy on Francois Chollet&amp;#39;s ARC Challenge by generating many Python programs. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sponsor:&lt;/p&gt;
&lt;p&gt;Sign up to Kalshi here https://kalshi.onelink.me/1r91/mlst -- the first 500 traders who deposit $100 will get a free $20 credit! Important disclaimer - In case it&amp;#39;s not obvious - this is basically gambling and a *high risk* activity - only trade what you can afford to lose.￼&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We discuss:&lt;/p&gt;
&lt;p&gt;- Ryan&amp;#39;s unique approach to solving the ARC Challenge and achieving impressive results.&lt;/p&gt;
&lt;p&gt;- The strengths and weaknesses of current AI models.&lt;/p&gt;
&lt;p&gt;- How AI and humans differ in learning and reasoning.&lt;/p&gt;
&lt;p&gt;- Combining various techniques to create smarter AI systems.&lt;/p&gt;
&lt;p&gt;- The potential risks and future advancements in AI, including the idea of agentic AI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://x.com/RyanPGreenblatt&lt;/p&gt;
&lt;p&gt;https://www.redwoodresearch.org/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;Getting 50% (SoTA) on ARC-AGI with GPT-4o [Ryan Greenblatt]&lt;/p&gt;
&lt;p&gt;https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;On the Measure of Intelligence [Chollet]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Connectionism and Cognitive Architecture: A Critical Analysis [Jerry A. Fodor and Zenon W. Pylyshyn]&lt;/p&gt;
&lt;p&gt;https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Software 2.0 [Andrej Karpathy]&lt;/p&gt;
&lt;p&gt;https://karpathy.medium.com/software-2-0-a64152b37c35&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Why Greatness Cannot Be Planned: The Myth of the Objective [Kenneth Stanley]&lt;/p&gt;
&lt;p&gt;https://amzn.to/3Wfy2E0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Biographical account of Terence Tao’s mathematical development. [M.A.(KEN) CLEMENTS]&lt;/p&gt;
&lt;p&gt;https://gwern.net/doc/iq/high/smpy/1984-clements.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Model Evaluation and Threat Research (METR)&lt;/p&gt;
&lt;p&gt;https://metr.org/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Why Tool AIs Want to Be Agent AIs&lt;/p&gt;
&lt;p&gt;https://gwern.net/tool-ai&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Simulators - Janus&lt;/p&gt;
&lt;p&gt;https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;AI Control: Improving Safety Despite Intentional Subversion&lt;/p&gt;
&lt;p&gt;https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2312.06942&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What a Compute-Centric Framework Says About Takeoff Speeds&lt;/p&gt;
&lt;p&gt;https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Global GDP over the long run&lt;/p&gt;
&lt;p&gt;https://ourworldindata.org/grapher/global-gdp-over-the-long-run?yScale=log&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Safety Cases: How to Justify the Safety of Advanced AI Systems&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2403.10462&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Danger of a “Safety Case&amp;quot;&lt;/p&gt;
&lt;p&gt;http://sunnyday.mit.edu/The-Danger-of-a-Safety-Case.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Future Of Work Looks Like A UPS Truck (~02:15:50)&lt;/p&gt;
&lt;p&gt;https://www.npr.org/sections/money/2014/05/02/308640135/episode-536-the-future-of-work-looks-like-a-ups-truck&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;SWE-bench&lt;/p&gt;
&lt;p&gt;https://www.swebench.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Using DeepSpeed and Megatron to Train Megatron-Turing NLG&lt;/p&gt;
&lt;p&gt;530B, A Large-Scale Generative Language Model&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2201.11990 &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Algorithmic Progress in Language Models &lt;/p&gt;
&lt;p&gt;https://epochai.org/blog/algorithmic-progress-in-language-models&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:18:01</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1720293234618-be8ca539c2fa9.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Aiden Gomez - CEO of Cohere (AI's 'Inner Monologue' – Crucial for Reasoning)]]></title>
			<description><![CDATA[<p>Aidan Gomez, CEO of Cohere, reveals how they&#39;re tackling AI hallucinations and improving reasoning abilities. He also explains why Cohere doesn&#39;t use any output from GPT-4 for training their models.</p>
<p><br></p>
<p>Aidan shares his personal insights into the world of AI and LLMs and Cohere&#39;s unique approach to solving real-world business problems, and how their models are set apart from the competition. Aidan reveals how they are making major strides in AI technology, discussing everything from last mile customer engineering to the robustness of prompts and future architectures. </p>
<p><br></p>
<p>He also touches on the broader implications of AI for society, including potential risks and the role of regulation. He discusses Cohere&#39;s guiding principles and the health the of startup scene. With a particular focus on enterprise applications. Aidan provides a rare look into the internal workings of Cohere and their vision for driving productivity and innovation.</p>
<p><br></p>
<p>https://cohere.com/</p>
<p>https://x.com/aidangomez</p>
<p><br></p>
<p>Check out Cohere&#39;s amazing new Command R* models here</p>
<p>https://cohere.com/command</p>
<p><br></p>
<p>Disclaimer: This is the second video from our Cohere partnership. We were not told what to say in the interview, and didn&#39;t edit anything out from the interview.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Aiden-Gomez---CEO-of-Cohere-AIs-Inner-Monologue--Crucial-for-Reasoning-e2lf8fg</link>
			<guid isPermaLink="false">41d7fda2-7841-4b49-8f01-fa662a74d4d9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 29 Jun 2024 21:00:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/88629168/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-29%2F8d59b65c-4d2c-5ef7-07aa-f848990d9725.mp3" length="87316778" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Aidan Gomez, CEO of Cohere, reveals how they&amp;#39;re tackling AI hallucinations and improving reasoning abilities. He also explains why Cohere doesn&amp;#39;t use any output from GPT-4 for training their models.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Aidan shares his personal insights into the world of AI and LLMs and Cohere&amp;#39;s unique approach to solving real-world business problems, and how their models are set apart from the competition. Aidan reveals how they are making major strides in AI technology, discussing everything from last mile customer engineering to the robustness of prompts and future architectures. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;He also touches on the broader implications of AI for society, including potential risks and the role of regulation. He discusses Cohere&amp;#39;s guiding principles and the health the of startup scene. With a particular focus on enterprise applications. Aidan provides a rare look into the internal workings of Cohere and their vision for driving productivity and innovation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://cohere.com/&lt;/p&gt;
&lt;p&gt;https://x.com/aidangomez&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Check out Cohere&amp;#39;s amazing new Command R* models here&lt;/p&gt;
&lt;p&gt;https://cohere.com/command&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: This is the second video from our Cohere partnership. We were not told what to say in the interview, and didn&amp;#39;t edit anything out from the interview.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:00:22</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1719694778690-2c53dc029669c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[New "50%" ARC result and current winners interviewed ]]></title>
			<description><![CDATA[<p>The ARC Challenge, created by Francois Chollet, tests how well AI systems can generalize from a few examples in a grid-based intelligence test. We interview the current winners of the ARC Challenge—Jack Cole, Mohammed Osman and their collaborator Michael Hodel. They discuss how they tackled ARC (Abstraction and Reasoning Corpus) using language models. We also discuss the new &quot;50%&quot; public set approach announced today from Redwood Research (Ryan Greenblatt).

Jack and Mohammed explain their winning approach, which involves fine-tuning a language model on a large, specifically-generated dataset and then doing additional fine-tuning at test-time, a technique known in this context as &quot;active inference&quot;. They use various strategies to represent the data for the language model and believe that with further improvements, the accuracy could reach above 50%. Michael talks about his work on generating new ARC-like tasks to help train the models. 

They also debate whether their methods stay true to the &quot;spirit&quot; of Chollet&#39;s measure of intelligence. Despite some concerns, they agree that their solutions are promising and adaptable for other similar problems. 

Note:
Jack&#39;s team is still the current official winner at 33% on the private set. Ryan&#39;s entry is not on the private leaderboard or eligible.
Chollet invented ARC in 2019 (not 2017 as stated)

&quot;Ryan&#39;s entry is not a new state of the art. We don&#39;t know exactly how well it does since it was only evaluated on 100 tasks from the evaluation set and does 50% on those, reportedly. Meanwhile Jacks team i.e. MindsAI&#39;s solution does 54% on the entire eval set and it is seemingly possible to do 60-70% with an ensemble&quot;

Jack Cole:
https://x.com/Jcole75Cole
https://lab42.global/community-interview-jack-cole/

Mohamed Osman:
Mohamed is looking to do a PhD in AI/ML, can you help him? 
Email: mothman198@outlook.com
https://www.linkedin.com/in/mohamedosman1905/

Michael Hodel:
https://arxiv.org/pdf/2404.07353v1
https://www.linkedin.com/in/michael-hodel/
https://x.com/bayesilicon
https://github.com/michaelhodel

Getting 50% (SoTA) on ARC-AGI with GPT-4o - Ryan Greenblatt
https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt

Neural networks for abstraction and reasoning: Towards broad generalization in machines [Mikel Bober-Irizar, Soumya Banerjee]
https://arxiv.org/pdf/2402.03507

Measure of intelligence:
https://arxiv.org/abs/1911.01547

YT version: https://youtu.be/jSAT_RuJ_Cg
</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/New-50-ARC-result-and-current-winners-interviewed-e2l1prl</link>
			<guid isPermaLink="false">8eb32119-f72a-4e75-a300-1eed3a9719ec</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 18 Jun 2024 21:57:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/88188213/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-18%2Fdb923632-5035-adf8-0949-08dd818b67e0.mp3" length="193802824" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;The ARC Challenge, created by Francois Chollet, tests how well AI systems can generalize from a few examples in a grid-based intelligence test. We interview the current winners of the ARC Challenge—Jack Cole, Mohammed Osman and their collaborator Michael Hodel. They discuss how they tackled ARC (Abstraction and Reasoning Corpus) using language models. We also discuss the new &amp;quot;50%&amp;quot; public set approach announced today from Redwood Research (Ryan Greenblatt).

Jack and Mohammed explain their winning approach, which involves fine-tuning a language model on a large, specifically-generated dataset and then doing additional fine-tuning at test-time, a technique known in this context as &amp;quot;active inference&amp;quot;. They use various strategies to represent the data for the language model and believe that with further improvements, the accuracy could reach above 50%. Michael talks about his work on generating new ARC-like tasks to help train the models. 

They also debate whether their methods stay true to the &amp;quot;spirit&amp;quot; of Chollet&amp;#39;s measure of intelligence. Despite some concerns, they agree that their solutions are promising and adaptable for other similar problems. 

Note:
Jack&amp;#39;s team is still the current official winner at 33% on the private set. Ryan&amp;#39;s entry is not on the private leaderboard or eligible.
Chollet invented ARC in 2019 (not 2017 as stated)

&amp;quot;Ryan&amp;#39;s entry is not a new state of the art. We don&amp;#39;t know exactly how well it does since it was only evaluated on 100 tasks from the evaluation set and does 50% on those, reportedly. Meanwhile Jacks team i.e. MindsAI&amp;#39;s solution does 54% on the entire eval set and it is seemingly possible to do 60-70% with an ensemble&amp;quot;

Jack Cole:
https://x.com/Jcole75Cole
https://lab42.global/community-interview-jack-cole/

Mohamed Osman:
Mohamed is looking to do a PhD in AI/ML, can you help him? 
Email: mothman198@outlook.com
https://www.linkedin.com/in/mohamedosman1905/

Michael Hodel:
https://arxiv.org/pdf/2404.07353v1
https://www.linkedin.com/in/michael-hodel/
https://x.com/bayesilicon
https://github.com/michaelhodel

Getting 50% (SoTA) on ARC-AGI with GPT-4o - Ryan Greenblatt
https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt

Neural networks for abstraction and reasoning: Towards broad generalization in machines [Mikel Bober-Irizar, Soumya Banerjee]
https://arxiv.org/pdf/2402.03507

Measure of intelligence:
https://arxiv.org/abs/1911.01547

YT version: https://youtu.be/jSAT_RuJ_Cg
&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:14:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1718747822086-85885af71ccba.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Cohere co-founder Nick Frosst on building LLM apps for business]]></title>
			<description><![CDATA[<p>Nick Frosst, co-founder of Cohere, on the future of LLMs, and AGI. Learn how Cohere is solving real problems for business with their new AI models.</p>
<p><br></p>
<p>This is the first podcast from our new Cohere partnership!</p>
<p><br></p>
<p>Nick talks about his journey at Google Brain, working with AI legends like Geoff Hinton, and the amazing things his company, Cohere, is doing. From creating the must useful language models for businesses to making tools for developers, Nick shares a lot of interesting insights. He even talks about his band, Good Kid! Nick said that RAG is one of the best features of Cohere&#39;s new Command R* models. We are about to release a deep-dive on RAG with Patrick Lewis from Cohere, keep an eye out for that - he explains why their models are specifically optimised for RAG use cases.</p>
<p><br></p>
<p>Learn more about Cohere Command R* models here:</p>
<p>https://cohere.com/commandhttps://github.com/cohere-ai/cohere-toolkit</p>
<p><br></p>
<p>Nick&#39;s band Good Kid:</p>
<p>https://goodkidofficial.com/</p>
<p><br></p>
<p>Nick on Twitter:</p>
<p>https://x.com/nickfrosst</p>
<p><br></p>
<p>Disclaimer: We are in a partnership with Cohere to release content for them. We were not told what to say in the interview, and didn&#39;t edit anything out from the interview. We are currently planning to release 2 shows per month under the partnership about their AI platform, research and strategy.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Cohere-co-founder-Nick-Frosst-on-building-LLM-apps-for-business-e2kuiu2</link>
			<guid isPermaLink="false">f0b80aa4-e77b-48de-b00f-bcee4505a9e3</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 16 Jun 2024 12:00:01 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/88082818/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-16%2Faf493afb-1f13-ef9c-a566-c75b56696a06.mp3" length="60034327" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Nick Frosst, co-founder of Cohere, on the future of LLMs, and AGI. Learn how Cohere is solving real problems for business with their new AI models.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This is the first podcast from our new Cohere partnership!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Nick talks about his journey at Google Brain, working with AI legends like Geoff Hinton, and the amazing things his company, Cohere, is doing. From creating the must useful language models for businesses to making tools for developers, Nick shares a lot of interesting insights. He even talks about his band, Good Kid! Nick said that RAG is one of the best features of Cohere&amp;#39;s new Command R* models. We are about to release a deep-dive on RAG with Patrick Lewis from Cohere, keep an eye out for that - he explains why their models are specifically optimised for RAG use cases.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Learn more about Cohere Command R* models here:&lt;/p&gt;
&lt;p&gt;https://cohere.com/commandhttps://github.com/cohere-ai/cohere-toolkit&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Nick&amp;#39;s band Good Kid:&lt;/p&gt;
&lt;p&gt;https://goodkidofficial.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Nick on Twitter:&lt;/p&gt;
&lt;p&gt;https://x.com/nickfrosst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: We are in a partnership with Cohere to release content for them. We were not told what to say in the interview, and didn&amp;#39;t edit anything out from the interview. We are currently planning to release 2 shows per month under the partnership about their AI platform, research and strategy.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:41:25</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1718539112020-e5f9d89f9e34a.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[What’s the Magic Word? A Control Theory of LLM Prompting.]]></title>
			<description><![CDATA[<p>These two scientists have mapped out the insides or “reachable space” of a language model using control theory, what they discovered was extremely surprising. </p>
<p><br></p>
<p>Please support us on Patreon to get access to the private Discord server, bi-weekly calls, early access and ad-free listening.</p>
<p>https://patreon.com/mlst</p>
<p><br></p>
<p>YT version: https://youtu.be/Bpgloy1dDn0</p>
<p><br></p>
<p>Aman Bhargava from Caltech and Cameron Witkowski from the University of Toronto to discuss their groundbreaking paper, “What’s the Magic Word? A Control Theory of LLM Prompting.” (the main theorem on self-attention controllability was developed in collaboration with Dr. Shi-Zhuo Looi from Caltech).</p>
<p><br></p>
<p>They frame LLM systems as discrete stochastic dynamical systems. This means they look at LLMs in a structured way, similar to how we analyze control systems in engineering. They explore the “reachable set” of outputs for an LLM. Essentially, this is the range of possible outputs the model can generate from a given starting point when influenced by different prompts. The research highlights that prompt engineering, or optimizing the input tokens, can significantly influence LLM outputs. They show that even short prompts can drastically alter the likelihood of specific outputs. Aman and Cameron’s work might be a boon for understanding and improving LLMs. They suggest that a deeper exploration of control theory concepts could lead to more reliable and capable language models.</p>
<p><br></p>
<p>We dropped an additional, more technical video on the research on our Twitter account here: https://x.com/MLStreetTalk/status/1795093759471890606</p>
<p><br></p>
<p>Additional 20 minutes of unreleased footage on our Patreon here: https://www.patreon.com/posts/whats-magic-word-104922629</p>
<p><br></p>
<p>What&#39;s the Magic Word? A Control Theory of LLM Prompting (Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson)</p>
<p>https://arxiv.org/abs/2310.04444</p>
<p><br></p>
<p>LLM Control Theory Seminar (April 2024)</p>
<p>https://www.youtube.com/watch?v=9QtS9sVBFM0</p>
<p><br></p>
<p>Society for the pursuit of AGI (Cameron founded it)</p>
<p>https://agisociety.mydurable.com/</p>
<p><br></p>
<p>Roger Federer demo</p>
<p>http://conway.languagegame.io/inference</p>
<p><br></p>
<p>Neural Cellular Automata, Active Inference, and the Mystery of Biological Computation (Aman)</p>
<p>https://aman-bhargava.com/ai/neuro/neuromorphic/2024/03/25/nca-do-active-inference.html </p>
<p><br></p>
<p>Aman and Cameron also want to thank Dr. Shi-Zhuo Looi and Prof. Matt Thomson from from Caltech for help and advice on their research. (https://thomsonlab.caltech.edu/ and https://pma.caltech.edu/people/looi-shi-zhuo)</p>
<p><br></p>
<p>https://x.com/ABhargava2000</p>
<p>https://x.com/witkowski_cam</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Whats-the-Magic-Word--A-Control-Theory-of-LLM-Prompting-e2khs2t</link>
			<guid isPermaLink="false">1e61ace0-8450-4186-8a0c-e1323ee437a0</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 05 Jun 2024 20:22:54 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/87666205/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-5-5%2Fedcc7f6f-d812-d535-6844-d68532b9a96b.mp3" length="111291602" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;These two scientists have mapped out the insides or “reachable space” of a language model using control theory, what they discovered was extremely surprising. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Please support us on Patreon to get access to the private Discord server, bi-weekly calls, early access and ad-free listening.&lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/Bpgloy1dDn0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Aman Bhargava from Caltech and Cameron Witkowski from the University of Toronto to discuss their groundbreaking paper, “What’s the Magic Word? A Control Theory of LLM Prompting.” (the main theorem on self-attention controllability was developed in collaboration with Dr. Shi-Zhuo Looi from Caltech).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;They frame LLM systems as discrete stochastic dynamical systems. This means they look at LLMs in a structured way, similar to how we analyze control systems in engineering. They explore the “reachable set” of outputs for an LLM. Essentially, this is the range of possible outputs the model can generate from a given starting point when influenced by different prompts. The research highlights that prompt engineering, or optimizing the input tokens, can significantly influence LLM outputs. They show that even short prompts can drastically alter the likelihood of specific outputs. Aman and Cameron’s work might be a boon for understanding and improving LLMs. They suggest that a deeper exploration of control theory concepts could lead to more reliable and capable language models.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We dropped an additional, more technical video on the research on our Twitter account here: https://x.com/MLStreetTalk/status/1795093759471890606&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Additional 20 minutes of unreleased footage on our Patreon here: https://www.patreon.com/posts/whats-magic-word-104922629&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What&amp;#39;s the Magic Word? A Control Theory of LLM Prompting (Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2310.04444&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;LLM Control Theory Seminar (April 2024)&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=9QtS9sVBFM0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Society for the pursuit of AGI (Cameron founded it)&lt;/p&gt;
&lt;p&gt;https://agisociety.mydurable.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Roger Federer demo&lt;/p&gt;
&lt;p&gt;http://conway.languagegame.io/inference&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Neural Cellular Automata, Active Inference, and the Mystery of Biological Computation (Aman)&lt;/p&gt;
&lt;p&gt;https://aman-bhargava.com/ai/neuro/neuromorphic/2024/03/25/nca-do-active-inference.html &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Aman and Cameron also want to thank Dr. Shi-Zhuo Looi and Prof. Matt Thomson from from Caltech for help and advice on their research. (https://thomsonlab.caltech.edu/ and https://pma.caltech.edu/people/looi-shi-zhuo)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://x.com/ABhargava2000&lt;/p&gt;
&lt;p&gt;https://x.com/witkowski_cam&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:17:07</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1717618950945-9cdba599df988.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[CAN MACHINES REPLACE US? (AI vs Humanity) - Maria Santacaterina]]></title>
			<description><![CDATA[<p>Maria Santacaterina, with her background in the humanities, brings a critical perspective on the current state and future implications of AI technology, its impact on society, and the nature of human intelligence and creativity. She emphasizes that despite technological advancements, AI lacks fundamental human traits such as consciousness, empathy, intuition, and the ability to engage in genuine creative processes. Maria argues that AI, at its core, processes data but does not have the capability to understand or generate new, intrinsic meaning or ideas as humans do.</p>
<p><br></p>
<p>Throughout the conversation, Maria highlights her concern about the overreliance on AI in critical sectors such as healthcare, the justice system, and business. She stresses that while AI can serve as a tool, it should not replace human judgment and decision-making. Maria points out that AI systems often operate on past data, which may lead to outdated or incorrect decisions if not carefully managed.</p>
<p><br></p>
<p>The discussion also touches upon the concept of &quot;adaptive resilience&quot;, which Maria describes in her book. She explains adaptive resilience as the capacity for individuals and enterprises to evolve and thrive amidst challenges by leveraging technology responsibly, without undermining human values and capabilities.</p>
<p><br></p>
<p>A significant portion of the conversation focussed on ethical considerations surrounding AI. Tim and Maria agree that there&#39;s a pressing need for strong governance and ethical frameworks to guide AI development and deployment. They discuss how AI, without proper ethical considerations, risks exacerbating issues like privacy invasion, misinformation, and unintended discrimination.</p>
<p><br></p>
<p>Maria is skeptical about claims of achieving Artificial General Intelligence (AGI) or a technological singularity where machines surpass human intelligence in all aspects. She argues that such scenarios neglect the complex, dynamic nature of human intelligence and consciousness, which cannot be fully replicated or replaced by machines.</p>
<p><br></p>
<p>Tim and Maria discuss the importance of keeping human agency and creativity at the forefront of technology development. Maria asserts that efforts to automate or standardize complex human actions and decisions are misguided and could lead to dehumanizing outcomes. They both advocate for using AI as an aid to enhance human capabilities rather than a substitute.</p>
<p><br></p>
<p>In closing, Maria encourages a balanced approach to AI adoption, urging stakeholders to prioritize human well-being, ethical standards, and societal benefit above mere technological advancement. The conversation ends with Maria pointing people to her book for more in-depth analysis and thoughts on the future interaction between humans and technology.</p>
<p><br></p>
<p>Buy Maria&#39;s book here: https://amzn.to/4avF6kq</p>
<p>https://www.linkedin.com/in/mariasantacaterina</p>
<p><br></p>
<p>TOC</p>
<p>00:00:00 - Intro to Book</p>
<p>00:03:23 - What Life Is</p>
<p>00:10:10 - Agency</p>
<p>00:18:04 - Tech and Society</p>
<p>00:21:51 - System 1 and 2</p>
<p>00:22:59 - We Are Being Pigeonholed</p>
<p>00:30:22 - Agency vs Autonomy</p>
<p>00:36:37 - Explanations</p>
<p>00:40:24 - AI Reductionism</p>
<p>00:49:50 - How Are Humans Intelligent</p>
<p>01:00:22 - Semantics</p>
<p>01:01:53 - Emotive AI and Pavlovian Dogs</p>
<p>01:04:05 - Technology, Social Media and Organisation</p>
<p>01:18:34 - Systems Are Not That Automated</p>
<p>01:19:33 - Hiring</p>
<p>01:22:34 - Subjectivity in Orgs</p>
<p>01:32:28 - The AGI Delusion</p>
<p>01:45:37 - GPT-laziness Syndrome</p>
<p>01:54:58 - Diversity Preservation</p>
<p>01:58:24 - Ethics</p>
<p>02:11:43 - Moral Realism</p>
<p>02:16:17 - Utopia</p>
<p>02:18:02 - Reciprocity</p>
<p>02:20:52 - Tyranny of Categorisation</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/CAN-MACHINES-REPLACE-US--AI-vs-Humanity---Maria-Santacaterina-e2jaa4p</link>
			<guid isPermaLink="false">8409f249-7892-4055-9395-84231c463117</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 06 May 2024 10:48:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/86369881/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-4-6%2F8ede7e72-d5ef-7870-f16d-ab499569ea1c.mp3" length="218382683" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Maria Santacaterina, with her background in the humanities, brings a critical perspective on the current state and future implications of AI technology, its impact on society, and the nature of human intelligence and creativity. She emphasizes that despite technological advancements, AI lacks fundamental human traits such as consciousness, empathy, intuition, and the ability to engage in genuine creative processes. Maria argues that AI, at its core, processes data but does not have the capability to understand or generate new, intrinsic meaning or ideas as humans do.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Throughout the conversation, Maria highlights her concern about the overreliance on AI in critical sectors such as healthcare, the justice system, and business. She stresses that while AI can serve as a tool, it should not replace human judgment and decision-making. Maria points out that AI systems often operate on past data, which may lead to outdated or incorrect decisions if not carefully managed.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The discussion also touches upon the concept of &amp;quot;adaptive resilience&amp;quot;, which Maria describes in her book. She explains adaptive resilience as the capacity for individuals and enterprises to evolve and thrive amidst challenges by leveraging technology responsibly, without undermining human values and capabilities.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A significant portion of the conversation focussed on ethical considerations surrounding AI. Tim and Maria agree that there&amp;#39;s a pressing need for strong governance and ethical frameworks to guide AI development and deployment. They discuss how AI, without proper ethical considerations, risks exacerbating issues like privacy invasion, misinformation, and unintended discrimination.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Maria is skeptical about claims of achieving Artificial General Intelligence (AGI) or a technological singularity where machines surpass human intelligence in all aspects. She argues that such scenarios neglect the complex, dynamic nature of human intelligence and consciousness, which cannot be fully replicated or replaced by machines.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tim and Maria discuss the importance of keeping human agency and creativity at the forefront of technology development. Maria asserts that efforts to automate or standardize complex human actions and decisions are misguided and could lead to dehumanizing outcomes. They both advocate for using AI as an aid to enhance human capabilities rather than a substitute.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In closing, Maria encourages a balanced approach to AI adoption, urging stakeholders to prioritize human well-being, ethical standards, and societal benefit above mere technological advancement. The conversation ends with Maria pointing people to her book for more in-depth analysis and thoughts on the future interaction between humans and technology.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy Maria&amp;#39;s book here: https://amzn.to/4avF6kq&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/mariasantacaterina&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;00:00:00 - Intro to Book&lt;/p&gt;
&lt;p&gt;00:03:23 - What Life Is&lt;/p&gt;
&lt;p&gt;00:10:10 - Agency&lt;/p&gt;
&lt;p&gt;00:18:04 - Tech and Society&lt;/p&gt;
&lt;p&gt;00:21:51 - System 1 and 2&lt;/p&gt;
&lt;p&gt;00:22:59 - We Are Being Pigeonholed&lt;/p&gt;
&lt;p&gt;00:30:22 - Agency vs Autonomy&lt;/p&gt;
&lt;p&gt;00:36:37 - Explanations&lt;/p&gt;
&lt;p&gt;00:40:24 - AI Reductionism&lt;/p&gt;
&lt;p&gt;00:49:50 - How Are Humans Intelligent&lt;/p&gt;
&lt;p&gt;01:00:22 - Semantics&lt;/p&gt;
&lt;p&gt;01:01:53 - Emotive AI and Pavlovian Dogs&lt;/p&gt;
&lt;p&gt;01:04:05 - Technology, Social Media and Organisation&lt;/p&gt;
&lt;p&gt;01:18:34 - Systems Are Not That Automated&lt;/p&gt;
&lt;p&gt;01:19:33 - Hiring&lt;/p&gt;
&lt;p&gt;01:22:34 - Subjectivity in Orgs&lt;/p&gt;
&lt;p&gt;01:32:28 - The AGI Delusion&lt;/p&gt;
&lt;p&gt;01:45:37 - GPT-laziness Syndrome&lt;/p&gt;
&lt;p&gt;01:54:58 - Diversity Preservation&lt;/p&gt;
&lt;p&gt;01:58:24 - Ethics&lt;/p&gt;
&lt;p&gt;02:11:43 - Moral Realism&lt;/p&gt;
&lt;p&gt;02:16:17 - Utopia&lt;/p&gt;
&lt;p&gt;02:18:02 - Reciprocity&lt;/p&gt;
&lt;p&gt;02:20:52 - Tyranny of Categorisation&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:31:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1714992458992-f69df034ea5bd.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dr. Thomas Parr - Active Inference Book]]></title>
			<description><![CDATA[<p>Thomas Parr and his collaborators wrote a book titled &quot;Active Inference: The Free Energy Principle in Mind, Brain and Behavior&quot; which introduces Active Inference from both a high-level conceptual perspective and a low-level mechanistic, mathematical perspective.</p>
<p><br></p>
<p>Active inference, developed by the legendary neuroscientist Prof. Karl Friston - is a unifying mathematical framework which frames living systems as agents which minimize surprise and free energy in order to resist entropy and persist over time. It unifies various perspectives from physics, biology, statistics, and psychology - and allows us to explore deep questions about agency, biology, causality, modelling, and consciousness.</p>
<p><br></p>
<p>Buy Active Inference: The Free Energy Principle in Mind, Brain, and Behavior </p>
<p>https://amzn.to/4dj0iMj</p>
<p><br></p>
<p>YT version: https://youtu.be/lbb-Si5wa_o</p>
<p><br></p>
<p>Please support us on Patreon to get access to the private Discord server, bi-weekly calls, early access and ad-free listening. </p>
<p>https://patreon.com/mlst</p>
<p><br></p>
<p>Chapters should be embedded in the mp3, let me me know if issues</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Dr--Thomas-Parr---Active-Inference-Book-e2j4sdn</link>
			<guid isPermaLink="false">d5d40cea-2292-4218-9bda-bb911b145934</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 01 May 2024 21:24:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/86191991/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-4-1%2F54df4213-e234-c151-3bfa-b5e2a923ab29.mp3" length="140137250" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Thomas Parr and his collaborators wrote a book titled &amp;quot;Active Inference: The Free Energy Principle in Mind, Brain and Behavior&amp;quot; which introduces Active Inference from both a high-level conceptual perspective and a low-level mechanistic, mathematical perspective.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Active inference, developed by the legendary neuroscientist Prof. Karl Friston - is a unifying mathematical framework which frames living systems as agents which minimize surprise and free energy in order to resist entropy and persist over time. It unifies various perspectives from physics, biology, statistics, and psychology - and allows us to explore deep questions about agency, biology, causality, modelling, and consciousness.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy Active Inference: The Free Energy Principle in Mind, Brain, and Behavior &lt;/p&gt;
&lt;p&gt;https://amzn.to/4dj0iMj&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/lbb-Si5wa_o&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Please support us on Patreon to get access to the private Discord server, bi-weekly calls, early access and ad-free listening. &lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chapters should be embedded in the mp3, let me me know if issues&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:37:09</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1714598638789-5ff43fac12c42.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Connor Leahy - e/acc, AGI and the future.]]></title>
			<description><![CDATA[<p>Connor is the CEO of Conjecture and one of the most famous names in the AI alignment movement. This is the &quot;behind the scenes footage&quot; and bonus Patreon interviews from the day of the Beff Jezos debate, including an interview with Daniel Clothiaux. It&#39;s a great insight into Connor&#39;s philosophy. At the end there is an unreleased additional interview with Beff.</p>
<p><br></p>
<p>Support MLST:</p>
<p>Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, very early-access + exclusive content and lots more. </p>
<p>https://patreon.com/mlst</p>
<p>Donate: https://www.paypal.com/donate/?hosted_button_id=K2TYRVPBGXVNA</p>
<p>If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail</p>
<p><br></p>
<p>Topics:</p>
<p>Externalized cognition and the role of society and culture in human intelligence</p>
<p>The potential for AI systems to develop agency and autonomy</p>
<p>The future of AGI as a complex mixture of various components</p>
<p>The concept of agency and its relationship to power</p>
<p>The importance of coherence in AI systems</p>
<p>The balance between coherence and variance in exploring potential upsides</p>
<p>The role of dynamic, competent, and incorruptible institutions in handling risks and developing technology</p>
<p>Concerns about AI widening the gap between the haves and have-nots</p>
<p>The concept of equal access to opportunity and maintaining dynamism in the system</p>
<p>Leahy&#39;s perspective on life as a process that &quot;rides entropy&quot;</p>
<p>The importance of distinguishing between epistemological, decision-theoretic, and aesthetic aspects of morality (inc ref to Hume&#39;s Guillotine)</p>
<p>The concept of continuous agency and the idea that the first AGI will be a messy admixture of various components</p>
<p>The potential for AI systems to become more physically embedded in the future</p>
<p>The challenges of aligning AI systems and the societal impacts of AI technologies like ChatGPT and Bing</p>
<p>The importance of humility in the face of complexity when considering the future of AI and its societal implications</p>
<p><br></p>
<p>Disclaimer: this video is not an endorsement of e/acc or AGI agential existential risk from us - the hosts of MLST consider both of these views to be quite extreme. We seek diverse views on the channel.</p>
<p><br></p>
<p>00:00:00 Intro</p>
<p>00:00:56 Connor&#39;s Philosophy</p>
<p>00:03:53 Office Skit</p>
<p>00:05:08 Connor on e/acc and Beff</p>
<p>00:07:28 Intro to Daniel&#39;s Philosophy</p>
<p>00:08:35 Connor on Entropy, Life, and Morality</p>
<p>00:19:10 Connor on London</p>
<p>00:20:21 Connor Office Interview</p>
<p>00:20:46 Friston Patreon Preview</p>
<p>00:21:48 Why Are We So Dumb?</p>
<p>00:23:52 The Voice of the People, the Voice of God / Populism</p>
<p>00:26:35 Mimetics</p>
<p>00:30:03 Governance</p>
<p>00:33:19 Agency</p>
<p>00:40:25 Daniel Interview - Externalised Cognition, Bing GPT, AGI</p>
<p>00:56:29 Beff + Connor Bonus Patreons Interview</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Connor-Leahy---eacc--AGI-and-the-future-e2im26o</link>
			<guid isPermaLink="false">ce28373f-f12c-4cb6-a7e9-41002f7a7e9a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 21 Apr 2024 15:05:53 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/85706392/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-3-21%2F43e6978e-0033-d0f1-05e9-ac3b88108ced.mp3" length="114814407" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Connor is the CEO of Conjecture and one of the most famous names in the AI alignment movement. This is the &amp;quot;behind the scenes footage&amp;quot; and bonus Patreon interviews from the day of the Beff Jezos debate, including an interview with Daniel Clothiaux. It&amp;#39;s a great insight into Connor&amp;#39;s philosophy. At the end there is an unreleased additional interview with Beff.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support MLST:&lt;/p&gt;
&lt;p&gt;Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, very early-access + exclusive content and lots more. &lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Donate: https://www.paypal.com/donate/?hosted_button_id=K2TYRVPBGXVNA&lt;/p&gt;
&lt;p&gt;If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Topics:&lt;/p&gt;
&lt;p&gt;Externalized cognition and the role of society and culture in human intelligence&lt;/p&gt;
&lt;p&gt;The potential for AI systems to develop agency and autonomy&lt;/p&gt;
&lt;p&gt;The future of AGI as a complex mixture of various components&lt;/p&gt;
&lt;p&gt;The concept of agency and its relationship to power&lt;/p&gt;
&lt;p&gt;The importance of coherence in AI systems&lt;/p&gt;
&lt;p&gt;The balance between coherence and variance in exploring potential upsides&lt;/p&gt;
&lt;p&gt;The role of dynamic, competent, and incorruptible institutions in handling risks and developing technology&lt;/p&gt;
&lt;p&gt;Concerns about AI widening the gap between the haves and have-nots&lt;/p&gt;
&lt;p&gt;The concept of equal access to opportunity and maintaining dynamism in the system&lt;/p&gt;
&lt;p&gt;Leahy&amp;#39;s perspective on life as a process that &amp;quot;rides entropy&amp;quot;&lt;/p&gt;
&lt;p&gt;The importance of distinguishing between epistemological, decision-theoretic, and aesthetic aspects of morality (inc ref to Hume&amp;#39;s Guillotine)&lt;/p&gt;
&lt;p&gt;The concept of continuous agency and the idea that the first AGI will be a messy admixture of various components&lt;/p&gt;
&lt;p&gt;The potential for AI systems to become more physically embedded in the future&lt;/p&gt;
&lt;p&gt;The challenges of aligning AI systems and the societal impacts of AI technologies like ChatGPT and Bing&lt;/p&gt;
&lt;p&gt;The importance of humility in the face of complexity when considering the future of AI and its societal implications&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Disclaimer: this video is not an endorsement of e/acc or AGI agential existential risk from us - the hosts of MLST consider both of these views to be quite extreme. We seek diverse views on the channel.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:00:56 Connor&amp;#39;s Philosophy&lt;/p&gt;
&lt;p&gt;00:03:53 Office Skit&lt;/p&gt;
&lt;p&gt;00:05:08 Connor on e/acc and Beff&lt;/p&gt;
&lt;p&gt;00:07:28 Intro to Daniel&amp;#39;s Philosophy&lt;/p&gt;
&lt;p&gt;00:08:35 Connor on Entropy, Life, and Morality&lt;/p&gt;
&lt;p&gt;00:19:10 Connor on London&lt;/p&gt;
&lt;p&gt;00:20:21 Connor Office Interview&lt;/p&gt;
&lt;p&gt;00:20:46 Friston Patreon Preview&lt;/p&gt;
&lt;p&gt;00:21:48 Why Are We So Dumb?&lt;/p&gt;
&lt;p&gt;00:23:52 The Voice of the People, the Voice of God / Populism&lt;/p&gt;
&lt;p&gt;00:26:35 Mimetics&lt;/p&gt;
&lt;p&gt;00:30:03 Governance&lt;/p&gt;
&lt;p&gt;00:33:19 Agency&lt;/p&gt;
&lt;p&gt;00:40:25 Daniel Interview - Externalised Cognition, Bing GPT, AGI&lt;/p&gt;
&lt;p&gt;00:56:29 Beff + Connor Bonus Patreons Interview&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:19:34</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1713711873100-c7dc82294861.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Chris Bishop's NEW Deep Learning Textbook!]]></title>
			<description><![CDATA[<p>Professor Chris Bishop is a Technical Fellow and Director at Microsoft Research AI4Science, in Cambridge. He is also Honorary Professor of Computer Science at the University of Edinburgh, and a Fellow of Darwin College, Cambridge. In 2004, he was elected Fellow of the Royal Academy of Engineering, in 2007 he was elected Fellow of the Royal Society of Edinburgh, and in 2017 he was elected Fellow of the Royal Society. Chris was a founding member of the UK AI Council, and in 2019 he was appointed to the Prime Minister’s Council for Science and Technology.</p>
<p><br></p>
<p>At Microsoft Research, Chris oversees a global portfolio of industrial research and development, with a strong focus on machine learning and the natural sciences.</p>
<p>Chris obtained a BA in Physics from Oxford, and a PhD in Theoretical Physics from the University of Edinburgh, with a thesis on quantum field theory. </p>
<p><br></p>
<p>Chris&#39;s contributions to the field of machine learning have been truly remarkable. He has authored (what is arguably) the original textbook in the field - &#39;Pattern Recognition and Machine Learning&#39; (PRML) which has served as an essential reference for countless students and researchers around the world, and that was his second textbook after his highly acclaimed first textbook Neural Networks for Pattern Recognition. </p>
<p><br></p>
<p>Recently, Chris has co-authored a new book with his son, Hugh, titled &#39;Deep Learning: Foundations and Concepts.&#39; This book aims to provide a comprehensive understanding of the key ideas and techniques underpinning the rapidly evolving field of deep learning. It covers both the foundational concepts and the latest advances, making it an invaluable resource for newcomers and experienced practitioners alike.</p>
<p><br></p>
<p>Buy Chris&#39; textbook here:</p>
<p>https://amzn.to/3vvLcCh</p>
<p><br></p>
<p>More about Prof. Chris Bishop:</p>
<p>https://en.wikipedia.org/wiki/Christopher_Bishop</p>
<p>https://www.microsoft.com/en-us/research/people/cmbishop/</p>
<p><br></p>
<p>Support MLST:</p>
<p>Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, early-access + exclusive content and lots more. </p>
<p>https://patreon.com/mlst</p>
<p>Donate: https://www.paypal.com/donate/?hosted_button_id=K2TYRVPBGXVNA</p>
<p>If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 - Intro to Chris</p>
<p>00:06:54 - Changing Landscape of AI</p>
<p>00:08:16 - Symbolism</p>
<p>00:09:32 - PRML</p>
<p>00:11:02 - Bayesian Approach</p>
<p>00:14:49 - Are NNs One Model or Many, Special vs General</p>
<p>00:20:04 - Can Language Models Be Creative</p>
<p>00:22:35 - Sparks of AGI</p>
<p>00:25:52 - Creativity Gap in LLMs</p>
<p>00:35:40 - New Deep Learning Book</p>
<p>00:39:01 - Favourite Chapters</p>
<p>00:44:11 - Probability Theory</p>
<p>00:45:42 - AI4Science</p>
<p>00:48:31 - Inductive Priors</p>
<p>00:58:52 - Drug Discovery</p>
<p>01:05:19 - Foundational Bias Models</p>
<p>01:07:46 - How Fundamental Is Our Physics Knowledge?</p>
<p>01:12:05 - Transformers</p>
<p>01:12:59 - Why Does Deep Learning Work?</p>
<p>01:16:59 - Inscrutability of NNs</p>
<p>01:18:01 - Example of Simulator</p>
<p>01:21:09 - Control</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Chris-Bishops-NEW-Deep-Learning-Textbook-e2i63r0</link>
			<guid isPermaLink="false">b9aba838-5518-4be3-8d96-560b18cade35</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 10 Apr 2024 14:50:34 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/85183776/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-3-9%2F90ce9a86-62a0-eb9d-1e3a-380c3dc56c43.mp3" length="119884711" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Chris Bishop is a Technical Fellow and Director at Microsoft Research AI4Science, in Cambridge. He is also Honorary Professor of Computer Science at the University of Edinburgh, and a Fellow of Darwin College, Cambridge. In 2004, he was elected Fellow of the Royal Academy of Engineering, in 2007 he was elected Fellow of the Royal Society of Edinburgh, and in 2017 he was elected Fellow of the Royal Society. Chris was a founding member of the UK AI Council, and in 2019 he was appointed to the Prime Minister’s Council for Science and Technology.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;At Microsoft Research, Chris oversees a global portfolio of industrial research and development, with a strong focus on machine learning and the natural sciences.&lt;/p&gt;
&lt;p&gt;Chris obtained a BA in Physics from Oxford, and a PhD in Theoretical Physics from the University of Edinburgh, with a thesis on quantum field theory. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chris&amp;#39;s contributions to the field of machine learning have been truly remarkable. He has authored (what is arguably) the original textbook in the field - &amp;#39;Pattern Recognition and Machine Learning&amp;#39; (PRML) which has served as an essential reference for countless students and researchers around the world, and that was his second textbook after his highly acclaimed first textbook Neural Networks for Pattern Recognition. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Recently, Chris has co-authored a new book with his son, Hugh, titled &amp;#39;Deep Learning: Foundations and Concepts.&amp;#39; This book aims to provide a comprehensive understanding of the key ideas and techniques underpinning the rapidly evolving field of deep learning. It covers both the foundational concepts and the latest advances, making it an invaluable resource for newcomers and experienced practitioners alike.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy Chris&amp;#39; textbook here:&lt;/p&gt;
&lt;p&gt;https://amzn.to/3vvLcCh&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;More about Prof. Chris Bishop:&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Christopher_Bishop&lt;/p&gt;
&lt;p&gt;https://www.microsoft.com/en-us/research/people/cmbishop/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support MLST:&lt;/p&gt;
&lt;p&gt;Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, early-access + exclusive content and lots more. &lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Donate: https://www.paypal.com/donate/?hosted_button_id=K2TYRVPBGXVNA&lt;/p&gt;
&lt;p&gt;If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 - Intro to Chris&lt;/p&gt;
&lt;p&gt;00:06:54 - Changing Landscape of AI&lt;/p&gt;
&lt;p&gt;00:08:16 - Symbolism&lt;/p&gt;
&lt;p&gt;00:09:32 - PRML&lt;/p&gt;
&lt;p&gt;00:11:02 - Bayesian Approach&lt;/p&gt;
&lt;p&gt;00:14:49 - Are NNs One Model or Many, Special vs General&lt;/p&gt;
&lt;p&gt;00:20:04 - Can Language Models Be Creative&lt;/p&gt;
&lt;p&gt;00:22:35 - Sparks of AGI&lt;/p&gt;
&lt;p&gt;00:25:52 - Creativity Gap in LLMs&lt;/p&gt;
&lt;p&gt;00:35:40 - New Deep Learning Book&lt;/p&gt;
&lt;p&gt;00:39:01 - Favourite Chapters&lt;/p&gt;
&lt;p&gt;00:44:11 - Probability Theory&lt;/p&gt;
&lt;p&gt;00:45:42 - AI4Science&lt;/p&gt;
&lt;p&gt;00:48:31 - Inductive Priors&lt;/p&gt;
&lt;p&gt;00:58:52 - Drug Discovery&lt;/p&gt;
&lt;p&gt;01:05:19 - Foundational Bias Models&lt;/p&gt;
&lt;p&gt;01:07:46 - How Fundamental Is Our Physics Knowledge?&lt;/p&gt;
&lt;p&gt;01:12:05 - Transformers&lt;/p&gt;
&lt;p&gt;01:12:59 - Why Does Deep Learning Work?&lt;/p&gt;
&lt;p&gt;01:16:59 - Inscrutability of NNs&lt;/p&gt;
&lt;p&gt;01:18:01 - Example of Simulator&lt;/p&gt;
&lt;p&gt;01:21:09 - Control&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:22:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1712670516348-81fbc90b0ec2.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Philip Ball - How Life Works]]></title>
			<description><![CDATA[<p>Dr. Philip Ball is a freelance science writer. He just wrote a book called &quot;How Life Works&quot;, discussing the how the science of Biology has advanced in the last 20 years. We focus on the concept of Agency in particular. </p>
<p><br></p>
<p>He trained as a chemist at the University of Oxford, and as a physicist at the University of Bristol. He worked previously at Nature for over 20 years, first as an editor for physical sciences and then as a consultant editor. His writings on science for the popular press have covered topical issues ranging from cosmology to the future of molecular biology.</p>
<p><br></p>
<p>YT: https://www.youtube.com/watch?v=n6nxUiqiz9I</p>
<p><br></p>
<p>Transcript link on YT description</p>
<p><br></p>
<p>Philip is the author of many popular books on science, including H2O: A Biography of Water, Bright Earth: The Invention of Colour, The Music Instinct and Curiosity: How Science Became Interested in Everything. His book Critical Mass won the 2005 Aventis Prize for Science Books, while Serving the Reich was shortlisted for the Royal Society Winton Science Book Prize in 2014.</p>
<p><br></p>
<p>This is one of Tim&#39;s personal favourite MLST shows, so we have designated it a special edition. Enjoy!</p>
<p><br></p>
<p>Buy Philip&#39;s book &quot;How Life Works&quot; here: https://amzn.to/3vSmNqp</p>
<p><br></p>
<p>Support MLST:
Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, early-access + exclusive content and lots more. 
<a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUlTQVlTQWhmTmV2elFIUUYycEVMcXVXOHpMQXxBQ3Jtc0tuQnR5SjVONnZHTVprbFAzSkZ0Y0ZPREUwLVRaaU16UE5OcUdCaXRTQjFkUVluV1lUc2ZGRWNZNnNOSlFIVTJTM281VEF6MDAzYU5VcEFQLXRXNGRVRzhWdi10N2NzRUoxSTdIYm1wbHlEeTNmMURtZw&q=https%3A%2F%2Fpatreon.com%2Fmlst&v=n6nxUiqiz9I" target="_blank" rel="nofollow">https://patreon.com/mlst</a>
Donate: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjJZVnRjS2Q3eUlfaXBuT1F5TnhiRzZaR25rQXxBQ3Jtc0tuUmVjTmxnSjlRLWtHaWQwM3g0YVVmd2FzSnpQRVJHVU41TTF4ZlB5RG5WOV9PbDZfWXdJaU1GeWpaSmJ3QlJSY1BJVzJxZ2JnSExWM1NHWEpXcVd0XzZqV1pGTlptd3hkYWZUZC15TG1lSVRGRGJScw&q=https%3A%2F%2Fwww.paypal.com%2Fdonate%2F%3Fhosted_button_id%3DK2TYRVPBGXVNA&v=n6nxUiqiz9I" target="_blank" rel="nofollow">https://www.paypal.com/donate/?hosted...</a>
If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Philip-Ball---How-Life-Works-e2i39dj</link>
			<guid isPermaLink="false">bcbcbe80-9ed2-42a2-9e15-86e086d15913</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 07 Apr 2024 16:24:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/85091187/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-3-7%2F7f812191-d67c-4b88-41d6-2dbf817ed901.mp3" length="186485738" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Philip Ball is a freelance science writer. He just wrote a book called &amp;quot;How Life Works&amp;quot;, discussing the how the science of Biology has advanced in the last 20 years. We focus on the concept of Agency in particular. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;He trained as a chemist at the University of Oxford, and as a physicist at the University of Bristol. He worked previously at Nature for over 20 years, first as an editor for physical sciences and then as a consultant editor. His writings on science for the popular press have covered topical issues ranging from cosmology to the future of molecular biology.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT: https://www.youtube.com/watch?v=n6nxUiqiz9I&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Transcript link on YT description&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Philip is the author of many popular books on science, including H2O: A Biography of Water, Bright Earth: The Invention of Colour, The Music Instinct and Curiosity: How Science Became Interested in Everything. His book Critical Mass won the 2005 Aventis Prize for Science Books, while Serving the Reich was shortlisted for the Royal Society Winton Science Book Prize in 2014.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This is one of Tim&amp;#39;s personal favourite MLST shows, so we have designated it a special edition. Enjoy!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy Philip&amp;#39;s book &amp;quot;How Life Works&amp;quot; here: https://amzn.to/3vSmNqp&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support MLST:
Please support us on Patreon. We are entirely funded from Patreon donations right now. Patreon supports get private discord access, biweekly calls, early-access + exclusive content and lots more. 
&lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbUlTQVlTQWhmTmV2elFIUUYycEVMcXVXOHpMQXxBQ3Jtc0tuQnR5SjVONnZHTVprbFAzSkZ0Y0ZPREUwLVRaaU16UE5OcUdCaXRTQjFkUVluV1lUc2ZGRWNZNnNOSlFIVTJTM281VEF6MDAzYU5VcEFQLXRXNGRVRzhWdi10N2NzRUoxSTdIYm1wbHlEeTNmMURtZw&amp;q=https%3A%2F%2Fpatreon.com%2Fmlst&amp;v=n6nxUiqiz9I&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://patreon.com/mlst&lt;/a&gt;
Donate: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbjJZVnRjS2Q3eUlfaXBuT1F5TnhiRzZaR25rQXxBQ3Jtc0tuUmVjTmxnSjlRLWtHaWQwM3g0YVVmd2FzSnpQRVJHVU41TTF4ZlB5RG5WOV9PbDZfWXdJaU1GeWpaSmJ3QlJSY1BJVzJxZ2JnSExWM1NHWEpXcVd0XzZqV1pGTlptd3hkYWZUZC15TG1lSVRGRGJScw&amp;q=https%3A%2F%2Fwww.paypal.com%2Fdonate%2F%3Fhosted_button_id%3DK2TYRVPBGXVNA&amp;v=n6nxUiqiz9I&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://www.paypal.com/donate/?hosted...&lt;/a&gt;
If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:09:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1712507018678-6e13812b8f2a7.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dr. Paul Lessard - Categorical/Structured Deep Learning]]></title>
			<description><![CDATA[<p>Dr. Paul Lessard and his collaborators have written a paper on &quot;Categorical Deep Learning and Algebraic Theory of Architectures&quot;. They aim to make neural networks more interpretable, composable and amenable to formal reasoning. The key is mathematical abstraction, as exemplified by category theory - using monads to develop a more principled, algebraic approach to structuring neural networks.</p>
<p><br></p>
<p>We also discussed the limitations of current neural network architectures in terms of their ability to generalise and reason in a human-like way. In particular, the inability of neural networks to do unbounded computation equivalent to a Turing machine. Paul expressed optimism that this is not a fundamental limitation, but an artefact of current architectures and training procedures.</p>
<p><br></p>
<p>The power of abstraction - allowing us to focus on the essential structure while ignoring extraneous details. This can make certain problems more tractable to reason about. Paul sees category theory as providing a powerful &quot;Lego set&quot; for productively thinking about many practical problems.</p>
<p><br></p>
<p>Towards the end, Paul gave an accessible introduction to some core concepts in category theory like categories, morphisms, functors, monads etc. We explained how these abstract constructs can capture essential patterns that arise across different domains of mathematics.</p>
<p><br></p>
<p>Paul is optimistic about the potential of category theory and related mathematical abstractions to put AI and neural networks on a more robust conceptual foundation to enable interpretability and reasoning. However, significant theoretical and engineering challenges remain in realising this vision.</p>
<p><br></p>
<p>Please support us on Patreon. We are entirely funded from Patreon donations right now. </p>
<p>https://patreon.com/mlst</p>
<p>If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail</p>
<p><br></p>
<p>Links:</p>
<p>Categorical Deep Learning: An Algebraic Theory of Architectures</p>
<p>Bruno Gavranović, Paul Lessard, Andrew Dudzik,</p>
<p>Tamara von Glehn, João G. M. Araújo, Petar Veličković</p>
<p>Paper: https://categoricaldeeplearning.com/</p>
<p><br></p>
<p>Symbolica:</p>
<p>https://twitter.com/symbolica</p>
<p>https://www.symbolica.ai/</p>
<p><br></p>
<p>Dr. Paul Lessard (Principal Scientist - Symbolica)</p>
<p>https://www.linkedin.com/in/paul-roy-lessard/</p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 - Intro</p>
<p>00:05:07 - What is the category paper all about</p>
<p>00:07:19 - Composition</p>
<p>00:10:42 - Abstract Algebra</p>
<p>00:23:01 - DSLs for machine learning</p>
<p>00:24:10 - Inscrutibility</p>
<p>00:29:04 - Limitations with current NNs</p>
<p>00:30:41 - Generative code / NNs don&#39;t recurse</p>
<p>00:34:34 - NNs are not Turing machines (special edition)</p>
<p>00:53:09 - Abstraction</p>
<p>00:55:11 - Category theory objects</p>
<p>00:58:06 - Cat theory vs number theory</p>
<p>00:59:43 - Data and Code are one in the same</p>
<p>01:08:05 - Syntax and semantics</p>
<p>01:14:32 - Category DL elevator pitch</p>
<p>01:17:05 - Abstraction again</p>
<p>01:20:25 - Lego set for the universe</p>
<p>01:23:04 - Reasoning</p>
<p>01:28:05 - Category theory 101</p>
<p>01:37:42 - Monads</p>
<p>01:45:59 - Where to learn more cat theory</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Dr--Paul-Lessard---CategoricalStructured-Deep-Learning-e2hqqlq</link>
			<guid isPermaLink="false">fd102195-267d-4644-afda-0e598517f9a4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 01 Apr 2024 12:53:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/84813946/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-3-1%2Fcb2afcb9-86d2-c3ee-def3-dc54f73377c9.mp3" length="157669228" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Paul Lessard and his collaborators have written a paper on &amp;quot;Categorical Deep Learning and Algebraic Theory of Architectures&amp;quot;. They aim to make neural networks more interpretable, composable and amenable to formal reasoning. The key is mathematical abstraction, as exemplified by category theory - using monads to develop a more principled, algebraic approach to structuring neural networks.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We also discussed the limitations of current neural network architectures in terms of their ability to generalise and reason in a human-like way. In particular, the inability of neural networks to do unbounded computation equivalent to a Turing machine. Paul expressed optimism that this is not a fundamental limitation, but an artefact of current architectures and training procedures.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The power of abstraction - allowing us to focus on the essential structure while ignoring extraneous details. This can make certain problems more tractable to reason about. Paul sees category theory as providing a powerful &amp;quot;Lego set&amp;quot; for productively thinking about many practical problems.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Towards the end, Paul gave an accessible introduction to some core concepts in category theory like categories, morphisms, functors, monads etc. We explained how these abstract constructs can capture essential patterns that arise across different domains of mathematics.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Paul is optimistic about the potential of category theory and related mathematical abstractions to put AI and neural networks on a more robust conceptual foundation to enable interpretability and reasoning. However, significant theoretical and engineering challenges remain in realising this vision.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Please support us on Patreon. We are entirely funded from Patreon donations right now. &lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst&lt;/p&gt;
&lt;p&gt;If you would like to sponsor us, so we can tell your story - reach out on mlstreettalk at gmail&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;p&gt;Categorical Deep Learning: An Algebraic Theory of Architectures&lt;/p&gt;
&lt;p&gt;Bruno Gavranović, Paul Lessard, Andrew Dudzik,&lt;/p&gt;
&lt;p&gt;Tamara von Glehn, João G. M. Araújo, Petar Veličković&lt;/p&gt;
&lt;p&gt;Paper: https://categoricaldeeplearning.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Symbolica:&lt;/p&gt;
&lt;p&gt;https://twitter.com/symbolica&lt;/p&gt;
&lt;p&gt;https://www.symbolica.ai/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Paul Lessard (Principal Scientist - Symbolica)&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/paul-roy-lessard/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 - Intro&lt;/p&gt;
&lt;p&gt;00:05:07 - What is the category paper all about&lt;/p&gt;
&lt;p&gt;00:07:19 - Composition&lt;/p&gt;
&lt;p&gt;00:10:42 - Abstract Algebra&lt;/p&gt;
&lt;p&gt;00:23:01 - DSLs for machine learning&lt;/p&gt;
&lt;p&gt;00:24:10 - Inscrutibility&lt;/p&gt;
&lt;p&gt;00:29:04 - Limitations with current NNs&lt;/p&gt;
&lt;p&gt;00:30:41 - Generative code / NNs don&amp;#39;t recurse&lt;/p&gt;
&lt;p&gt;00:34:34 - NNs are not Turing machines (special edition)&lt;/p&gt;
&lt;p&gt;00:53:09 - Abstraction&lt;/p&gt;
&lt;p&gt;00:55:11 - Category theory objects&lt;/p&gt;
&lt;p&gt;00:58:06 - Cat theory vs number theory&lt;/p&gt;
&lt;p&gt;00:59:43 - Data and Code are one in the same&lt;/p&gt;
&lt;p&gt;01:08:05 - Syntax and semantics&lt;/p&gt;
&lt;p&gt;01:14:32 - Category DL elevator pitch&lt;/p&gt;
&lt;p&gt;01:17:05 - Abstraction again&lt;/p&gt;
&lt;p&gt;01:20:25 - Lego set for the universe&lt;/p&gt;
&lt;p&gt;01:23:04 - Reasoning&lt;/p&gt;
&lt;p&gt;01:28:05 - Category theory 101&lt;/p&gt;
&lt;p&gt;01:37:42 - Monads&lt;/p&gt;
&lt;p&gt;01:45:59 - Where to learn more cat theory&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:49:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1711975934857-eea6bfe774bd7.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Can we build a generalist agent? Dr. Minqi Jiang and Dr. Marc Rigter]]></title>
			<description><![CDATA[<p>Dr. Minqi Jiang and Dr. Marc Rigter explain an innovative new method to make the intelligence of agents more general-purpose by training them to learn many worlds before their usual goal-directed training, which we call &quot;reinforcement learning&quot;. 

Their new paper is called &quot;Reward-free curricula for training robust world models&quot; https://arxiv.org/pdf/2306.09205.pdf

https://twitter.com/MinqiJiang
https://twitter.com/MarcRigter

Interviewer: Dr. Tim Scarfe

Please support us on Patreon, Tim is now doing MLST full-time and taking a massive financial hit. If you love MLST and want this to continue, please show your support! In return you get access to shows very early and private discord and networking. https://patreon.com/mlst

We are also looking for show sponsors, please get in touch if interested mlstreettalk at gmail. 

MLST Discord: https://discord.gg/machine-learning-street-talk-mlst-937356144060530778</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Can-we-build-a-generalist-agent--Dr--Minqi-Jiang-and-Dr--Marc-Rigter-e2hbgmm</link>
			<guid isPermaLink="false">3bcac65f-8965-475a-82e6-f3bf40ced60d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 20 Mar 2024 18:36:23 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/84312214/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-2-20%2F1bbd0c9c-d5cd-0177-632d-f6c191a25928.mp3" length="169095940" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Minqi Jiang and Dr. Marc Rigter explain an innovative new method to make the intelligence of agents more general-purpose by training them to learn many worlds before their usual goal-directed training, which we call &amp;quot;reinforcement learning&amp;quot;. 

Their new paper is called &amp;quot;Reward-free curricula for training robust world models&amp;quot; https://arxiv.org/pdf/2306.09205.pdf

https://twitter.com/MinqiJiang
https://twitter.com/MarcRigter

Interviewer: Dr. Tim Scarfe

Please support us on Patreon, Tim is now doing MLST full-time and taking a massive financial hit. If you love MLST and want this to continue, please show your support! In return you get access to shows very early and private discord and networking. https://patreon.com/mlst

We are also looking for show sponsors, please get in touch if interested mlstreettalk at gmail. 

MLST Discord: https://discord.gg/machine-learning-street-talk-mlst-937356144060530778&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:57:11</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1710959766195-f000be6a8b6e4.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Nick Chater - The Language Game (Part 1)]]></title>
			<description><![CDATA[<p>Nick Chater is Professor of Behavioural Science at Warwick Business School, who works on rationality and language using a range of theoretical and experimental approaches. We discuss his books The Mind is Flat, and the Language Game. </p>
<p><br></p>
<p>Please support me on Patreon (this is now my main job!) - https://patreon.com/mlst - Access the private Discord, networking, and early access to content. </p>
<p>MLST Discord: https://discord.gg/machine-learning-street-talk-mlst-937356144060530778</p>
<p>https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Buy The Language Game:</p>
<p>https://amzn.to/3SRHjPm</p>
<p><br></p>
<p>Buy The Mind is Flat: </p>
<p>https://amzn.to/3P3BUUC</p>
<p><br></p>
<p>YT version: https://youtu.be/5cBS6COzLN4</p>
<p><br></p>
<p>https://www.wbs.ac.uk/about/person/nick-chater/</p>
<p>https://twitter.com/nickjchater?lang=en</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Nick-Chater---The-Language-Game-Part-1-e2gh3dv</link>
			<guid isPermaLink="false">dbef7495-afd6-4fdb-928c-87d759ce38ff</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 01 Mar 2024 19:37:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/83446655/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-2-1%2Fd41cc5ea-41e4-cb35-8648-003762e2ea38.mp3" length="149760133" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Nick Chater is Professor of Behavioural Science at Warwick Business School, who works on rationality and language using a range of theoretical and experimental approaches. We discuss his books The Mind is Flat, and the Language Game. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Please support me on Patreon (this is now my main job!) - https://patreon.com/mlst - Access the private Discord, networking, and early access to content. &lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/machine-learning-street-talk-mlst-937356144060530778&lt;/p&gt;
&lt;p&gt;https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy The Language Game:&lt;/p&gt;
&lt;p&gt;https://amzn.to/3SRHjPm&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Buy The Mind is Flat: &lt;/p&gt;
&lt;p&gt;https://amzn.to/3P3BUUC&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/5cBS6COzLN4&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.wbs.ac.uk/about/person/nick-chater/&lt;/p&gt;
&lt;p&gt;https://twitter.com/nickjchater?lang=en&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:43:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1709321825600-3bdc95fce32a4.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Kenneth Stanley created a new social network based on serendipity and divergence ]]></title>
			<description><![CDATA[<p>See what Sam Altman advised Kenneth when he left OpenAI! Professor Kenneth Stanley has just launched a brand new type of social network, which he calls a &quot;Serendipity network&quot;. The idea is that you follow interests, NOT people. It&#39;s a social network without the popularity contest. We discuss the phgilosophy and technology behind the venture in great detail. The main ideas of which came from Kenneth&#39;s famous book &quot;Why greatness cannot be planned&quot;. </p>
<p><br></p>
<p>See what Sam Altman advised Kenneth when he left OpenAI! Professor Kenneth Stanley has just launched a brand new type of social network, which he calls a &quot;Serendipity network&quot;.The idea is that you follow interests, NOT people. It&#39;s a social network without the popularity contest.
</p>
<p>YT version: https://www.youtube.com/watch?v=pWIrXN-yy8g</p>
<p><br></p>
<p>Chapters should be baked into the MP3 file now</p>
<p>
MLST public Discord: https://discord.gg/machine-learning-street-talk-mlst-937356144060530778

Please support our work on Patreon  - get access to interviews months early, private Patreon, networking, exclusive content and regular calls with Tim and Keith.  
https://patreon.com/mlst

Get Maven here:
https://www.heymaven.com/

Kenneth: 
https://twitter.com/kenneth0stanley
https://www.kenstanley.net/home

Host - Tim Scarfe:
https://www.linkedin.com/in/ecsquizor/
https://www.mlst.ai/

Original MLST show with Kenneth:
https://www.youtube.com/watch?v=lhYGXYeMq_E</p>
<p>Tim explains the book more here:</p>
<p>https://www.youtube.com/watch?v=wNhaz81OOqw</p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Kenneth-Stanley-created-a-new-social-network-based-on-serendipity-and-divergence-e2gcpda</link>
			<guid isPermaLink="false">8c1ed114-7997-4736-a733-b58abb79a476</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 28 Feb 2024 10:26:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/83305322/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-1-28%2Fd5ee117f-75e1-6b8b-a6bb-f3dadc46d5df.mp3" length="281906695" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;See what Sam Altman advised Kenneth when he left OpenAI! Professor Kenneth Stanley has just launched a brand new type of social network, which he calls a &amp;quot;Serendipity network&amp;quot;. The idea is that you follow interests, NOT people. It&amp;#39;s a social network without the popularity contest. We discuss the phgilosophy and technology behind the venture in great detail. The main ideas of which came from Kenneth&amp;#39;s famous book &amp;quot;Why greatness cannot be planned&amp;quot;. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;See what Sam Altman advised Kenneth when he left OpenAI! Professor Kenneth Stanley has just launched a brand new type of social network, which he calls a &amp;quot;Serendipity network&amp;quot;.The idea is that you follow interests, NOT people. It&amp;#39;s a social network without the popularity contest.
&lt;/p&gt;
&lt;p&gt;YT version: https://www.youtube.com/watch?v=pWIrXN-yy8g&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chapters should be baked into the MP3 file now&lt;/p&gt;
&lt;p&gt;
MLST public Discord: https://discord.gg/machine-learning-street-talk-mlst-937356144060530778

Please support our work on Patreon  - get access to interviews months early, private Patreon, networking, exclusive content and regular calls with Tim and Keith.  
https://patreon.com/mlst

Get Maven here:
https://www.heymaven.com/

Kenneth: 
https://twitter.com/kenneth0stanley
https://www.kenstanley.net/home

Host - Tim Scarfe:
https://www.linkedin.com/in/ecsquizor/
https://www.mlst.ai/

Original MLST show with Kenneth:
https://www.youtube.com/watch?v=lhYGXYeMq_E&lt;/p&gt;
&lt;p&gt;Tim explains the book more here:&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=wNhaz81OOqw&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>03:15:27</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1709115957077-866dbf4bbedb4.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dr. Brandon Rohrer - Robotics, Creativity and Intelligence]]></title>
			<description><![CDATA[<p>Brandon Rohrer who obtained his Ph.D from MIT is driven by understanding algorithms ALL the way down to their nuts and bolts, so he can make them accessible to everyone by first explaining them in the way HE himself would have wanted to learn!</p>
<p><br></p>
<p>Please support us on Patreon for loads of exclusive content and private Discord:</p>
<p>https://patreon.com/mlst (public discord)</p>
<p>https://discord.gg/aNPkGUQtc5</p>
<p>https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Brandon Rohrer is a seasoned data science leader and educator with a rich background in creating robust, efficient machine learning algorithms and tools. With a Ph.D. in Mechanical Engineering from MIT, his expertise encompasses a broad spectrum of AI applications — from computer vision and natural language processing to reinforcement learning and robotics. Brandon&#39;s career has seen him in Principle-level roles at Microsoft and Facebook. An educator at heart, he also shares his knowledge through detailed tutorials, courses, and his forthcoming book, &quot;How to Train Your Robot.&quot; </p>
<p><br></p>
<p>YT version: https://www.youtube.com/watch?v=4Ps7ahonRCY</p>
<p><br></p>
<p>Brandon&#39;s links:</p>
<p>https://github.com/brohrer</p>
<p>https://www.youtube.com/channel/UCsBKTrp45lTfHa_p49I2AEQ</p>
<p>https://www.linkedin.com/in/brohrer/</p>
<p><br></p>
<p>How transformers work:</p>
<p>https://e2eml.school/transformers</p>
<p><br></p>
<p>Brandon&#39;s End-to-End Machine Learning school courses, posts, and tutorials</p>
<p>https://e2eml.school</p>
<p><br></p>
<p>Free course:</p>
<p>https://end-to-end-machine-learning.teachable.com/p/complete-course-library-full-end-to-end-machine-learning-catalog</p>
<p><br></p>
<p>Blog: https://e2eml.school/blog.html</p>
<p><br></p>
<p>Ziptie: Learning Useful Features [Brandon Rohrer]</p>
<p>https://www.brandonrohrer.com/ziptie</p>
<p><br></p>
<p>TOC should be baked into the MP3 file now</p>
<p>00:00:00 - Intro to Brandon</p>
<p>00:00:36 - RLHF</p>
<p>00:01:09 - Limitations of transformers</p>
<p>00:07:23 - Agency - we are all GPTs</p>
<p>00:09:07 - BPE / representation bias</p>
<p>00:12:00 - LLM true believers</p>
<p>00:16:42 - Brandon&#39;s style of teaching</p>
<p>00:19:50 - ML vs real world = Robotics</p>
<p>00:29:59 - Reward shaping</p>
<p>00:37:08 - No true Scotsman - when do we accept capabilities as real</p>
<p>00:38:50 - Externalism</p>
<p>00:43:03 - Building flexible robots</p>
<p>00:45:37 - Is reward enough</p>
<p>00:54:30 - Optimization curse</p>
<p>00:58:15 - Collective intelligence</p>
<p>01:01:51 - Intelligence + creativity</p>
<p>01:13:35 - ChatGPT + Creativity</p>
<p>01:25:19 - Transformers Tutorial</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Dr--Brandon-Rohrer---Robotics--Creativity-and-Intelligence-e2foau9</link>
			<guid isPermaLink="false">70ee1cdc-ebd3-41cf-85ea-73434c5aee54</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 13 Feb 2024 21:39:39 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/82635145/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-1-13%2F918a5e58-b1e8-4844-ad4c-b04feb4939fc.mp3" length="132330277" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Brandon Rohrer who obtained his Ph.D from MIT is driven by understanding algorithms ALL the way down to their nuts and bolts, so he can make them accessible to everyone by first explaining them in the way HE himself would have wanted to learn!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Please support us on Patreon for loads of exclusive content and private Discord:&lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst (public discord)&lt;/p&gt;
&lt;p&gt;https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Brandon Rohrer is a seasoned data science leader and educator with a rich background in creating robust, efficient machine learning algorithms and tools. With a Ph.D. in Mechanical Engineering from MIT, his expertise encompasses a broad spectrum of AI applications — from computer vision and natural language processing to reinforcement learning and robotics. Brandon&amp;#39;s career has seen him in Principle-level roles at Microsoft and Facebook. An educator at heart, he also shares his knowledge through detailed tutorials, courses, and his forthcoming book, &amp;quot;How to Train Your Robot.&amp;quot; &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://www.youtube.com/watch?v=4Ps7ahonRCY&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Brandon&amp;#39;s links:&lt;/p&gt;
&lt;p&gt;https://github.com/brohrer&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/channel/UCsBKTrp45lTfHa_p49I2AEQ&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/brohrer/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;How transformers work:&lt;/p&gt;
&lt;p&gt;https://e2eml.school/transformers&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Brandon&amp;#39;s End-to-End Machine Learning school courses, posts, and tutorials&lt;/p&gt;
&lt;p&gt;https://e2eml.school&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Free course:&lt;/p&gt;
&lt;p&gt;https://end-to-end-machine-learning.teachable.com/p/complete-course-library-full-end-to-end-machine-learning-catalog&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Blog: https://e2eml.school/blog.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ziptie: Learning Useful Features [Brandon Rohrer]&lt;/p&gt;
&lt;p&gt;https://www.brandonrohrer.com/ziptie&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC should be baked into the MP3 file now&lt;/p&gt;
&lt;p&gt;00:00:00 - Intro to Brandon&lt;/p&gt;
&lt;p&gt;00:00:36 - RLHF&lt;/p&gt;
&lt;p&gt;00:01:09 - Limitations of transformers&lt;/p&gt;
&lt;p&gt;00:07:23 - Agency - we are all GPTs&lt;/p&gt;
&lt;p&gt;00:09:07 - BPE / representation bias&lt;/p&gt;
&lt;p&gt;00:12:00 - LLM true believers&lt;/p&gt;
&lt;p&gt;00:16:42 - Brandon&amp;#39;s style of teaching&lt;/p&gt;
&lt;p&gt;00:19:50 - ML vs real world = Robotics&lt;/p&gt;
&lt;p&gt;00:29:59 - Reward shaping&lt;/p&gt;
&lt;p&gt;00:37:08 - No true Scotsman - when do we accept capabilities as real&lt;/p&gt;
&lt;p&gt;00:38:50 - Externalism&lt;/p&gt;
&lt;p&gt;00:43:03 - Building flexible robots&lt;/p&gt;
&lt;p&gt;00:45:37 - Is reward enough&lt;/p&gt;
&lt;p&gt;00:54:30 - Optimization curse&lt;/p&gt;
&lt;p&gt;00:58:15 - Collective intelligence&lt;/p&gt;
&lt;p&gt;01:01:51 - Intelligence + creativity&lt;/p&gt;
&lt;p&gt;01:13:35 - ChatGPT + Creativity&lt;/p&gt;
&lt;p&gt;01:25:19 - Transformers Tutorial&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:31:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1707860307738-a8755eb31b41d.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Showdown Between e/acc Leader And Doomer - Connor Leahy + Beff Jezos]]></title>
			<description><![CDATA[<p>The world&#39;s second-most famous AI doomer Connor Leahy sits down with Beff Jezos, the founder of the e/acc movement debating technology, AI policy, and human values. As the two discuss technology, AI safety, civilization advancement, and the future of institutions, they clash on their opposing perspectives on how we steer humanity towards a more optimal path.</p>
<p><br></p>
<p>Watch behind the scenes, get early access and join the private Discord by supporting us on Patreon. We have some amazing content going up there with Max Bennett and Kenneth Stanley this week! 
https://patreon.com/mlst (public discord)
https://discord.gg/aNPkGUQtc5
https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Post-interview with Beff and Connor: https://www.patreon.com/posts/97905213</p>
<p>Pre-interview with Connor and his colleague Dan Clothiaux: https://www.patreon.com/posts/connor-leahy-and-97631416</p>
<p><br></p>
<p>Leahy, known for his critical perspectives on AI and technology, challenges Jezos on a variety of assertions related to the accelerationist movement, market dynamics, and the need for regulation in the face of rapid technological advancements. Jezos, on the other hand, provides insights into the e/acc movement&#39;s core philosophies, emphasizing growth, adaptability, and the dangers of over-legislation and centralized control in current institutions.</p>
<p><br></p>
<p>Throughout the discussion, both speakers explore the concept of entropy, the role of competition in fostering innovation, and the balance needed to mediate order and chaos to ensure the prosperity and survival of civilization. They weigh up the risks and rewards of AI, the importance of maintaining a power equilibrium in society, and the significance of cultural and institutional dynamism.</p>
<p><br></p>
<p>Beff Jezos (Guillaume Verdon): 
https://twitter.com/BasedBeffJezos
https://twitter.com/GillVerd

Connor Leahy:
https://twitter.com/npcollapse</p>
<p><br></p>
<p>YT: https://www.youtube.com/watch?v=0zxi0xSBOaQ</p>
<p><br></p>
<p>TOC: </p>
<p>00:00:00 - Intro</p>
<p>00:03:05 - Society library reference</p>
<p>00:03:35 - Debate starts</p>
<p>00:05:08 - Should any tech be banned?</p>
<p>00:20:39 - Leaded Gasoline</p>
<p>00:28:57 - False vacuum collapse method?</p>
<p>00:34:56 - What if there are dangerous aliens?</p>
<p>00:36:56 - Risk tolerances</p>
<p>00:39:26 - Optimizing for growth vs value</p>
<p>00:52:38 - Is vs ought</p>
<p>01:02:29 - AI discussion</p>
<p>01:07:38 - War / global competition</p>
<p>01:11:02 - Open source F16 designs</p>
<p>01:20:37 - Offense vs defense</p>
<p>01:28:49 - Morality / value</p>
<p>01:43:34 - What would Conor do</p>
<p>01:50:36 - Institutions/regulation</p>
<p>02:26:41 - Competition vs. Regulation Dilemma</p>
<p>02:32:50 - Existential Risks and Future Planning</p>
<p>02:41:46 - Conclusion and Reflection</p>
<p><br></p>
<p>Note from Tim: I baked the chapter metadata into the mp3 file this time, does that help the chapters show up in your app? Let me know. Also I accidentally exported a few minutes of dead audio at the end of the file - sorry about that just skip on when the episode finishes. </p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Showdown-Between-eacc-Leader-And-Doomer---Connor-Leahy--Beff-Jezos-e2far3q</link>
			<guid isPermaLink="false">a199ec26-abf2-4235-90b2-b57051f1e51a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 03 Feb 2024 21:00:16 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/82192954/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-1-3%2F71511474-0933-6a0d-b298-475e741e13ab.mp3" length="260016600" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;The world&amp;#39;s second-most famous AI doomer Connor Leahy sits down with Beff Jezos, the founder of the e/acc movement debating technology, AI policy, and human values. As the two discuss technology, AI safety, civilization advancement, and the future of institutions, they clash on their opposing perspectives on how we steer humanity towards a more optimal path.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Watch behind the scenes, get early access and join the private Discord by supporting us on Patreon. We have some amazing content going up there with Max Bennett and Kenneth Stanley this week! 
https://patreon.com/mlst (public discord)
https://discord.gg/aNPkGUQtc5
https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Post-interview with Beff and Connor: https://www.patreon.com/posts/97905213&lt;/p&gt;
&lt;p&gt;Pre-interview with Connor and his colleague Dan Clothiaux: https://www.patreon.com/posts/connor-leahy-and-97631416&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Leahy, known for his critical perspectives on AI and technology, challenges Jezos on a variety of assertions related to the accelerationist movement, market dynamics, and the need for regulation in the face of rapid technological advancements. Jezos, on the other hand, provides insights into the e/acc movement&amp;#39;s core philosophies, emphasizing growth, adaptability, and the dangers of over-legislation and centralized control in current institutions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Throughout the discussion, both speakers explore the concept of entropy, the role of competition in fostering innovation, and the balance needed to mediate order and chaos to ensure the prosperity and survival of civilization. They weigh up the risks and rewards of AI, the importance of maintaining a power equilibrium in society, and the significance of cultural and institutional dynamism.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Beff Jezos (Guillaume Verdon): 
https://twitter.com/BasedBeffJezos
https://twitter.com/GillVerd

Connor Leahy:
https://twitter.com/npcollapse&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT: https://www.youtube.com/watch?v=0zxi0xSBOaQ&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC: &lt;/p&gt;
&lt;p&gt;00:00:00 - Intro&lt;/p&gt;
&lt;p&gt;00:03:05 - Society library reference&lt;/p&gt;
&lt;p&gt;00:03:35 - Debate starts&lt;/p&gt;
&lt;p&gt;00:05:08 - Should any tech be banned?&lt;/p&gt;
&lt;p&gt;00:20:39 - Leaded Gasoline&lt;/p&gt;
&lt;p&gt;00:28:57 - False vacuum collapse method?&lt;/p&gt;
&lt;p&gt;00:34:56 - What if there are dangerous aliens?&lt;/p&gt;
&lt;p&gt;00:36:56 - Risk tolerances&lt;/p&gt;
&lt;p&gt;00:39:26 - Optimizing for growth vs value&lt;/p&gt;
&lt;p&gt;00:52:38 - Is vs ought&lt;/p&gt;
&lt;p&gt;01:02:29 - AI discussion&lt;/p&gt;
&lt;p&gt;01:07:38 - War / global competition&lt;/p&gt;
&lt;p&gt;01:11:02 - Open source F16 designs&lt;/p&gt;
&lt;p&gt;01:20:37 - Offense vs defense&lt;/p&gt;
&lt;p&gt;01:28:49 - Morality / value&lt;/p&gt;
&lt;p&gt;01:43:34 - What would Conor do&lt;/p&gt;
&lt;p&gt;01:50:36 - Institutions/regulation&lt;/p&gt;
&lt;p&gt;02:26:41 - Competition vs. Regulation Dilemma&lt;/p&gt;
&lt;p&gt;02:32:50 - Existential Risks and Future Planning&lt;/p&gt;
&lt;p&gt;02:41:46 - Conclusion and Reflection&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Note from Tim: I baked the chapter metadata into the mp3 file this time, does that help the chapters show up in your app? Let me know. Also I accidentally exported a few minutes of dead audio at the end of the file - sorry about that just skip on when the episode finishes. &lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>03:00:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1706993818893-66494d4e5fa33.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Mahault Albarracin - Cognitive Science ]]></title>
			<description><![CDATA[<p>Watch behind the scenes, get early access and join the private Discord by supporting us on Patreon: </p>
<p>https://patreon.com/mlst (public discord)</p>
<p>https://discord.gg/aNPkGUQtc5</p>
<p>https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>YT version: https://youtu.be/n8G50ynU0Vg</p>
<p><br></p>
<p>In this interview on MLST, Dr. Tim Scarfe interviews Mahault Albarracin, who is the director of product for R&amp;D at VERSES and also a PhD student in cognitive computing at the University of Quebec in Montreal. They discuss a range of topics related to consciousness, cognition, and machine learning.</p>
<p><br></p>
<p>Throughout the conversation, they touch upon various philosophical and computational concepts such as panpsychism, computationalism, and materiality. They consider the &quot;hard problem&quot; of consciousness, which is the question of how and why we have subjective experiences.</p>
<p><br></p>
<p>Albarracin shares her views on the controversial Integrated Information Theory and the open letter of opposition it received from the scientific community. She reflects on the nature of scientific critique and rivalry, advising caution in declaring entire fields of study as pseudoscientific.</p>
<p><br></p>
<p>A substantial part of the discussion is dedicated to the topic of science itself, where Albarracin talks about thresholds between legitimate science and pseudoscience, the role of evidence, and the importance of validating scientific methods and claims.</p>
<p><br></p>
<p>They touch upon language models, discussing whether they can be considered as having a &quot;theory of mind&quot; and the implications of assigning such properties to AI systems. Albarracin challenges the idea that there is a pure form of intelligence independent of material constraints and emphasizes the role of sociality in the development of our cognitive abilities.</p>
<p><br></p>
<p>Albarracin offers her thoughts on scientific endeavors, the predictability of systems, the nature of intelligence, and the processes of learning and adaptation. She gives insights into the concept of using degeneracy as a way to increase resilience within systems and the role of maintaining a degree of redundancy or extra capacity as a buffer against unforeseen events.</p>
<p><br></p>
<p>The conversation concludes with her discussing the potential benefits of collective intelligence, likening the adaptability and resilience of interconnected agent systems to those found in natural ecosystems.</p>
<p><br></p>
<p>https://www.linkedin.com/in/mahault-albarracin-1742bb153/</p>
<p><br></p>
<p>00:00:00 - Intro / IIT scandal </p>
<p>00:05:54 - Gaydar paper / What makes good science</p>
<p>00:10:51 - Language</p>
<p>00:18:16 - Intelligence</p>
<p>00:29:06 - X-risk</p>
<p>00:40:49 - Self modelling</p>
<p>00:43:56 - Anthropomorphisation</p>
<p>00:46:41 - Mediation and subjectivity</p>
<p>00:51:03 - Understanding</p>
<p>00:56:33 - Resiliency</p>
<p><br></p>
<p>Technical topics: </p>
<p>1. Integrated Information Theory (IIT) - Giulio Tononi</p>
<p>2. The &quot;hard problem&quot; of consciousness - David Chalmers</p>
<p>3. Panpsychism and Computationalism in philosophy of mind</p>
<p>4. Active Inference Framework - Karl Friston</p>
<p>5. Theory of Mind and its computation in AI systems</p>
<p>6. Noam Chomsky&#39;s views on language models and linguistics</p>
<p>7. Daniel Dennett&#39;s Intentional Stance theory</p>
<p>8. Collective intelligence and system resilience</p>
<p>9. Redundancy and degeneracy in complex systems</p>
<p>10. Michael Levin&#39;s research on bioelectricity and pattern formation</p>
<p>11. The role of phenomenology in cognitive science</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Mahault-Albarracin---Cognitive-Science-e2ee5m0</link>
			<guid isPermaLink="false">cf1628b1-5421-4442-8d81-0e5efba02fa4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 14 Jan 2024 14:16:36 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/81253504/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-14%2F4725de70-1169-18b6-e9b9-d060c4d35f9d.mp3" length="161118720" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Watch behind the scenes, get early access and join the private Discord by supporting us on Patreon: &lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst (public discord)&lt;/p&gt;
&lt;p&gt;https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/n8G50ynU0Vg&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this interview on MLST, Dr. Tim Scarfe interviews Mahault Albarracin, who is the director of product for R&amp;amp;D at VERSES and also a PhD student in cognitive computing at the University of Quebec in Montreal. They discuss a range of topics related to consciousness, cognition, and machine learning.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Throughout the conversation, they touch upon various philosophical and computational concepts such as panpsychism, computationalism, and materiality. They consider the &amp;quot;hard problem&amp;quot; of consciousness, which is the question of how and why we have subjective experiences.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Albarracin shares her views on the controversial Integrated Information Theory and the open letter of opposition it received from the scientific community. She reflects on the nature of scientific critique and rivalry, advising caution in declaring entire fields of study as pseudoscientific.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A substantial part of the discussion is dedicated to the topic of science itself, where Albarracin talks about thresholds between legitimate science and pseudoscience, the role of evidence, and the importance of validating scientific methods and claims.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;They touch upon language models, discussing whether they can be considered as having a &amp;quot;theory of mind&amp;quot; and the implications of assigning such properties to AI systems. Albarracin challenges the idea that there is a pure form of intelligence independent of material constraints and emphasizes the role of sociality in the development of our cognitive abilities.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Albarracin offers her thoughts on scientific endeavors, the predictability of systems, the nature of intelligence, and the processes of learning and adaptation. She gives insights into the concept of using degeneracy as a way to increase resilience within systems and the role of maintaining a degree of redundancy or extra capacity as a buffer against unforeseen events.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The conversation concludes with her discussing the potential benefits of collective intelligence, likening the adaptability and resilience of interconnected agent systems to those found in natural ecosystems.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/mahault-albarracin-1742bb153/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 - Intro / IIT scandal &lt;/p&gt;
&lt;p&gt;00:05:54 - Gaydar paper / What makes good science&lt;/p&gt;
&lt;p&gt;00:10:51 - Language&lt;/p&gt;
&lt;p&gt;00:18:16 - Intelligence&lt;/p&gt;
&lt;p&gt;00:29:06 - X-risk&lt;/p&gt;
&lt;p&gt;00:40:49 - Self modelling&lt;/p&gt;
&lt;p&gt;00:43:56 - Anthropomorphisation&lt;/p&gt;
&lt;p&gt;00:46:41 - Mediation and subjectivity&lt;/p&gt;
&lt;p&gt;00:51:03 - Understanding&lt;/p&gt;
&lt;p&gt;00:56:33 - Resiliency&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Technical topics: &lt;/p&gt;
&lt;p&gt;1. Integrated Information Theory (IIT) - Giulio Tononi&lt;/p&gt;
&lt;p&gt;2. The &amp;quot;hard problem&amp;quot; of consciousness - David Chalmers&lt;/p&gt;
&lt;p&gt;3. Panpsychism and Computationalism in philosophy of mind&lt;/p&gt;
&lt;p&gt;4. Active Inference Framework - Karl Friston&lt;/p&gt;
&lt;p&gt;5. Theory of Mind and its computation in AI systems&lt;/p&gt;
&lt;p&gt;6. Noam Chomsky&amp;#39;s views on language models and linguistics&lt;/p&gt;
&lt;p&gt;7. Daniel Dennett&amp;#39;s Intentional Stance theory&lt;/p&gt;
&lt;p&gt;8. Collective intelligence and system resilience&lt;/p&gt;
&lt;p&gt;9. Redundancy and degeneracy in complex systems&lt;/p&gt;
&lt;p&gt;10. Michael Levin&amp;#39;s research on bioelectricity and pattern formation&lt;/p&gt;
&lt;p&gt;11. The role of phenomenology in cognitive science&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:07</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1705241740920-e0848d862120d.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[$450M AI Startup In 3 Years | Chai AI]]></title>
			<description><![CDATA[<p>Chai AI is the leading platform for conversational chat artificial intelligence. 
</p>
<p>Note: this is a sponsored episode of MLST. </p>
<p>
William Beauchamp is the founder of two $100M+ companies - Chai Research, an AI startup, and Seamless Capital, a hedge fund based in Cambridge, UK.

Chaiverse is the Chai AI developer platform, where developers can train, submit and evaluate on millions of real users to win their share of $1,000,000.

https://www.chai-research.com
https://www.chaiverse.com
https://twitter.com/chai_research
https://facebook.com/chairesearch/
https://www.instagram.com/chairesearch/
Download the app on iOS and Android (https://onelink.to/kqzhy9 )

#chai #chai_ai #chai_research #chaiverse #generative_ai #LLMs</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/450M-AI-Startup-In-3-Years--Chai-AI-e2e7pma</link>
			<guid isPermaLink="false">418866e7-1e82-41c3-8282-479dd632720c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 09 Jan 2024 22:11:35 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/81044618/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-9%2Fda5f159a-22f5-5db0-2c50-5348da2678e5.mp3" length="71494080" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Chai AI is the leading platform for conversational chat artificial intelligence. 
&lt;/p&gt;
&lt;p&gt;Note: this is a sponsored episode of MLST. &lt;/p&gt;
&lt;p&gt;
William Beauchamp is the founder of two $100M+ companies - Chai Research, an AI startup, and Seamless Capital, a hedge fund based in Cambridge, UK.

Chaiverse is the Chai AI developer platform, where developers can train, submit and evaluate on millions of real users to win their share of $1,000,000.

https://www.chai-research.com
https://www.chaiverse.com
https://twitter.com/chai_research
https://facebook.com/chairesearch/
https://www.instagram.com/chairesearch/
Download the app on iOS and Android (https://onelink.to/kqzhy9 )

#chai #chai_ai #chai_research #chaiverse #generative_ai #LLMs&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:29:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1704838100945-6bf3cf352483a.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[DOES AI HAVE AGENCY? With Professor. Karl Friston and Riddhi J. Pitliya]]></title>
			<description><![CDATA[<p>Watch behind the scenes, get early access and join the private Discord by supporting us on Patreon: </p>
<p>https://patreon.com/mlst (public discord)</p>
<p>https://discord.gg/aNPkGUQtc5</p>
<p>https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>DOES AI HAVE AGENCY? With Professor. Karl Friston and Riddhi J. Pitliya</p>
<p><br></p>
<p>Agency in the context of cognitive science, particularly when considering the free energy principle, extends beyond just human decision-making and autonomy. It encompasses a broader understanding of how all living systems, including non-human entities, interact with their environment to maintain their existence by minimising sensory surprise.</p>
<p><br></p>
<p>According to the free energy principle, living organisms strive to minimize the difference between their predicted states and the actual sensory inputs they receive. This principle suggests that agency arises as a natural consequence of this process, particularly when organisms appear to plan ahead many steps in the future. </p>
<p><br></p>
<p>Riddhi J. Pitliya is based in the computational psychopathology lab doing her Ph.D at the University of Oxford and works with Professor Karl Friston at VERSES. </p>
<p>https://twitter.com/RiddhiJP</p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>THE FREE ENERGY PRINCIPLE—A PRECIS [Ramstead]</p>
<p>https://www.dialecticalsystems.eu/contributions/the-free-energy-principle-a-precis/</p>
<p><br></p>
<p>Active Inference: The Free Energy Principle in Mind, Brain, and Behavior [Thomas Parr, Giovanni Pezzulo, Karl J. Friston]</p>
<p>https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind</p>
<p><br></p>
<p>The beauty of collective intelligence, explained by a developmental biologist | Michael Levin</p>
<p>https://www.youtube.com/watch?v=U93x9AWeuOA</p>
<p><br></p>
<p>Growing Neural Cellular Automata</p>
<p>https://distill.pub/2020/growing-ca</p>
<p><br></p>
<p>Carcinisation</p>
<p>https://en.wikipedia.org/wiki/Carcinisation</p>
<p><br></p>
<p>Prof. KENNETH STANLEY - Why Greatness Cannot Be Planned</p>
<p>https://www.youtube.com/watch?v=lhYGXYeMq_E</p>
<p><br></p>
<p>On Defining Artificial Intelligence [Pei Wang]</p>
<p>https://sciendo.com/article/10.2478/jagi-2019-0002</p>
<p><br></p>
<p>Why? The Purpose of the Universe [Goff]</p>
<p>https://amzn.to/4aEqpfm</p>
<p><br></p>
<p>Umwelt</p>
<p>https://en.wikipedia.org/wiki/Umwelt</p>
<p><br></p>
<p>An Immense World: How Animal Senses Reveal the Hidden Realms [Yong]</p>
<p>https://amzn.to/3tzzTb7</p>
<p><br></p>
<p>What&#39;s it like to be a bat [Nagal]</p>
<p>https://www.sas.upenn.edu/~cavitch/pdf-library/Nagel_Bat.pdf</p>
<p><br></p>
<p>COUNTERFEIT PEOPLE. DANIEL DENNETT. (SPECIAL EDITION)</p>
<p>https://www.youtube.com/watch?v=axJtywd9Tbo</p>
<p><br></p>
<p>We live in the infosphere [FLORIDI]</p>
<p>https://www.youtube.com/watch?v=YLNGvvgq3eg</p>
<p><br></p>
<p>Mark Zuckerberg: First Interview in the Metaverse | Lex Fridman Podcast #398</p>
<p>https://www.youtube.com/watch?v=MVYrJJNdrEg</p>
<p><br></p>
<p>Black Mirror: Rachel, Jack and Ashley Too | Official Trailer | Netflix</p>
<p>https://www.youtube.com/watch?v=-qIlCo9yqpY</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/DOES-AI-HAVE-AGENCY--With-Professor--Karl-Friston-and-Riddhi-J--Pitliya-e2e4f2n</link>
			<guid isPermaLink="false">45c4ddaa-cd42-455d-9fe4-cdeb02d3a855</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 07 Jan 2024 19:37:46 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/80935447/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2024-0-7%2F43178d4d-663b-e236-7650-35007b52702c.mp3" length="90220608" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Watch behind the scenes, get early access and join the private Discord by supporting us on Patreon: &lt;/p&gt;
&lt;p&gt;https://patreon.com/mlst (public discord)&lt;/p&gt;
&lt;p&gt;https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;DOES AI HAVE AGENCY? With Professor. Karl Friston and Riddhi J. Pitliya&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Agency in the context of cognitive science, particularly when considering the free energy principle, extends beyond just human decision-making and autonomy. It encompasses a broader understanding of how all living systems, including non-human entities, interact with their environment to maintain their existence by minimising sensory surprise.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;According to the free energy principle, living organisms strive to minimize the difference between their predicted states and the actual sensory inputs they receive. This principle suggests that agency arises as a natural consequence of this process, particularly when organisms appear to plan ahead many steps in the future. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Riddhi J. Pitliya is based in the computational psychopathology lab doing her Ph.D at the University of Oxford and works with Professor Karl Friston at VERSES. &lt;/p&gt;
&lt;p&gt;https://twitter.com/RiddhiJP&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;THE FREE ENERGY PRINCIPLE—A PRECIS [Ramstead]&lt;/p&gt;
&lt;p&gt;https://www.dialecticalsystems.eu/contributions/the-free-energy-principle-a-precis/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Active Inference: The Free Energy Principle in Mind, Brain, and Behavior [Thomas Parr, Giovanni Pezzulo, Karl J. Friston]&lt;/p&gt;
&lt;p&gt;https://direct.mit.edu/books/oa-monograph/5299/Active-InferenceThe-Free-Energy-Principle-in-Mind&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The beauty of collective intelligence, explained by a developmental biologist | Michael Levin&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=U93x9AWeuOA&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Growing Neural Cellular Automata&lt;/p&gt;
&lt;p&gt;https://distill.pub/2020/growing-ca&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Carcinisation&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Carcinisation&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. KENNETH STANLEY - Why Greatness Cannot Be Planned&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=lhYGXYeMq_E&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;On Defining Artificial Intelligence [Pei Wang]&lt;/p&gt;
&lt;p&gt;https://sciendo.com/article/10.2478/jagi-2019-0002&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Why? The Purpose of the Universe [Goff]&lt;/p&gt;
&lt;p&gt;https://amzn.to/4aEqpfm&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Umwelt&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Umwelt&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;An Immense World: How Animal Senses Reveal the Hidden Realms [Yong]&lt;/p&gt;
&lt;p&gt;https://amzn.to/3tzzTb7&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What&amp;#39;s it like to be a bat [Nagal]&lt;/p&gt;
&lt;p&gt;https://www.sas.upenn.edu/~cavitch/pdf-library/Nagel_Bat.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;COUNTERFEIT PEOPLE. DANIEL DENNETT. (SPECIAL EDITION)&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=axJtywd9Tbo&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We live in the infosphere [FLORIDI]&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=YLNGvvgq3eg&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Mark Zuckerberg: First Interview in the Metaverse | Lex Fridman Podcast #398&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=MVYrJJNdrEg&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Black Mirror: Rachel, Jack and Ashley Too | Official Trailer | Netflix&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=-qIlCo9yqpY&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:39</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1704656213831-10e515578118.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Understanding Deep Learning - Prof. SIMON PRINCE [STAFF FAVOURITE]]]></title>
			<description><![CDATA[<p>Watch behind the scenes, get early access and join private Discord by supporting us on Patreon: https://patreon.com/mlst</p>
<p>https://discord.gg/aNPkGUQtc5</p>
<p>https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>In this comprehensive exploration of the field of deep learning with Professor Simon Prince who has just authored an entire text book on Deep Learning, we investigate the technical underpinnings that contribute to the field&#39;s unexpected success and confront the enduring conundrums that still perplex AI researchers.</p>
<p><br></p>
<p>Key points discussed include the surprising efficiency of deep learning models, where high-dimensional loss functions are optimized in ways which defy traditional statistical expectations. Professor Prince provides an exposition on the choice of activation functions, architecture design considerations, and overparameterization. We scrutinize the generalization capabilities of neural networks, addressing the seeming paradox of well-performing overparameterized models. Professor Prince challenges popular misconceptions, shedding light on the manifold hypothesis and the role of data geometry in informing the training process. Professor Prince speaks about how layers within neural networks collaborate, recursively reconfiguring instance representations that contribute to both the stability of learning and the emergence of hierarchical feature representations. In addition to the primary discussion on technical elements and learning dynamics, the conversation briefly diverts to audit the implications of AI advancements with ethical concerns.</p>
<p><br></p>
<p>Follow Prof. Prince: </p>
<p>https://twitter.com/SimonPrinceAI</p>
<p>https://www.linkedin.com/in/simon-prince-615bb9165/</p>
<p><br></p>
<p>Get the book now!</p>
<p>https://mitpress.mit.edu/9780262048644/understanding-deep-learning/</p>
<p>https://udlbook.github.io/udlbook/</p>
<p><br></p>
<p>Panel: Dr. Tim Scarfe - </p>
<p>https://www.linkedin.com/in/ecsquizor/ </p>
<p>https://twitter.com/ecsquendor</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction</p>
<p>[00:11:03] General Book Discussion</p>
<p>[00:15:30] The Neural Metaphor</p>
<p>[00:17:56] Back to Book Discussion</p>
<p>[00:18:33] Emergence and the Mind</p>
<p>[00:29:10] Computation in Transformers</p>
<p>[00:31:12] Studio Interview with Prof. Simon Prince</p>
<p>[00:31:46] Why Deep Neural Networks Work: Spline Theory</p>
<p>[00:40:29] Overparameterization in Deep Learning</p>
<p>[00:43:42] Inductive Priors and the Manifold Hypothesis</p>
<p>[00:49:31] Universal Function Approximation and Deep Networks</p>
<p>[00:59:25] Training vs Inference: Model Bias</p>
<p>[01:03:43] Model Generalization Challenges</p>
<p>[01:11:47] Purple Segment: Unknown Topic</p>
<p>[01:12:45] Visualizations in Deep Learning</p>
<p>[01:18:03] Deep Learning Theories Overview</p>
<p>[01:24:29] Tricks in Neural Networks</p>
<p>[01:30:37] Critiques of ChatGPT</p>
<p>[01:42:45] Ethical Considerations in AI</p>
<p><br></p>
<p>References on YT version VD: https://youtu.be/sJXn4Cl4oww</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Understanding-Deep-Learning---Prof--SIMON-PRINCE-STAFF-FAVOURITE-e2dmd3i</link>
			<guid isPermaLink="false">84b9ea9e-310c-4c3b-b348-9cd280b2b6bd</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 26 Dec 2023 20:33:13 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/80474674/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-11-26%2F74a865bd-0387-d8d3-2ffd-ed59ee431f03.mp3" length="182352384" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Watch behind the scenes, get early access and join private Discord by supporting us on Patreon: https://patreon.com/mlst&lt;/p&gt;
&lt;p&gt;https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this comprehensive exploration of the field of deep learning with Professor Simon Prince who has just authored an entire text book on Deep Learning, we investigate the technical underpinnings that contribute to the field&amp;#39;s unexpected success and confront the enduring conundrums that still perplex AI researchers.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Key points discussed include the surprising efficiency of deep learning models, where high-dimensional loss functions are optimized in ways which defy traditional statistical expectations. Professor Prince provides an exposition on the choice of activation functions, architecture design considerations, and overparameterization. We scrutinize the generalization capabilities of neural networks, addressing the seeming paradox of well-performing overparameterized models. Professor Prince challenges popular misconceptions, shedding light on the manifold hypothesis and the role of data geometry in informing the training process. Professor Prince speaks about how layers within neural networks collaborate, recursively reconfiguring instance representations that contribute to both the stability of learning and the emergence of hierarchical feature representations. In addition to the primary discussion on technical elements and learning dynamics, the conversation briefly diverts to audit the implications of AI advancements with ethical concerns.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Follow Prof. Prince: &lt;/p&gt;
&lt;p&gt;https://twitter.com/SimonPrinceAI&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/simon-prince-615bb9165/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Get the book now!&lt;/p&gt;
&lt;p&gt;https://mitpress.mit.edu/9780262048644/understanding-deep-learning/&lt;/p&gt;
&lt;p&gt;https://udlbook.github.io/udlbook/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe - &lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/ecsquizor/ &lt;/p&gt;
&lt;p&gt;https://twitter.com/ecsquendor&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:11:03] General Book Discussion&lt;/p&gt;
&lt;p&gt;[00:15:30] The Neural Metaphor&lt;/p&gt;
&lt;p&gt;[00:17:56] Back to Book Discussion&lt;/p&gt;
&lt;p&gt;[00:18:33] Emergence and the Mind&lt;/p&gt;
&lt;p&gt;[00:29:10] Computation in Transformers&lt;/p&gt;
&lt;p&gt;[00:31:12] Studio Interview with Prof. Simon Prince&lt;/p&gt;
&lt;p&gt;[00:31:46] Why Deep Neural Networks Work: Spline Theory&lt;/p&gt;
&lt;p&gt;[00:40:29] Overparameterization in Deep Learning&lt;/p&gt;
&lt;p&gt;[00:43:42] Inductive Priors and the Manifold Hypothesis&lt;/p&gt;
&lt;p&gt;[00:49:31] Universal Function Approximation and Deep Networks&lt;/p&gt;
&lt;p&gt;[00:59:25] Training vs Inference: Model Bias&lt;/p&gt;
&lt;p&gt;[01:03:43] Model Generalization Challenges&lt;/p&gt;
&lt;p&gt;[01:11:47] Purple Segment: Unknown Topic&lt;/p&gt;
&lt;p&gt;[01:12:45] Visualizations in Deep Learning&lt;/p&gt;
&lt;p&gt;[01:18:03] Deep Learning Theories Overview&lt;/p&gt;
&lt;p&gt;[01:24:29] Tricks in Neural Networks&lt;/p&gt;
&lt;p&gt;[01:30:37] Critiques of ChatGPT&lt;/p&gt;
&lt;p&gt;[01:42:45] Ethical Considerations in AI&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References on YT version VD: https://youtu.be/sJXn4Cl4oww&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:06:38</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1703622724067-e592633b5a08e.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. BERT DE VRIES - ON ACTIVE INFERENCE]]></title>
			<description><![CDATA[<p>Watch behind the scenes with Bert on Patreon: https://www.patreon.com/posts/bert-de-vries-93230722
https://discord.gg/aNPkGUQtc5
https://twitter.com/MLStreetTalk</p>
<p>Note, there is some mild background music on chapter 1 (Least Action), 3 (Friston) and 5 (Variational Methods) - please skip ahead if annoying. It&#39;s a tiny fraction of the overall podcast. 
</p>
<p>YT version: https://youtu.be/2wnJ6E6rQsU</p>
<p>
Bert de Vries is Professor in the Signal Processing Systems group at Eindhoven University. His research focuses on the development of intelligent autonomous agents that learn from in-situ interactions with their environment. His research draws inspiration from diverse fields including computational neuroscience, Bayesian machine learning, Active Inference and signal processing. 

Bert believes that development of signal processing systems will in the future be largely automated by autonomously operating agents that learn purposeful from situated environmental interactions.

Bert received nis M.Sc. (1986) and Ph.D. (1991) degrees in Electrical Engineering from Eindhoven University of Technology (TU/e) and the University of Florida, respectively. From 1992 to 1999, he worked as a research scientist at Sarnoff Research Center in Princeton (NJ, USA). Since 1999, he has been employed in the hearing aids industry, both in engineering and managerial positions. De Vries was appointed part-time professor in the Signal Processing Systems Group at TU/e in 2012.

Contact:
https://twitter.com/bertdv0
https://www.tue.nl/en/research/researchers/bert-de-vries
https://www.verses.ai/about-us

Panel: Dr. Tim Scarfe / Dr. Keith Duggar

TOC:
[00:00:00] Principle of Least Action
[00:05:10] Patreon Teaser
[00:05:46] On Friston
[00:07:34] Capm Peterson (VERSES)
[00:08:20] Variational Methods
[00:16:13] Dan Mapes (VERSES)
[00:17:12] Engineering with Active Inference
[00:20:23] Jason Fox (VERSES)
[00:20:51] Riddhi Jain Pitliya
[00:21:49] Hearing Aids as Adaptive Agents
[00:33:38] Steven Swanson (VERSES)
[00:35:46] Main Interview Kick Off, Engineering and Active Inference
[00:43:35] Actor / Streaming / Message Passing
[00:56:21] Do Agents Lose Flexibility with Maturity?
[01:00:50] Language Compression
[01:04:37] Marginalisation to Abstraction
[01:12:45] Online Structural Learning
[01:18:40] Efficiency in Active Inference
[01:26:25] SEs become Neuroscientists
[01:35:11] Building an Automated Engineer
[01:38:58] Robustness and Design vs Grow
[01:42:38] RXInfer
[01:51:12] Resistance to Active Inference?
[01:57:39] Diffusion of Responsibility in a System
[02:10:33] Chauvinism in &quot;Understanding&quot;
[02:20:08] On Becoming a Bayesian

Refs:

RXInfer
https://biaslab.github.io/rxinfer-website/

Prof. Ariel Caticha
https://www.albany.edu/physics/faculty/ariel-caticha

Pattern recognition and machine learning (Bishop)
https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf

Data Analysis: A Bayesian Tutorial (Sivia)
https://www.amazon.co.uk/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320

Probability Theory: The Logic of Science (E. T. Jaynes)
https://www.amazon.co.uk/Probability-Theory-Principles-Elementary-Applications/dp/0521592712/

#activeinference #artificialintelligence</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--BERT-DE-VRIES---ON-ACTIVE-INFERENCE-e2c6l1u</link>
			<guid isPermaLink="false">b14f6711-ae9e-4f0c-864e-a94f7dab2204</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 20 Nov 2023 22:08:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/78909950/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-20%2Feaceb699-dd36-1cf1-f316-ba2102d1a009.mp3" length="212618880" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Watch behind the scenes with Bert on Patreon: https://www.patreon.com/posts/bert-de-vries-93230722
https://discord.gg/aNPkGUQtc5
https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;Note, there is some mild background music on chapter 1 (Least Action), 3 (Friston) and 5 (Variational Methods) - please skip ahead if annoying. It&amp;#39;s a tiny fraction of the overall podcast. 
&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/2wnJ6E6rQsU&lt;/p&gt;
&lt;p&gt;
Bert de Vries is Professor in the Signal Processing Systems group at Eindhoven University. His research focuses on the development of intelligent autonomous agents that learn from in-situ interactions with their environment. His research draws inspiration from diverse fields including computational neuroscience, Bayesian machine learning, Active Inference and signal processing. 

Bert believes that development of signal processing systems will in the future be largely automated by autonomously operating agents that learn purposeful from situated environmental interactions.

Bert received nis M.Sc. (1986) and Ph.D. (1991) degrees in Electrical Engineering from Eindhoven University of Technology (TU/e) and the University of Florida, respectively. From 1992 to 1999, he worked as a research scientist at Sarnoff Research Center in Princeton (NJ, USA). Since 1999, he has been employed in the hearing aids industry, both in engineering and managerial positions. De Vries was appointed part-time professor in the Signal Processing Systems Group at TU/e in 2012.

Contact:
https://twitter.com/bertdv0
https://www.tue.nl/en/research/researchers/bert-de-vries
https://www.verses.ai/about-us

Panel: Dr. Tim Scarfe / Dr. Keith Duggar

TOC:
[00:00:00] Principle of Least Action
[00:05:10] Patreon Teaser
[00:05:46] On Friston
[00:07:34] Capm Peterson (VERSES)
[00:08:20] Variational Methods
[00:16:13] Dan Mapes (VERSES)
[00:17:12] Engineering with Active Inference
[00:20:23] Jason Fox (VERSES)
[00:20:51] Riddhi Jain Pitliya
[00:21:49] Hearing Aids as Adaptive Agents
[00:33:38] Steven Swanson (VERSES)
[00:35:46] Main Interview Kick Off, Engineering and Active Inference
[00:43:35] Actor / Streaming / Message Passing
[00:56:21] Do Agents Lose Flexibility with Maturity?
[01:00:50] Language Compression
[01:04:37] Marginalisation to Abstraction
[01:12:45] Online Structural Learning
[01:18:40] Efficiency in Active Inference
[01:26:25] SEs become Neuroscientists
[01:35:11] Building an Automated Engineer
[01:38:58] Robustness and Design vs Grow
[01:42:38] RXInfer
[01:51:12] Resistance to Active Inference?
[01:57:39] Diffusion of Responsibility in a System
[02:10:33] Chauvinism in &amp;quot;Understanding&amp;quot;
[02:20:08] On Becoming a Bayesian

Refs:

RXInfer
https://biaslab.github.io/rxinfer-website/

Prof. Ariel Caticha
https://www.albany.edu/physics/faculty/ariel-caticha

Pattern recognition and machine learning (Bishop)
https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf

Data Analysis: A Bayesian Tutorial (Sivia)
https://www.amazon.co.uk/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320

Probability Theory: The Logic of Science (E. T. Jaynes)
https://www.amazon.co.uk/Probability-Theory-Principles-Elementary-Applications/dp/0521592712/

#activeinference #artificialintelligence&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:27:39</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1700518047432-4cbb80a155115.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[MULTI AGENT LEARNING - LANCELOT DA COSTA]]></title>
			<description><![CDATA[<p>Please support us https://www.patreon.com/mlst </p>
<p>https://discord.gg/aNPkGUQtc5</p>
<p>https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Lance Da Costa aims to advance our understanding of intelligent systems by modelling cognitive systems and improving artificial systems.</p>
<p>He&#39;s a PhD candidate with Greg Pavliotis and Karl Friston jointly at Imperial College London and UCL, and a student in the Mathematics of Random Systems CDT run by Imperial College London and the University of Oxford. He completed an MRes in Brain Sciences at UCL with Karl Friston and Biswa Sengupta, an MASt in Pure Mathematics at the University of Cambridge with Oscar Randal-Williams, and a BSc in Mathematics at EPFL and the University of Toronto. </p>
<p><br></p>
<p>Summary:</p>
<p>Lance did pure math originally but became interested in the brain and AI. He started working with Karl Friston on the free energy principle, which claims all intelligent agents minimize free energy for perception, action, and decision-making. Lance has worked to provide mathematical foundations and proofs for why the free energy principle is true, starting from basic assumptions about agents interacting with their environment. This aims to justify the principle from first physics principles. Dr. Scarfe and Da Costa discuss different approaches to AI - the free energy/active inference approach focused on mimicking human intelligence vs approaches focused on maximizing capability like deep reinforcement learning. Lance argues active inference provides advantages for explainability and safety compared to black box AI systems. It provides a simple, sparse description of intelligence based on a generative model and free energy minimization. They discuss the need for structured learning and acquiring core knowledge to achieve more human-like intelligence. Lance highlights work from Josh Tenenbaum&#39;s lab that shows similar learning trajectories to humans in a simple Atari-like environment.</p>
<p>Incorporating core knowledge constraints the space of possible generative models the agent can use to represent the world, making learning more sample efficient. Lance argues active inference agents with core knowledge can match human learning capabilities.</p>
<p>They discuss how to make generative models interpretable, such as through factor graphs. The goal is to be able to understand the representations and message passing in the model that leads to decisions.</p>
<p>In summary, Lance argues active inference provides a principled approach to AI with advantages for explainability, safety, and human-like learning. Combining it with core knowledge and structural learning aims to achieve more human-like artificial intelligence.</p>
<p><br></p>
<p>https://www.lancelotdacosta.com/</p>
<p>https://twitter.com/lancelotdacosta</p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC</p>
<p>00:00:00 - Start</p>
<p>00:09:27 - Intelligence</p>
<p>00:12:37 - Priors / structure learning</p>
<p>00:17:21 - Core knowledge</p>
<p>00:29:05 - Intelligence is specialised</p>
<p>00:33:21 - The magic of agents</p>
<p>00:39:30 - Intelligibility of structure learning</p>
<p><br></p>
<p>#artificialintelligence #activeinference</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/MULTI-AGENT-LEARNING---LANCELOT-DA-COSTA-e2bgsjs</link>
			<guid isPermaLink="false">0b978bda-9030-469f-93bb-ed26c6899fce</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 05 Nov 2023 15:15:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/78196796/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-10-5%2F1891ba52-58fa-c7c3-6e74-5a39fbf25723.mp3" length="71921224" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please support us https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Lance Da Costa aims to advance our understanding of intelligent systems by modelling cognitive systems and improving artificial systems.&lt;/p&gt;
&lt;p&gt;He&amp;#39;s a PhD candidate with Greg Pavliotis and Karl Friston jointly at Imperial College London and UCL, and a student in the Mathematics of Random Systems CDT run by Imperial College London and the University of Oxford. He completed an MRes in Brain Sciences at UCL with Karl Friston and Biswa Sengupta, an MASt in Pure Mathematics at the University of Cambridge with Oscar Randal-Williams, and a BSc in Mathematics at EPFL and the University of Toronto. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;p&gt;Lance did pure math originally but became interested in the brain and AI. He started working with Karl Friston on the free energy principle, which claims all intelligent agents minimize free energy for perception, action, and decision-making. Lance has worked to provide mathematical foundations and proofs for why the free energy principle is true, starting from basic assumptions about agents interacting with their environment. This aims to justify the principle from first physics principles. Dr. Scarfe and Da Costa discuss different approaches to AI - the free energy/active inference approach focused on mimicking human intelligence vs approaches focused on maximizing capability like deep reinforcement learning. Lance argues active inference provides advantages for explainability and safety compared to black box AI systems. It provides a simple, sparse description of intelligence based on a generative model and free energy minimization. They discuss the need for structured learning and acquiring core knowledge to achieve more human-like intelligence. Lance highlights work from Josh Tenenbaum&amp;#39;s lab that shows similar learning trajectories to humans in a simple Atari-like environment.&lt;/p&gt;
&lt;p&gt;Incorporating core knowledge constraints the space of possible generative models the agent can use to represent the world, making learning more sample efficient. Lance argues active inference agents with core knowledge can match human learning capabilities.&lt;/p&gt;
&lt;p&gt;They discuss how to make generative models interpretable, such as through factor graphs. The goal is to be able to understand the representations and message passing in the model that leads to decisions.&lt;/p&gt;
&lt;p&gt;In summary, Lance argues active inference provides a principled approach to AI with advantages for explainability, safety, and human-like learning. Combining it with core knowledge and structural learning aims to achieve more human-like artificial intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.lancelotdacosta.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/lancelotdacosta&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;00:00:00 - Start&lt;/p&gt;
&lt;p&gt;00:09:27 - Intelligence&lt;/p&gt;
&lt;p&gt;00:12:37 - Priors / structure learning&lt;/p&gt;
&lt;p&gt;00:17:21 - Core knowledge&lt;/p&gt;
&lt;p&gt;00:29:05 - Intelligence is specialised&lt;/p&gt;
&lt;p&gt;00:33:21 - The magic of agents&lt;/p&gt;
&lt;p&gt;00:39:30 - Intelligibility of structure learning&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#artificialintelligence #activeinference&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:56</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1699197298544-43fc6b6746593.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[THE HARD PROBLEM OF OBSERVERS - WOLFRAM & FRISTON [SPECIAL EDITION]]]></title>
			<description><![CDATA[<p>Please support us! https://www.patreon.com/mlst 
https://discord.gg/aNPkGUQtc5
https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>YT version (with intro not found here) https://youtu.be/6iaT-0Dvhnc

This is the epic special edition show you have been waiting for! With two of the most brilliant scientists alive today. 

Atoms, things, agents, ... observers. What even defines an &quot;observer&quot; and what properties must all observers share? How do objects persist in our universe given that their material composition changes over time? What does it mean for a thing to be a thing? And do things supervene on our lower-level physical reality? What does it mean for a thing to have agency? What&#39;s the difference between a complex dynamical system with and without agency? Could a rock or an AI catflap have agency? Can the universe be factorised into distinct agents, or is agency diffused? Have you ever pondered about these deep questions about reality?

Prof. Friston and Dr. Wolfram have spent their entire careers, some 40+ years each thinking long and hard about these very questions and have developed significant frameworks of reference on their respective journeys (the Wolfram Physics project and the Free Energy principle). </p>
<p>
Panel: MIT Ph.D Keith Duggar
Production: Dr. Tim Scarfe

Refs:
TED Talk with Stephen:
https://www.ted.com/talks/stephen_wolfram_how_to_think_computationally_about_ai_the_universe_and_everything
https://writings.stephenwolfram.com/2023/10/how-to-think-computationally-about-ai-the-universe-and-everything/

TOC
00:00:00 - Show kickoff</p>
<p>00:02:38 - Wolfram gets to grips with FEP</p>
<p>00:27:08 - How much control does an agent/observer have</p>
<p>00:34:52 - Observer persistence, what universe seems like to us</p>
<p>00:40:31 - Black holes</p>
<p>00:45:07 - Inside vs outside</p>
<p>00:52:20 - Moving away from the predictable path</p>
<p>00:55:26 - What can observers do</p>
<p>01:06:50 - Self modelling gives agency</p>
<p>01:11:26 - How do you know a thing has agency?</p>
<p>01:22:48 - Deep link between dynamics, ruliad and AI</p>
<p>01:25:52 - Does agency entail free will? Defining Agency</p>
<p>01:32:57 - Where do I probe for agency?</p>
<p>01:39:13 - Why is the universe the way we see it?</p>
<p>01:42:50 - Alien intelligence</p>
<p>01:43:40 - The hard problem of Observers</p>
<p>01:46:20 - Summary thoughts from Wolfram</p>
<p>01:49:35 - Factorisability of FEP</p>
<p>01:57:05 - Patreon interview teaser</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/THE-HARD-PROBLEM-OF-OBSERVERS---WOLFRAM--FRISTON-SPECIAL-EDITION-e2b7ib5</link>
			<guid isPermaLink="false">1eb58c44-2c13-4552-96d2-dff031d7bda2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 29 Oct 2023 22:38:11 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/77891365/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-9-29%2F1e458bfd-b223-3678-e645-ce1d53be4687.mp3" length="172059264" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please support us! https://www.patreon.com/mlst 
https://discord.gg/aNPkGUQtc5
https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version (with intro not found here) https://youtu.be/6iaT-0Dvhnc

This is the epic special edition show you have been waiting for! With two of the most brilliant scientists alive today. 

Atoms, things, agents, ... observers. What even defines an &amp;quot;observer&amp;quot; and what properties must all observers share? How do objects persist in our universe given that their material composition changes over time? What does it mean for a thing to be a thing? And do things supervene on our lower-level physical reality? What does it mean for a thing to have agency? What&amp;#39;s the difference between a complex dynamical system with and without agency? Could a rock or an AI catflap have agency? Can the universe be factorised into distinct agents, or is agency diffused? Have you ever pondered about these deep questions about reality?

Prof. Friston and Dr. Wolfram have spent their entire careers, some 40+ years each thinking long and hard about these very questions and have developed significant frameworks of reference on their respective journeys (the Wolfram Physics project and the Free Energy principle). &lt;/p&gt;
&lt;p&gt;
Panel: MIT Ph.D Keith Duggar
Production: Dr. Tim Scarfe

Refs:
TED Talk with Stephen:
https://www.ted.com/talks/stephen_wolfram_how_to_think_computationally_about_ai_the_universe_and_everything
https://writings.stephenwolfram.com/2023/10/how-to-think-computationally-about-ai-the-universe-and-everything/

TOC
00:00:00 - Show kickoff&lt;/p&gt;
&lt;p&gt;00:02:38 - Wolfram gets to grips with FEP&lt;/p&gt;
&lt;p&gt;00:27:08 - How much control does an agent/observer have&lt;/p&gt;
&lt;p&gt;00:34:52 - Observer persistence, what universe seems like to us&lt;/p&gt;
&lt;p&gt;00:40:31 - Black holes&lt;/p&gt;
&lt;p&gt;00:45:07 - Inside vs outside&lt;/p&gt;
&lt;p&gt;00:52:20 - Moving away from the predictable path&lt;/p&gt;
&lt;p&gt;00:55:26 - What can observers do&lt;/p&gt;
&lt;p&gt;01:06:50 - Self modelling gives agency&lt;/p&gt;
&lt;p&gt;01:11:26 - How do you know a thing has agency?&lt;/p&gt;
&lt;p&gt;01:22:48 - Deep link between dynamics, ruliad and AI&lt;/p&gt;
&lt;p&gt;01:25:52 - Does agency entail free will? Defining Agency&lt;/p&gt;
&lt;p&gt;01:32:57 - Where do I probe for agency?&lt;/p&gt;
&lt;p&gt;01:39:13 - Why is the universe the way we see it?&lt;/p&gt;
&lt;p&gt;01:42:50 - Alien intelligence&lt;/p&gt;
&lt;p&gt;01:43:40 - The hard problem of Observers&lt;/p&gt;
&lt;p&gt;01:46:20 - Summary thoughts from Wolfram&lt;/p&gt;
&lt;p&gt;01:49:35 - Factorisability of FEP&lt;/p&gt;
&lt;p&gt;01:57:05 - Patreon interview teaser&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:59:29</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1698618975963-4cbeb1ca14ddd.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[DR. JEFF BECK - THE BAYESIAN BRAIN]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst </p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT version: https://www.youtube.com/watch?v=c4praCiy9qU</p>
<p><br></p>
<p>Dr. Jeff Beck is a computational neuroscientist studying probabilistic reasoning (decision making under uncertainty) in humans and animals with emphasis on neural representations of uncertainty and cortical implementations of probabilistic inference and learning. His line of research incorporates information theoretic and hierarchical statistical analysis of neural and behavioural data as well as reinforcement learning and active inference.</p>
<p><br></p>
<p>https://www.linkedin.com/in/jeff-beck...</p>
<p>https://scholar.google.com/citations?...</p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC</p>
<p>00:00:00 Intro</p>
<p>00:00:51 Bayesian / Knowledge</p>
<p>00:14:57 Active inference </p>
<p>00:18:58 Mediation</p>
<p>00:23:44 Philosophy of mind / science</p>
<p>00:29:25 Optimisation </p>
<p>00:42:54 Emergence</p>
<p>00:56:38 Steering emergent systems</p>
<p>01:04:31 Work plan</p>
<p>01:06:06 Representations/Core knowledge</p>
<p><br></p>
<p>#activeinference</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/DR--JEFF-BECK---THE-BAYESIAN-BRAIN-e2akqa1</link>
			<guid isPermaLink="false">850f7f92-e5d3-4bf8-83c6-ce4d46543af0</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 16 Oct 2023 10:47:07 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/77276929/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-9-16%2F2548666f-3f72-9008-5898-d9f59132c94f.mp3" length="100946304" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT version: https://www.youtube.com/watch?v=c4praCiy9qU&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Jeff Beck is a computational neuroscientist studying probabilistic reasoning (decision making under uncertainty) in humans and animals with emphasis on neural representations of uncertainty and cortical implementations of probabilistic inference and learning. His line of research incorporates information theoretic and hierarchical statistical analysis of neural and behavioural data as well as reinforcement learning and active inference.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/jeff-beck...&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?...&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:00:51 Bayesian / Knowledge&lt;/p&gt;
&lt;p&gt;00:14:57 Active inference &lt;/p&gt;
&lt;p&gt;00:18:58 Mediation&lt;/p&gt;
&lt;p&gt;00:23:44 Philosophy of mind / science&lt;/p&gt;
&lt;p&gt;00:29:25 Optimisation &lt;/p&gt;
&lt;p&gt;00:42:54 Emergence&lt;/p&gt;
&lt;p&gt;00:56:38 Steering emergent systems&lt;/p&gt;
&lt;p&gt;01:04:31 Work plan&lt;/p&gt;
&lt;p&gt;01:06:06 Representations/Core knowledge&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#activeinference&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:10:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1697453170511-9c65dc670e974.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Melanie Mitchell 2.0 - AI Benchmarks are Broken!]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

Prof. Melanie Mitchell argues that the concept of &quot;understanding&quot; in AI is ill-defined and multidimensional - we can&#39;t simply say an AI system does or doesn&#39;t understand. She advocates for rigorously testing AI systems&#39; capabilities using proper experimental methods from cognitive science. Popular benchmarks for intelligence often rely on the assumption that if a human can perform a task, an AI that performs the task must have human-like general intelligence. But benchmarks should evolve as capabilities improve.

Large language models show surprising skill on many human tasks but lack common sense and fail at simple things young children can do. Their knowledge comes from statistical relationships in text, not grounded concepts about the world. We don&#39;t know if their internal representations actually align with human-like concepts. More granular testing focused on generalization is needed.

There are open questions around whether large models&#39; abilities constitute a fundamentally different non-human form of intelligence based on vast statistical correlations across text. Mitchell argues intelligence is situated, domain-specific and grounded in physical experience and evolution. The brain computes but in a specialized way honed by evolution for controlling the body. Extracting &quot;pure&quot; intelligence may not work.

Other key points:

- Need more focus on proper experimental method in AI research. Developmental psychology offers examples for rigorous testing of cognition.
- Reporting instance-level failures rather than just aggregate accuracy can provide insights.
- Scaling laws and complex systems science are an interesting area of complexity theory, with applications to understanding cities.
- Concepts like &quot;understanding&quot; and &quot;intelligence&quot; in AI force refinement of fuzzy definitions.
- Human intelligence may be more collective and social than we realize. AI forces us to rethink concepts we apply anthropomorphically.

The overall emphasis is on rigorously building the science of machine cognition through proper experimentation and benchmarking as we assess emerging capabilities.

TOC:

[00:00:00] Introduction and Munk AI Risk Debate Highlights
[05:00:00] Douglas Hofstadter on AI Risk
[00:06:56] The Complexity of Defining Intelligence
[00:11:20] Examining Understanding in AI Models
[00:16:48] Melanie&#39;s Insights on AI Understanding Debate
[00:22:23] Unveiling the Concept Arc
[00:27:57] AI Goals: A Human vs Machine Perspective
[00:31:10] Addressing the Extrapolation Challenge in AI
[00:36:05] Brain Computation: The Human-AI Parallel
[00:38:20] The Arc Challenge: Implications and Insights
[00:43:20] The Need for Detailed AI Performance Reporting
[00:44:31] Exploring Scaling in Complexity Theory

Eratta: 

Note Tim said around 39 mins that a recent Stanford/DM paper modelling ARC “on GPT-4 got around 60%”. This is not correct and he misremembered. It was actually davinci3, and around 10%, which is still extremely good for a blank slate approach with an LLM and no ARC specific knowledge. Folks on our forum couldn’t reproduce the result. See paper linked below. 

Books (MUST READ):

Artificial Intelligence: A Guide for Thinking Humans (Melanie Mitchell)
https://www.amazon.co.uk/Artificial-Intelligence-Guide-Thinking-Humans/dp/B07YBHNM1C/?&amp;_encoding=UTF8&amp;tag=mlst00-21&amp;linkCode=ur2&amp;linkId=44ccac78973f47e59d745e94967c0f30&amp;camp=1634&amp;creative=6738

Complexity: A Guided Tour (Melanie Mitchell)
https://www.amazon.co.uk/Audible-Complexity-A-Guided-Tour?&amp;_encoding=UTF8&amp;tag=mlst00-21&amp;linkCode=ur2&amp;linkId=3f8bd505d86865c50c02dd7f10b27c05&amp;camp=1634&amp;creative=6738</p>
<p><br></p>
<p>Show notes (transcript, full references etc)</p>
<p>https://atlantic-papyrus-d68.notion.site/Melanie-Mitchell-2-0-15e212560e8e445d8b0131712bad3000?pvs=25</p>
<p>YT version: https://youtu.be/29gkDpR2orc</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Melanie-Mitchell-2-0---AI-Benchmarks-are-Broken-e2959li</link>
			<guid isPermaLink="false">69502d64-cf64-4426-b69c-9153320504ea</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 10 Sep 2023 18:28:11 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/75719794/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-8-10%2F5127448f-aab3-b628-ec12-ca8103f1ded7.mp3" length="88990272" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

Prof. Melanie Mitchell argues that the concept of &amp;quot;understanding&amp;quot; in AI is ill-defined and multidimensional - we can&amp;#39;t simply say an AI system does or doesn&amp;#39;t understand. She advocates for rigorously testing AI systems&amp;#39; capabilities using proper experimental methods from cognitive science. Popular benchmarks for intelligence often rely on the assumption that if a human can perform a task, an AI that performs the task must have human-like general intelligence. But benchmarks should evolve as capabilities improve.

Large language models show surprising skill on many human tasks but lack common sense and fail at simple things young children can do. Their knowledge comes from statistical relationships in text, not grounded concepts about the world. We don&amp;#39;t know if their internal representations actually align with human-like concepts. More granular testing focused on generalization is needed.

There are open questions around whether large models&amp;#39; abilities constitute a fundamentally different non-human form of intelligence based on vast statistical correlations across text. Mitchell argues intelligence is situated, domain-specific and grounded in physical experience and evolution. The brain computes but in a specialized way honed by evolution for controlling the body. Extracting &amp;quot;pure&amp;quot; intelligence may not work.

Other key points:

- Need more focus on proper experimental method in AI research. Developmental psychology offers examples for rigorous testing of cognition.
- Reporting instance-level failures rather than just aggregate accuracy can provide insights.
- Scaling laws and complex systems science are an interesting area of complexity theory, with applications to understanding cities.
- Concepts like &amp;quot;understanding&amp;quot; and &amp;quot;intelligence&amp;quot; in AI force refinement of fuzzy definitions.
- Human intelligence may be more collective and social than we realize. AI forces us to rethink concepts we apply anthropomorphically.

The overall emphasis is on rigorously building the science of machine cognition through proper experimentation and benchmarking as we assess emerging capabilities.

TOC:

[00:00:00] Introduction and Munk AI Risk Debate Highlights
[05:00:00] Douglas Hofstadter on AI Risk
[00:06:56] The Complexity of Defining Intelligence
[00:11:20] Examining Understanding in AI Models
[00:16:48] Melanie&amp;#39;s Insights on AI Understanding Debate
[00:22:23] Unveiling the Concept Arc
[00:27:57] AI Goals: A Human vs Machine Perspective
[00:31:10] Addressing the Extrapolation Challenge in AI
[00:36:05] Brain Computation: The Human-AI Parallel
[00:38:20] The Arc Challenge: Implications and Insights
[00:43:20] The Need for Detailed AI Performance Reporting
[00:44:31] Exploring Scaling in Complexity Theory

Eratta: 

Note Tim said around 39 mins that a recent Stanford/DM paper modelling ARC “on GPT-4 got around 60%”. This is not correct and he misremembered. It was actually davinci3, and around 10%, which is still extremely good for a blank slate approach with an LLM and no ARC specific knowledge. Folks on our forum couldn’t reproduce the result. See paper linked below. 

Books (MUST READ):

Artificial Intelligence: A Guide for Thinking Humans (Melanie Mitchell)
https://www.amazon.co.uk/Artificial-Intelligence-Guide-Thinking-Humans/dp/B07YBHNM1C/?&amp;amp;_encoding=UTF8&amp;amp;tag=mlst00-21&amp;amp;linkCode=ur2&amp;amp;linkId=44ccac78973f47e59d745e94967c0f30&amp;amp;camp=1634&amp;amp;creative=6738

Complexity: A Guided Tour (Melanie Mitchell)
https://www.amazon.co.uk/Audible-Complexity-A-Guided-Tour?&amp;amp;_encoding=UTF8&amp;amp;tag=mlst00-21&amp;amp;linkCode=ur2&amp;amp;linkId=3f8bd505d86865c50c02dd7f10b27c05&amp;amp;camp=1634&amp;amp;creative=6738&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Show notes (transcript, full references etc)&lt;/p&gt;
&lt;p&gt;https://atlantic-papyrus-d68.notion.site/Melanie-Mitchell-2-0-15e212560e8e445d8b0131712bad3000?pvs=25&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/29gkDpR2orc&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:01:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1694370431283-692adeb11b9e4.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Autopoitic Enactivism and the Free Energy Principle - Prof. Friston, Prof Buckley, Dr. Ramstead]]></title>
			<description><![CDATA[<p>We explore connections between FEP and enactivism, including tensions raised in a paper critiquing FEP from an enactivist perspective.</p>
<p><br></p>
<p>Dr. Maxwell Ramstead provides background on enactivism emerging from autopoiesis, with a focus on embodied cognition and rejecting information processing/computational views of mind. </p>
<p><br></p>
<p>Chris shares his journey from robotics into FEP, starting as a skeptic but becoming convinced it&#39;s the right framework. He notes there are both &quot;high road&quot; and &quot;low road&quot; versions, ranging from embodied to more radically anti-representational stances. He doesn&#39;t see a definitive fork between dynamical systems and information theory as the source of conflict. Rather, the notion of operational closure in enactivism seems to be the main sticking point.</p>
<p><br></p>
<p>The group explores definitional issues around structure/organization, boundaries, and operational closure. Maxwell argues the generative model in FEP captures organizational dependencies akin to operational closure. The Markov blanket formalism models structural interfaces.</p>
<p><br></p>
<p>We discuss the concept of goals in cognitive systems - Chris advocates an intentional stance perspective - using notions of goals/intentions if they help explain system dynamics. Goals emerge from beliefs about dynamical trajectories. Prof Friston provides an elegant explanation of how goal-directed behavior naturally falls out of the FEP mathematics in a particular &quot;goldilocks&quot; regime of system scale/dynamics. The conversation explores the idea that many systems simply act &quot;as if&quot; they have goals or models, without necessarily possessing explicit representations. This helps resolve tensions between enactivist and computational perspectives.</p>
<p><br></p>
<p>Throughout the dialogue, Maxwell presses philosophical points about the FEP abolishing what he perceives as false dichotomies in cognitive science such as internalism/externalism. He is critical of enactivists&#39; commitment to bright line divides between subject areas.</p>
<p><br></p>
<p>Prof. Karl Friston - Inventor of the free energy principle https://scholar.google.com/citations?user=q_4u0aoAAAAJ</p>
<p>Prof. Chris Buckley - Professor of Neural Computation at Sussex University https://scholar.google.co.uk/citations?user=nWuZ0XcAAAAJ&amp;hl=en</p>
<p>Dr. Maxwell Ramstead - Director of Research at VERSES https://scholar.google.ca/citations?user=ILpGOMkAAAAJ&amp;hl=fr</p>
<p><br></p>
<p>We address critique in this paper: </p>
<p>Laying down a forking path: Tensions between enaction and the free energy principle (Ezequiel A. Di Paolo, Evan Thompson, Randall D. Beere)</p>
<p>https://philosophymindscience.org/index.php/phimisci/article/download/9187/8975</p>
<p><br></p>
<p>Other refs:</p>
<p>Multiscale integration: beyond internalism and externalism (Maxwell J D Ramstead) </p>
<p>https://pubmed.ncbi.nlm.nih.gov/33627890/</p>
<p><br></p>
<p>MLST panel: Dr. Tim Scarfe and Dr. Keith Duggar</p>
<p><br></p>
<p>TOC (auto generated):
0:00 - Introduction
0:41 - Defining enactivism and its variants
6:58 - The source of the conflict between dynamical systems and information theory
8:56 - Operational closure in enactivism
10:03 - Goals and intentions
12:35 - The link between dynamical systems and information theory
15:02 - Path integrals and non-equilibrium dynamics
18:38 - Operational closure defined
21:52 - Structure vs. organization in enactivism
24:24 - Markov blankets as interfaces
28:48 - Operational closure in FEP
30:28 - Structure and organization again
31:08 - Dynamics vs. information theory
33:55 - Goals and intentions emerge in the FEP mathematics
36:58 - The Good Regulator Theorem
49:30 - enactivism and its relation to ecological psychology
52:00 - Goals, intentions and beliefs
55:21 - Boundaries and meaning
58:55 - Enactivism&#39;s rejection of information theory
1:02:08 - Beliefs vs goals
1:05:06 - Ecological psychology and FEP
1:08:41 - The Good Regulator Theorem
1:18:38 - How goal-directed behavior emerges
1:23:13 - Ontological vs metaphysical boundaries
1:25:20 - Boundaries as maps
1:31:08 - Connections to the maximum entropy principle
1:33:45 - Relations to quantum and relational physics</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Autopoitic-Enactivism-and-the-Free-Energy-Principle---Prof--Friston--Prof-Buckley--Dr--Ramstead-e28v7rp</link>
			<guid isPermaLink="false">ebdb88ae-8b52-475d-b7d3-8dca24857ab1</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 05 Sep 2023 21:39:06 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/75521337/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-8-5%2F7ab707f2-38b9-28d9-e473-a5a99cca111a.mp3" length="136468418" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We explore connections between FEP and enactivism, including tensions raised in a paper critiquing FEP from an enactivist perspective.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Maxwell Ramstead provides background on enactivism emerging from autopoiesis, with a focus on embodied cognition and rejecting information processing/computational views of mind. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chris shares his journey from robotics into FEP, starting as a skeptic but becoming convinced it&amp;#39;s the right framework. He notes there are both &amp;quot;high road&amp;quot; and &amp;quot;low road&amp;quot; versions, ranging from embodied to more radically anti-representational stances. He doesn&amp;#39;t see a definitive fork between dynamical systems and information theory as the source of conflict. Rather, the notion of operational closure in enactivism seems to be the main sticking point.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The group explores definitional issues around structure/organization, boundaries, and operational closure. Maxwell argues the generative model in FEP captures organizational dependencies akin to operational closure. The Markov blanket formalism models structural interfaces.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We discuss the concept of goals in cognitive systems - Chris advocates an intentional stance perspective - using notions of goals/intentions if they help explain system dynamics. Goals emerge from beliefs about dynamical trajectories. Prof Friston provides an elegant explanation of how goal-directed behavior naturally falls out of the FEP mathematics in a particular &amp;quot;goldilocks&amp;quot; regime of system scale/dynamics. The conversation explores the idea that many systems simply act &amp;quot;as if&amp;quot; they have goals or models, without necessarily possessing explicit representations. This helps resolve tensions between enactivist and computational perspectives.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Throughout the dialogue, Maxwell presses philosophical points about the FEP abolishing what he perceives as false dichotomies in cognitive science such as internalism/externalism. He is critical of enactivists&amp;#39; commitment to bright line divides between subject areas.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. Karl Friston - Inventor of the free energy principle https://scholar.google.com/citations?user=q_4u0aoAAAAJ&lt;/p&gt;
&lt;p&gt;Prof. Chris Buckley - Professor of Neural Computation at Sussex University https://scholar.google.co.uk/citations?user=nWuZ0XcAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;Dr. Maxwell Ramstead - Director of Research at VERSES https://scholar.google.ca/citations?user=ILpGOMkAAAAJ&amp;amp;hl=fr&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We address critique in this paper: &lt;/p&gt;
&lt;p&gt;Laying down a forking path: Tensions between enaction and the free energy principle (Ezequiel A. Di Paolo, Evan Thompson, Randall D. Beere)&lt;/p&gt;
&lt;p&gt;https://philosophymindscience.org/index.php/phimisci/article/download/9187/8975&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Other refs:&lt;/p&gt;
&lt;p&gt;Multiscale integration: beyond internalism and externalism (Maxwell J D Ramstead) &lt;/p&gt;
&lt;p&gt;https://pubmed.ncbi.nlm.nih.gov/33627890/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST panel: Dr. Tim Scarfe and Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC (auto generated):
0:00 - Introduction
0:41 - Defining enactivism and its variants
6:58 - The source of the conflict between dynamical systems and information theory
8:56 - Operational closure in enactivism
10:03 - Goals and intentions
12:35 - The link between dynamical systems and information theory
15:02 - Path integrals and non-equilibrium dynamics
18:38 - Operational closure defined
21:52 - Structure vs. organization in enactivism
24:24 - Markov blankets as interfaces
28:48 - Operational closure in FEP
30:28 - Structure and organization again
31:08 - Dynamics vs. information theory
33:55 - Goals and intentions emerge in the FEP mathematics
36:58 - The Good Regulator Theorem
49:30 - enactivism and its relation to ecological psychology
52:00 - Goals, intentions and beliefs
55:21 - Boundaries and meaning
58:55 - Enactivism&amp;#39;s rejection of information theory
1:02:08 - Beliefs vs goals
1:05:06 - Ecological psychology and FEP
1:08:41 - The Good Regulator Theorem
1:18:38 - How goal-directed behavior emerges
1:23:13 - Ontological vs metaphysical boundaries
1:25:20 - Boundaries as maps
1:31:08 - Connections to the maximum entropy principle
1:33:45 - Relations to quantum and relational physics&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:34:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1693949914879-8ee4781ce295c.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[STEPHEN WOLFRAM 2.0 - Resolving the Mystery of the Second Law of Thermodynamics]]></title>
			<description><![CDATA[<p>Please check out Numerai - our sponsor @
http://numer.ai/mlst

Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

The Second Law: Resolving the Mystery of the Second Law of Thermodynamics
Buy Stephen&#39;s book here - https://tinyurl.com/2jj2t9wa

The Language Game: How Improvisation Created Language and Changed the World by Morten H. Christiansen and Nick Chater
Buy here: https://tinyurl.com/35bvs8be 

Stephen Wolfram starts by discussing the second law of thermodynamics - the idea that entropy, or disorder, tends to increase over time. He talks about how this law seems intuitively true, but has been difficult to prove. Wolfram outlines his decades-long quest to fully understand the second law, including failed early attempts to simulate particles mixing as a 12-year-old. He explains how irreversibility arises from the computational irreducibility of underlying physical processes coupled with our limited ability as observers to do the computations needed to &quot;decrypt&quot; the microscopic details.

The conversation then shifts to discussing language and how concepts allow us to communicate shared ideas between minds positioned in different parts of &quot;rule space.&quot; Wolfram talks about the successes and limitations of using large language models to generate Wolfram Language code from natural language prompts. He sees it as a useful tool for getting started programming, but one still needs human refinement.

The final part of the conversation focuses on AI safety and governance. Wolfram notes uncontrolled actuation is where things can go wrong with AI systems. He discusses whether AI agents could have intrinsic experiences and goals, how we might build trust networks between AIs, and that managing a system of many AIs may be easier than a single AI. Wolfram emphasizes the need for more philosophical depth in thinking about AI aims, and draws connections between potential solutions and his work on computational irreducibility and physics.

Show notes: https://docs.google.com/document/d/1hXNHtvv8KDR7PxCfMh9xOiDFhU3SVDW8ijyxeTq9LHo/edit?usp=sharing
Pod version: TBA

https://twitter.com/stephen_wolfram

TOC:
00:00:00 - Introduction
00:02:34 - Second law book
00:14:01 - Reversibility / entropy / observers / equivalence
00:34:22 - Concepts/language in the ruliad
00:49:04 - Comparison to free energy principle
00:53:58 - ChatGPT / Wolfram / Language
01:00:17 - AI risk

Panel: Dr. Tim Scarfe @ecsquendor / Dr. Keith Duggar @DoctorDuggar</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/STEPHEN-WOLFRAM-2-0---Resolving-the-Mystery-of-the-Second-Law-of-Thermodynamics-e2847qh</link>
			<guid isPermaLink="false">8f4228db-f7e6-4e3d-b61b-d2937193d415</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 15 Aug 2023 02:13:47 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/74636561/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-15%2F10889ed6-9b45-bf9b-712a-f509dddb0716.mp3" length="121126600" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please check out Numerai - our sponsor @
http://numer.ai/mlst

Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

The Second Law: Resolving the Mystery of the Second Law of Thermodynamics
Buy Stephen&amp;#39;s book here - https://tinyurl.com/2jj2t9wa

The Language Game: How Improvisation Created Language and Changed the World by Morten H. Christiansen and Nick Chater
Buy here: https://tinyurl.com/35bvs8be 

Stephen Wolfram starts by discussing the second law of thermodynamics - the idea that entropy, or disorder, tends to increase over time. He talks about how this law seems intuitively true, but has been difficult to prove. Wolfram outlines his decades-long quest to fully understand the second law, including failed early attempts to simulate particles mixing as a 12-year-old. He explains how irreversibility arises from the computational irreducibility of underlying physical processes coupled with our limited ability as observers to do the computations needed to &amp;quot;decrypt&amp;quot; the microscopic details.

The conversation then shifts to discussing language and how concepts allow us to communicate shared ideas between minds positioned in different parts of &amp;quot;rule space.&amp;quot; Wolfram talks about the successes and limitations of using large language models to generate Wolfram Language code from natural language prompts. He sees it as a useful tool for getting started programming, but one still needs human refinement.

The final part of the conversation focuses on AI safety and governance. Wolfram notes uncontrolled actuation is where things can go wrong with AI systems. He discusses whether AI agents could have intrinsic experiences and goals, how we might build trust networks between AIs, and that managing a system of many AIs may be easier than a single AI. Wolfram emphasizes the need for more philosophical depth in thinking about AI aims, and draws connections between potential solutions and his work on computational irreducibility and physics.

Show notes: https://docs.google.com/document/d/1hXNHtvv8KDR7PxCfMh9xOiDFhU3SVDW8ijyxeTq9LHo/edit?usp=sharing
Pod version: TBA

https://twitter.com/stephen_wolfram

TOC:
00:00:00 - Introduction
00:02:34 - Second law book
00:14:01 - Reversibility / entropy / observers / equivalence
00:34:22 - Concepts/language in the ruliad
00:49:04 - Comparison to free energy principle
00:53:58 - ChatGPT / Wolfram / Language
01:00:17 - AI risk

Panel: Dr. Tim Scarfe @ecsquendor / Dr. Keith Duggar @DoctorDuggar&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:24:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1692065521748-25f78f59a1e93.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>128</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Jürgen Schmidhuber - FATHER OF AI ON ITS DANGERS]]></title>
			<description><![CDATA[<p>Please check out Numerai - our sponsor @
http://numer.ai/mlst

Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

Professor Jürgen Schmidhuber, the father of artificial intelligence, joins us today. Schmidhuber discussed the history of machine learning, the current state of AI, and his career researching recursive self-improvement, artificial general intelligence and its risks. 

Schmidhuber pointed out the importance of studying the history of machine learning to properly assign credit for key breakthroughs. He discussed some of the earliest machine learning algorithms. He also highlighted the foundational work of Leibniz, who discovered the chain rule that enables training of deep neural networks, and the ancient Antikythera mechanism, the first known gear-based computer. 

Schmidhuber discussed limits to recursive self-improvement and artificial general intelligence, including physical constraints like the speed of light and what can be computed. He noted we have no evidence the human brain can do more than traditional computing. Schmidhuber sees humankind as a potential stepping stone to more advanced, spacefaring machine life which may have little interest in humanity. However, he believes commercial incentives point AGI development towards being beneficial and that open-source innovation can help to achieve &quot;AI for all&quot; symbolised by his company&#39;s motto &quot;AI∀&quot;.

Schmidhuber discussed approaches he believes will lead to more general AI, including meta-learning, reinforcement learning, building predictive world models, and curiosity-driven learning. His &quot;fast weight programming&quot; approach from the 1990s involved one network altering another network&#39;s connections. This was actually the first Transformer variant, now called an unnormalised linear Transformer. He also described the first GANs in 1990, to implement artificial curiosity.

Schmidhuber reflected on his career researching AI. He said his fondest memories were gaining insights that seemed to solve longstanding problems, though new challenges always arose: &quot;then for a brief moment it looks like the greatest thing since sliced bread and and then you get excited ... but then suddenly you realize, oh, it&#39;s still not finished. Something important is missing.” Since 1985 he has worked on systems that can recursively improve themselves, constrained only by the limits of physics and computability. He believes continual progress, shaped by both competition and collaboration, will lead to increasingly advanced AI. 

On AI Risk: Schmidhuber: &quot;To me it&#39;s indeed weird. Now there are all these letters coming out warning of the dangers of AI. And I think some of the guys who are writing these letters, they are just seeking attention because they know that AI dystopia are attracting more attention than documentaries about the benefits of AI in healthcare.&quot;

Schmidhuber believes we should be more concerned with existing threats like nuclear weapons than speculative risks from advanced AI. He said: &quot;As far as I can judge, all of this cannot be stopped but it can be channeled in a very natural way that is good for humankind...there is a tremendous bias towards good AI, meaning AI that is good for humans...I am much more worried about 60 year old technology that can wipe out civilization within two hours, without any AI.” </p>
<p>[this is truncated, read show notes]
</p>
<p>YT: https://youtu.be/q27XMPm5wg8</p>
<p>Show notes: https://docs.google.com/document/d/13-vIetOvhceZq5XZnELRbaazpQbxLbf5Yi7M25CixEE/edit?usp=sharing

Note: Interview was recorded 15th June 2023.
https://twitter.com/SchmidhuberAI

Panel: Dr. Tim Scarfe @ecsquendor / Dr. Keith Duggar @DoctorDuggar

Pod version: TBA

TOC: 
[00:00:00] Intro / Numerai 
[00:00:51] Show Kick Off 
[00:02:24] Credit Assignment in ML 
[00:12:51] XRisk 
[00:20:45] First Transformer variant of 1991
[00:47:20] Which Current Approaches are Good 
[00:52:42] Autonomy / Curiosity 
[00:58:42] GANs of 1990
[01:11:29] OpenAI, Moats, Legislation</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Jrgen-Schmidhuber---FATHER-OF-AI-ON-ITS-DANGERS-e2839av</link>
			<guid isPermaLink="false">a77e5c11-db67-4871-9535-c8a6ebf4cd71</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 14 Aug 2023 11:12:35 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/74605343/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-14%2F31c302ae-c16f-ca62-f807-dac6c80dc9d7.mp3" length="116726400" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please check out Numerai - our sponsor @
http://numer.ai/mlst

Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

Professor Jürgen Schmidhuber, the father of artificial intelligence, joins us today. Schmidhuber discussed the history of machine learning, the current state of AI, and his career researching recursive self-improvement, artificial general intelligence and its risks. 

Schmidhuber pointed out the importance of studying the history of machine learning to properly assign credit for key breakthroughs. He discussed some of the earliest machine learning algorithms. He also highlighted the foundational work of Leibniz, who discovered the chain rule that enables training of deep neural networks, and the ancient Antikythera mechanism, the first known gear-based computer. 

Schmidhuber discussed limits to recursive self-improvement and artificial general intelligence, including physical constraints like the speed of light and what can be computed. He noted we have no evidence the human brain can do more than traditional computing. Schmidhuber sees humankind as a potential stepping stone to more advanced, spacefaring machine life which may have little interest in humanity. However, he believes commercial incentives point AGI development towards being beneficial and that open-source innovation can help to achieve &amp;quot;AI for all&amp;quot; symbolised by his company&amp;#39;s motto &amp;quot;AI∀&amp;quot;.

Schmidhuber discussed approaches he believes will lead to more general AI, including meta-learning, reinforcement learning, building predictive world models, and curiosity-driven learning. His &amp;quot;fast weight programming&amp;quot; approach from the 1990s involved one network altering another network&amp;#39;s connections. This was actually the first Transformer variant, now called an unnormalised linear Transformer. He also described the first GANs in 1990, to implement artificial curiosity.

Schmidhuber reflected on his career researching AI. He said his fondest memories were gaining insights that seemed to solve longstanding problems, though new challenges always arose: &amp;quot;then for a brief moment it looks like the greatest thing since sliced bread and and then you get excited ... but then suddenly you realize, oh, it&amp;#39;s still not finished. Something important is missing.” Since 1985 he has worked on systems that can recursively improve themselves, constrained only by the limits of physics and computability. He believes continual progress, shaped by both competition and collaboration, will lead to increasingly advanced AI. 

On AI Risk: Schmidhuber: &amp;quot;To me it&amp;#39;s indeed weird. Now there are all these letters coming out warning of the dangers of AI. And I think some of the guys who are writing these letters, they are just seeking attention because they know that AI dystopia are attracting more attention than documentaries about the benefits of AI in healthcare.&amp;quot;

Schmidhuber believes we should be more concerned with existing threats like nuclear weapons than speculative risks from advanced AI. He said: &amp;quot;As far as I can judge, all of this cannot be stopped but it can be channeled in a very natural way that is good for humankind...there is a tremendous bias towards good AI, meaning AI that is good for humans...I am much more worried about 60 year old technology that can wipe out civilization within two hours, without any AI.” &lt;/p&gt;
&lt;p&gt;[this is truncated, read show notes]
&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/q27XMPm5wg8&lt;/p&gt;
&lt;p&gt;Show notes: https://docs.google.com/document/d/13-vIetOvhceZq5XZnELRbaazpQbxLbf5Yi7M25CixEE/edit?usp=sharing

Note: Interview was recorded 15th June 2023.
https://twitter.com/SchmidhuberAI

Panel: Dr. Tim Scarfe @ecsquendor / Dr. Keith Duggar @DoctorDuggar

Pod version: TBA

TOC: 
[00:00:00] Intro / Numerai 
[00:00:51] Show Kick Off 
[00:02:24] Credit Assignment in ML 
[00:12:51] XRisk 
[00:20:45] First Transformer variant of 1991
[00:47:20] Which Current Approaches are Good 
[00:52:42] Autonomy / Curiosity 
[00:58:42] GANs of 1990
[01:11:29] OpenAI, Moats, Legislation&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:21:03</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1692011439356-7a3f54b2456e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>127</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Can We Develop Truly Beneficial AI? George Hotz and Connor Leahy]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB</p>
<p><br></p>
<p>George Hotz and Connor Leahy discuss the crucial challenge of developing beneficial AI that is aligned with human values. Hotz believes truly aligned AI is impossible, while Leahy argues it&#39;s a solvable technical challenge.<br>Hotz contends that AI will inevitably pursue power, but distributing AI widely would prevent any single AI from dominating. He advocates open-sourcing AI developments to democratize access. Leahy counters that alignment is necessary to ensure AIs respect human values. Without solving alignment, general AI could ignore or harm humans.<br>They discuss whether AI&#39;s tendency to seek power stems from optimization pressure or human-instilled goals. Leahy argues goal-seeking behavior naturally emerges while Hotz believes it reflects human values. Though agreeing on AI&#39;s potential dangers, they differ on solutions. Hotz favors accelerating AI progress and distributing capabilities while Leahy wants safeguards put in place.<br>While acknowledging risks like AI-enabled weapons, they debate whether broad access or restrictions better manage threats. Leahy suggests limiting dangerous knowledge, but Hotz insists openness checks government overreach. They concur that coordination and balance of power are key to navigating the AI revolution. Both eagerly anticipate seeing whose ideas prevail as AI progresses.</p>
<p>
Transcript and notes: https://docs.google.com/document/d/1smkmBY7YqcrhejdbqJOoZHq-59LZVwu-DNdM57IgFcU/edit?usp=sharing
</p>
<p>Note: this is not a normal episode i.e. the hosts are not part of the debate (and for the record don&#39;t agree with Connor or George).</p>
<p>
TOC:
[00:00:00] Introduction to George Hotz and Connor Leahy
[00:03:10] George Hotz&#39;s Opening Statement: Intelligence and Power
[00:08:50] Connor Leahy&#39;s Opening Statement: Technical Problem of Alignment and Coordination
[00:15:18] George Hotz&#39;s Response: Nature of Cooperation and Individual Sovereignty
[00:17:32] Discussion on individual sovereignty and defense
[00:18:45] Debate on living conditions in America versus Somalia
[00:21:57] Talk on the nature of freedom and the aesthetics of life
[00:24:02] Discussion on the implications of coordination and conflict in politics
[00:33:41] Views on the speed of AI development / hard takeoff
[00:35:17] Discussion on potential dangers of AI
[00:36:44] Discussion on the effectiveness of current AI
[00:40:59] Exploration of potential risks in technology
[00:45:01] Discussion on memetic mutation risk
[00:52:36] AI alignment and exploitability
[00:53:13] Superintelligent AIs and the assumption of good intentions
[00:54:52] Humanity’s inconsistency and AI alignment
[00:57:57] Stability of the world and the impact of superintelligent AIs
[01:02:30] Personal utopia and the limitations of AI alignment
[01:05:10] Proposed regulation on limiting the total number of flops
[01:06:20] Having access to a powerful AI system
[01:18:00] Power dynamics and coordination issues with AI
[01:25:44] Humans vs AI in Optimization
[01:27:05] The Impact of AI&#39;s Power Seeking Behavior
[01:29:32] A Debate on the Future of AI</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Can-We-Develop-Truly-Beneficial-AI--George-Hotz-and-Connor-Leahy-e27nhtg</link>
			<guid isPermaLink="false">dd18d5a1-1559-416a-bc25-7ed0ce2a7f0e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 04 Aug 2023 00:10:20 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/74220912/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-7-4%2Fca4227d5-5ea7-4432-36fc-1e9fbda9549d.mp3" length="129580552" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;George Hotz and Connor Leahy discuss the crucial challenge of developing beneficial AI that is aligned with human values. Hotz believes truly aligned AI is impossible, while Leahy argues it&amp;#39;s a solvable technical challenge.&lt;br&gt;Hotz contends that AI will inevitably pursue power, but distributing AI widely would prevent any single AI from dominating. He advocates open-sourcing AI developments to democratize access. Leahy counters that alignment is necessary to ensure AIs respect human values. Without solving alignment, general AI could ignore or harm humans.&lt;br&gt;They discuss whether AI&amp;#39;s tendency to seek power stems from optimization pressure or human-instilled goals. Leahy argues goal-seeking behavior naturally emerges while Hotz believes it reflects human values. Though agreeing on AI&amp;#39;s potential dangers, they differ on solutions. Hotz favors accelerating AI progress and distributing capabilities while Leahy wants safeguards put in place.&lt;br&gt;While acknowledging risks like AI-enabled weapons, they debate whether broad access or restrictions better manage threats. Leahy suggests limiting dangerous knowledge, but Hotz insists openness checks government overreach. They concur that coordination and balance of power are key to navigating the AI revolution. Both eagerly anticipate seeing whose ideas prevail as AI progresses.&lt;/p&gt;
&lt;p&gt;
Transcript and notes: https://docs.google.com/document/d/1smkmBY7YqcrhejdbqJOoZHq-59LZVwu-DNdM57IgFcU/edit?usp=sharing
&lt;/p&gt;
&lt;p&gt;Note: this is not a normal episode i.e. the hosts are not part of the debate (and for the record don&amp;#39;t agree with Connor or George).&lt;/p&gt;
&lt;p&gt;
TOC:
[00:00:00] Introduction to George Hotz and Connor Leahy
[00:03:10] George Hotz&amp;#39;s Opening Statement: Intelligence and Power
[00:08:50] Connor Leahy&amp;#39;s Opening Statement: Technical Problem of Alignment and Coordination
[00:15:18] George Hotz&amp;#39;s Response: Nature of Cooperation and Individual Sovereignty
[00:17:32] Discussion on individual sovereignty and defense
[00:18:45] Debate on living conditions in America versus Somalia
[00:21:57] Talk on the nature of freedom and the aesthetics of life
[00:24:02] Discussion on the implications of coordination and conflict in politics
[00:33:41] Views on the speed of AI development / hard takeoff
[00:35:17] Discussion on potential dangers of AI
[00:36:44] Discussion on the effectiveness of current AI
[00:40:59] Exploration of potential risks in technology
[00:45:01] Discussion on memetic mutation risk
[00:52:36] AI alignment and exploitability
[00:53:13] Superintelligent AIs and the assumption of good intentions
[00:54:52] Humanity’s inconsistency and AI alignment
[00:57:57] Stability of the world and the impact of superintelligent AIs
[01:02:30] Personal utopia and the limitations of AI alignment
[01:05:10] Proposed regulation on limiting the total number of flops
[01:06:20] Having access to a powerful AI system
[01:18:00] Power dynamics and coordination issues with AI
[01:25:44] Humans vs AI in Optimization
[01:27:05] The Impact of AI&amp;#39;s Power Seeking Behavior
[01:29:32] A Debate on the Future of AI&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>true</itunes:explicit>
			<itunes:duration>01:29:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1691107789227-41ddc05aac9ae.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>126</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Dr. MAXWELL RAMSTEAD - The Physics of Survival]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

Join us for a fascinating discussion of the free energy principle with Dr. Maxwell Ramsted, a leading thinker exploring the intersection of math, physics, and philosophy and Director of Research at VERSES. The FEP was proposed by renowned neuroscientist Karl Friston, this principle offers a unifying theory explaining how systems maintain order and their identity.

The free energy principle inverts traditional survival logic. Rather than asking what behaviors promote survival, it queries - given things exist, what must they do? The answer: minimizing free energy, or &quot;surprise.&quot; Systems persist by constantly ensuring their internal states match anticipated states based on a model of the world. Failure to minimize surprise leads to chaos as systems dissolve into disorder.

Thus, the free energy principle elucidates why lifeforms relentlessly model and predict their surroundings. It is an existential imperative counterbalancing entropy. Essentially, this principle describes the mind&#39;s pursuit of harmony between expectations and reality. Its relevance spans from cells to societies, underlying order wherever longevity is found.

Our discussion explores the technical details and philosophical implications of this paradigm-shifting theory. How does it further our understanding of cognition and intelligence? What insights does it offer about the fundamental patterns and properties of existence? Can it precipitate breakthroughs in disciplines like neuroscience and artificial intelligence?

Dr. Ramstead completed his Ph.D. at McGill University in Montreal, Canada in 2019, with frequent research visits to UCL in London, under the supervision of the world’s most cited neuroscientist, Professor Karl Friston (UCL).</p>
<p><br></p>
<p>YT version: https://youtu.be/8qb28P7ksyE

https://scholar.google.ca/citations?user=ILpGOMkAAAAJ&amp;hl=frhttps://spatialwebfoundation.org/team/maxwell-ramstead/https://www.linkedin.com/in/maxwell-ramstead-43a1991b7/https://twitter.com/mjdramstead

VERSES AI: https://www.verses.ai/

Intro: Tim Scarfe (Ph.D)
Interviewer: Keith Duggar (Ph.D MIT)

TOC:
0:00:00 - Tim Intro
0:08:10 - Intro and philosophy
0:14:26 - Intro to Maxwell
0:18:00 - FEP
0:29:08 - Markov Blankets
0:51:15 - Verses AI / Applications of FEP
1:05:55 - Potential issues with deploying FEP
1:10:50 - Shared knowledge graphs
1:14:29 - XRisk / Ethics
1:24:57 - Strength of Verses
1:28:30 - Misconceptions about FEP, Physics vs philosophy/criticism
1:44:41 - Emergence / consciousness

References:
Principia Mathematica
https://www.abebooks.co.uk/servlet/BookDetailsPL?bi=30567249049

Andy Clark&#39;s paper &quot;Whatever Next? Predictive Brains, Situated Agents, and the Future of Cognitive Science&quot; (Behavioral and Brain Sciences, 2013)
https://pubmed.ncbi.nlm.nih.gov/23663408/

&quot;Math Does Not Represent&quot; by Erik Curiel
https://www.youtube.com/watch?v=aA_T20HAzyY

A free energy principle for generic quantum systems (Chris Fields et al)
https://arxiv.org/pdf/2112.15242.pdf

Designing explainable artificial intelligence with active inference
https://arxiv.org/abs/2306.04025

Am I Self-Conscious? (Friston)
https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full

The Meta-Problem of Consciousness
https://philarchive.org/archive/CHATMO-32v1

The Map-Territory Fallacy Fallacy
https://arxiv.org/abs/2208.06924

A Technical Critique of Some Parts of the Free Energy Principle - Martin Biehl et al
https://arxiv.org/abs/2001.06408

WEAK MARKOV BLANKETS IN HIGH-DIMENSIONAL, SPARSELY-COUPLED RANDOM DYNAMICAL SYSTEMS - DALTON A R SAKTHIVADIVEL
https://arxiv.org/pdf/2207.07620.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Dr--MAXWELL-RAMSTEAD---The-Physics-of-Survival-e2704fa</link>
			<guid isPermaLink="false">4b5a0b1e-68fd-44b4-bb2f-deddc039604b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 16 Jul 2023 00:23:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/73453482/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-6-16%2F54cf5039-4acd-6a69-a628-c2c18ba8f1cb.mp3" length="181222272" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

Join us for a fascinating discussion of the free energy principle with Dr. Maxwell Ramsted, a leading thinker exploring the intersection of math, physics, and philosophy and Director of Research at VERSES. The FEP was proposed by renowned neuroscientist Karl Friston, this principle offers a unifying theory explaining how systems maintain order and their identity.

The free energy principle inverts traditional survival logic. Rather than asking what behaviors promote survival, it queries - given things exist, what must they do? The answer: minimizing free energy, or &amp;quot;surprise.&amp;quot; Systems persist by constantly ensuring their internal states match anticipated states based on a model of the world. Failure to minimize surprise leads to chaos as systems dissolve into disorder.

Thus, the free energy principle elucidates why lifeforms relentlessly model and predict their surroundings. It is an existential imperative counterbalancing entropy. Essentially, this principle describes the mind&amp;#39;s pursuit of harmony between expectations and reality. Its relevance spans from cells to societies, underlying order wherever longevity is found.

Our discussion explores the technical details and philosophical implications of this paradigm-shifting theory. How does it further our understanding of cognition and intelligence? What insights does it offer about the fundamental patterns and properties of existence? Can it precipitate breakthroughs in disciplines like neuroscience and artificial intelligence?

Dr. Ramstead completed his Ph.D. at McGill University in Montreal, Canada in 2019, with frequent research visits to UCL in London, under the supervision of the world’s most cited neuroscientist, Professor Karl Friston (UCL).&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/8qb28P7ksyE

https://scholar.google.ca/citations?user=ILpGOMkAAAAJ&amp;amp;hl=frhttps://spatialwebfoundation.org/team/maxwell-ramstead/https://www.linkedin.com/in/maxwell-ramstead-43a1991b7/https://twitter.com/mjdramstead

VERSES AI: https://www.verses.ai/

Intro: Tim Scarfe (Ph.D)
Interviewer: Keith Duggar (Ph.D MIT)

TOC:
0:00:00 - Tim Intro
0:08:10 - Intro and philosophy
0:14:26 - Intro to Maxwell
0:18:00 - FEP
0:29:08 - Markov Blankets
0:51:15 - Verses AI / Applications of FEP
1:05:55 - Potential issues with deploying FEP
1:10:50 - Shared knowledge graphs
1:14:29 - XRisk / Ethics
1:24:57 - Strength of Verses
1:28:30 - Misconceptions about FEP, Physics vs philosophy/criticism
1:44:41 - Emergence / consciousness

References:
Principia Mathematica
https://www.abebooks.co.uk/servlet/BookDetailsPL?bi=30567249049

Andy Clark&amp;#39;s paper &amp;quot;Whatever Next? Predictive Brains, Situated Agents, and the Future of Cognitive Science&amp;quot; (Behavioral and Brain Sciences, 2013)
https://pubmed.ncbi.nlm.nih.gov/23663408/

&amp;quot;Math Does Not Represent&amp;quot; by Erik Curiel
https://www.youtube.com/watch?v=aA_T20HAzyY

A free energy principle for generic quantum systems (Chris Fields et al)
https://arxiv.org/pdf/2112.15242.pdf

Designing explainable artificial intelligence with active inference
https://arxiv.org/abs/2306.04025

Am I Self-Conscious? (Friston)
https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full

The Meta-Problem of Consciousness
https://philarchive.org/archive/CHATMO-32v1

The Map-Territory Fallacy Fallacy
https://arxiv.org/abs/2208.06924

A Technical Critique of Some Parts of the Free Energy Principle - Martin Biehl et al
https://arxiv.org/abs/2001.06408

WEAK MARKOV BLANKETS IN HIGH-DIMENSIONAL, SPARSELY-COUPLED RANDOM DYNAMICAL SYSTEMS - DALTON A R SAKTHIVADIVEL
https://arxiv.org/pdf/2207.07620.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:05:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1689466958634-70db8ec6a2472.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>125</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[MUNK DEBATE ON AI (COMMENTARY) [DAVID FOSTER]]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p><br></p>
<p>The discussion between Tim Scarfe and David Foster provided an in-depth critique of the arguments made by panelists at the Munk AI Debate on whether artificial intelligence poses an existential threat to humanity. While the panelists made thought-provoking points, Scarfe and Foster found their arguments largely speculative, lacking crucial details and evidence to support claims of an impending existential threat.</p>
<p><br></p>
<p>Scarfe and Foster strongly disagreed with Max Tegmark’s position that AI has an unparalleled “blast radius” that could lead to human extinction. Tegmark failed to provide a credible mechanism for how this scenario would unfold in reality. His arguments relied more on speculation about advanced future technologies than on present capabilities and trends. As Foster argued, we cannot conclude AI poses a threat based on speculation alone. Evidence is needed to ground discussions of existential risks in science rather than science fiction fantasies or doomsday scenarios.</p>
<p><br></p>
<p>They found Yann LeCun’s statements too broad and high-level, critiquing him for not providing sufficiently strong arguments or specifics to back his position. While LeCun aptly noted AI remains narrow in scope and far from achieving human-level intelligence, his arguments lacked crucial details on current limitations and why we should not fear superintelligence emerging in the near future. As Scarfe argued, without these details the discussion descended into “philosophy” rather than focusing on evidence and data.</p>
<p><br></p>
<p>Scarfe and Foster also took issue with Yoshua Bengio’s unsubstantiated speculation that machines would necessarily develop a desire for self-preservation that threatens humanity. There is no evidence today’s AI systems are developing human-like general intelligence or desires, let alone that these attributes would manifest in ways dangerous to humans. The question is not whether machines will eventually surpass human intelligence, but how and when this might realistically unfold based on present technological capabilities. Bengio’s arguments relied more on speculation about advanced future technologies than on evidence from current systems and research.</p>
<p><br></p>
<p>In contrast, they strongly agreed with Melanie Mitchell’s view that scenarios of malevolent or misguided superintelligence are speculation, not backed by evidence from AI as it exists today. Claims of an impending “existential threat” from AI are overblown, harmful to progress, and inspire undue fear of technology rather than consideration of its benefits. Mitchell sensibly argued discussions of risks from emerging technologies must be grounded in science and data, not speculation, if we are to make balanced policy and development decisions.</p>
<p><br></p>
<p>Overall, while the debate raised thought-provoking questions about advanced technologies that could eventually transform our world, none of the speakers made a credible evidence-based case that today’s AI poses an existential threat. Scarfe and Foster argued the debate failed to discuss concrete details about current capabilities and limitations of technologies like language models, which remain narrow in scope. General human-level AI is still missing many components, including physical embodiment, emotions, and the &quot;common sense&quot; reasoning that underlies human thinking. Claims of existential threats require extraordinary evidence to justify policy or research restrictions, not speculation. By discussing possibilities rather than probabilities grounded in evidence, the debate failed to substantively advance our thinking on risks from AI and its plausible development in the coming decades. </p>
<p><br></p>
<p>David&#39;s new podcast: https://podcasts.apple.com/us/podcast/the-ai-canvas/id1692538973</p>
<p>Generative AI book: https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/MUNK-DEBATE-ON-AI-COMMENTARY-DAVID-FOSTER-e26f787</link>
			<guid isPermaLink="false">1e15c652-13e2-4d65-b49f-47d796b78f9d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 02 Jul 2023 18:02:15 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/72899271/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-6-2%2F5b72d680-bae9-0b56-44f7-c49750ce828f.mp3" length="184672648" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The discussion between Tim Scarfe and David Foster provided an in-depth critique of the arguments made by panelists at the Munk AI Debate on whether artificial intelligence poses an existential threat to humanity. While the panelists made thought-provoking points, Scarfe and Foster found their arguments largely speculative, lacking crucial details and evidence to support claims of an impending existential threat.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Scarfe and Foster strongly disagreed with Max Tegmark’s position that AI has an unparalleled “blast radius” that could lead to human extinction. Tegmark failed to provide a credible mechanism for how this scenario would unfold in reality. His arguments relied more on speculation about advanced future technologies than on present capabilities and trends. As Foster argued, we cannot conclude AI poses a threat based on speculation alone. Evidence is needed to ground discussions of existential risks in science rather than science fiction fantasies or doomsday scenarios.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;They found Yann LeCun’s statements too broad and high-level, critiquing him for not providing sufficiently strong arguments or specifics to back his position. While LeCun aptly noted AI remains narrow in scope and far from achieving human-level intelligence, his arguments lacked crucial details on current limitations and why we should not fear superintelligence emerging in the near future. As Scarfe argued, without these details the discussion descended into “philosophy” rather than focusing on evidence and data.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Scarfe and Foster also took issue with Yoshua Bengio’s unsubstantiated speculation that machines would necessarily develop a desire for self-preservation that threatens humanity. There is no evidence today’s AI systems are developing human-like general intelligence or desires, let alone that these attributes would manifest in ways dangerous to humans. The question is not whether machines will eventually surpass human intelligence, but how and when this might realistically unfold based on present technological capabilities. Bengio’s arguments relied more on speculation about advanced future technologies than on evidence from current systems and research.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In contrast, they strongly agreed with Melanie Mitchell’s view that scenarios of malevolent or misguided superintelligence are speculation, not backed by evidence from AI as it exists today. Claims of an impending “existential threat” from AI are overblown, harmful to progress, and inspire undue fear of technology rather than consideration of its benefits. Mitchell sensibly argued discussions of risks from emerging technologies must be grounded in science and data, not speculation, if we are to make balanced policy and development decisions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Overall, while the debate raised thought-provoking questions about advanced technologies that could eventually transform our world, none of the speakers made a credible evidence-based case that today’s AI poses an existential threat. Scarfe and Foster argued the debate failed to discuss concrete details about current capabilities and limitations of technologies like language models, which remain narrow in scope. General human-level AI is still missing many components, including physical embodiment, emotions, and the &amp;quot;common sense&amp;quot; reasoning that underlies human thinking. Claims of existential threats require extraordinary evidence to justify policy or research restrictions, not speculation. By discussing possibilities rather than probabilities grounded in evidence, the debate failed to substantively advance our thinking on risks from AI and its plausible development in the coming decades. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;David&amp;#39;s new podcast: https://podcasts.apple.com/us/podcast/the-ai-canvas/id1692538973&lt;/p&gt;
&lt;p&gt;Generative AI book: https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:08:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1688320777337-06e52bf986cac.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>124</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[[SPONSORED] The Digitized Self: AI, Identity and the Human Psyche (YouAi)]]></title>
			<description><![CDATA[<p>Sponsored Episode - YouAi

What if an AI truly knew you—your thoughts, values, aptitudes, and dreams? An AI that could enhance your life in profound ways by amplifying your strengths, augmenting your weaknesses, and connecting you with like-minded souls. That is the vision of YouAi.

YouAi founder Dmitri Shapiro believes digitizing our inner lives could unlock tremendous benefits. But mapping the human psyche also poses deep questions. As technology mediates our self-understanding, what risks rendering our minds in bits and algorithms? Could we gain a new means of flourishing or lose something intangible?

There are no easy answers, but YouAi offers a vision balanced by hard thinking. Shapiro discussed YouAi&#39;s app, which builds personalized AI assistants by learning how individuals think through interactive questions. As people share, YouAi develops a multidimensional model of their mind. Users get a tailored feed of prompts to continue engaging and teaching their AI.

YouAi&#39;s vision provides a glimpse into a future that could unsettle or fulfill our hopes. As technology mediates understanding ourselves and others, will we risk losing what makes us human or find new means of flourishing? YouAI believes that together, we can build a future where our minds contain infinite potential—and their technology helps unlock it. But we must proceed thoughtfully, upholding human dignity above all else. Our minds shape who we are. And who we can become.Digitise your mind today:
YouAi - https://YouAi.aiMIndStudio – https://YouAi.ai/mindstudioYouAi Mind Indexer - https://YouAi.ai/trainJoin the MLST discord and register for the YouAi event on July 13th: https://discord.gg/ESrGqhf5CB

TOC:
0:00:00 - Introduction to Mind Digitization
0:09:31 - The YouAi Platform and Personal Applications
0:27:54 - The Potential of Group Alignment
0:30:28 - Applications in Human-to-Human Communication
0:35:43 - Applications in Interfacing with Digital Technology
0:43:41 - Introduction to the Project
0:44:51 - Brain digitization and mind vs. brain
0:49:55 - The Extended Mind and Neurofeedback
0:54:16 - Personalized Learning and the Future of Education
1:02:19 - Privacy and Data Security
1:14:20 - Ethical Considerations of Digitizing the Mind
1:19:49 - The Metaverse and the Future of Digital Identity
1:25:17 - Digital Immortality and Legacy
1:29:09 - The Nature of Consciousness
1:34:11 - Digitization of the Mind
1:35:06 - Potential Inequality in a Digital World
1:38:00 - The Role of Technology in Equalizing or Democratizing Society
1:40:51 - The Future of the Startup and Community Involvement</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/SPONSORED-The-Digitized-Self-AI--Identity-and-the-Human-Psyche-YouAi-e26b90f</link>
			<guid isPermaLink="false">f18e52ad-7c64-49e0-b824-55d5b4a62785</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 29 Jun 2023 08:25:27 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/72769999/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-29%2F4ccdedce-34a4-29f5-0a6a-2b014c76ef99.mp3" length="152641152" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Sponsored Episode - YouAi

What if an AI truly knew you—your thoughts, values, aptitudes, and dreams? An AI that could enhance your life in profound ways by amplifying your strengths, augmenting your weaknesses, and connecting you with like-minded souls. That is the vision of YouAi.

YouAi founder Dmitri Shapiro believes digitizing our inner lives could unlock tremendous benefits. But mapping the human psyche also poses deep questions. As technology mediates our self-understanding, what risks rendering our minds in bits and algorithms? Could we gain a new means of flourishing or lose something intangible?

There are no easy answers, but YouAi offers a vision balanced by hard thinking. Shapiro discussed YouAi&amp;#39;s app, which builds personalized AI assistants by learning how individuals think through interactive questions. As people share, YouAi develops a multidimensional model of their mind. Users get a tailored feed of prompts to continue engaging and teaching their AI.

YouAi&amp;#39;s vision provides a glimpse into a future that could unsettle or fulfill our hopes. As technology mediates understanding ourselves and others, will we risk losing what makes us human or find new means of flourishing? YouAI believes that together, we can build a future where our minds contain infinite potential—and their technology helps unlock it. But we must proceed thoughtfully, upholding human dignity above all else. Our minds shape who we are. And who we can become.Digitise your mind today:
YouAi - https://YouAi.aiMIndStudio – https://YouAi.ai/mindstudioYouAi Mind Indexer - https://YouAi.ai/trainJoin the MLST discord and register for the YouAi event on July 13th: https://discord.gg/ESrGqhf5CB

TOC:
0:00:00 - Introduction to Mind Digitization
0:09:31 - The YouAi Platform and Personal Applications
0:27:54 - The Potential of Group Alignment
0:30:28 - Applications in Human-to-Human Communication
0:35:43 - Applications in Interfacing with Digital Technology
0:43:41 - Introduction to the Project
0:44:51 - Brain digitization and mind vs. brain
0:49:55 - The Extended Mind and Neurofeedback
0:54:16 - Personalized Learning and the Future of Education
1:02:19 - Privacy and Data Security
1:14:20 - Ethical Considerations of Digitizing the Mind
1:19:49 - The Metaverse and the Future of Digital Identity
1:25:17 - Digital Immortality and Legacy
1:29:09 - The Nature of Consciousness
1:34:11 - Digitization of the Mind
1:35:06 - Potential Inequality in a Digital World
1:38:00 - The Role of Technology in Equalizing or Democratizing Society
1:40:51 - The Future of the Startup and Community Involvement&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:46:00</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1688027069040-9760187f3dbc7.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>123</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Joscha Bach and Connor Leahy on AI risk]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5
Twitter: https://twitter.com/MLStreetTalk

The first 10 mins of audio from Joscha isn&#39;t great, it improves after.</p>
<p>
Transcript and longer summary: https://docs.google.com/document/d/1TUJhlSVbrHf2vWoe6p7xL5tlTK_BGZ140QqqTudF8UI/edit?usp=sharing

Dr. Joscha Bach argued that general intelligence emerges from civilization, not individuals. Given our biological constraints, humans cannot achieve a high level of general intelligence on our own. Bach believes AGI may become integrated into all parts of the world, including human minds and bodies. He thinks a future where humans and AGI harmoniously coexist is possible if we develop a shared purpose and incentive to align. However, Bach is uncertain about how AI progress will unfold or which scenarios are most likely.

Bach argued that global control and regulation of AI is unrealistic. While regulation may address some concerns, it cannot stop continued progress in AI. He believes individuals determine their own values, so &quot;human values&quot; cannot be formally specified and aligned across humanity. For Bach, the possibility of building beneficial AGI is exciting but much work is still needed to ensure a positive outcome.

Connor Leahy believes we have more control over the future than the default outcome might suggest. With sufficient time and effort, humanity could develop the technology and coordination to build a beneficial AGI. However, the default outcome likely leads to an undesirable scenario if we do not actively work to build a better future. Leahy thinks finding values and priorities most humans endorse could help align AI, even if individuals disagree on some values.

Leahy argued a future where humans and AGI harmoniously coexist is ideal but will require substantial work to achieve. While regulation faces challenges, it remains worth exploring. Leahy believes limits to progress in AI exist but we are unlikely to reach them before humanity is at risk. He worries even modestly superhuman intelligence could disrupt the status quo if misaligned with human values and priorities.

Overall, Bach and Leahy expressed optimism about the possibility of building beneficial AGI but believe we must address risks and challenges proactively. They agreed substantial uncertainty remains around how AI will progress and what scenarios are most plausible. But developing a shared purpose between humans and AI, improving coordination and control, and finding human values to help guide progress could all improve the odds of a beneficial outcome. With openness to new ideas and willingness to consider multiple perspectives, continued discussions like this one could help ensure the future of AI is one that benefits and inspires humanity.

TOC:
00:00:00 - Introduction and Background
00:02:54 - Different Perspectives on AGI
00:13:59 - The Importance of AGI
00:23:24 - Existential Risks and the Future of Humanity
00:36:21 - Coherence and Coordination in Society
00:40:53 - Possibilities and Future of AGI
00:44:08 - Coherence and alignment
01:08:32 - The role of values in AI alignment
01:18:33 - The future of AGI and merging with AI
01:22:14 - The limits of AI alignment
01:23:06 - The scalability of intelligence
01:26:15 - Closing statements and future prospects</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Joscha-Bach-and-Connor-Leahy-on-AI-risk-e25ukc5</link>
			<guid isPermaLink="false">c3a0a6b0-6d7a-4e3d-bc97-5ccb6d8d8fb2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 20 Jun 2023 01:14:46 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/72355653/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-20%2F81fd6c77-364d-2af5-b47e-98f121101d8a.mp3" length="87808840" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5
Twitter: https://twitter.com/MLStreetTalk

The first 10 mins of audio from Joscha isn&amp;#39;t great, it improves after.&lt;/p&gt;
&lt;p&gt;
Transcript and longer summary: https://docs.google.com/document/d/1TUJhlSVbrHf2vWoe6p7xL5tlTK_BGZ140QqqTudF8UI/edit?usp=sharing

Dr. Joscha Bach argued that general intelligence emerges from civilization, not individuals. Given our biological constraints, humans cannot achieve a high level of general intelligence on our own. Bach believes AGI may become integrated into all parts of the world, including human minds and bodies. He thinks a future where humans and AGI harmoniously coexist is possible if we develop a shared purpose and incentive to align. However, Bach is uncertain about how AI progress will unfold or which scenarios are most likely.

Bach argued that global control and regulation of AI is unrealistic. While regulation may address some concerns, it cannot stop continued progress in AI. He believes individuals determine their own values, so &amp;quot;human values&amp;quot; cannot be formally specified and aligned across humanity. For Bach, the possibility of building beneficial AGI is exciting but much work is still needed to ensure a positive outcome.

Connor Leahy believes we have more control over the future than the default outcome might suggest. With sufficient time and effort, humanity could develop the technology and coordination to build a beneficial AGI. However, the default outcome likely leads to an undesirable scenario if we do not actively work to build a better future. Leahy thinks finding values and priorities most humans endorse could help align AI, even if individuals disagree on some values.

Leahy argued a future where humans and AGI harmoniously coexist is ideal but will require substantial work to achieve. While regulation faces challenges, it remains worth exploring. Leahy believes limits to progress in AI exist but we are unlikely to reach them before humanity is at risk. He worries even modestly superhuman intelligence could disrupt the status quo if misaligned with human values and priorities.

Overall, Bach and Leahy expressed optimism about the possibility of building beneficial AGI but believe we must address risks and challenges proactively. They agreed substantial uncertainty remains around how AI will progress and what scenarios are most plausible. But developing a shared purpose between humans and AI, improving coordination and control, and finding human values to help guide progress could all improve the odds of a beneficial outcome. With openness to new ideas and willingness to consider multiple perspectives, continued discussions like this one could help ensure the future of AI is one that benefits and inspires humanity.

TOC:
00:00:00 - Introduction and Background
00:02:54 - Different Perspectives on AGI
00:13:59 - The Importance of AGI
00:23:24 - Existential Risks and the Future of Humanity
00:36:21 - Coherence and Coordination in Society
00:40:53 - Possibilities and Future of AGI
00:44:08 - Coherence and alignment
01:08:32 - The role of values in AI alignment
01:18:33 - The future of AGI and merging with AI
01:22:14 - The limits of AI alignment
01:23:06 - The scalability of intelligence
01:26:15 - Closing statements and future prospects&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:31:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>122</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Neel Nanda - Mechanistic Interpretability ]]></title>
			<description><![CDATA[<p>In this wide-ranging conversation, Tim Scarfe interviews Neel Nanda, a researcher at DeepMind working on mechanistic interpretability, which aims to understand the algorithms and representations learned by machine learning models. Neel discusses how models can represent their thoughts using motifs, circuits, and linear directional features which are often communicated via a &quot;residual stream&quot;, an information highway models use to pass information between layers.</p>
<p>Neel argues that &quot;superposition&quot;, the ability for models to represent more features than they have neurons, is one of the biggest open problems in interpretability. This is because superposition thwarts our ability to understand models by decomposing them into individual units of analysis. Despite this, Neel remains optimistic that ambitious interpretability is possible, citing examples like his work reverse engineering how models do modular addition. However, Neel notes we must start small, build rigorous foundations, and not assume our theoretical frameworks perfectly match reality.</p>
<p>The conversation turns to whether models can have goals or agency, with Neel arguing they likely can based on heuristics like models executing long term plans towards some objective. However, we currently lack techniques to build models with specific goals, meaning any goals would likely be learned or emergent. Neel highlights how induction heads, circuits models use to track long range dependencies, seem crucial for phenomena like in-context learning to emerge.</p>
<p>On the existential risks from AI, Neel believes we should avoid overly confident claims that models will or will not be dangerous, as we do not understand them enough to make confident theoretical assertions. However, models could pose risks through being misused, having undesirable emergent properties, or being imperfectly aligned. Neel argues we must pursue rigorous empirical work to better understand and ensure model safety, avoid &quot;philosophizing&quot; about definitions of intelligence, and focus on ensuring researchers have standards for what it means to decide a system is &quot;safe&quot; before deploying it. Overall, a thoughtful conversation on one of the most important issues of our time.</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst </p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Twitter: https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Neel Nanda: https://www.neelnanda.io/</p>
<p><br></p>
<p>TOC</p>
<p>[00:00:00] Introduction and Neel Nanda&#39;s Interests (walk and talk)</p>
<p>[00:03:15] Mechanistic Interpretability: Reverse Engineering Neural Networks</p>
<p>[00:13:23] Discord questions</p>
<p>[00:21:16] Main interview kick-off in studio</p>
<p>[00:49:26] Grokking and Sudden Generalization</p>
<p>[00:53:18] The Debate on Systematicity and Compositionality</p>
<p>[01:19:16] How do ML models represent their thoughts</p>
<p>[01:25:51] Do Large Language Models Learn World Models?</p>
<p>[01:53:36] Superposition and Interference in Language Models</p>
<p>[02:43:15] Transformers discussion</p>
<p>[02:49:49] Emergence and In-Context Learning</p>
<p>[03:20:02] Superintelligence/XRisk discussion</p>
<p><br></p>
<p>Transcript: https://docs.google.com/document/d/1FK1OepdJMrqpFK-_1Q3LQN6QLyLBvBwWW_5z8WrS1RI/edit?usp=sharing</p>
<p>Refs: https://docs.google.com/document/d/115dAroX0PzSduKr5F1V4CWggYcqIoSXYBhcxYktCnqY/edit?usp=sharing</p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Neel-Nanda---Mechanistic-Interpretability-e25sibc</link>
			<guid isPermaLink="false">d7e27836-5dab-4440-9846-53a7a790d88a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 18 Jun 2023 13:52:48 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/72288044/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-18%2Fd25a054b-86a0-d2b6-dc25-292faf13246d.mp3" length="240012591" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this wide-ranging conversation, Tim Scarfe interviews Neel Nanda, a researcher at DeepMind working on mechanistic interpretability, which aims to understand the algorithms and representations learned by machine learning models. Neel discusses how models can represent their thoughts using motifs, circuits, and linear directional features which are often communicated via a &amp;quot;residual stream&amp;quot;, an information highway models use to pass information between layers.&lt;/p&gt;
&lt;p&gt;Neel argues that &amp;quot;superposition&amp;quot;, the ability for models to represent more features than they have neurons, is one of the biggest open problems in interpretability. This is because superposition thwarts our ability to understand models by decomposing them into individual units of analysis. Despite this, Neel remains optimistic that ambitious interpretability is possible, citing examples like his work reverse engineering how models do modular addition. However, Neel notes we must start small, build rigorous foundations, and not assume our theoretical frameworks perfectly match reality.&lt;/p&gt;
&lt;p&gt;The conversation turns to whether models can have goals or agency, with Neel arguing they likely can based on heuristics like models executing long term plans towards some objective. However, we currently lack techniques to build models with specific goals, meaning any goals would likely be learned or emergent. Neel highlights how induction heads, circuits models use to track long range dependencies, seem crucial for phenomena like in-context learning to emerge.&lt;/p&gt;
&lt;p&gt;On the existential risks from AI, Neel believes we should avoid overly confident claims that models will or will not be dangerous, as we do not understand them enough to make confident theoretical assertions. However, models could pose risks through being misused, having undesirable emergent properties, or being imperfectly aligned. Neel argues we must pursue rigorous empirical work to better understand and ensure model safety, avoid &amp;quot;philosophizing&amp;quot; about definitions of intelligence, and focus on ensuring researchers have standards for what it means to decide a system is &amp;quot;safe&amp;quot; before deploying it. Overall, a thoughtful conversation on one of the most important issues of our time.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Twitter: https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Neel Nanda: https://www.neelnanda.io/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction and Neel Nanda&amp;#39;s Interests (walk and talk)&lt;/p&gt;
&lt;p&gt;[00:03:15] Mechanistic Interpretability: Reverse Engineering Neural Networks&lt;/p&gt;
&lt;p&gt;[00:13:23] Discord questions&lt;/p&gt;
&lt;p&gt;[00:21:16] Main interview kick-off in studio&lt;/p&gt;
&lt;p&gt;[00:49:26] Grokking and Sudden Generalization&lt;/p&gt;
&lt;p&gt;[00:53:18] The Debate on Systematicity and Compositionality&lt;/p&gt;
&lt;p&gt;[01:19:16] How do ML models represent their thoughts&lt;/p&gt;
&lt;p&gt;[01:25:51] Do Large Language Models Learn World Models?&lt;/p&gt;
&lt;p&gt;[01:53:36] Superposition and Interference in Language Models&lt;/p&gt;
&lt;p&gt;[02:43:15] Transformers discussion&lt;/p&gt;
&lt;p&gt;[02:49:49] Emergence and In-Context Learning&lt;/p&gt;
&lt;p&gt;[03:20:02] Superintelligence/XRisk discussion&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Transcript: https://docs.google.com/document/d/1FK1OepdJMrqpFK-_1Q3LQN6QLyLBvBwWW_5z8WrS1RI/edit?usp=sharing&lt;/p&gt;
&lt;p&gt;Refs: https://docs.google.com/document/d/115dAroX0PzSduKr5F1V4CWggYcqIoSXYBhcxYktCnqY/edit?usp=sharing&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>04:10:00</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1687096332929-d0e3a3d6e38d4.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>121</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Prof. Daniel Dennett - Could AI Counterfeit People Destroy Civilization? (SPECIAL EDITION)]]></title>
			<description><![CDATA[<p>Please check out Numerai - our sponsor using our link @</p>
<p>http://numer.ai/mlst</p>
<p><br></p>
<p>Numerai is a groundbreaking platform which is taking the data science world by storm. Tim has been using Numerai to build state-of-the-art models which predict the stock market, all while being a part of an inspiring community of data scientists from around the globe. They host the Numerai Data Science Tournament, where data scientists like us use their financial dataset to predict future stock market performance.</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst </p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Twitter: https://twitter.com/MLStreetTalk</p>
<p>YT version: https://youtu.be/axJtywd9Tbo</p>
<p><br></p>
<p>In this fascinating interview, Dr. Tim Scarfe speaks with renowned philosopher Daniel Dennett about the potential dangers of AI and the concept of &quot;Counterfeit People.&quot; Dennett raises concerns about AI being used to create artificial colleagues, and argues that preventing counterfeit AI individuals is crucial for societal trust and security.</p>
<p><br></p>
<p>They delve into Dennett&#39;s &quot;Two Black Boxes&quot; thought experiment, the Chinese Room Argument by John Searle, and discuss the implications of AI in terms of reversibility, reontologisation, and realism. Dr. Scarfe and Dennett also examine adversarial LLMs, mental trajectories, and the emergence of consciousness and semanticity in AI systems.</p>
<p><br></p>
<p>Throughout the conversation, they touch upon various philosophical perspectives, including Gilbert Ryle&#39;s Ghost in the Machine, Chomsky&#39;s work, and the importance of competition in academia. Dennett concludes by highlighting the need for legal and technological barriers to protect against the dangers of counterfeit AI creations.</p>
<p><br></p>
<p>Join Dr. Tim Scarfe and Daniel Dennett in this thought-provoking discussion about the future of AI and the potential challenges we face in preserving our civilization. Don&#39;t miss this insightful conversation!</p>
<p><br></p>
<p>TOC:</p>
<p>00:00:00 Intro</p>
<p>00:09:56 Main show kick off</p>
<p>00:12:04 Counterfeit People</p>
<p>00:16:03 Reversibility</p>
<p>00:20:55 Reontologisation</p>
<p>00:24:43 Realism</p>
<p>00:27:48 Adversarial LLMs are out to get us</p>
<p>00:32:34 Exploring mental trajectories and Chomsky</p>
<p>00:38:53 Gilbert Ryle and Ghost in machine and competition in academia</p>
<p>00:44:32 2 Black boxes thought experiment / intentional stance</p>
<p>01:00:11 Chinese room</p>
<p>01:04:49 Singularitarianism</p>
<p>01:07:22 Emergence of consciousness and semanticity</p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</p>
<p>https://arxiv.org/abs/2305.10601</p>
<p><br></p>
<p>The Problem With Counterfeit People (Daniel Dennett)</p>
<p>https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/</p>
<p><br></p>
<p>The knowledge argument</p>
<p>https://en.wikipedia.org/wiki/Knowledge_argument</p>
<p><br></p>
<p>The Intentional Stance</p>
<p>https://www.researchgate.net/publication/271180035_The_Intentional_Stance</p>
<p><br></p>
<p>Two Black Boxes: a Fable (Daniel Dennett)</p>
<p>https://www.researchgate.net/publication/28762339_Two_Black_Boxes_a_Fable</p>
<p><br></p>
<p>The Chinese Room Argument (John Searle)</p>
<p>https://plato.stanford.edu/entries/chinese-room/</p>
<p>https://web-archive.southampton.ac.uk/cogprints.org/7150/1/10.1.1.83.5248.pdf</p>
<p><br></p>
<p>From Bacteria to Bach and Back: The Evolution of Minds (Daniel Dennett)</p>
<p>https://www.amazon.co.uk/Bacteria-Bach-Back-Evolution-Minds/dp/014197804X</p>
<p><br></p>
<p>Consciousness Explained (Daniel Dennett)</p>
<p>https://www.amazon.co.uk/Consciousness-Explained-Penguin-Science-Dennett/dp/0140128670/</p>
<p><br></p>
<p>The Mind&#39;s I: Fantasies and Reflections on Self and Soul (Hofstadter, Douglas R; Dennett, Daniel C.)</p>
<p>https://www.abebooks.co.uk/servlet/BookDetailsPL?bi=31494476184</p>
<p><br></p>
<p>#DanielDennett #ArtificialIntelligence #CounterfeitPeople</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Prof--Daniel-Dennett---Could-AI-Counterfeit-People-Destroy-Civilization--SPECIAL-EDITION-e257pcm</link>
			<guid isPermaLink="false">14f68ceb-8c07-4b8e-bea5-fa33381af744</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 04 Jun 2023 23:49:02 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/71607126/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-5-4%2F171638b5-c3f5-ed57-dff0-3f500ae34bfc.mp3" length="107563392" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please check out Numerai - our sponsor using our link @&lt;/p&gt;
&lt;p&gt;http://numer.ai/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Numerai is a groundbreaking platform which is taking the data science world by storm. Tim has been using Numerai to build state-of-the-art models which predict the stock market, all while being a part of an inspiring community of data scientists from around the globe. They host the Numerai Data Science Tournament, where data scientists like us use their financial dataset to predict future stock market performance.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Twitter: https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/axJtywd9Tbo&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this fascinating interview, Dr. Tim Scarfe speaks with renowned philosopher Daniel Dennett about the potential dangers of AI and the concept of &amp;quot;Counterfeit People.&amp;quot; Dennett raises concerns about AI being used to create artificial colleagues, and argues that preventing counterfeit AI individuals is crucial for societal trust and security.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;They delve into Dennett&amp;#39;s &amp;quot;Two Black Boxes&amp;quot; thought experiment, the Chinese Room Argument by John Searle, and discuss the implications of AI in terms of reversibility, reontologisation, and realism. Dr. Scarfe and Dennett also examine adversarial LLMs, mental trajectories, and the emergence of consciousness and semanticity in AI systems.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Throughout the conversation, they touch upon various philosophical perspectives, including Gilbert Ryle&amp;#39;s Ghost in the Machine, Chomsky&amp;#39;s work, and the importance of competition in academia. Dennett concludes by highlighting the need for legal and technological barriers to protect against the dangers of counterfeit AI creations.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Join Dr. Tim Scarfe and Daniel Dennett in this thought-provoking discussion about the future of AI and the potential challenges we face in preserving our civilization. Don&amp;#39;t miss this insightful conversation!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:09:56 Main show kick off&lt;/p&gt;
&lt;p&gt;00:12:04 Counterfeit People&lt;/p&gt;
&lt;p&gt;00:16:03 Reversibility&lt;/p&gt;
&lt;p&gt;00:20:55 Reontologisation&lt;/p&gt;
&lt;p&gt;00:24:43 Realism&lt;/p&gt;
&lt;p&gt;00:27:48 Adversarial LLMs are out to get us&lt;/p&gt;
&lt;p&gt;00:32:34 Exploring mental trajectories and Chomsky&lt;/p&gt;
&lt;p&gt;00:38:53 Gilbert Ryle and Ghost in machine and competition in academia&lt;/p&gt;
&lt;p&gt;00:44:32 2 Black boxes thought experiment / intentional stance&lt;/p&gt;
&lt;p&gt;01:00:11 Chinese room&lt;/p&gt;
&lt;p&gt;01:04:49 Singularitarianism&lt;/p&gt;
&lt;p&gt;01:07:22 Emergence of consciousness and semanticity&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2305.10601&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Problem With Counterfeit People (Daniel Dennett)&lt;/p&gt;
&lt;p&gt;https://www.theatlantic.com/technology/archive/2023/05/problem-counterfeit-people/674075/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The knowledge argument&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Knowledge_argument&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Intentional Stance&lt;/p&gt;
&lt;p&gt;https://www.researchgate.net/publication/271180035_The_Intentional_Stance&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Two Black Boxes: a Fable (Daniel Dennett)&lt;/p&gt;
&lt;p&gt;https://www.researchgate.net/publication/28762339_Two_Black_Boxes_a_Fable&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Chinese Room Argument (John Searle)&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/chinese-room/&lt;/p&gt;
&lt;p&gt;https://web-archive.southampton.ac.uk/cogprints.org/7150/1/10.1.1.83.5248.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;From Bacteria to Bach and Back: The Evolution of Minds (Daniel Dennett)&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Bacteria-Bach-Back-Evolution-Minds/dp/014197804X&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Consciousness Explained (Daniel Dennett)&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Consciousness-Explained-Penguin-Science-Dennett/dp/0140128670/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Mind&amp;#39;s I: Fantasies and Reflections on Self and Soul (Hofstadter, Douglas R; Dennett, Daniel C.)&lt;/p&gt;
&lt;p&gt;https://www.abebooks.co.uk/servlet/BookDetailsPL?bi=31494476184&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#DanielDennett #ArtificialIntelligence #CounterfeitPeople&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:14:41</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1685922480219-bade45786ab96.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>120</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Decoding the Genome: Unraveling the Complexities with AI and Creativity [Prof. Jim Hughes, Oxford]]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5
Twitter: https://twitter.com/MLStreetTalk

 In this eye-opening discussion between Tim Scarfe and Prof. Jim Hughes, a professor of gene regulation at Oxford University, they explore the intersection of creativity, genomics, and artificial intelligence. Prof. Hughes brings his expertise in genomics and insights from his interdisciplinary research group, which includes machine learning experts, mathematicians, and molecular biologists.

The conversation begins with an overview of Prof. Hughes&#39; background and the importance of creativity in scientific research. They delve into the challenges of unlocking the secrets of the human genome and how machine learning, specifically convolutional neural networks, can assist in decoding genome function. 

As they discuss validation and interpretability concerns in machine learning, they acknowledge the need for experimental tests and ponder the complex nature of understanding the basic code of life. They touch upon the fascinating world of morphogenesis and emergence, considering the potential crossovers into AI and their implications for self-repairing systems in medicine.

Examining the ethical and regulatory aspects of genomics and AI, the duo explores the implications of having access to someone&#39;s genome, the potential to predict traits or diseases, and the role of AI in understanding complex genetic signals. They also consider the challenges of keeping up with the rapidly expanding body of scientific research and the pressures faced by researchers in academia.

To wrap up the discussion, Tim and Prof. Hughes shed light on the significance of creativity and diversity in scientific research, emphasizing the need for divergent processes and diverse perspectives to foster innovation and avoid consensus-driven convergence.
Filmed at https://www.creativemachine.io/Prof. Jim Hughes: https://www.rdm.ox.ac.uk/people/jim-hughesDr. Tim Scarfe: https://xrai.glass/

Table of Contents:

1. [0:00:00] Introduction and Prof. Jim Hughes&#39; background
2. [0:02:48] Creativity and its role in science
3. [0:07:13] Challenges in understanding the human genome
4. [0:13:20] Using convolutional neural networks to decode genome function
5. [0:15:32] Validation and interpretability concerns in machine learning
6. [0:17:56] Challenges in understanding the basic code of life
7. [0:19:36] Morphogenesis, emergence, and potential crossovers into AI
8. [0:21:38] Ethics and regulation in genomics and AI
9. [0:23:30] The role of AI in understanding and managing genetic risks
10. [0:32:37] Creativity and diversity in scientific research</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Decoding-the-Genome-Unraveling-the-Complexities-with-AI-and-Creativity-Prof--Jim-Hughes--Oxford-e250mck</link>
			<guid isPermaLink="false">9a919cd7-5a5d-4ab5-97c9-892ed4702ac7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 31 May 2023 23:05:16 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/71374676/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-31%2Fb0588e48-adf6-8c94-abb3-c6a2baaa001d.mp3" length="61855755" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5
Twitter: https://twitter.com/MLStreetTalk

 In this eye-opening discussion between Tim Scarfe and Prof. Jim Hughes, a professor of gene regulation at Oxford University, they explore the intersection of creativity, genomics, and artificial intelligence. Prof. Hughes brings his expertise in genomics and insights from his interdisciplinary research group, which includes machine learning experts, mathematicians, and molecular biologists.

The conversation begins with an overview of Prof. Hughes&amp;#39; background and the importance of creativity in scientific research. They delve into the challenges of unlocking the secrets of the human genome and how machine learning, specifically convolutional neural networks, can assist in decoding genome function. 

As they discuss validation and interpretability concerns in machine learning, they acknowledge the need for experimental tests and ponder the complex nature of understanding the basic code of life. They touch upon the fascinating world of morphogenesis and emergence, considering the potential crossovers into AI and their implications for self-repairing systems in medicine.

Examining the ethical and regulatory aspects of genomics and AI, the duo explores the implications of having access to someone&amp;#39;s genome, the potential to predict traits or diseases, and the role of AI in understanding complex genetic signals. They also consider the challenges of keeping up with the rapidly expanding body of scientific research and the pressures faced by researchers in academia.

To wrap up the discussion, Tim and Prof. Hughes shed light on the significance of creativity and diversity in scientific research, emphasizing the need for divergent processes and diverse perspectives to foster innovation and avoid consensus-driven convergence.
Filmed at https://www.creativemachine.io/Prof. Jim Hughes: https://www.rdm.ox.ac.uk/people/jim-hughesDr. Tim Scarfe: https://xrai.glass/

Table of Contents:

1. [0:00:00] Introduction and Prof. Jim Hughes&amp;#39; background
2. [0:02:48] Creativity and its role in science
3. [0:07:13] Challenges in understanding the human genome
4. [0:13:20] Using convolutional neural networks to decode genome function
5. [0:15:32] Validation and interpretability concerns in machine learning
6. [0:17:56] Challenges in understanding the basic code of life
7. [0:19:36] Morphogenesis, emergence, and potential crossovers into AI
8. [0:21:38] Ethics and regulation in genomics and AI
9. [0:23:30] The role of AI in understanding and managing genetic risks
10. [0:32:37] Creativity and diversity in scientific research&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:42:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1685574298363-ba80d3c2e3072.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>119</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ROBERT MILES - "There is a good chance this kills everyone"]]></title>
			<description><![CDATA[<p>Please check out Numerai - our sponsor @</p>
<p>https://numerai.com/mlst</p>
<p><br></p>
<p>Numerai is a groundbreaking platform which is taking the data science world by storm. Tim has been using Numerai to build state-of-the-art models which predict the stock market, all while being a part of an inspiring community of data scientists from around the globe. They host the Numerai Data Science Tournament, where data scientists like us use their financial dataset to predict future stock market performance.</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst </p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Twitter: https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Welcome to an exciting episode featuring an outstanding guest, Robert Miles! Renowned for his extraordinary contributions to understanding AI and its potential impacts on our lives, Robert is an artificial intelligence advocate, researcher, and YouTube sensation. He combines engaging discussions with entertaining content, captivating millions of viewers from around the world.</p>
<p>With a strong computer science background, Robert has been actively involved in AI safety projects, focusing on raising awareness about potential risks and benefits of advanced AI systems. His YouTube channel is celebrated for making AI safety discussions accessible to a diverse audience through breaking down complex topics into easy-to-understand nuggets of knowledge, and you might also recognise him from his appearances on Computerphile.</p>
<p>In this episode, join us as we dive deep into Robert&#39;s journey in the world of AI, exploring his insights on AI alignment, superintelligence, and the role of AI shaping our society and future. We&#39;ll discuss topics such as the limits of AI capabilities and physics, AI progress and timelines, human-machine hybrid intelligence, AI in conflict and cooperation with humans, and the convergence of AI communities.</p>
<p><br></p>
<p>Robert Miles: </p>
<p> @RobertMilesAI </p>
<p>https://twitter.com/robertskmiles</p>
<p>https://aisafety.info/</p>
<p><br></p>
<p>YT version: https://www.youtube.com/watch?v=kMLKbhY0ji0</p>
<p><br></p>
<p>Panel:</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p>Joint CTOs - https://xrai.glass/</p>
<p><br></p>
<p>Refs:</p>
<p>Are Emergent Abilities of Large Language Models a Mirage? (Rylan Schaeffer)</p>
<p>https://arxiv.org/abs/2304.15004</p>
<p><br></p>
<p>TOC:</p>
<p>Intro [00:00:00]</p>
<p>Numerai Sponsor Messsage [00:02:17]</p>
<p>AI Alignment [00:04:27]</p>
<p>Limits of AI Capabilities and Physics [00:18:00]</p>
<p>AI Progress and Timelines [00:23:52]</p>
<p>AI Arms Race and Innovation [00:31:11]</p>
<p>Human-Machine Hybrid Intelligence [00:38:30]</p>
<p>Understanding and Defining Intelligence [00:42:48]</p>
<p>AI in Conflict and Cooperation with Humans [00:50:13]</p>
<p>Interpretability and Mind Reading in AI [01:03:46]</p>
<p>Mechanistic Interpretability and Deconfusion Research [01:05:53]</p>
<p>Understanding the core concepts of AI [01:07:40]</p>
<p>Moon landing analogy and AI alignment [01:09:42]</p>
<p>Cognitive horizon and limits of human intelligence [01:11:42]</p>
<p>Funding and focus on AI alignment [01:16:18]</p>
<p>Regulating AI technology and potential risks [01:19:17]</p>
<p>Aligning AI with human values and its dynamic nature [01:27:04]</p>
<p>Cooperation and Allyship [01:29:33]</p>
<p>Orthogonality Thesis and Goal Preservation [01:33:15]</p>
<p>Anthropomorphic Language and Intelligent Agents [01:35:31]</p>
<p>Maintaining Variety and Open-ended Existence [01:36:27]</p>
<p>Emergent Abilities of Large Language Models [01:39:22]</p>
<p>Convergence vs Emergence [01:44:04]</p>
<p>Criticism of X-risk and Alignment Communities [01:49:40]</p>
<p>Fusion of AI communities and addressing biases [01:52:51]</p>
<p>AI systems integration into society and understanding them [01:53:29]</p>
<p>Changing opinions on AI topics and learning from past videos [01:54:23]</p>
<p>Utility functions and von Neumann-Morgenstern theorems [01:54:47]</p>
<p>AI Safety FAQ project [01:58:06]</p>
<p>Building a conversation agent using AI safety dataset [02:00:36]</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/ROBERT-MILES---There-is-a-good-chance-this-kills-everyone-e24eio7</link>
			<guid isPermaLink="false">e567446a-e5e6-4af5-9779-87eb42bc64a4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 21 May 2023 16:42:51 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/70781127/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-21%2Fb355263e-1788-fabd-ebe6-f7ed528dd89c.mp3" length="175546368" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please check out Numerai - our sponsor @&lt;/p&gt;
&lt;p&gt;https://numerai.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Numerai is a groundbreaking platform which is taking the data science world by storm. Tim has been using Numerai to build state-of-the-art models which predict the stock market, all while being a part of an inspiring community of data scientists from around the globe. They host the Numerai Data Science Tournament, where data scientists like us use their financial dataset to predict future stock market performance.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Twitter: https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Welcome to an exciting episode featuring an outstanding guest, Robert Miles! Renowned for his extraordinary contributions to understanding AI and its potential impacts on our lives, Robert is an artificial intelligence advocate, researcher, and YouTube sensation. He combines engaging discussions with entertaining content, captivating millions of viewers from around the world.&lt;/p&gt;
&lt;p&gt;With a strong computer science background, Robert has been actively involved in AI safety projects, focusing on raising awareness about potential risks and benefits of advanced AI systems. His YouTube channel is celebrated for making AI safety discussions accessible to a diverse audience through breaking down complex topics into easy-to-understand nuggets of knowledge, and you might also recognise him from his appearances on Computerphile.&lt;/p&gt;
&lt;p&gt;In this episode, join us as we dive deep into Robert&amp;#39;s journey in the world of AI, exploring his insights on AI alignment, superintelligence, and the role of AI shaping our society and future. We&amp;#39;ll discuss topics such as the limits of AI capabilities and physics, AI progress and timelines, human-machine hybrid intelligence, AI in conflict and cooperation with humans, and the convergence of AI communities.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Robert Miles: &lt;/p&gt;
&lt;p&gt; @RobertMilesAI &lt;/p&gt;
&lt;p&gt;https://twitter.com/robertskmiles&lt;/p&gt;
&lt;p&gt;https://aisafety.info/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://www.youtube.com/watch?v=kMLKbhY0ji0&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel:&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Joint CTOs - https://xrai.glass/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;Are Emergent Abilities of Large Language Models a Mirage? (Rylan Schaeffer)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2304.15004&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;Intro [00:00:00]&lt;/p&gt;
&lt;p&gt;Numerai Sponsor Messsage [00:02:17]&lt;/p&gt;
&lt;p&gt;AI Alignment [00:04:27]&lt;/p&gt;
&lt;p&gt;Limits of AI Capabilities and Physics [00:18:00]&lt;/p&gt;
&lt;p&gt;AI Progress and Timelines [00:23:52]&lt;/p&gt;
&lt;p&gt;AI Arms Race and Innovation [00:31:11]&lt;/p&gt;
&lt;p&gt;Human-Machine Hybrid Intelligence [00:38:30]&lt;/p&gt;
&lt;p&gt;Understanding and Defining Intelligence [00:42:48]&lt;/p&gt;
&lt;p&gt;AI in Conflict and Cooperation with Humans [00:50:13]&lt;/p&gt;
&lt;p&gt;Interpretability and Mind Reading in AI [01:03:46]&lt;/p&gt;
&lt;p&gt;Mechanistic Interpretability and Deconfusion Research [01:05:53]&lt;/p&gt;
&lt;p&gt;Understanding the core concepts of AI [01:07:40]&lt;/p&gt;
&lt;p&gt;Moon landing analogy and AI alignment [01:09:42]&lt;/p&gt;
&lt;p&gt;Cognitive horizon and limits of human intelligence [01:11:42]&lt;/p&gt;
&lt;p&gt;Funding and focus on AI alignment [01:16:18]&lt;/p&gt;
&lt;p&gt;Regulating AI technology and potential risks [01:19:17]&lt;/p&gt;
&lt;p&gt;Aligning AI with human values and its dynamic nature [01:27:04]&lt;/p&gt;
&lt;p&gt;Cooperation and Allyship [01:29:33]&lt;/p&gt;
&lt;p&gt;Orthogonality Thesis and Goal Preservation [01:33:15]&lt;/p&gt;
&lt;p&gt;Anthropomorphic Language and Intelligent Agents [01:35:31]&lt;/p&gt;
&lt;p&gt;Maintaining Variety and Open-ended Existence [01:36:27]&lt;/p&gt;
&lt;p&gt;Emergent Abilities of Large Language Models [01:39:22]&lt;/p&gt;
&lt;p&gt;Convergence vs Emergence [01:44:04]&lt;/p&gt;
&lt;p&gt;Criticism of X-risk and Alignment Communities [01:49:40]&lt;/p&gt;
&lt;p&gt;Fusion of AI communities and addressing biases [01:52:51]&lt;/p&gt;
&lt;p&gt;AI systems integration into society and understanding them [01:53:29]&lt;/p&gt;
&lt;p&gt;Changing opinions on AI topics and learning from past videos [01:54:23]&lt;/p&gt;
&lt;p&gt;Utility functions and von Neumann-Morgenstern theorems [01:54:47]&lt;/p&gt;
&lt;p&gt;AI Safety FAQ project [01:58:06]&lt;/p&gt;
&lt;p&gt;Building a conversation agent using AI safety dataset [02:00:36]&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:01:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1684687318838-722dfad8832d4.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>118</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[AI Senate Hearing - Executive Summary (Sam Altman, Gary Marcus)]]></title>
			<description><![CDATA[<p>Support us! <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkd0MFR1eGxSUkxyM29UNmV6VTY0UjQ2Nk5aUXxBQ3Jtc0tteEg4X3liTnJvY25qamNyZ3l3dDJIWFZOallOcFpGRXhxVlpndmJnQ1FndVJjN0pFLTJydkdNVVJTRllKbmFPN21JekpHR1UwX1BEcEk0SUFvQnNVNjA0WE4xUFVBVmRvLS0xcEJXS0ZEWVBvbWZOdw&q=https%3A%2F%2Fwww.patreon.com%2Fmlst&v=CTA7-Bsa9U4" target="_blank" rel="nofollow">https://www.patreon.com/mlst</a> 
MLST Discord: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa21PZmx2UVBBdllkR25tSjFGSC1uLUhaQlJCd3xBQ3Jtc0ttNlFSWVR6NjFpcnFMMDBTc2MtU2tUV1loRGJFY0QwWTZNODJOVWU4M3RtTUtmdTFVZl90ME4zc1JDS0FoTzI1U1lNZlFfbUJnQkl1LTFNVFJRTmNsUEdpT3lkdlplb3RleWw0bEtIUFNqM0Rvc1ZOTQ&q=https%3A%2F%2Fdiscord.gg%2FaNPkGUQtc5&v=CTA7-Bsa9U4" target="_blank" rel="nofollow">https://discord.gg/aNPkGUQtc5</a>
Twitter: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbGxJb3FjSlUtUHFCQ0IxUExZeEMzTE9aZmtiUXxBQ3Jtc0trdEh0M2pIVDNKZWd5Z0RJZWZuMzdqUVUwOTNLYmo1dXhhNGN3MHlBZlZ6TF9fU05nN1loTEdySWVlYTV6VmcwX3Rzb0dQdHRIMmpVX3h4bVhoc2g5X3NFRG10bk5rMllGRlUzeUNxemFKNVpTY1phWQ&q=https%3A%2F%2Ftwitter.com%2FMLStreetTalk&v=CTA7-Bsa9U4" target="_blank" rel="nofollow">https://twitter.com/MLStreetTalk</a></p>
<p><br></p>
<p>In a historic and candid Senate hearing, OpenAI CEO Sam Altman, Professor Gary Marcus, and IBM&#39;s Christina Montgomery discussed the regulatory landscape of AI in the US. The discussion was particularly interesting due to its timing, as it followed the recent release of the EU&#39;s proposed AI Act, which could potentially ban American companies like OpenAI and Google from providing API access to generative AI models and impose massive fines for non-compliance.</p>
<p><br></p>
<p>The speakers openly addressed potential risks of AI technology and emphasized the need for precision regulation. This was a unique approach, as historically, US companies have tried their hardest to avoid regulation. The hearing not only showcased the willingness of industry leaders to engage in discussions on regulation but also demonstrated the need for a balanced approach to avoid stifling innovation.</p>
<p><br></p>
<p>The EU AI Act, scheduled to come into power in 2026, is still just a proposal, but it has already raised concerns about its impact on the American tech ecosystem and potential conflicts between US and EU laws. With extraterritorial jurisdiction and provisions targeting open-source developers and software distributors like GitHub, the Act could create more problems than it solves by encouraging unsafe AI practices and limiting access to advanced AI technologies.</p>
<p><br></p>
<p>One core issue with the Act is the designation of foundation models in the highest risk category, primarily due to their open-ended nature. A significant risk theme revolves around users creating harmful content and determining who should be held accountable – the users or the platforms. The Senate hearing served as an essential platform to discuss these pressing concerns and work towards a regulatory framework that promotes both safety and innovation in AI.</p>
<p><br></p>
<p>00:00 Show</p>
<p>01:35 Legals</p>
<p>03:44 Intro </p>
<p>10:33 Altman intro</p>
<p>14:16 Christina Montgomery</p>
<p>18:20 Gary Marcus</p>
<p>23:15 Jobs</p>
<p>26:01 Scorecards</p>
<p>28:08 Harmful content </p>
<p>29:47 Startups</p>
<p>31:35 What meets the definition of harmful?</p>
<p>32:08 Moratorium</p>
<p>36:11 Social Media</p>
<p>46:17 Gary&#39;s take on BingGPT and pivot into policy</p>
<p>48:05 Democratisation</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/AI-Senate-Hearing---Executive-Summary-Sam-Altman--Gary-Marcus-e246bab</link>
			<guid isPermaLink="false">92114a10-2c71-424a-ac55-3752f2397a42</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 16 May 2023 22:00:30 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/70511371/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-16%2F50b698b1-2c54-c842-6da1-fad6b777c706.mp3" length="71615888" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbkd0MFR1eGxSUkxyM29UNmV6VTY0UjQ2Nk5aUXxBQ3Jtc0tteEg4X3liTnJvY25qamNyZ3l3dDJIWFZOallOcFpGRXhxVlpndmJnQ1FndVJjN0pFLTJydkdNVVJTRllKbmFPN21JekpHR1UwX1BEcEk0SUFvQnNVNjA0WE4xUFVBVmRvLS0xcEJXS0ZEWVBvbWZOdw&amp;q=https%3A%2F%2Fwww.patreon.com%2Fmlst&amp;v=CTA7-Bsa9U4&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://www.patreon.com/mlst&lt;/a&gt; 
MLST Discord: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa21PZmx2UVBBdllkR25tSjFGSC1uLUhaQlJCd3xBQ3Jtc0ttNlFSWVR6NjFpcnFMMDBTc2MtU2tUV1loRGJFY0QwWTZNODJOVWU4M3RtTUtmdTFVZl90ME4zc1JDS0FoTzI1U1lNZlFfbUJnQkl1LTFNVFJRTmNsUEdpT3lkdlplb3RleWw0bEtIUFNqM0Rvc1ZOTQ&amp;q=https%3A%2F%2Fdiscord.gg%2FaNPkGUQtc5&amp;v=CTA7-Bsa9U4&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://discord.gg/aNPkGUQtc5&lt;/a&gt;
Twitter: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbGxJb3FjSlUtUHFCQ0IxUExZeEMzTE9aZmtiUXxBQ3Jtc0trdEh0M2pIVDNKZWd5Z0RJZWZuMzdqUVUwOTNLYmo1dXhhNGN3MHlBZlZ6TF9fU05nN1loTEdySWVlYTV6VmcwX3Rzb0dQdHRIMmpVX3h4bVhoc2g5X3NFRG10bk5rMllGRlUzeUNxemFKNVpTY1phWQ&amp;q=https%3A%2F%2Ftwitter.com%2FMLStreetTalk&amp;v=CTA7-Bsa9U4&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://twitter.com/MLStreetTalk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In a historic and candid Senate hearing, OpenAI CEO Sam Altman, Professor Gary Marcus, and IBM&amp;#39;s Christina Montgomery discussed the regulatory landscape of AI in the US. The discussion was particularly interesting due to its timing, as it followed the recent release of the EU&amp;#39;s proposed AI Act, which could potentially ban American companies like OpenAI and Google from providing API access to generative AI models and impose massive fines for non-compliance.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The speakers openly addressed potential risks of AI technology and emphasized the need for precision regulation. This was a unique approach, as historically, US companies have tried their hardest to avoid regulation. The hearing not only showcased the willingness of industry leaders to engage in discussions on regulation but also demonstrated the need for a balanced approach to avoid stifling innovation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The EU AI Act, scheduled to come into power in 2026, is still just a proposal, but it has already raised concerns about its impact on the American tech ecosystem and potential conflicts between US and EU laws. With extraterritorial jurisdiction and provisions targeting open-source developers and software distributors like GitHub, the Act could create more problems than it solves by encouraging unsafe AI practices and limiting access to advanced AI technologies.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;One core issue with the Act is the designation of foundation models in the highest risk category, primarily due to their open-ended nature. A significant risk theme revolves around users creating harmful content and determining who should be held accountable – the users or the platforms. The Senate hearing served as an essential platform to discuss these pressing concerns and work towards a regulatory framework that promotes both safety and innovation in AI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00 Show&lt;/p&gt;
&lt;p&gt;01:35 Legals&lt;/p&gt;
&lt;p&gt;03:44 Intro &lt;/p&gt;
&lt;p&gt;10:33 Altman intro&lt;/p&gt;
&lt;p&gt;14:16 Christina Montgomery&lt;/p&gt;
&lt;p&gt;18:20 Gary Marcus&lt;/p&gt;
&lt;p&gt;23:15 Jobs&lt;/p&gt;
&lt;p&gt;26:01 Scorecards&lt;/p&gt;
&lt;p&gt;28:08 Harmful content &lt;/p&gt;
&lt;p&gt;29:47 Startups&lt;/p&gt;
&lt;p&gt;31:35 What meets the definition of harmful?&lt;/p&gt;
&lt;p&gt;32:08 Moratorium&lt;/p&gt;
&lt;p&gt;36:11 Social Media&lt;/p&gt;
&lt;p&gt;46:17 Gary&amp;#39;s take on BingGPT and pivot into policy&lt;/p&gt;
&lt;p&gt;48:05 Democratisation&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:49:43</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1684274065996-ead47b53ebac7.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>117</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Future of Generative AI [David Foster]]]></title>
			<description><![CDATA[<p>Generative Deep Learning, 2nd Edition [David Foster]</p>
<p>https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst </p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Twitter: https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>In this conversation, Tim Scarfe and David Foster, the author of &#39;Generative Deep Learning,&#39; dive deep into the world of generative AI, discussing topics ranging from model families and auto regressive models to the democratization of AI technology and its potential impact on various industries. They explore the connection between language and true intelligence, as well as the limitations of GPT and other large language models. The discussion also covers the importance of task-independent world models, the concept of active inference, and the potential of combining these ideas with transformer and GPT-style models.</p>
<p><br></p>
<p>Ethics and regulation in AI development are also discussed, including the need for transparency in data used to train AI models and the responsibility of developers to ensure their creations are not destructive. The conversation touches on the challenges posed by AI-generated content on copyright laws and the diminishing role of effort and skill in copyright due to generative models.</p>
<p><br></p>
<p>The impact of AI on education and creativity is another key area of discussion, with Tim and David exploring the potential benefits and drawbacks of using AI in the classroom, the need for a balance between traditional learning methods and AI-assisted learning, and the importance of teaching students to use AI tools critically and responsibly.</p>
<p><br></p>
<p>Generative AI in music is also explored, with David and Tim discussing the potential for AI-generated music to change the way we create and consume art, as well as the challenges in training AI models to generate music that captures human emotions and experiences.</p>
<p><br></p>
<p>Throughout the conversation, Tim and David touch on the potential risks and consequences of AI becoming too powerful, the importance of maintaining control over the technology, and the possibility of government intervention and regulation. The discussion concludes with a thought experiment about AI predicting human actions and creating transient capabilities that could lead to doom.</p>
<p><br></p>
<p>TOC: </p>
<p>Introducing Generative Deep Learning [00:00:00]</p>
<p>Model Families in Generative Modeling [00:02:25]</p>
<p>Auto Regressive Models and Recurrence [00:06:26]</p>
<p>Language and True Intelligence [00:15:07]</p>
<p>Language, Reality, and World Models [00:19:10]</p>
<p>AI, Human Experience, and Understanding [00:23:09]</p>
<p>GPTs Limitations and World Modeling [00:27:52]</p>
<p>Task-Independent Modeling and Cybernetic Loop [00:33:55]</p>
<p>Collective Intelligence and Emergence [00:36:01]</p>
<p>Active Inference vs. Reinforcement Learning [00:38:02]</p>
<p>Combining Active Inference with Transformers [00:41:55]</p>
<p>Decentralized AI and Collective Intelligence [00:47:46]</p>
<p>Regulation and Ethics in AI Development [00:53:59]</p>
<p>AI-Generated Content and Copyright Laws [00:57:06]</p>
<p>Effort, Skill, and AI Models in Copyright [00:57:59]</p>
<p>AI Alignment and Scale of AI Models [00:59:51]</p>
<p>Democratization of AI: GPT-3 and GPT-4 [01:03:20]</p>
<p>Context Window Size and Vector Databases [01:10:31]</p>
<p>Attention Mechanisms and Hierarchies [01:15:04]</p>
<p>Benefits and Limitations of Language Models [01:16:04]</p>
<p>AI in Education: Risks and Benefits [01:19:41]</p>
<p>AI Tools and Critical Thinking in the Classroom [01:29:26]</p>
<p>Impact of Language Models on Assessment and Creativity [01:35:09]</p>
<p>Generative AI in Music and Creative Arts [01:47:55]</p>
<p>Challenges and Opportunities in Generative Music [01:52:11]</p>
<p>AI-Generated Music and Human Emotions [01:54:31]</p>
<p>Language Modeling vs. Music Modeling [02:01:58]</p>
<p>Democratization of AI and Industry Impact [02:07:38]</p>
<p>Recursive Self-Improving Superintelligence [02:12:48]</p>
<p>AI Technologies: Positive and Negative Impacts [02:14:44]</p>
<p>Runaway AGI and Control Over AI [02:20:35]</p>
<p>AI Dangers, Cybercrime, and Ethics [02:23:42]</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Future-of-Generative-AI-David-Foster-e23sut7</link>
			<guid isPermaLink="false">4bfdf781-c14e-4e03-98c6-9023788c5170</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 11 May 2023 16:44:57 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/70203751/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-11%2Fd36a6cd7-8f50-67bb-80ba-474a04e402e7.mp3" length="218312490" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Generative Deep Learning, 2nd Edition [David Foster]&lt;/p&gt;
&lt;p&gt;https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Twitter: https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this conversation, Tim Scarfe and David Foster, the author of &amp;#39;Generative Deep Learning,&amp;#39; dive deep into the world of generative AI, discussing topics ranging from model families and auto regressive models to the democratization of AI technology and its potential impact on various industries. They explore the connection between language and true intelligence, as well as the limitations of GPT and other large language models. The discussion also covers the importance of task-independent world models, the concept of active inference, and the potential of combining these ideas with transformer and GPT-style models.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ethics and regulation in AI development are also discussed, including the need for transparency in data used to train AI models and the responsibility of developers to ensure their creations are not destructive. The conversation touches on the challenges posed by AI-generated content on copyright laws and the diminishing role of effort and skill in copyright due to generative models.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The impact of AI on education and creativity is another key area of discussion, with Tim and David exploring the potential benefits and drawbacks of using AI in the classroom, the need for a balance between traditional learning methods and AI-assisted learning, and the importance of teaching students to use AI tools critically and responsibly.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Generative AI in music is also explored, with David and Tim discussing the potential for AI-generated music to change the way we create and consume art, as well as the challenges in training AI models to generate music that captures human emotions and experiences.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Throughout the conversation, Tim and David touch on the potential risks and consequences of AI becoming too powerful, the importance of maintaining control over the technology, and the possibility of government intervention and regulation. The discussion concludes with a thought experiment about AI predicting human actions and creating transient capabilities that could lead to doom.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC: &lt;/p&gt;
&lt;p&gt;Introducing Generative Deep Learning [00:00:00]&lt;/p&gt;
&lt;p&gt;Model Families in Generative Modeling [00:02:25]&lt;/p&gt;
&lt;p&gt;Auto Regressive Models and Recurrence [00:06:26]&lt;/p&gt;
&lt;p&gt;Language and True Intelligence [00:15:07]&lt;/p&gt;
&lt;p&gt;Language, Reality, and World Models [00:19:10]&lt;/p&gt;
&lt;p&gt;AI, Human Experience, and Understanding [00:23:09]&lt;/p&gt;
&lt;p&gt;GPTs Limitations and World Modeling [00:27:52]&lt;/p&gt;
&lt;p&gt;Task-Independent Modeling and Cybernetic Loop [00:33:55]&lt;/p&gt;
&lt;p&gt;Collective Intelligence and Emergence [00:36:01]&lt;/p&gt;
&lt;p&gt;Active Inference vs. Reinforcement Learning [00:38:02]&lt;/p&gt;
&lt;p&gt;Combining Active Inference with Transformers [00:41:55]&lt;/p&gt;
&lt;p&gt;Decentralized AI and Collective Intelligence [00:47:46]&lt;/p&gt;
&lt;p&gt;Regulation and Ethics in AI Development [00:53:59]&lt;/p&gt;
&lt;p&gt;AI-Generated Content and Copyright Laws [00:57:06]&lt;/p&gt;
&lt;p&gt;Effort, Skill, and AI Models in Copyright [00:57:59]&lt;/p&gt;
&lt;p&gt;AI Alignment and Scale of AI Models [00:59:51]&lt;/p&gt;
&lt;p&gt;Democratization of AI: GPT-3 and GPT-4 [01:03:20]&lt;/p&gt;
&lt;p&gt;Context Window Size and Vector Databases [01:10:31]&lt;/p&gt;
&lt;p&gt;Attention Mechanisms and Hierarchies [01:15:04]&lt;/p&gt;
&lt;p&gt;Benefits and Limitations of Language Models [01:16:04]&lt;/p&gt;
&lt;p&gt;AI in Education: Risks and Benefits [01:19:41]&lt;/p&gt;
&lt;p&gt;AI Tools and Critical Thinking in the Classroom [01:29:26]&lt;/p&gt;
&lt;p&gt;Impact of Language Models on Assessment and Creativity [01:35:09]&lt;/p&gt;
&lt;p&gt;Generative AI in Music and Creative Arts [01:47:55]&lt;/p&gt;
&lt;p&gt;Challenges and Opportunities in Generative Music [01:52:11]&lt;/p&gt;
&lt;p&gt;AI-Generated Music and Human Emotions [01:54:31]&lt;/p&gt;
&lt;p&gt;Language Modeling vs. Music Modeling [02:01:58]&lt;/p&gt;
&lt;p&gt;Democratization of AI and Industry Impact [02:07:38]&lt;/p&gt;
&lt;p&gt;Recursive Self-Improving Superintelligence [02:12:48]&lt;/p&gt;
&lt;p&gt;AI Technologies: Positive and Negative Impacts [02:14:44]&lt;/p&gt;
&lt;p&gt;Runaway AGI and Control Over AI [02:20:35]&lt;/p&gt;
&lt;p&gt;AI Dangers, Cybercrime, and Ethics [02:23:42]&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:31:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1683823478895-a1a6a6ef9b683.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>115</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[PERPLEXITY AI - The future of search.]]></title>
			<description><![CDATA[<p>https://www.perplexity.ai/</p>
<p>https://www.perplexity.ai/iphone</p>
<p>https://www.perplexity.ai/android

Interview with Aravind Srinivas, CEO and Co-Founder of Perplexity AI – Revolutionizing Learning with Conversational Search Engines

Dr. Tim Scarfe talks with Dr. Aravind Srinivas, CEO and Co-Founder of Perplexity AI, about his journey from studying AI and reinforcement learning at UC Berkeley to launching Perplexity – a startup that aims to revolutionize learning through the power of conversational search engines. By combining the strengths of large language models like GPT-* with search engines, Perplexity provides users with direct answers to their questions in a decluttered user interface, making the learning process not only more efficient but also enjoyable.

Aravind shares his insights on how advertising can be made more relevant and less intrusive with the help of large language models, emphasizing the importance of transparency in relevance ranking to improve user experience. He also discusses the challenge of balancing the interests of users and advertisers for long-term success.

The interview delves into the challenges of maintaining truthfulness and balancing opinions and facts in a world where algorithmic truth is difficult to achieve. Aravind believes that opinionated models can be useful as long as they don&#39;t spread misinformation and are transparent about being opinions. He also emphasizes the importance of allowing users to correct or update information, making the platform more adaptable and dynamic.

Lastly, Aravind shares his thoughts on embracing a digital society with large language models, stressing the need for frequent and iterative deployments of these models to reduce fear of AI and misinformation. He envisions a future where using AI tools effectively requires clear thinking and first-principle reasoning, ultimately benefiting society as a whole. Education and transparency are crucial to counter potential misuse of AI for political or malicious purposes.
</p>
<p>YT version: https://youtu.be/_vMOWw3uYvk

Aravind Srinivas: 
https://www.linkedin.com/in/aravind-srinivas-16051987/</p>
<p>https://scholar.google.com/citations?user=GhrKC1gAAAAJ&amp;hl=en</p>
<p>https://twitter.com/aravsrinivas?lang=en

Interviewer: Dr. Tim Scarfe (CTO XRAI Glass)
Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

TOC:
Introduction and Background of Perplexity AI [00:00:00]</p>
<p>The Importance of a Decluttered UI and User Experience [00:04:19]</p>
<p>Advertising in Search Engines and Potential Improvements [00:09:02]</p>
<p>Challenges and Opportunities in this new Search Modality [00:18:17]</p>
<p>Benefits of Perplexity and Personalized Learning [00:21:27]</p>
<p>Objective Truth and Personalized Wikipedia [00:26:34]</p>
<p>Opinions and Truth in Answer Engines [00:30:53]</p>
<p>Embracing the Digital Society with Language Models [00:37:30]</p>
<p>Impact on Jobs and Future of Learning [00:40:13]</p>
<p>Educating users on when perplexity works and doesn&#39;t work [00:43:13]</p>
<p>Improving user experience and the possibilities of voice-to-voice interaction [00:45:04]</p>
<p>The future of language models and auto-regressive models [00:49:51]</p>
<p>Performance of GPT-4 and potential improvements [00:52:31]</p>
<p>Building the ultimate research and knowledge assistant [00:55:33]</p>
<p>Revolutionizing note-taking and personal knowledge stores [00:58:16]

References:
Evaluating Verifiability in Generative Search Engines (Nelson F. Liu et al, Stanford University)
https://arxiv.org/pdf/2304.09848.pdf

Note: this was a sponsored interview.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/PERPLEXITY-AI---The-future-of-search-e23i5fq</link>
			<guid isPermaLink="false">c308b2a8-0344-4c07-a361-88d75ef02b34</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 08 May 2023 18:58:01 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/69850042/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-4-5%2F8e48f567-a6ef-5a83-09d0-37a535267a10.mp3" length="86133312" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;https://www.perplexity.ai/&lt;/p&gt;
&lt;p&gt;https://www.perplexity.ai/iphone&lt;/p&gt;
&lt;p&gt;https://www.perplexity.ai/android

Interview with Aravind Srinivas, CEO and Co-Founder of Perplexity AI – Revolutionizing Learning with Conversational Search Engines

Dr. Tim Scarfe talks with Dr. Aravind Srinivas, CEO and Co-Founder of Perplexity AI, about his journey from studying AI and reinforcement learning at UC Berkeley to launching Perplexity – a startup that aims to revolutionize learning through the power of conversational search engines. By combining the strengths of large language models like GPT-* with search engines, Perplexity provides users with direct answers to their questions in a decluttered user interface, making the learning process not only more efficient but also enjoyable.

Aravind shares his insights on how advertising can be made more relevant and less intrusive with the help of large language models, emphasizing the importance of transparency in relevance ranking to improve user experience. He also discusses the challenge of balancing the interests of users and advertisers for long-term success.

The interview delves into the challenges of maintaining truthfulness and balancing opinions and facts in a world where algorithmic truth is difficult to achieve. Aravind believes that opinionated models can be useful as long as they don&amp;#39;t spread misinformation and are transparent about being opinions. He also emphasizes the importance of allowing users to correct or update information, making the platform more adaptable and dynamic.

Lastly, Aravind shares his thoughts on embracing a digital society with large language models, stressing the need for frequent and iterative deployments of these models to reduce fear of AI and misinformation. He envisions a future where using AI tools effectively requires clear thinking and first-principle reasoning, ultimately benefiting society as a whole. Education and transparency are crucial to counter potential misuse of AI for political or malicious purposes.
&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/_vMOWw3uYvk

Aravind Srinivas: 
https://www.linkedin.com/in/aravind-srinivas-16051987/&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=GhrKC1gAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;https://twitter.com/aravsrinivas?lang=en

Interviewer: Dr. Tim Scarfe (CTO XRAI Glass)
Patreon: https://www.patreon.com/mlst
Discord: https://discord.gg/ESrGqhf5CB

TOC:
Introduction and Background of Perplexity AI [00:00:00]&lt;/p&gt;
&lt;p&gt;The Importance of a Decluttered UI and User Experience [00:04:19]&lt;/p&gt;
&lt;p&gt;Advertising in Search Engines and Potential Improvements [00:09:02]&lt;/p&gt;
&lt;p&gt;Challenges and Opportunities in this new Search Modality [00:18:17]&lt;/p&gt;
&lt;p&gt;Benefits of Perplexity and Personalized Learning [00:21:27]&lt;/p&gt;
&lt;p&gt;Objective Truth and Personalized Wikipedia [00:26:34]&lt;/p&gt;
&lt;p&gt;Opinions and Truth in Answer Engines [00:30:53]&lt;/p&gt;
&lt;p&gt;Embracing the Digital Society with Language Models [00:37:30]&lt;/p&gt;
&lt;p&gt;Impact on Jobs and Future of Learning [00:40:13]&lt;/p&gt;
&lt;p&gt;Educating users on when perplexity works and doesn&amp;#39;t work [00:43:13]&lt;/p&gt;
&lt;p&gt;Improving user experience and the possibilities of voice-to-voice interaction [00:45:04]&lt;/p&gt;
&lt;p&gt;The future of language models and auto-regressive models [00:49:51]&lt;/p&gt;
&lt;p&gt;Performance of GPT-4 and potential improvements [00:52:31]&lt;/p&gt;
&lt;p&gt;Building the ultimate research and knowledge assistant [00:55:33]&lt;/p&gt;
&lt;p&gt;Revolutionizing note-taking and personal knowledge stores [00:58:16]

References:
Evaluating Verifiability in Generative Search Engines (Nelson F. Liu et al, Stanford University)
https://arxiv.org/pdf/2304.09848.pdf

Note: this was a sponsored interview.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:59:48</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1683294467651-148be05378e4d.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>115</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#114 - Secrets of Deep Reinforcement Learning (Minqi Jiang)]]></title>
			<description><![CDATA[<p>Patreon: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXYyNVEwZ1FzLXJ4Y2F3VHVSMEFJYzJGR1BDUXxBQ3Jtc0ttUmhOYUhwcDZOMHljbWpXbUFDbkUwSnhhSWhZajl3M1hTRERXekd6cW1oaFFuX1doSW5BU05xUnQ5bldEODJqY3hNWHA0MEpteTFZZGE4SnUxOHVJMGxoakFlci1XVkFkTHVDME91cXVCMWFmd2NqTQ&q=https%3A%2F%2Fwww.patreon.com%2Fmlst&v=6Zv6A_9urh4" target="_blank" rel="nofollow">https://www.patreon.com/mlst</a>
Discord: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjFnclotUTJPcGRfclpJTHhvZVk4dm9uQm9id3xBQ3Jtc0tsVjVCSWVsWmxGTGF2eFhvYmx2SzFqNE50TVJXWUIyaWI1b0U1X25NanFwVHFxR3VNSWFPWU01R2hyX05qY0RGZXhqM284OGRpcHN4Z2UzMjJYbnZhLUxSSXlJdWxwZFJ3Vk1CbXItUHVtRkljcTYxMA&q=https%3A%2F%2Fdiscord.gg%2FESrGqhf5CB&v=6Zv6A_9urh4" target="_blank" rel="nofollow">https://discord.gg/ESrGqhf5CB</a>
Twitter: <a href="https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqazlCc2FjU0RMMmtCRFZsa3RTVTBsbDNMTDBLd3xBQ3Jtc0tsQWNOOVk3SHQ2d3Z5bGx0X3hsd29uLVZvOFR6Z2tkX0thWnhKWVBPaTdxUGNxelFZTHFSUEFQM1hKTkVjOW0yMG9JSTUzUnhhNWl0c3dBSzgtMmZsM2tnakpKSUtXYkRmWWtuQmpCaURTbmZkbXJNbw&q=https%3A%2F%2Ftwitter.com%2FMLStreetTalk&v=6Zv6A_9urh4" target="_blank" rel="nofollow">https://twitter.com/MLStreetTalk</a></p>
<p><br></p>
<p>In this exclusive interview, Dr. Tim Scarfe sits down with Minqi Jiang, a leading PhD student at University College London and Meta AI, as they delve into the fascinating world of deep reinforcement learning (RL) and its impact on technology, startups, and research. Discover how Minqi made the crucial decision to pursue a PhD in this exciting field, and learn from his valuable startup experiences and lessons.</p>
<p>Minqi shares his insights into balancing serendipity and planning in life and research, and explains the role of objectives and Goodhart&#39;s Law in decision-making. Get ready to explore the depths of robustness in RL, two-player zero-sum games, and the differences between RL and supervised learning.</p>
<p>As they discuss the role of environment in intelligence, emergence, and abstraction, prepare to be blown away by the possibilities of open-endedness and the intelligence explosion. Learn how language models generate their own training data, the limitations of RL, and the future of software 2.0 with interpretability concerns.</p>
<p>From robotics and open-ended learning applications to learning potential metrics and MDPs, this interview is a goldmine of information for anyone interested in AI, RL, and the cutting edge of technology. Don&#39;t miss out on this incredible opportunity to learn from a rising star in the AI world!</p>
<p>TOC</p>
<p>Tech &amp; Startup Background [00:00:00]</p>
<p>Pursuing PhD in Deep RL [00:03:59]</p>
<p>Startup Lessons [00:11:33]</p>
<p>Serendipity vs Planning [00:12:30]</p>
<p>Objectives &amp; Decision Making [00:19:19]</p>
<p>Minimax Regret &amp; Uncertainty [00:22:57]</p>
<p>Robustness in RL &amp; Zero-Sum Games [00:26:14]</p>
<p>RL vs Supervised Learning [00:34:04]</p>
<p>Exploration &amp; Intelligence [00:41:27]</p>
<p>Environment, Emergence, Abstraction [00:46:31]</p>
<p>Open-endedness &amp; Intelligence Explosion [00:54:28]</p>
<p>Language Models &amp; Training Data [01:04:59]</p>
<p>RLHF &amp; Language Models [01:16:37]</p>
<p>Creativity in Language Models [01:27:25]</p>
<p>Limitations of RL [01:40:58]</p>
<p>Software 2.0 &amp; Interpretability [01:45:11]</p>
<p>Language Models &amp; Code Reliability [01:48:23]</p>
<p>Robust Prioritized Level Replay [01:51:42]</p>
<p>Open-ended Learning [01:55:57]</p>
<p>Auto-curriculum &amp; Deep RL [02:08:48]</p>
<p>Robotics &amp; Open-ended Learning [02:31:05]</p>
<p>Learning Potential &amp; MDPs [02:36:20]</p>
<p>Universal Function Space [02:42:02]</p>
<p>Goal-Directed Learning &amp; Auto-Curricula [02:42:48]</p>
<p>Advice &amp; Closing Thoughts [02:44:47]</p>
<p><br></p>
<p>References:</p>
<p>- Why Greatness Cannot Be Planned: The Myth of the Objective by Kenneth O. Stanley and Joel Lehman</p>
<p> https://www.springer.com/gp/book/9783319155234</p>
<p>- Rethinking Exploration: General Intelligence Requires Rethinking Exploration</p>
<p> https://arxiv.org/abs/2106.06860</p>
<p>- The Case for Strong Emergence (Sabine Hossenfelder)</p>
<p>  https://arxiv.org/abs/2102.07740</p>
<p>- The Game of Life (Conway)</p>
<p>  https://www.conwaylife.com/</p>
<p>- Toolformer: Teaching Language Models to Generate APIs (Meta AI)</p>
<p>https://arxiv.org/abs/2302.04761</p>
<p>- OpenAI&#39;s POET: Paired Open-Ended Trailblazer</p>
<p>https://arxiv.org/abs/1901.01753</p>
<p>- Schmidhuber&#39;s Artificial Curiosity</p>
<p>  https://people.idsia.ch/~juergen/interest.html</p>
<p>- Gödel Machines</p>
<p>  https://people.idsia.ch/~juergen/goedelmachine.html</p>
<p>- PowerPlay</p>
<p>  https://arxiv.org/abs/1112.5309</p>
<p>- Robust Prioritized Level Replay: https://openreview.net/forum?id=NfZ6g2OmXEk</p>
<p>- Unsupervised Environment Design: https://arxiv.org/abs/2012.02096</p>
<p>- Excel: Evolving Curriculum Learning for Deep Reinforcement Learning</p>
<p>  https://arxiv.org/abs/1901.05431</p>
<p>- Go-Explore: A New Approach for Hard-Exploration Problems</p>
<p>  https://arxiv.org/abs/1901.10995</p>
<p>- Learning with AMIGo: Adversarially Motivated Intrinsic Goals</p>
<p>  https://www.researchgate.net/publication/342377312_Learning_with_AMIGo_Adversarially_Motivated_Intrinsic_Goals</p>
<p><br></p>
<p>PRML</p>
<p>https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</p>
<p>Sutton and Barto</p>
<p>https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/114---Secrets-of-Deep-Reinforcement-Learning-Minqi-Jiang-e22fkp7</link>
			<guid isPermaLink="false">c1ad6f36-06e0-4a17-bdf0-004651c2f554</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 16 Apr 2023 18:00:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/68718823/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-16%2F9d4f6981-9471-b94a-e360-4e90ddd666f4.mp3" length="240857280" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbXYyNVEwZ1FzLXJ4Y2F3VHVSMEFJYzJGR1BDUXxBQ3Jtc0ttUmhOYUhwcDZOMHljbWpXbUFDbkUwSnhhSWhZajl3M1hTRERXekd6cW1oaFFuX1doSW5BU05xUnQ5bldEODJqY3hNWHA0MEpteTFZZGE4SnUxOHVJMGxoakFlci1XVkFkTHVDME91cXVCMWFmd2NqTQ&amp;q=https%3A%2F%2Fwww.patreon.com%2Fmlst&amp;v=6Zv6A_9urh4&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://www.patreon.com/mlst&lt;/a&gt;
Discord: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbjFnclotUTJPcGRfclpJTHhvZVk4dm9uQm9id3xBQ3Jtc0tsVjVCSWVsWmxGTGF2eFhvYmx2SzFqNE50TVJXWUIyaWI1b0U1X25NanFwVHFxR3VNSWFPWU01R2hyX05qY0RGZXhqM284OGRpcHN4Z2UzMjJYbnZhLUxSSXlJdWxwZFJ3Vk1CbXItUHVtRkljcTYxMA&amp;q=https%3A%2F%2Fdiscord.gg%2FESrGqhf5CB&amp;v=6Zv6A_9urh4&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://discord.gg/ESrGqhf5CB&lt;/a&gt;
Twitter: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqazlCc2FjU0RMMmtCRFZsa3RTVTBsbDNMTDBLd3xBQ3Jtc0tsQWNOOVk3SHQ2d3Z5bGx0X3hsd29uLVZvOFR6Z2tkX0thWnhKWVBPaTdxUGNxelFZTHFSUEFQM1hKTkVjOW0yMG9JSTUzUnhhNWl0c3dBSzgtMmZsM2tnakpKSUtXYkRmWWtuQmpCaURTbmZkbXJNbw&amp;q=https%3A%2F%2Ftwitter.com%2FMLStreetTalk&amp;v=6Zv6A_9urh4&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;https://twitter.com/MLStreetTalk&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this exclusive interview, Dr. Tim Scarfe sits down with Minqi Jiang, a leading PhD student at University College London and Meta AI, as they delve into the fascinating world of deep reinforcement learning (RL) and its impact on technology, startups, and research. Discover how Minqi made the crucial decision to pursue a PhD in this exciting field, and learn from his valuable startup experiences and lessons.&lt;/p&gt;
&lt;p&gt;Minqi shares his insights into balancing serendipity and planning in life and research, and explains the role of objectives and Goodhart&amp;#39;s Law in decision-making. Get ready to explore the depths of robustness in RL, two-player zero-sum games, and the differences between RL and supervised learning.&lt;/p&gt;
&lt;p&gt;As they discuss the role of environment in intelligence, emergence, and abstraction, prepare to be blown away by the possibilities of open-endedness and the intelligence explosion. Learn how language models generate their own training data, the limitations of RL, and the future of software 2.0 with interpretability concerns.&lt;/p&gt;
&lt;p&gt;From robotics and open-ended learning applications to learning potential metrics and MDPs, this interview is a goldmine of information for anyone interested in AI, RL, and the cutting edge of technology. Don&amp;#39;t miss out on this incredible opportunity to learn from a rising star in the AI world!&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;Tech &amp;amp; Startup Background [00:00:00]&lt;/p&gt;
&lt;p&gt;Pursuing PhD in Deep RL [00:03:59]&lt;/p&gt;
&lt;p&gt;Startup Lessons [00:11:33]&lt;/p&gt;
&lt;p&gt;Serendipity vs Planning [00:12:30]&lt;/p&gt;
&lt;p&gt;Objectives &amp;amp; Decision Making [00:19:19]&lt;/p&gt;
&lt;p&gt;Minimax Regret &amp;amp; Uncertainty [00:22:57]&lt;/p&gt;
&lt;p&gt;Robustness in RL &amp;amp; Zero-Sum Games [00:26:14]&lt;/p&gt;
&lt;p&gt;RL vs Supervised Learning [00:34:04]&lt;/p&gt;
&lt;p&gt;Exploration &amp;amp; Intelligence [00:41:27]&lt;/p&gt;
&lt;p&gt;Environment, Emergence, Abstraction [00:46:31]&lt;/p&gt;
&lt;p&gt;Open-endedness &amp;amp; Intelligence Explosion [00:54:28]&lt;/p&gt;
&lt;p&gt;Language Models &amp;amp; Training Data [01:04:59]&lt;/p&gt;
&lt;p&gt;RLHF &amp;amp; Language Models [01:16:37]&lt;/p&gt;
&lt;p&gt;Creativity in Language Models [01:27:25]&lt;/p&gt;
&lt;p&gt;Limitations of RL [01:40:58]&lt;/p&gt;
&lt;p&gt;Software 2.0 &amp;amp; Interpretability [01:45:11]&lt;/p&gt;
&lt;p&gt;Language Models &amp;amp; Code Reliability [01:48:23]&lt;/p&gt;
&lt;p&gt;Robust Prioritized Level Replay [01:51:42]&lt;/p&gt;
&lt;p&gt;Open-ended Learning [01:55:57]&lt;/p&gt;
&lt;p&gt;Auto-curriculum &amp;amp; Deep RL [02:08:48]&lt;/p&gt;
&lt;p&gt;Robotics &amp;amp; Open-ended Learning [02:31:05]&lt;/p&gt;
&lt;p&gt;Learning Potential &amp;amp; MDPs [02:36:20]&lt;/p&gt;
&lt;p&gt;Universal Function Space [02:42:02]&lt;/p&gt;
&lt;p&gt;Goal-Directed Learning &amp;amp; Auto-Curricula [02:42:48]&lt;/p&gt;
&lt;p&gt;Advice &amp;amp; Closing Thoughts [02:44:47]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;- Why Greatness Cannot Be Planned: The Myth of the Objective by Kenneth O. Stanley and Joel Lehman&lt;/p&gt;
&lt;p&gt; https://www.springer.com/gp/book/9783319155234&lt;/p&gt;
&lt;p&gt;- Rethinking Exploration: General Intelligence Requires Rethinking Exploration&lt;/p&gt;
&lt;p&gt; https://arxiv.org/abs/2106.06860&lt;/p&gt;
&lt;p&gt;- The Case for Strong Emergence (Sabine Hossenfelder)&lt;/p&gt;
&lt;p&gt;  https://arxiv.org/abs/2102.07740&lt;/p&gt;
&lt;p&gt;- The Game of Life (Conway)&lt;/p&gt;
&lt;p&gt;  https://www.conwaylife.com/&lt;/p&gt;
&lt;p&gt;- Toolformer: Teaching Language Models to Generate APIs (Meta AI)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2302.04761&lt;/p&gt;
&lt;p&gt;- OpenAI&amp;#39;s POET: Paired Open-Ended Trailblazer&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1901.01753&lt;/p&gt;
&lt;p&gt;- Schmidhuber&amp;#39;s Artificial Curiosity&lt;/p&gt;
&lt;p&gt;  https://people.idsia.ch/~juergen/interest.html&lt;/p&gt;
&lt;p&gt;- Gödel Machines&lt;/p&gt;
&lt;p&gt;  https://people.idsia.ch/~juergen/goedelmachine.html&lt;/p&gt;
&lt;p&gt;- PowerPlay&lt;/p&gt;
&lt;p&gt;  https://arxiv.org/abs/1112.5309&lt;/p&gt;
&lt;p&gt;- Robust Prioritized Level Replay: https://openreview.net/forum?id=NfZ6g2OmXEk&lt;/p&gt;
&lt;p&gt;- Unsupervised Environment Design: https://arxiv.org/abs/2012.02096&lt;/p&gt;
&lt;p&gt;- Excel: Evolving Curriculum Learning for Deep Reinforcement Learning&lt;/p&gt;
&lt;p&gt;  https://arxiv.org/abs/1901.05431&lt;/p&gt;
&lt;p&gt;- Go-Explore: A New Approach for Hard-Exploration Problems&lt;/p&gt;
&lt;p&gt;  https://arxiv.org/abs/1901.10995&lt;/p&gt;
&lt;p&gt;- Learning with AMIGo: Adversarially Motivated Intrinsic Goals&lt;/p&gt;
&lt;p&gt;  https://www.researchgate.net/publication/342377312_Learning_with_AMIGo_Adversarially_Motivated_Intrinsic_Goals&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;PRML&lt;/p&gt;
&lt;p&gt;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&lt;/p&gt;
&lt;p&gt;Sutton and Barto&lt;/p&gt;
&lt;p&gt;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:47:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1681668013664-9ab4ee6622305.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>114</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Unlocking the Brain's Mysteries: Chris Eliasmith on Spiking Neural Networks and the Future of Human-Machine Interaction]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p>Twitter: https://twitter.com/MLStreetTalk</p>
<p><br></p>
<p>Chris Eliasmith is a renowned interdisciplinary researcher, author, and professor at the University of Waterloo, where he holds the prestigious Canada Research Chair in Theoretical Neuroscience. As the Founding Director of the Centre for Theoretical Neuroscience, Eliasmith leads the Computational Neuroscience Research Group in exploring the mysteries of the brain and its complex functions. His groundbreaking work, including the Neural Engineering Framework, Neural Engineering Objects software environment, and the Semantic Pointer Architecture, has led to the development of Spaun, the most advanced functional brain simulation to date. Among his numerous achievements, Eliasmith has received the 2015 NSERC &quot;Polany-ee&quot; Award and authored two influential books, &quot;How to Build a Brain&quot; and &quot;Neural Engineering.&quot;</p>
<p><br></p>
<p>Chris&#39; homepage:</p>
<p>http://arts.uwaterloo.ca/~celiasmi/</p>
<p><br></p>
<p>Interviewers: Dr. Tim Scarfe and Dr. Keith Duggar</p>
<p><br></p>
<p>TOC:</p>
<p><br></p>
<p>Intro to Chris [00:00:00]</p>
<p>Continuous Representation in Biologically Plausible Neural Networks [00:06:49]</p>
<p>Legendre Memory Unit and Spatial Semantic Pointer [00:14:36]</p>
<p>Large Contexts and Data in Language Models [00:20:30]</p>
<p>Spatial Semantic Pointers and Continuous Representations [00:24:38]</p>
<p>Auto Convolution [00:30:12]</p>
<p>Abstractions and the Continuity [00:36:33]</p>
<p>Compression, Sparsity, and Brain Representations [00:42:52]</p>
<p>Continual Learning and Real-World Interactions [00:48:05]</p>
<p>Robust Generalization in LLMs and Priors [00:56:11]</p>
<p>Chip design [01:00:41]</p>
<p>Chomsky + Computational Power of NNs and Recursion [01:04:02]</p>
<p>Spiking Neural Networks and Applications [01:13:07]</p>
<p>Limits of Empirical Learning [01:22:43]</p>
<p>Philosophy of Mind, Consciousness etc [01:25:35]</p>
<p>Future of human machine interaction [01:41:28]</p>
<p>Future research and advice to young researchers [01:45:06]</p>
<p><br></p>
<p>Refs:</p>
<p><a href="http://compneuro.uwaterloo.ca/publications/dumont2023.html" target="_blank">http://compneuro.uwaterloo.ca/publications/dumont2023.html</a> </p>
<p><a href="http://compneuro.uwaterloo.ca/publications/voelker2019lmu.html" target="_blank">http://compneuro.uwaterloo.ca/publications/voelker2019lmu.html</a>  </p>
<p><a href="http://compneuro.uwaterloo.ca/publications/voelker2018.html" target="_blank">http://compneuro.uwaterloo.ca/publications/voelker2018.html</a></p>
<p><a href="http://compneuro.uwaterloo.ca/publications/lu2019.html" target="_blank">http://compneuro.uwaterloo.ca/publications/lu2019.html</a> </p>
<p><a href="https://www.youtube.com/watch?v=I5h-xjddzlY" target="_blank">https://www.youtube.com/watch?v=I5h-xjddzlY</a></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Unlocking-the-Brains-Mysteries-Chris-Eliasmith-on-Spiking-Neural-Networks-and-the-Future-of-Human-Machine-Interaction-e224fh6</link>
			<guid isPermaLink="false">c334c034-8c4c-45f4-ba6f-e07e9a7523a0</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 10 Apr 2023 18:07:39 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/68352998/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-10%2Faa6b8b83-e7b8-6164-495a-a94b9aebc9d7.mp3" length="157843732" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;Twitter: https://twitter.com/MLStreetTalk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chris Eliasmith is a renowned interdisciplinary researcher, author, and professor at the University of Waterloo, where he holds the prestigious Canada Research Chair in Theoretical Neuroscience. As the Founding Director of the Centre for Theoretical Neuroscience, Eliasmith leads the Computational Neuroscience Research Group in exploring the mysteries of the brain and its complex functions. His groundbreaking work, including the Neural Engineering Framework, Neural Engineering Objects software environment, and the Semantic Pointer Architecture, has led to the development of Spaun, the most advanced functional brain simulation to date. Among his numerous achievements, Eliasmith has received the 2015 NSERC &amp;quot;Polany-ee&amp;quot; Award and authored two influential books, &amp;quot;How to Build a Brain&amp;quot; and &amp;quot;Neural Engineering.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chris&amp;#39; homepage:&lt;/p&gt;
&lt;p&gt;http://arts.uwaterloo.ca/~celiasmi/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewers: Dr. Tim Scarfe and Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Intro to Chris [00:00:00]&lt;/p&gt;
&lt;p&gt;Continuous Representation in Biologically Plausible Neural Networks [00:06:49]&lt;/p&gt;
&lt;p&gt;Legendre Memory Unit and Spatial Semantic Pointer [00:14:36]&lt;/p&gt;
&lt;p&gt;Large Contexts and Data in Language Models [00:20:30]&lt;/p&gt;
&lt;p&gt;Spatial Semantic Pointers and Continuous Representations [00:24:38]&lt;/p&gt;
&lt;p&gt;Auto Convolution [00:30:12]&lt;/p&gt;
&lt;p&gt;Abstractions and the Continuity [00:36:33]&lt;/p&gt;
&lt;p&gt;Compression, Sparsity, and Brain Representations [00:42:52]&lt;/p&gt;
&lt;p&gt;Continual Learning and Real-World Interactions [00:48:05]&lt;/p&gt;
&lt;p&gt;Robust Generalization in LLMs and Priors [00:56:11]&lt;/p&gt;
&lt;p&gt;Chip design [01:00:41]&lt;/p&gt;
&lt;p&gt;Chomsky + Computational Power of NNs and Recursion [01:04:02]&lt;/p&gt;
&lt;p&gt;Spiking Neural Networks and Applications [01:13:07]&lt;/p&gt;
&lt;p&gt;Limits of Empirical Learning [01:22:43]&lt;/p&gt;
&lt;p&gt;Philosophy of Mind, Consciousness etc [01:25:35]&lt;/p&gt;
&lt;p&gt;Future of human machine interaction [01:41:28]&lt;/p&gt;
&lt;p&gt;Future research and advice to young researchers [01:45:06]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://compneuro.uwaterloo.ca/publications/dumont2023.html&quot; target=&quot;_blank&quot;&gt;http://compneuro.uwaterloo.ca/publications/dumont2023.html&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://compneuro.uwaterloo.ca/publications/voelker2019lmu.html&quot; target=&quot;_blank&quot;&gt;http://compneuro.uwaterloo.ca/publications/voelker2019lmu.html&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://compneuro.uwaterloo.ca/publications/voelker2018.html&quot; target=&quot;_blank&quot;&gt;http://compneuro.uwaterloo.ca/publications/voelker2018.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://compneuro.uwaterloo.ca/publications/lu2019.html&quot; target=&quot;_blank&quot;&gt;http://compneuro.uwaterloo.ca/publications/lu2019.html&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=I5h-xjddzlY&quot; target=&quot;_blank&quot;&gt;https://www.youtube.com/watch?v=I5h-xjddzlY&lt;/a&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:49:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1681149883590-45cb36a77671f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>113</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#112 AVOIDING AGI APOCALYPSE - CONNOR LEAHY]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5

In this podcast with the legendary Connor Leahy (CEO Conjecture) recorded in Dec 2022, we discuss various topics related to artificial intelligence (AI), including AI alignment, the success of ChatGPT, the potential threats of artificial general intelligence (AGI), and the challenges of balancing research and product development at his company, Conjecture. He emphasizes the importance of empathy, dehumanizing our thinking to avoid anthropomorphic biases, and the value of real-world experiences in learning and personal growth. The conversation also covers the Orthogonality Thesis, AI preferences, the mystery of mode collapse, and the paradox of AI alignment.

Connor Leahy expresses concern about the rapid development of AI and the potential dangers it poses, especially as AI systems become more powerful and integrated into society. He argues that we need a better understanding of AI systems to ensure their safe and beneficial development. The discussion also touches on the concept of &quot;futuristic whack-a-mole,&quot; where futurists predict potential AGI threats, and others try to come up with solutions for those specific scenarios. However, the problem lies in the fact that there could be many more scenarios that neither party can think of, especially when dealing with a system that&#39;s smarter than humans.

https://www.linkedin.com/in/connor-j-leahy/https://twitter.com/NPCollapse

Interviewer: Dr. Tim Scarfe (Innovation CTO @ XRAI Glass https://xrai.glass/)

TOC:
The success of ChatGPT and its impact on the AI field [00:00:00]
Subjective experience [00:15:12]
AI Architectural discussion including RLHF [00:18:04]
The paradox of AI alignment and the future of AI in society [00:31:44]
The impact of AI on society and politics [00:36:11]
Future shock levels and the challenges of predicting the future [00:45:58]
Long termism and existential risk [00:48:23]
Consequentialism vs. deontology in rationalism [00:53:39]
The Rationalist Community and its Challenges [01:07:37]
AI Alignment and Conjecture [01:14:15]
Orthogonality Thesis and AI Preferences [01:17:01]
Challenges in AI Alignment [01:20:28]
Mechanistic Interpretability in Neural Networks [01:24:54]
Building Cleaner Neural Networks [01:31:36]
Cognitive horizons / The problem with rapid AI development [01:34:52]
Founding Conjecture and raising funds [01:39:36]
Inefficiencies in the market and seizing opportunities [01:45:38]
Charisma, authenticity, and leadership in startups [01:52:13]
Autistic culture and empathy [01:55:26]
Learning from real-world experiences [02:01:57]
Technical empathy and transhumanism [02:07:18]
Moral status and the limits of empathy [02:15:33]
Anthropomorphic Thinking and Consequentialism [02:17:42]
Conjecture: Balancing Research and Product Development [02:20:37]
Epistemology Team at Conjecture [02:31:07]
Interpretability and Deception in AGI [02:36:23]
Futuristic whack-a-mole and predicting AGI threats [02:38:27]

Refs:
1. OpenAI&#39;s ChatGPT: https://chat.openai.com/
2. The Mystery of Mode Collapse (Article): https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse 
3. The Rationalist Guide to the Galaxy https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795
5. Alfred Korzybski: https://en.wikipedia.org/wiki/Alfred_Korzybski
6. Instrumental Convergence: https://en.wikipedia.org/wiki/Instrumental_convergence
7. Orthogonality Thesis: https://en.wikipedia.org/wiki/Orthogonality_thesis
8. Brian Tomasik&#39;s Essays on Reducing Suffering: https://reducing-suffering.org/
9. Epistemological Framing for AI Alignment Research: https://www.lesswrong.com/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research
10. How to Defeat Mind readers: https://www.alignmentforum.org/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers
11. Society of mind: https://www.amazon.co.uk/Society-Mind-Marvin-Minsky/dp/0671607405</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/112-AVOIDING-AGI-APOCALYPSE---CONNOR-LEAHY-e21ji45</link>
			<guid isPermaLink="false">199fd723-606a-43ea-b8a4-d0adc373a9cb</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 02 Apr 2023 18:00:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/67798597/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-2%2F2b84c075-edce-9bd9-9a26-800f67c3441d.mp3" length="230736526" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5

In this podcast with the legendary Connor Leahy (CEO Conjecture) recorded in Dec 2022, we discuss various topics related to artificial intelligence (AI), including AI alignment, the success of ChatGPT, the potential threats of artificial general intelligence (AGI), and the challenges of balancing research and product development at his company, Conjecture. He emphasizes the importance of empathy, dehumanizing our thinking to avoid anthropomorphic biases, and the value of real-world experiences in learning and personal growth. The conversation also covers the Orthogonality Thesis, AI preferences, the mystery of mode collapse, and the paradox of AI alignment.

Connor Leahy expresses concern about the rapid development of AI and the potential dangers it poses, especially as AI systems become more powerful and integrated into society. He argues that we need a better understanding of AI systems to ensure their safe and beneficial development. The discussion also touches on the concept of &amp;quot;futuristic whack-a-mole,&amp;quot; where futurists predict potential AGI threats, and others try to come up with solutions for those specific scenarios. However, the problem lies in the fact that there could be many more scenarios that neither party can think of, especially when dealing with a system that&amp;#39;s smarter than humans.

https://www.linkedin.com/in/connor-j-leahy/https://twitter.com/NPCollapse

Interviewer: Dr. Tim Scarfe (Innovation CTO @ XRAI Glass https://xrai.glass/)

TOC:
The success of ChatGPT and its impact on the AI field [00:00:00]
Subjective experience [00:15:12]
AI Architectural discussion including RLHF [00:18:04]
The paradox of AI alignment and the future of AI in society [00:31:44]
The impact of AI on society and politics [00:36:11]
Future shock levels and the challenges of predicting the future [00:45:58]
Long termism and existential risk [00:48:23]
Consequentialism vs. deontology in rationalism [00:53:39]
The Rationalist Community and its Challenges [01:07:37]
AI Alignment and Conjecture [01:14:15]
Orthogonality Thesis and AI Preferences [01:17:01]
Challenges in AI Alignment [01:20:28]
Mechanistic Interpretability in Neural Networks [01:24:54]
Building Cleaner Neural Networks [01:31:36]
Cognitive horizons / The problem with rapid AI development [01:34:52]
Founding Conjecture and raising funds [01:39:36]
Inefficiencies in the market and seizing opportunities [01:45:38]
Charisma, authenticity, and leadership in startups [01:52:13]
Autistic culture and empathy [01:55:26]
Learning from real-world experiences [02:01:57]
Technical empathy and transhumanism [02:07:18]
Moral status and the limits of empathy [02:15:33]
Anthropomorphic Thinking and Consequentialism [02:17:42]
Conjecture: Balancing Research and Product Development [02:20:37]
Epistemology Team at Conjecture [02:31:07]
Interpretability and Deception in AGI [02:36:23]
Futuristic whack-a-mole and predicting AGI threats [02:38:27]

Refs:
1. OpenAI&amp;#39;s ChatGPT: https://chat.openai.com/
2. The Mystery of Mode Collapse (Article): https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse 
3. The Rationalist Guide to the Galaxy https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795
5. Alfred Korzybski: https://en.wikipedia.org/wiki/Alfred_Korzybski
6. Instrumental Convergence: https://en.wikipedia.org/wiki/Instrumental_convergence
7. Orthogonality Thesis: https://en.wikipedia.org/wiki/Orthogonality_thesis
8. Brian Tomasik&amp;#39;s Essays on Reducing Suffering: https://reducing-suffering.org/
9. Epistemological Framing for AI Alignment Research: https://www.lesswrong.com/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research
10. How to Defeat Mind readers: https://www.alignmentforum.org/posts/EhAbh2pQoAXkm9yor/circumventing-interpretability-how-to-defeat-mind-readers
11. Society of mind: https://www.amazon.co.uk/Society-Mind-Marvin-Minsky/dp/0671607405&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:40:13</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1680458385148-82a93eb5b3131.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>112</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#111 - AI moratorium, Eliezer Yudkowsky, AGI risk etc]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Send us a voice message which you want us to publish: https://podcasters.spotify.com/pod/show/machinelearningstreettalk/message

In a recent open letter, over 1500 individuals called for a six-month pause on the development of advanced AI systems, expressing concerns over the potential risks AI poses to society and humanity. However, there are issues with this approach, including global competition, unstoppable progress, potential benefits, and the need to manage risks instead of avoiding them.

Decision theorist Eliezer Yudkowsky took it a step further in a Time magazine article, calling for an indefinite and worldwide moratorium on Artificial General Intelligence (AGI) development, warning of potential catastrophe if AGI exceeds human intelligence. Yudkowsky urged for an immediate halt to all large AI training runs and the shutdown of major GPU clusters, calling for international cooperation to enforce these measures.

However, several counterarguments question the validity of Yudkowsky&#39;s concerns:
</p>
<p>1. Hard limits on AGI
2. Dismissing AI extinction risk
3. Collective action problem
4. Misplaced focus on AI threats

While the potential risks of AGI cannot be ignored, it is essential to consider various arguments and potential solutions before making drastic decisions. As AI continues to advance, it is crucial for researchers, policymakers, and society as a whole to engage in open and honest discussions about the potential consequences and the best path forward. With a balanced approach to AGI development, we may be able to harness its power for the betterment of humanity while mitigating its risks.

Eliezer Yudkowsky: https://en.wikipedia.org/wiki/Eliezer_Yudkowsky
Connor Leahy: https://twitter.com/NPCollapse (we will release that interview soon)
Gary Marcus: http://garymarcus.com/index.html
Tim Scarfe is the innovation CTO of XRAI Glass: https://xrai.glass/ 

Gary clip filmed at AIUK https://ai-uk.turing.ac.uk/programme/ and our appreciation to them for giving us a press pass. Check out their conference next year!
WIRED clip from Gary came from here: https://www.youtube.com/watch?v=Puo3VkPkNZ4

Refs:</p>
<p><br></p>
<p>Statement from the listed authors of Stochastic Parrots on the “AI pause” letterTimnit Gebru, Emily M. Bender, Angelina McMillan-Major, Margaret Mitchell</p>
<p>https://www.dair-institute.org/blog/letter-statement-March2023

Eliezer Yudkowsky on Lex: https://www.youtube.com/watch?v=AaTRHFaaPG8

Pause Giant AI Experiments: An Open Letter
https://futureoflife.org/open-letter/pause-giant-ai-experiments/

Pausing AI Developments Isn&#39;t Enough. We Need to Shut it All Down (Eliezer Yudkowsky)
https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/111---AI-moratorium--Eliezer-Yudkowsky--AGI-risk-etc-e21hnrq</link>
			<guid isPermaLink="false">ddcb4c8a-e882-454e-ab59-20d61068bd5e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 01 Apr 2023 18:45:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/67738938/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-3-1%2F321607929-44100-2-6282408081976.m4a" length="26161595" type="audio/x-m4a"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Send us a voice message which you want us to publish: https://podcasters.spotify.com/pod/show/machinelearningstreettalk/message

In a recent open letter, over 1500 individuals called for a six-month pause on the development of advanced AI systems, expressing concerns over the potential risks AI poses to society and humanity. However, there are issues with this approach, including global competition, unstoppable progress, potential benefits, and the need to manage risks instead of avoiding them.

Decision theorist Eliezer Yudkowsky took it a step further in a Time magazine article, calling for an indefinite and worldwide moratorium on Artificial General Intelligence (AGI) development, warning of potential catastrophe if AGI exceeds human intelligence. Yudkowsky urged for an immediate halt to all large AI training runs and the shutdown of major GPU clusters, calling for international cooperation to enforce these measures.

However, several counterarguments question the validity of Yudkowsky&amp;#39;s concerns:
&lt;/p&gt;
&lt;p&gt;1. Hard limits on AGI
2. Dismissing AI extinction risk
3. Collective action problem
4. Misplaced focus on AI threats

While the potential risks of AGI cannot be ignored, it is essential to consider various arguments and potential solutions before making drastic decisions. As AI continues to advance, it is crucial for researchers, policymakers, and society as a whole to engage in open and honest discussions about the potential consequences and the best path forward. With a balanced approach to AGI development, we may be able to harness its power for the betterment of humanity while mitigating its risks.

Eliezer Yudkowsky: https://en.wikipedia.org/wiki/Eliezer_Yudkowsky
Connor Leahy: https://twitter.com/NPCollapse (we will release that interview soon)
Gary Marcus: http://garymarcus.com/index.html
Tim Scarfe is the innovation CTO of XRAI Glass: https://xrai.glass/ 

Gary clip filmed at AIUK https://ai-uk.turing.ac.uk/programme/ and our appreciation to them for giving us a press pass. Check out their conference next year!
WIRED clip from Gary came from here: https://www.youtube.com/watch?v=Puo3VkPkNZ4

Refs:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Statement from the listed authors of Stochastic Parrots on the “AI pause” letterTimnit Gebru, Emily M. Bender, Angelina McMillan-Major, Margaret Mitchell&lt;/p&gt;
&lt;p&gt;https://www.dair-institute.org/blog/letter-statement-March2023

Eliezer Yudkowsky on Lex: https://www.youtube.com/watch?v=AaTRHFaaPG8

Pause Giant AI Experiments: An Open Letter
https://futureoflife.org/open-letter/pause-giant-ai-experiments/

Pausing AI Developments Isn&amp;#39;t Enough. We Need to Shut it All Down (Eliezer Yudkowsky)
https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:26:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode/4981699/4981699-1680374241577-4a15f2506500b.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>111</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#110 Dr. STEPHEN WOLFRAM - HUGE ChatGPT+Wolfram announcement!]]></title>
			<description><![CDATA[<p>HUGE ANNOUNCEMENT, CHATGPT+WOLFRAM! You saw it HERE first!

YT version: https://youtu.be/z5WZhCBRDpU
Support us! https://www.patreon.com/mlst </p>
<p>
MLST Discord: https://discord.gg/aNPkGUQtc5

Stephen&#39;s announcement post: https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/ 
OpenAI&#39;s announcement post: https://openai.com/blog/chatgpt-plugins 

In an era of technology and innovation, few individuals have left as indelible a mark on the fabric of modern science as our esteemed guest, Dr. Steven Wolfram. 

Dr. Wolfram is a renowned polymath who has made significant contributions to the fields of physics, computer science, and mathematics. A prodigious young man too, Wolfram earned a Ph.D. in theoretical physics from the California Institute of Technology by the age of 20. He became the youngest recipient of the prestigious MacArthur Fellowship at the age of 21.

Wolfram&#39;s groundbreaking computational tool, Mathematica, was launched in 1988 and has become a cornerstone for researchers and innovators worldwide. In 2002, he published &quot;A New Kind of Science,&quot; a paradigm-shifting work that explores the foundations of science through the lens of computational systems.

In 2009, Wolfram created Wolfram Alpha, a computational knowledge engine utilized by millions of users worldwide. His current focus is on the Wolfram Language, a powerful programming language designed to democratize access to cutting-edge technology.

Wolfram&#39;s numerous accolades include honorary doctorates and fellowships from prestigious institutions. As an influential thinker, Dr. Wolfram has dedicated his life to unraveling the mysteries of the universe and making computation accessible to all.

First of all... we have an announcement to make, you heard it FIRST here on MLST! ....

Intro [00:00:00]
Big announcement! Wolfram + ChatGPT! [00:02:57]
What does it mean to understand? [00:05:33]
Feeding information back into the model [00:13:48]
Semantics and cognitive categories [00:20:09]
Navigating the ruliad [00:23:50]
Computational irreducibility [00:31:39]
Conceivability and interestingness [00:38:43]
Human intelligible sciences [00:43:43]</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/110-Dr--STEPHEN-WOLFRAM---HUGE-ChatGPTWolfram-announcement-e210qda</link>
			<guid isPermaLink="false">e3662b59-6c66-4aa5-be32-c5ce9e42474e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 23 Mar 2023 20:59:11 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/67184490/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-23%2Fd509e7a4-bb85-d958-be7c-721bb996eb57.mp3" length="82805291" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;HUGE ANNOUNCEMENT, CHATGPT+WOLFRAM! You saw it HERE first!

YT version: https://youtu.be/z5WZhCBRDpU
Support us! https://www.patreon.com/mlst &lt;/p&gt;
&lt;p&gt;
MLST Discord: https://discord.gg/aNPkGUQtc5

Stephen&amp;#39;s announcement post: https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/ 
OpenAI&amp;#39;s announcement post: https://openai.com/blog/chatgpt-plugins 

In an era of technology and innovation, few individuals have left as indelible a mark on the fabric of modern science as our esteemed guest, Dr. Steven Wolfram. 

Dr. Wolfram is a renowned polymath who has made significant contributions to the fields of physics, computer science, and mathematics. A prodigious young man too, Wolfram earned a Ph.D. in theoretical physics from the California Institute of Technology by the age of 20. He became the youngest recipient of the prestigious MacArthur Fellowship at the age of 21.

Wolfram&amp;#39;s groundbreaking computational tool, Mathematica, was launched in 1988 and has become a cornerstone for researchers and innovators worldwide. In 2002, he published &amp;quot;A New Kind of Science,&amp;quot; a paradigm-shifting work that explores the foundations of science through the lens of computational systems.

In 2009, Wolfram created Wolfram Alpha, a computational knowledge engine utilized by millions of users worldwide. His current focus is on the Wolfram Language, a powerful programming language designed to democratize access to cutting-edge technology.

Wolfram&amp;#39;s numerous accolades include honorary doctorates and fellowships from prestigious institutions. As an influential thinker, Dr. Wolfram has dedicated his life to unraveling the mysteries of the universe and making computation accessible to all.

First of all... we have an announcement to make, you heard it FIRST here on MLST! ....

Intro [00:00:00]
Big announcement! Wolfram + ChatGPT! [00:02:57]
What does it mean to understand? [00:05:33]
Feeding information back into the model [00:13:48]
Semantics and cognitive categories [00:20:09]
Navigating the ruliad [00:23:50]
Computational irreducibility [00:31:39]
Conceivability and interestingness [00:38:43]
Human intelligible sciences [00:43:43]&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:29</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_episode400/4981699/4981699-1679605138536-5929853036825.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>110</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#109 - Dr. DAN MCQUILLAN - Resisting AI]]></title>
			<description><![CDATA[<p>YT version: https://youtu.be/P1j3VoKBxbc (references in pinned comment)
Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5

Dan McQuillan, a visionary in digital culture and social innovation, emphasizes the importance of understanding technology&#39;s complex relationship with society. As an academic at Goldsmiths, University of London, he fosters interdisciplinary collaboration and champions data-driven equity and ethical technology. Dan&#39;s career includes roles at Amnesty International and Social Innovation Camp, showcasing technology&#39;s potential to empower and bring about positive change. In this conversation, we discuss the challenges and opportunities at the intersection of technology and society, exploring the profound impact of our digital world.

Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>[00:00:00] Dan&#39;s background and journey to academia </p>
<p>[00:03:30] Dan&#39;s background and journey to academia </p>
<p>[00:04:10] Writing the book &quot;Resisting AI&quot; </p>
<p>[00:08:30] Necropolitics and its relation to AI </p>
<p>[00:10:06] AI as a new form of colonization </p>
<p>[00:12:57] LLMs as a new form of neo-techno-imperialism </p>
<p>[00:15:47] Technology for good and AGI&#39;s skewed worldview </p>
<p>[00:17:49] Transhumanism, eugenics, and intelligence </p>
<p>[00:20:45] Valuing differences (disability) and challenging societal norms </p>
<p>[00:26:08] Re-ontologizing and the philosophy of information </p>
<p>[00:28:19] New materialism and the impact of technology on society </p>
<p>[00:30:32] Intelligence, meaning, and materiality </p>
<p>[00:31:43] The constraints of physical laws and the importance of science </p>
<p>[00:32:44] Exploring possibilities to reduce suffering and increase well-being </p>
<p>[00:33:29] The division between meaning and material in our experiences </p>
<p>[00:35:36] Machine learning, data science, and neoplatonic approach to understanding reality </p>
<p>[00:37:56] Different understandings of cognition, thought, and consciousness </p>
<p>[00:39:15] Enactivism and its variants in cognitive science </p>
<p>[00:40:58] Jordan Peterson </p>
<p>[00:44:47] Relationism, relativism, and finding the correct relational framework </p>
<p>[00:47:42] Recognizing privilege and its impact on social interactions </p>
<p>[00:49:10] Intersectionality / Feminist thinking and the concept of care in social structures </p>
<p>[00:51:46] Intersectionality and its role in understanding social inequalities </p>
<p>[00:54:26] The entanglement of history, technology, and politics </p>
<p>[00:57:39] ChatGPT article - we come to bury ChatGPT </p>
<p>[00:59:41] Statistical pattern learning and convincing patterns in AI </p>
<p>[01:01:27] Anthropomorphization and understanding in AI </p>
<p>[01:03:26] AI in education and critical thinking </p>
<p>[01:06:09] European Union policies and trustable AI </p>
<p>[01:07:52] AI reliability and the halo effect </p>
<p>[01:09:26] AI as a tool enmeshed in society </p>
<p>[01:13:49] Luddites </p>
<p>[01:15:16] AI is a scam </p>
<p>[01:15:31] AI and Social Relations </p>
<p>[01:16:49] Invisible Labor in AI and Machine Learning </p>
<p>[01:21:09] Exploititative AI / alignment </p>
<p>[01:23:50] Science fiction AI / moral frameworks </p>
<p>[01:27:22] Discussing Stochastic Parrots and Nihilism </p>
<p>[01:30:36] Human Intelligence vs. Language Models </p>
<p>[01:32:22] Image Recognition and Emulation vs. Experience </p>
<p>[01:34:32] Thought Experiments and Philosophy in AI Ethics (mimicry) </p>
<p>[01:41:23] Abstraction, reduction, and grounding in reality </p>
<p>[01:43:13] Process philosophy and the possibility of change </p>
<p>[01:49:55] Mental health, AI, and epistemic injustice </p>
<p>[01:50:30] Hermeneutic injustice and gendered techniques </p>
<p>[01:53:57] AI and politics </p>
<p>[01:59:24] Epistemic injustice and testimonial injustice </p>
<p>[02:11:46] Fascism and AI discussion </p>
<p>[02:13:24] Violence in various systems </p>
<p>[02:16:52] Recognizing systemic violence </p>
<p>[02:22:35] Fascism in Today&#39;s Society </p>
<p>[02:33:33] Pace and Scale of Technological Change </p>
<p>[02:37:38] Alternative approaches to AI and society </p>
<p>[02:44:09] Self-Organization at Successive Scales / cybernetics </p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/109---Dr--DAN-MCQUILLAN---Resisting-AI-e20oplg</link>
			<guid isPermaLink="false">a0488e11-6e2f-47f5-87ee-6d6c49374872</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 20 Mar 2023 08:56:43 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/66921584/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-20%2Feca0ead2-ee1a-d115-3a84-c60336c4008d.mp3" length="246318444" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;YT version: https://youtu.be/P1j3VoKBxbc (references in pinned comment)
Support us! https://www.patreon.com/mlst 
MLST Discord: https://discord.gg/aNPkGUQtc5

Dan McQuillan, a visionary in digital culture and social innovation, emphasizes the importance of understanding technology&amp;#39;s complex relationship with society. As an academic at Goldsmiths, University of London, he fosters interdisciplinary collaboration and champions data-driven equity and ethical technology. Dan&amp;#39;s career includes roles at Amnesty International and Social Innovation Camp, showcasing technology&amp;#39;s potential to empower and bring about positive change. In this conversation, we discuss the challenges and opportunities at the intersection of technology and society, exploring the profound impact of our digital world.

Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Dan&amp;#39;s background and journey to academia &lt;/p&gt;
&lt;p&gt;[00:03:30] Dan&amp;#39;s background and journey to academia &lt;/p&gt;
&lt;p&gt;[00:04:10] Writing the book &amp;quot;Resisting AI&amp;quot; &lt;/p&gt;
&lt;p&gt;[00:08:30] Necropolitics and its relation to AI &lt;/p&gt;
&lt;p&gt;[00:10:06] AI as a new form of colonization &lt;/p&gt;
&lt;p&gt;[00:12:57] LLMs as a new form of neo-techno-imperialism &lt;/p&gt;
&lt;p&gt;[00:15:47] Technology for good and AGI&amp;#39;s skewed worldview &lt;/p&gt;
&lt;p&gt;[00:17:49] Transhumanism, eugenics, and intelligence &lt;/p&gt;
&lt;p&gt;[00:20:45] Valuing differences (disability) and challenging societal norms &lt;/p&gt;
&lt;p&gt;[00:26:08] Re-ontologizing and the philosophy of information &lt;/p&gt;
&lt;p&gt;[00:28:19] New materialism and the impact of technology on society &lt;/p&gt;
&lt;p&gt;[00:30:32] Intelligence, meaning, and materiality &lt;/p&gt;
&lt;p&gt;[00:31:43] The constraints of physical laws and the importance of science &lt;/p&gt;
&lt;p&gt;[00:32:44] Exploring possibilities to reduce suffering and increase well-being &lt;/p&gt;
&lt;p&gt;[00:33:29] The division between meaning and material in our experiences &lt;/p&gt;
&lt;p&gt;[00:35:36] Machine learning, data science, and neoplatonic approach to understanding reality &lt;/p&gt;
&lt;p&gt;[00:37:56] Different understandings of cognition, thought, and consciousness &lt;/p&gt;
&lt;p&gt;[00:39:15] Enactivism and its variants in cognitive science &lt;/p&gt;
&lt;p&gt;[00:40:58] Jordan Peterson &lt;/p&gt;
&lt;p&gt;[00:44:47] Relationism, relativism, and finding the correct relational framework &lt;/p&gt;
&lt;p&gt;[00:47:42] Recognizing privilege and its impact on social interactions &lt;/p&gt;
&lt;p&gt;[00:49:10] Intersectionality / Feminist thinking and the concept of care in social structures &lt;/p&gt;
&lt;p&gt;[00:51:46] Intersectionality and its role in understanding social inequalities &lt;/p&gt;
&lt;p&gt;[00:54:26] The entanglement of history, technology, and politics &lt;/p&gt;
&lt;p&gt;[00:57:39] ChatGPT article - we come to bury ChatGPT &lt;/p&gt;
&lt;p&gt;[00:59:41] Statistical pattern learning and convincing patterns in AI &lt;/p&gt;
&lt;p&gt;[01:01:27] Anthropomorphization and understanding in AI &lt;/p&gt;
&lt;p&gt;[01:03:26] AI in education and critical thinking &lt;/p&gt;
&lt;p&gt;[01:06:09] European Union policies and trustable AI &lt;/p&gt;
&lt;p&gt;[01:07:52] AI reliability and the halo effect &lt;/p&gt;
&lt;p&gt;[01:09:26] AI as a tool enmeshed in society &lt;/p&gt;
&lt;p&gt;[01:13:49] Luddites &lt;/p&gt;
&lt;p&gt;[01:15:16] AI is a scam &lt;/p&gt;
&lt;p&gt;[01:15:31] AI and Social Relations &lt;/p&gt;
&lt;p&gt;[01:16:49] Invisible Labor in AI and Machine Learning &lt;/p&gt;
&lt;p&gt;[01:21:09] Exploititative AI / alignment &lt;/p&gt;
&lt;p&gt;[01:23:50] Science fiction AI / moral frameworks &lt;/p&gt;
&lt;p&gt;[01:27:22] Discussing Stochastic Parrots and Nihilism &lt;/p&gt;
&lt;p&gt;[01:30:36] Human Intelligence vs. Language Models &lt;/p&gt;
&lt;p&gt;[01:32:22] Image Recognition and Emulation vs. Experience &lt;/p&gt;
&lt;p&gt;[01:34:32] Thought Experiments and Philosophy in AI Ethics (mimicry) &lt;/p&gt;
&lt;p&gt;[01:41:23] Abstraction, reduction, and grounding in reality &lt;/p&gt;
&lt;p&gt;[01:43:13] Process philosophy and the possibility of change &lt;/p&gt;
&lt;p&gt;[01:49:55] Mental health, AI, and epistemic injustice &lt;/p&gt;
&lt;p&gt;[01:50:30] Hermeneutic injustice and gendered techniques &lt;/p&gt;
&lt;p&gt;[01:53:57] AI and politics &lt;/p&gt;
&lt;p&gt;[01:59:24] Epistemic injustice and testimonial injustice &lt;/p&gt;
&lt;p&gt;[02:11:46] Fascism and AI discussion &lt;/p&gt;
&lt;p&gt;[02:13:24] Violence in various systems &lt;/p&gt;
&lt;p&gt;[02:16:52] Recognizing systemic violence &lt;/p&gt;
&lt;p&gt;[02:22:35] Fascism in Today&amp;#39;s Society &lt;/p&gt;
&lt;p&gt;[02:33:33] Pace and Scale of Technological Change &lt;/p&gt;
&lt;p&gt;[02:37:38] Alternative approaches to AI and society &lt;/p&gt;
&lt;p&gt;[02:44:09] Self-Organization at Successive Scales / cybernetics &lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:51:02</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1679302568606-7a5f7cedd12d4.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>109</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#108 - Dr. JOEL LEHMAN - Machine Love [Staff Favourite]]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst &nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p><br></p>
<p>We are honoured to welcome Dr. Joel Lehman, an eminent machine learning research scientist, whose work in AI safety, reinforcement learning, creative open-ended search algorithms, and indeed the philosophy of open-endedness and abandoning objectives has paved the way for innovative ideas that challenge our preconceptions and inspire new visions for the future.</p>
<p>Dr. Lehman's thought-provoking book, "Why Greatness Cannot Be Planned" penned with with our MLST favourite Professor Kenneth Stanley has left an indelible mark on the field and profoundly impacted the way we view innovation and the serendipitous nature of discovery. Those of you who haven't watched our special edition show on that, should do so at your earliest convenience! Building upon this foundation, Dr. Lehman has ventured into the domain of AI systems that embody principles of love, care, responsibility, respect, and knowledge, drawing from the works of Maslow, Erich Fromm, and positive psychology.</p>
<p><br></p>
<p>YT version: https://youtu.be/23-TXgJEv-Q</p>
<p><br></p>
<p>http://joellehman.com/</p>
<p>https://twitter.com/joelbot3000</p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>Intro [00:00:00]</p>
<p>Model [00:04:26]</p>
<p>Intro and Paper Intro [00:08:52]</p>
<p>Subjectivity [00:16:07]</p>
<p>Reflections on Greatness Book [00:19:30]</p>
<p>Representing Subjectivity [00:29:24]</p>
<p>Nagal's Bat [00:31:49]</p>
<p>Abstraction [00:38:58]</p>
<p>Love as Action Rather Than Feeling [00:42:58]</p>
<p>Reontologisation [00:57:38]</p>
<p>Self Help [01:04:15]</p>
<p>Meditation [01:09:02]</p>
<p>The Human Reward Function / Effective... [01:16:52]</p>
<p>Machine Hate [01:28:32]</p>
<p>Societal Harms [01:31:41]</p>
<p>Lenses We Use Obscuring Reality [01:56:36]</p>
<p>Meta Optimisation and Evolution [02:03:14]</p>
<p>Conclusion [02:07:06]</p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>What Is It Like to Be a Bat? (Thomas Nagel)</p>
<p>https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf</p>
<p><br></p>
<p>Why Greatness Cannot Be Planned: The Myth of the Objective (Kenneth O. Stanley and Joel Lehman)</p>
<p>https://link.springer.com/book/10.1007/978-3-319-15524-1&nbsp;</p>
<p><br></p>
<p>Machine Love (Joel Lehman)</p>
<p>https://arxiv.org/abs/2302.09248&nbsp;</p>
<p><br></p>
<p>How effective altruists ignored risk (Carla Cremer)</p>
<p>https://www.vox.com/future-perfect/23569519/effective-altrusim-sam-bankman-fried-will-macaskill-ea-risk-decentralization-philanthropy</p>
<p><br></p>
<p>Philosophy tube - The Rich Have Their Own Ethics: Effective Altruism</p>
<p>https://www.youtube.com/watch?v=Lm0vHQYKI-Y</p>
<p><br></p>
<p>Abandoning Objectives: Evolution through the Search for Novelty Alone (Joel Lehman and Kenneth O. Stanley)</p>
<p>https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/108---Dr--JOEL-LEHMAN---Machine-Love-Staff-Favourite-e20i0qt</link>
			<guid isPermaLink="false">9b1546d3-5786-4c63-8e53-37e2a4dd4a6d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 16 Mar 2023 16:24:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/66699549/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-16%2F712fbb29-0f50-e76d-1e7b-aee556e1f077.mp3" length="186685632" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst &amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We are honoured to welcome Dr. Joel Lehman, an eminent machine learning research scientist, whose work in AI safety, reinforcement learning, creative open-ended search algorithms, and indeed the philosophy of open-endedness and abandoning objectives has paved the way for innovative ideas that challenge our preconceptions and inspire new visions for the future.&lt;/p&gt;
&lt;p&gt;Dr. Lehman&apos;s thought-provoking book, &quot;Why Greatness Cannot Be Planned&quot; penned with with our MLST favourite Professor Kenneth Stanley has left an indelible mark on the field and profoundly impacted the way we view innovation and the serendipitous nature of discovery. Those of you who haven&apos;t watched our special edition show on that, should do so at your earliest convenience! Building upon this foundation, Dr. Lehman has ventured into the domain of AI systems that embody principles of love, care, responsibility, respect, and knowledge, drawing from the works of Maslow, Erich Fromm, and positive psychology.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/23-TXgJEv-Q&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;http://joellehman.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/joelbot3000&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;Intro [00:00:00]&lt;/p&gt;
&lt;p&gt;Model [00:04:26]&lt;/p&gt;
&lt;p&gt;Intro and Paper Intro [00:08:52]&lt;/p&gt;
&lt;p&gt;Subjectivity [00:16:07]&lt;/p&gt;
&lt;p&gt;Reflections on Greatness Book [00:19:30]&lt;/p&gt;
&lt;p&gt;Representing Subjectivity [00:29:24]&lt;/p&gt;
&lt;p&gt;Nagal&apos;s Bat [00:31:49]&lt;/p&gt;
&lt;p&gt;Abstraction [00:38:58]&lt;/p&gt;
&lt;p&gt;Love as Action Rather Than Feeling [00:42:58]&lt;/p&gt;
&lt;p&gt;Reontologisation [00:57:38]&lt;/p&gt;
&lt;p&gt;Self Help [01:04:15]&lt;/p&gt;
&lt;p&gt;Meditation [01:09:02]&lt;/p&gt;
&lt;p&gt;The Human Reward Function / Effective... [01:16:52]&lt;/p&gt;
&lt;p&gt;Machine Hate [01:28:32]&lt;/p&gt;
&lt;p&gt;Societal Harms [01:31:41]&lt;/p&gt;
&lt;p&gt;Lenses We Use Obscuring Reality [01:56:36]&lt;/p&gt;
&lt;p&gt;Meta Optimisation and Evolution [02:03:14]&lt;/p&gt;
&lt;p&gt;Conclusion [02:07:06]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What Is It Like to Be a Bat? (Thomas Nagel)&lt;/p&gt;
&lt;p&gt;https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Why Greatness Cannot Be Planned: The Myth of the Objective (Kenneth O. Stanley and Joel Lehman)&lt;/p&gt;
&lt;p&gt;https://link.springer.com/book/10.1007/978-3-319-15524-1&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Machine Love (Joel Lehman)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2302.09248&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;How effective altruists ignored risk (Carla Cremer)&lt;/p&gt;
&lt;p&gt;https://www.vox.com/future-perfect/23569519/effective-altrusim-sam-bankman-fried-will-macaskill-ea-risk-decentralization-philanthropy&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Philosophy tube - The Rich Have Their Own Ethics: Effective Altruism&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=Lm0vHQYKI-Y&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Abandoning Objectives: Evolution through the Search for Novelty Alone (Joel Lehman and Kenneth O. Stanley)&lt;/p&gt;
&lt;p&gt;https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:09:38</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1678983819088-05e46dfd02294.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>108</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#107 - Dr. RAPHAËL MILLIÈRE - Linguistics, Theory of Mind, Grounding]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Dr. Raphaël Millière is the 2020 Robert A. Burt Presidential Scholar in Society and Neuroscience in the Center for Science and Society, and a Lecturer in the Philosophy Department at Columbia University. His research draws from his expertise in philosophy and cognitive science to explore the implications of recent progress in deep learning for models of human cognition, as well as various issues in ethics and aesthetics. He is also investigating what underlies the capacity to represent oneself as oneself at a fundamental level, in humans and non-human animals; as well as the role that self-representation plays in perception, action, and memory. In a world where technology is rapidly advancing, Dr. Millière is striving to gain a better understanding of how artificial neural networks work, and to establish fair and meaningful comparisons between humans and machines in various domains in order to shed light on the implications of artificial intelligence for our lives.</p>
<p>https://www.raphaelmilliere.com/</p>
<p>https://twitter.com/raphaelmilliere</p>
<p><br></p>
<p>Here is a version with hesitation sounds like "um" removed if you prefer (I didn't notice them personally): https://share.descript.com/view/aGelyTl2xpN</p>
<p>YT: https://www.youtube.com/watch?v=fhn6ZtD6XeE</p>
<p><br></p>
<p>TOC:</p>
<p>Intro to Raphael [00:00:00]</p>
<p>Intro: Moving Beyond Mimicry in Artificial Intelligence (Raphael Millière) [00:01:18]</p>
<p>Show Kick off [00:07:10]</p>
<p>LLMs [00:08:37]</p>
<p>Semantic Competence/Understanding [00:18:28]</p>
<p>Forming Analogies/JPG Compression Article [00:30:17]</p>
<p>Compositional Generalisation [00:37:28]</p>
<p>Systematicity [00:47:08]</p>
<p>Language of Thought [00:51:28]</p>
<p>Bigbench (Conceptual Combinations) [00:57:37]</p>
<p>Symbol Grounding [01:11:13]</p>
<p>World Models [01:26:43]</p>
<p>Theory of Mind [01:30:57]</p>
<p><br></p>
<p>Refs (this is truncated, full list on YT video description):</p>
<p><br></p>
<p>Moving Beyond Mimicry in Artificial Intelligence (Raphael Millière)</p>
<p>https://nautil.us/moving-beyond-mimicry-in-artificial-intelligence-238504/</p>
<p><br></p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 (Bender et al)</p>
<p>https://dl.acm.org/doi/10.1145/3442188.3445922</p>
<p><br></p>
<p>ChatGPT Is a Blurry JPEG of the Web (Ted Chiang)</p>
<p>https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web</p>
<p><br></p>
<p>The Debate Over Understanding in AI's Large Language Models (Melanie Mitchell)</p>
<p>https://arxiv.org/abs/2210.13966</p>
<p><br></p>
<p>Talking About Large Language Models (Murray Shanahan)</p>
<p>https://arxiv.org/abs/2212.03551</p>
<p><br></p>
<p>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data (Bender)</p>
<p>https://aclanthology.org/2020.acl-main.463/</p>
<p><br></p>
<p>The symbol grounding problem (Stevan Harnad)</p>
<p>https://arxiv.org/html/cs/9906002</p>
<p><br></p>
<p>Why the Abstraction and Reasoning Corpus is interesting and important for AI (Mitchell)</p>
<p>https://aiguide.substack.com/p/why-the-abstraction-and-reasoning</p>
<p><br></p>
<p>Linguistic relativity (Sapir–Whorf hypothesis)</p>
<p>https://en.wikipedia.org/wiki/Linguistic_relativity</p>
<p><br></p>
<p>Cooperative principle (Grice's four maxims of conversation - quantity, quality, relation, and manner)</p>
<p>https://en.wikipedia.org/wiki/Cooperative_principle</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/107---Dr--RAPHAL-MILLIRE---Linguistics--Theory-of-Mind--Grounding-e20c60h</link>
			<guid isPermaLink="false">b2279203-a90c-423d-92af-4db59d56c077</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 13 Mar 2023 22:45:03 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/66508241/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-13%2F2d22c6b6-f49f-3546-40ff-1e6fb1a6d1bd.mp3" length="149639722" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Dr. Raphaël Millière is the 2020 Robert A. Burt Presidential Scholar in Society and Neuroscience in the Center for Science and Society, and a Lecturer in the Philosophy Department at Columbia University. His research draws from his expertise in philosophy and cognitive science to explore the implications of recent progress in deep learning for models of human cognition, as well as various issues in ethics and aesthetics. He is also investigating what underlies the capacity to represent oneself as oneself at a fundamental level, in humans and non-human animals; as well as the role that self-representation plays in perception, action, and memory. In a world where technology is rapidly advancing, Dr. Millière is striving to gain a better understanding of how artificial neural networks work, and to establish fair and meaningful comparisons between humans and machines in various domains in order to shed light on the implications of artificial intelligence for our lives.&lt;/p&gt;
&lt;p&gt;https://www.raphaelmilliere.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/raphaelmilliere&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Here is a version with hesitation sounds like &quot;um&quot; removed if you prefer (I didn&apos;t notice them personally): https://share.descript.com/view/aGelyTl2xpN&lt;/p&gt;
&lt;p&gt;YT: https://www.youtube.com/watch?v=fhn6ZtD6XeE&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;Intro to Raphael [00:00:00]&lt;/p&gt;
&lt;p&gt;Intro: Moving Beyond Mimicry in Artificial Intelligence (Raphael Millière) [00:01:18]&lt;/p&gt;
&lt;p&gt;Show Kick off [00:07:10]&lt;/p&gt;
&lt;p&gt;LLMs [00:08:37]&lt;/p&gt;
&lt;p&gt;Semantic Competence/Understanding [00:18:28]&lt;/p&gt;
&lt;p&gt;Forming Analogies/JPG Compression Article [00:30:17]&lt;/p&gt;
&lt;p&gt;Compositional Generalisation [00:37:28]&lt;/p&gt;
&lt;p&gt;Systematicity [00:47:08]&lt;/p&gt;
&lt;p&gt;Language of Thought [00:51:28]&lt;/p&gt;
&lt;p&gt;Bigbench (Conceptual Combinations) [00:57:37]&lt;/p&gt;
&lt;p&gt;Symbol Grounding [01:11:13]&lt;/p&gt;
&lt;p&gt;World Models [01:26:43]&lt;/p&gt;
&lt;p&gt;Theory of Mind [01:30:57]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Refs (this is truncated, full list on YT video description):&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Moving Beyond Mimicry in Artificial Intelligence (Raphael Millière)&lt;/p&gt;
&lt;p&gt;https://nautil.us/moving-beyond-mimicry-in-artificial-intelligence-238504/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 (Bender et al)&lt;/p&gt;
&lt;p&gt;https://dl.acm.org/doi/10.1145/3442188.3445922&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;ChatGPT Is a Blurry JPEG of the Web (Ted Chiang)&lt;/p&gt;
&lt;p&gt;https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Debate Over Understanding in AI&apos;s Large Language Models (Melanie Mitchell)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.13966&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Talking About Large Language Models (Murray Shanahan)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2212.03551&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data (Bender)&lt;/p&gt;
&lt;p&gt;https://aclanthology.org/2020.acl-main.463/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The symbol grounding problem (Stevan Harnad)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/html/cs/9906002&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Why the Abstraction and Reasoning Corpus is interesting and important for AI (Mitchell)&lt;/p&gt;
&lt;p&gt;https://aiguide.substack.com/p/why-the-abstraction-and-reasoning&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Linguistic relativity (Sapir–Whorf hypothesis)&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Linguistic_relativity&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Cooperative principle (Grice&apos;s four maxims of conversation - quantity, quality, relation, and manner)&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Cooperative_principle&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:43:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1678747425085-0ce9c95a1bfd7.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>107</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#106 - Prof. KARL FRISTON 3.0 - Collective Intelligence [Special Edition]]]></title>
			<description><![CDATA[<p>This show is sponsored by Numerai, please visit them here with our sponsor link (we would really appreciate it) http://numer.ai/mlst&nbsp;</p>
<p>Prof. Karl Friston recently proposed a vision of artificial intelligence that goes beyond machines and algorithms, and embraces humans and nature as part of a cyber-physical ecosystem of intelligence. This vision is based on the principle of active inference, which states that intelligent systems can learn from their observations and act on their environment to reduce uncertainty and achieve their goals. This leads to a formal account of collective intelligence that rests on shared narratives and goals.&nbsp;</p>
<p>To realize this vision, Friston suggests developing a shared hyper-spatial modelling language and transaction protocol, as well as novel methods for measuring and optimizing collective intelligence. This could harness the power of artificial intelligence for the common good, without compromising human dignity or autonomy. It also challenges us to rethink our relationship with technology, nature, and each other, and invites us to join a global community of sense-makers who are curious about the world and eager to improve it.</p>
<p><br></p>
<p>YT version: https://www.youtube.com/watch?v=V_VXOdf1NMw</p>
<p>Support us! https://www.patreon.com/mlst&nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p><br></p>
<p>TOC:&nbsp;</p>
<p>Intro [00:00:00]</p>
<p>Numerai (Sponsor segment) [00:07:10]</p>
<p>Designing Ecosystems of Intelligence from First Principles (Friston et al) [00:09:48]</p>
<p>Information / Infosphere and human agency [00:18:30]</p>
<p>Intelligence [00:31:38]</p>
<p>Reductionism [00:39:36]</p>
<p>Universalism [00:44:46]</p>
<p>Emergence [00:54:23]</p>
<p>Markov blankets [01:02:11]</p>
<p>Whole part relationships / structure learning [01:22:33]</p>
<p>Enactivism [01:29:23]</p>
<p>Knowledge and Language [01:43:53]</p>
<p>ChatGPT [01:50:56]</p>
<p>Ethics (is-ought) [02:07:55]</p>
<p>Can people be evil? [02:35:06]</p>
<p>Ethics in Al, subjectiveness [02:39:05]</p>
<p>Final thoughts [02:57:00]</p>
<p><br></p>
<p>References:</p>
<p>Designing Ecosystems of Intelligence from First Principles (Friston et al)</p>
<p>https://arxiv.org/abs/2212.01354</p>
<p><br></p>
<p>GLOM - How to represent part-whole hierarchies in a neural network (Hinton)</p>
<p>https://arxiv.org/pdf/2102.12627.pdf</p>
<p><br></p>
<p>Seven Brief Lessons on Physics (Carlo Rovelli)</p>
<p>https://www.amazon.co.uk/Seven-Brief-Lessons-Physics-Rovelli/dp/0141981725</p>
<p><br></p>
<p>How Emotions Are Made: The Secret Life of the Brain (Lisa Feldman Barrett)</p>
<p>https://www.amazon.co.uk/How-Emotions-Are-Made-Secret/dp/B01N3D4OON</p>
<p><br></p>
<p>Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?) (Karl Friston)</p>
<p>https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full</p>
<p><br></p>
<p>Integrated information theory (Giulio Tononi)</p>
<p>https://en.wikipedia.org/wiki/Integrated_information_theory</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/106---Prof--KARL-FRISTON-3-0---Collective-Intelligence-Special-Edition-e208f50</link>
			<guid isPermaLink="false">c576dd50-4533-4268-9e24-4ba38059ee76</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 11 Mar 2023 20:42:19 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/66386528/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-11%2F50d0286c-1634-44a2-466f-3eb6a7b11f8a.mp3" length="258260544" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This show is sponsored by Numerai, please visit them here with our sponsor link (we would really appreciate it) http://numer.ai/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Prof. Karl Friston recently proposed a vision of artificial intelligence that goes beyond machines and algorithms, and embraces humans and nature as part of a cyber-physical ecosystem of intelligence. This vision is based on the principle of active inference, which states that intelligent systems can learn from their observations and act on their environment to reduce uncertainty and achieve their goals. This leads to a formal account of collective intelligence that rests on shared narratives and goals.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;To realize this vision, Friston suggests developing a shared hyper-spatial modelling language and transaction protocol, as well as novel methods for measuring and optimizing collective intelligence. This could harness the power of artificial intelligence for the common good, without compromising human dignity or autonomy. It also challenges us to rethink our relationship with technology, nature, and each other, and invites us to join a global community of sense-makers who are curious about the world and eager to improve it.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://www.youtube.com/watch?v=V_VXOdf1NMw&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Intro [00:00:00]&lt;/p&gt;
&lt;p&gt;Numerai (Sponsor segment) [00:07:10]&lt;/p&gt;
&lt;p&gt;Designing Ecosystems of Intelligence from First Principles (Friston et al) [00:09:48]&lt;/p&gt;
&lt;p&gt;Information / Infosphere and human agency [00:18:30]&lt;/p&gt;
&lt;p&gt;Intelligence [00:31:38]&lt;/p&gt;
&lt;p&gt;Reductionism [00:39:36]&lt;/p&gt;
&lt;p&gt;Universalism [00:44:46]&lt;/p&gt;
&lt;p&gt;Emergence [00:54:23]&lt;/p&gt;
&lt;p&gt;Markov blankets [01:02:11]&lt;/p&gt;
&lt;p&gt;Whole part relationships / structure learning [01:22:33]&lt;/p&gt;
&lt;p&gt;Enactivism [01:29:23]&lt;/p&gt;
&lt;p&gt;Knowledge and Language [01:43:53]&lt;/p&gt;
&lt;p&gt;ChatGPT [01:50:56]&lt;/p&gt;
&lt;p&gt;Ethics (is-ought) [02:07:55]&lt;/p&gt;
&lt;p&gt;Can people be evil? [02:35:06]&lt;/p&gt;
&lt;p&gt;Ethics in Al, subjectiveness [02:39:05]&lt;/p&gt;
&lt;p&gt;Final thoughts [02:57:00]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Designing Ecosystems of Intelligence from First Principles (Friston et al)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2212.01354&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;GLOM - How to represent part-whole hierarchies in a neural network (Hinton)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2102.12627.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Seven Brief Lessons on Physics (Carlo Rovelli)&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Seven-Brief-Lessons-Physics-Rovelli/dp/0141981725&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;How Emotions Are Made: The Secret Life of the Brain (Lisa Feldman Barrett)&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/How-Emotions-Are-Made-Secret/dp/B01N3D4OON&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?) (Karl Friston)&lt;/p&gt;
&lt;p&gt;https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Integrated information theory (Giulio Tononi)&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Integrated_information_theory&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:59:20</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1678567320092-f2f627b6e381e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>106</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#105 - Dr. MICHAEL OLIVER [CSO - Numerai]]]></title>
			<description><![CDATA[<p>Access Numerai here: http://numer.ai/mlst</p>
<p><br></p>
<p>Michael Oliver is the Chief Scientist at Numerai, a hedge fund that crowdsources machine learning models from data scientists. He has a PhD in Computational Neuroscience from UC Berkeley and was a postdoctoral researcher at the Allen Institute for Brain Science before joining Numerai in 2020. He is also the host of Numerai Quant Club, a YouTube series where he discusses Numerai’s research, data and challenges.</p>
<p><br></p>
<p>YT version: https://youtu.be/61s8lLU7sFg</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction to Michael and Numerai</p>
<p>[00:02:03] Understanding / new Bing</p>
<p>[00:22:47] Quant vs Neuroscience</p>
<p>[00:36:43] Role of language in cognition and planning, and subjective...&nbsp;</p>
<p>[00:45:47] Boundaries in finance modelling</p>
<p>[00:48:00] Numerai</p>
<p>[00:57:37] Aggregation systems</p>
<p>[01:00:52] Getting started on Numeral</p>
<p>[01:03:21] What models are people using</p>
<p>[01:04:23] Numerai Problem Setup</p>
<p>[01:05:49] Regimes in financial data and quant talk</p>
<p>[01:11:18] Esoteric approaches used on Numeral?</p>
<p>[01:13:59] &nbsp;Curse of dimensionality</p>
<p>[01:16:32] Metrics</p>
<p>[01:19:10] Outro</p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>Growing Neural Cellular Automata (Alexander Mordvintsev)</p>
<p>https://distill.pub/2020/growing-ca/</p>
<p><br></p>
<p>A Thousand Brains: A New Theory of Intelligence (Jeff Hawkins)</p>
<p>https://www.amazon.fr/Thousand-Brains-New-Theory-Intelligence/dp/1541675819</p>
<p><br></p>
<p>Perceptual Neuroscience: The Cerebral Cortex (Vernon B. Mountcastle)</p>
<p>https://www.amazon.ca/Perceptual-Neuroscience-Cerebral-Vernon-Mountcastle/dp/0674661885</p>
<p><br></p>
<p>Numerai Quant Club with Michael Oliver</p>
<p>https://www.youtube.com/watch?v=eLIxarbDXuQ&amp;list=PLz3D6SeXhT3tTu8rhZmjwDZpkKi-UPO1F</p>
<p><br></p>
<p>Numerai YT channel</p>
<p>https://www.youtube.com/@Numerai/featured</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst&nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/105---Dr--MICHAEL-OLIVER-CSO---Numerai-e1vs6u5</link>
			<guid isPermaLink="false">769e06d7-c240-4c95-83d8-a11017046844</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 04 Mar 2023 21:54:48 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/65984901/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-2-4%2F6e350f12-9f3b-d80e-bc4a-dc342b58fe63.mp3" length="116219520" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Access Numerai here: http://numer.ai/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Michael Oliver is the Chief Scientist at Numerai, a hedge fund that crowdsources machine learning models from data scientists. He has a PhD in Computational Neuroscience from UC Berkeley and was a postdoctoral researcher at the Allen Institute for Brain Science before joining Numerai in 2020. He is also the host of Numerai Quant Club, a YouTube series where he discusses Numerai’s research, data and challenges.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/61s8lLU7sFg&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction to Michael and Numerai&lt;/p&gt;
&lt;p&gt;[00:02:03] Understanding / new Bing&lt;/p&gt;
&lt;p&gt;[00:22:47] Quant vs Neuroscience&lt;/p&gt;
&lt;p&gt;[00:36:43] Role of language in cognition and planning, and subjective...&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:45:47] Boundaries in finance modelling&lt;/p&gt;
&lt;p&gt;[00:48:00] Numerai&lt;/p&gt;
&lt;p&gt;[00:57:37] Aggregation systems&lt;/p&gt;
&lt;p&gt;[01:00:52] Getting started on Numeral&lt;/p&gt;
&lt;p&gt;[01:03:21] What models are people using&lt;/p&gt;
&lt;p&gt;[01:04:23] Numerai Problem Setup&lt;/p&gt;
&lt;p&gt;[01:05:49] Regimes in financial data and quant talk&lt;/p&gt;
&lt;p&gt;[01:11:18] Esoteric approaches used on Numeral?&lt;/p&gt;
&lt;p&gt;[01:13:59] &amp;nbsp;Curse of dimensionality&lt;/p&gt;
&lt;p&gt;[01:16:32] Metrics&lt;/p&gt;
&lt;p&gt;[01:19:10] Outro&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Growing Neural Cellular Automata (Alexander Mordvintsev)&lt;/p&gt;
&lt;p&gt;https://distill.pub/2020/growing-ca/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Thousand Brains: A New Theory of Intelligence (Jeff Hawkins)&lt;/p&gt;
&lt;p&gt;https://www.amazon.fr/Thousand-Brains-New-Theory-Intelligence/dp/1541675819&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Perceptual Neuroscience: The Cerebral Cortex (Vernon B. Mountcastle)&lt;/p&gt;
&lt;p&gt;https://www.amazon.ca/Perceptual-Neuroscience-Cerebral-Vernon-Mountcastle/dp/0674661885&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Numerai Quant Club with Michael Oliver&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=eLIxarbDXuQ&amp;amp;list=PLz3D6SeXhT3tTu8rhZmjwDZpkKi-UPO1F&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Numerai YT channel&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/@Numerai/featured&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:20:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1677966866904-8e77188145fd8.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>105</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#104 - Prof. CHRIS SUMMERFIELD - Natural General Intelligence [SPECIAL EDITION]]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst &nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p><br></p>
<p>Christopher Summerfield, Department of Experimental Psychology, University of Oxford is a Professor of Cognitive Neuroscience at the University of Oxford and a Research Scientist at Deepmind UK. His work focusses on the neural and computational mechanisms by which humans make decisions.</p>
<p>Chris has just released an incredible new book on AI called "Natural General Intelligence". It's my favourite book on AI I have read so so far.&nbsp;</p>
<p>The book explores the algorithms and architectures that are driving progress in AI research, and discusses intelligence in the language of psychology and biology, using examples and analogies to be comprehensible to a wide audience. It also tackles longstanding theoretical questions about the nature of thought and knowledge.</p>
<p>With Chris' permission, I read out a summarised version of Chapter 2 from his book on which was on Intelligence during the 30 minute MLST introduction. &nbsp;</p>
<p>Buy his book here:</p>
<p>https://global.oup.com/academic/product/natural-general-intelligence-9780192843883?cc=gb&amp;lang=en&amp;</p>
<p><br></p>
<p>YT version: https://youtu.be/31VRbxAl3t0</p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Walk and talk with Chris on Knowledge and Abstractions</p>
<p>[00:04:08] Intro to Chris and his book</p>
<p>[00:05:55] (Intro) Tim reads Chapter 2: Intelligence&nbsp;</p>
<p>[00:09:28] Intro continued: Goodhart's law</p>
<p>[00:15:37] Intro continued: The "swiss cheese" situation &nbsp;</p>
<p>[00:20:23] Intro continued: On Human Knowledge</p>
<p>[00:23:37] Intro continued: Neats and Scruffies</p>
<p>[00:30:22] Interview kick off&nbsp;</p>
<p>[00:31:59] What does it mean to understand?</p>
<p>[00:36:18] Aligning our language models</p>
<p>[00:40:17] Creativity&nbsp;</p>
<p>[00:41:40] "Meta" AI and basins of attraction&nbsp;</p>
<p>[00:51:23] What can Neuroscience impart to AI</p>
<p>[00:54:43] Sutton, neats and scruffies and human alignment</p>
<p>[01:02:05] Reward is enough</p>
<p>[01:19:46] Jon Von Neumann and Intelligence</p>
<p>[01:23:56] Compositionality</p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>The Language Game (Morten H. Christiansen, Nick Chater</p>
<p>https://www.penguin.co.uk/books/441689/the-language-game-by-morten-h-christiansen-and--nick-chater/9781787633483</p>
<p>Theory of general factor (Spearman)</p>
<p>https://www.proquest.com/openview/7c2c7dd23910c89e1fc401e8bb37c3d0/1?pq-origsite=gscholar&amp;cbl=1818401</p>
<p>Intelligence Reframed (Howard Gardner)</p>
<p>https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=Qkw4DgAAQBAJ&amp;oi=fnd&amp;pg=PT6&amp;dq=howard+gardner+multiple+intelligences&amp;ots=ERUU0u5Usq&amp;sig=XqiDgNUIkb3K9XBq0vNbFmXWKFs#v=onepage&amp;q=howard%20gardner%20multiple%20intelligences&amp;f=false</p>
<p>The master algorithm (Pedro Domingos)</p>
<p>https://www.amazon.co.uk/Master-Algorithm-Ultimate-Learning-Machine/dp/0241004543</p>
<p>A Thousand Brains: A New Theory of Intelligence (Jeff Hawkins)</p>
<p>https://www.amazon.co.uk/Thousand-Brains-New-Theory-Intelligence/dp/1541675819</p>
<p>The bitter lesson (Rich Sutton)</p>
<p>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/104---Prof--CHRIS-SUMMERFIELD---Natural-General-Intelligence-SPECIAL-EDITION-e1vb94c</link>
			<guid isPermaLink="false">873ac99b-6594-4f11-8419-5e831236a3ae</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 22 Feb 2023 00:13:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/65430092/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-22%2Ff4d615d7-9082-642b-d34d-45850c5d95a5.mp3" length="213392640" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst &amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Christopher Summerfield, Department of Experimental Psychology, University of Oxford is a Professor of Cognitive Neuroscience at the University of Oxford and a Research Scientist at Deepmind UK. His work focusses on the neural and computational mechanisms by which humans make decisions.&lt;/p&gt;
&lt;p&gt;Chris has just released an incredible new book on AI called &quot;Natural General Intelligence&quot;. It&apos;s my favourite book on AI I have read so so far.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The book explores the algorithms and architectures that are driving progress in AI research, and discusses intelligence in the language of psychology and biology, using examples and analogies to be comprehensible to a wide audience. It also tackles longstanding theoretical questions about the nature of thought and knowledge.&lt;/p&gt;
&lt;p&gt;With Chris&apos; permission, I read out a summarised version of Chapter 2 from his book on which was on Intelligence during the 30 minute MLST introduction. &amp;nbsp;&lt;/p&gt;
&lt;p&gt;Buy his book here:&lt;/p&gt;
&lt;p&gt;https://global.oup.com/academic/product/natural-general-intelligence-9780192843883?cc=gb&amp;amp;lang=en&amp;amp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/31VRbxAl3t0&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Walk and talk with Chris on Knowledge and Abstractions&lt;/p&gt;
&lt;p&gt;[00:04:08] Intro to Chris and his book&lt;/p&gt;
&lt;p&gt;[00:05:55] (Intro) Tim reads Chapter 2: Intelligence&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:09:28] Intro continued: Goodhart&apos;s law&lt;/p&gt;
&lt;p&gt;[00:15:37] Intro continued: The &quot;swiss cheese&quot; situation &amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:20:23] Intro continued: On Human Knowledge&lt;/p&gt;
&lt;p&gt;[00:23:37] Intro continued: Neats and Scruffies&lt;/p&gt;
&lt;p&gt;[00:30:22] Interview kick off&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:31:59] What does it mean to understand?&lt;/p&gt;
&lt;p&gt;[00:36:18] Aligning our language models&lt;/p&gt;
&lt;p&gt;[00:40:17] Creativity&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:41:40] &quot;Meta&quot; AI and basins of attraction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:51:23] What can Neuroscience impart to AI&lt;/p&gt;
&lt;p&gt;[00:54:43] Sutton, neats and scruffies and human alignment&lt;/p&gt;
&lt;p&gt;[01:02:05] Reward is enough&lt;/p&gt;
&lt;p&gt;[01:19:46] Jon Von Neumann and Intelligence&lt;/p&gt;
&lt;p&gt;[01:23:56] Compositionality&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Language Game (Morten H. Christiansen, Nick Chater&lt;/p&gt;
&lt;p&gt;https://www.penguin.co.uk/books/441689/the-language-game-by-morten-h-christiansen-and--nick-chater/9781787633483&lt;/p&gt;
&lt;p&gt;Theory of general factor (Spearman)&lt;/p&gt;
&lt;p&gt;https://www.proquest.com/openview/7c2c7dd23910c89e1fc401e8bb37c3d0/1?pq-origsite=gscholar&amp;amp;cbl=1818401&lt;/p&gt;
&lt;p&gt;Intelligence Reframed (Howard Gardner)&lt;/p&gt;
&lt;p&gt;https://books.google.co.uk/books?hl=en&amp;amp;lr=&amp;amp;id=Qkw4DgAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PT6&amp;amp;dq=howard+gardner+multiple+intelligences&amp;amp;ots=ERUU0u5Usq&amp;amp;sig=XqiDgNUIkb3K9XBq0vNbFmXWKFs#v=onepage&amp;amp;q=howard%20gardner%20multiple%20intelligences&amp;amp;f=false&lt;/p&gt;
&lt;p&gt;The master algorithm (Pedro Domingos)&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Master-Algorithm-Ultimate-Learning-Machine/dp/0241004543&lt;/p&gt;
&lt;p&gt;A Thousand Brains: A New Theory of Intelligence (Jeff Hawkins)&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Thousand-Brains-New-Theory-Intelligence/dp/1541675819&lt;/p&gt;
&lt;p&gt;The bitter lesson (Rich Sutton)&lt;/p&gt;
&lt;p&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:28:54</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1677024814307-292875e81fae1.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>104</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#103 - Prof. Edward Grefenstette - Language, Semantics, Philosophy]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst&nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT: https://youtu.be/i9VPPmQn9HQ</p>
<p><br></p>
<p>Edward Grefenstette is a Franco-American computer scientist who currently serves as Head of Machine Learning at Cohere and Honorary Professor at UCL. He has previously been a research scientist at Facebook AI Research and staff research scientist at DeepMind, and was also the CTO of Dark Blue Labs. Prior to his move to industry, Edward was a Fulford Junior Research Fellow at Somerville College, University of Oxford, and was lecturing at Hertford College. He obtained his BSc in Physics and Philosophy from the University of Sheffield and did graduate work in the philosophy departments at the University of St Andrews. His research draws on topics and methods from Machine Learning, Computational Linguistics and Quantum Information Theory, and has done work implementing and evaluating compositional vector-based models of natural language semantics and empirical semantic knowledge discovery.</p>
<p><br></p>
<p>https://www.egrefen.com/</p>
<p>https://cohere.ai/</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction</p>
<p>[00:02:52] Differential Semantics</p>
<p>[00:06:56] Concepts</p>
<p>[00:10:20] Ontology</p>
<p>[00:14:02] Pragmatics</p>
<p>[00:16:55] Code helps with language</p>
<p>[00:19:02] Montague</p>
<p>[00:22:13] RLHF</p>
<p>[00:31:54] Swiss cheese problem / retrieval augmented</p>
<p>[00:37:06] Intelligence / Agency</p>
<p>[00:43:33] Creativity</p>
<p>[00:46:41] Common sense</p>
<p>[00:53:46] Thinking vs knowing</p>
<p><br></p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>Large language models are not zero-shot communicators (Laura Ruis)</p>
<p>https://arxiv.org/abs/2210.14986</p>
<p><br></p>
<p>Some remarks on Large Language Models (Yoav Goldberg)</p>
<p>https://gist.github.com/yoavg/59d174608e92e845c8994ac2e234c8a9</p>
<p><br></p>
<p>Quantum Natural Language Processing (Bob Coecke)</p>
<p>https://www.cs.ox.ac.uk/people/bob.coecke/QNLP-ACT.pdf</p>
<p><br></p>
<p>Constitutional AI: Harmlessness from AI Feedback</p>
<p>https://www.anthropic.com/constitutional.pdf</p>
<p><br></p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Patrick Lewis)</p>
<p>https://www.patricklewis.io/publication/rag/</p>
<p><br></p>
<p>Natural General Intelligence (Prof. Christopher Summerfield)</p>
<p>https://global.oup.com/academic/product/natural-general-intelligence-9780192843883</p>
<p><br></p>
<p>ChatGPT with Rob Miles - Computerphile</p>
<p>https://www.youtube.com/watch?v=viJt_DXTfwA</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/103---Prof--Edward-Grefenstette---Language--Semantics--Philosophy-e1uqlsv</link>
			<guid isPermaLink="false">27fdf89c-f2df-4eb2-b2e5-72cf133e6d46</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 11 Feb 2023 21:31:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64886111/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-11%2F4c4a3c7e-b852-53d6-bd24-925cc62046b2.mp3" length="148264320" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/i9VPPmQn9HQ&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Edward Grefenstette is a Franco-American computer scientist who currently serves as Head of Machine Learning at Cohere and Honorary Professor at UCL. He has previously been a research scientist at Facebook AI Research and staff research scientist at DeepMind, and was also the CTO of Dark Blue Labs. Prior to his move to industry, Edward was a Fulford Junior Research Fellow at Somerville College, University of Oxford, and was lecturing at Hertford College. He obtained his BSc in Physics and Philosophy from the University of Sheffield and did graduate work in the philosophy departments at the University of St Andrews. His research draws on topics and methods from Machine Learning, Computational Linguistics and Quantum Information Theory, and has done work implementing and evaluating compositional vector-based models of natural language semantics and empirical semantic knowledge discovery.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.egrefen.com/&lt;/p&gt;
&lt;p&gt;https://cohere.ai/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:02:52] Differential Semantics&lt;/p&gt;
&lt;p&gt;[00:06:56] Concepts&lt;/p&gt;
&lt;p&gt;[00:10:20] Ontology&lt;/p&gt;
&lt;p&gt;[00:14:02] Pragmatics&lt;/p&gt;
&lt;p&gt;[00:16:55] Code helps with language&lt;/p&gt;
&lt;p&gt;[00:19:02] Montague&lt;/p&gt;
&lt;p&gt;[00:22:13] RLHF&lt;/p&gt;
&lt;p&gt;[00:31:54] Swiss cheese problem / retrieval augmented&lt;/p&gt;
&lt;p&gt;[00:37:06] Intelligence / Agency&lt;/p&gt;
&lt;p&gt;[00:43:33] Creativity&lt;/p&gt;
&lt;p&gt;[00:46:41] Common sense&lt;/p&gt;
&lt;p&gt;[00:53:46] Thinking vs knowing&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Large language models are not zero-shot communicators (Laura Ruis)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.14986&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Some remarks on Large Language Models (Yoav Goldberg)&lt;/p&gt;
&lt;p&gt;https://gist.github.com/yoavg/59d174608e92e845c8994ac2e234c8a9&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Quantum Natural Language Processing (Bob Coecke)&lt;/p&gt;
&lt;p&gt;https://www.cs.ox.ac.uk/people/bob.coecke/QNLP-ACT.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Constitutional AI: Harmlessness from AI Feedback&lt;/p&gt;
&lt;p&gt;https://www.anthropic.com/constitutional.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Patrick Lewis)&lt;/p&gt;
&lt;p&gt;https://www.patricklewis.io/publication/rag/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Natural General Intelligence (Prof. Christopher Summerfield)&lt;/p&gt;
&lt;p&gt;https://global.oup.com/academic/product/natural-general-intelligence-9780192843883&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;ChatGPT with Rob Miles - Computerphile&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=viJt_DXTfwA&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:01:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1676151068155-6bf23a822bd4e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>103</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#102 - Prof. MICHAEL LEVIN, Prof. IRINA RISH - Emergence, Intelligence, Transhumanism]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT: https://youtu.be/Vbi288CKgis</p>
<p><br></p>
<p>Michael Levin is a Distinguished Professor in the Biology department at Tufts University, and the holder of the Vannevar Bush endowed Chair. He is the Director of the Allen Discovery Center at Tufts and the Tufts Center for Regenerative and Developmental Biology. His research focuses on understanding the biophysical mechanisms of pattern regulation and harnessing endogenous bioelectric dynamics for rational control of growth and form.</p>
<p>The capacity to generate a complex, behaving organism from the single cell of a fertilized egg is one of the most amazing aspects of biology. Levin' lab integrates approaches from developmental biology, computer science, and cognitive science to investigate the emergence of form and function. Using biophysical and computational modeling approaches, they seek to understand the collective intelligence of cells, as they navigate physiological, transcriptional, morphognetic, and behavioral spaces. They develop conceptual frameworks for basal cognition and diverse intelligence, including synthetic organisms and AI.</p>
<p>Also joining us this evening is Irina Rish. Irina is a Full Professor at the Université de Montréal's Computer Science and Operations Research department, a core member of Mila - Quebec AI Institute, as well as the holder of the Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI. She has a PhD in AI from UC Irvine. Her research focuses on machine learning, neural data analysis, neuroscience-inspired AI, continual lifelong learning, optimization algorithms, sparse modelling, probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis.&nbsp;</p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction</p>
<p>[00:02:09] Emergence</p>
<p>[00:13:16] Scaling Laws</p>
<p>[00:23:12] Intelligence</p>
<p>[00:44:36] Transhumanism</p>
<p><br></p>
<p>Prof. Michael Levin</p>
<p>https://en.wikipedia.org/wiki/Michael_Levin_(biologist)</p>
<p>https://www.drmichaellevin.org/</p>
<p>https://twitter.com/drmichaellevin</p>
<p><br></p>
<p>Prof. Irina Rish</p>
<p>https://twitter.com/irinarish</p>
<p>https://irina-rish.com/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/102---Prof--MICHAEL-LEVIN--Prof--IRINA-RISH---Emergence--Intelligence--Transhumanism-e1upji9</link>
			<guid isPermaLink="false">49992a79-5cce-4334-9001-c80cd17e7cfa</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 11 Feb 2023 01:45:44 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64850953/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-11%2F3f321bd4-9a00-592b-3d46-2fd4e058bfb9.mp3" length="132682560" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/Vbi288CKgis&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Michael Levin is a Distinguished Professor in the Biology department at Tufts University, and the holder of the Vannevar Bush endowed Chair. He is the Director of the Allen Discovery Center at Tufts and the Tufts Center for Regenerative and Developmental Biology. His research focuses on understanding the biophysical mechanisms of pattern regulation and harnessing endogenous bioelectric dynamics for rational control of growth and form.&lt;/p&gt;
&lt;p&gt;The capacity to generate a complex, behaving organism from the single cell of a fertilized egg is one of the most amazing aspects of biology. Levin&apos; lab integrates approaches from developmental biology, computer science, and cognitive science to investigate the emergence of form and function. Using biophysical and computational modeling approaches, they seek to understand the collective intelligence of cells, as they navigate physiological, transcriptional, morphognetic, and behavioral spaces. They develop conceptual frameworks for basal cognition and diverse intelligence, including synthetic organisms and AI.&lt;/p&gt;
&lt;p&gt;Also joining us this evening is Irina Rish. Irina is a Full Professor at the Université de Montréal&apos;s Computer Science and Operations Research department, a core member of Mila - Quebec AI Institute, as well as the holder of the Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI. She has a PhD in AI from UC Irvine. Her research focuses on machine learning, neural data analysis, neuroscience-inspired AI, continual lifelong learning, optimization algorithms, sparse modelling, probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:02:09] Emergence&lt;/p&gt;
&lt;p&gt;[00:13:16] Scaling Laws&lt;/p&gt;
&lt;p&gt;[00:23:12] Intelligence&lt;/p&gt;
&lt;p&gt;[00:44:36] Transhumanism&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. Michael Levin&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Michael_Levin_(biologist)&lt;/p&gt;
&lt;p&gt;https://www.drmichaellevin.org/&lt;/p&gt;
&lt;p&gt;https://twitter.com/drmichaellevin&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. Irina Rish&lt;/p&gt;
&lt;p&gt;https://twitter.com/irinarish&lt;/p&gt;
&lt;p&gt;https://irina-rish.com/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1676079913383-c24d5e4e0653e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>102</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#100 Dr. PATRICK LEWIS (co:here) - Retrieval Augmented Generation]]></title>
			<description><![CDATA[<p>Dr. Patrick Lewis is a London-based AI and Natural Language Processing Research Scientist, working at co:here. Prior to this, Patrick worked as a research scientist at the Fundamental AI Research Lab (FAIR) at Meta AI. During his PhD, Patrick split his time between FAIR and University College London, working with Sebastian Riedel and Pontus Stenetorp.&nbsp;</p>
<p>Patrick’s research focuses on the intersection of information retrieval techniques (IR) and large language models (LLMs). He has done extensive work on Retrieval-Augmented Language Models. His current focus is on building more powerful, efficient, robust, and update-able models that can perform well on a wide range of NLP tasks, but also excel on knowledge-intensive NLP tasks such as Question Answering and Fact Checking.</p>
<p><br></p>
<p>YT version: https://youtu.be/Dm5sfALoL1Y</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Support us! https://www.patreon.com/mlst</p>
<p><br></p>
<p>References:</p>
<p>Patrick Lewis (Natural Language Processing Research Scientist @ co:here)</p>
<p>https://www.patricklewis.io/</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Patrick Lewis et al)</p>
<p>https://arxiv.org/abs/2005.11401</p>
<p>Atlas: Few-shot Learning with Retrieval Augmented Language Models (Gautier Izacard, Patrick Lewis, et al)</p>
<p>https://arxiv.org/abs/2208.03299</p>
<p>Improving language models by retrieving from trillions of tokens (RETRO) (Sebastian Borgeaud et al)</p>
<p>https://arxiv.org/abs/2112.04426</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/100-Dr--PATRICK-LEWIS-cohere---Retrieval-Augmented-Generation-e1uok1c</link>
			<guid isPermaLink="false">27c09745-67b4-4f55-8167-901fdb5fe3e2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 10 Feb 2023 11:18:53 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64818668/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-10%2F6ad66fd6-9a81-4ddb-db6f-7f12bcb34e8d.mp3" length="50830604" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Patrick Lewis is a London-based AI and Natural Language Processing Research Scientist, working at co:here. Prior to this, Patrick worked as a research scientist at the Fundamental AI Research Lab (FAIR) at Meta AI. During his PhD, Patrick split his time between FAIR and University College London, working with Sebastian Riedel and Pontus Stenetorp.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Patrick’s research focuses on the intersection of information retrieval techniques (IR) and large language models (LLMs). He has done extensive work on Retrieval-Augmented Language Models. His current focus is on building more powerful, efficient, robust, and update-able models that can perform well on a wide range of NLP tasks, but also excel on knowledge-intensive NLP tasks such as Question Answering and Fact Checking.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/Dm5sfALoL1Y&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Patrick Lewis (Natural Language Processing Research Scientist @ co:here)&lt;/p&gt;
&lt;p&gt;https://www.patricklewis.io/&lt;/p&gt;
&lt;p&gt;Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Patrick Lewis et al)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2005.11401&lt;/p&gt;
&lt;p&gt;Atlas: Few-shot Learning with Retrieval Augmented Language Models (Gautier Izacard, Patrick Lewis, et al)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2208.03299&lt;/p&gt;
&lt;p&gt;Improving language models by retrieving from trillions of tokens (RETRO) (Sebastian Borgeaud et al)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2112.04426&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:26:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1676027920204-1b1723515b0d9.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>100</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#99 - CARLA CREMER & IGOR KRAWCZUK - X-Risk, Governance, Effective Altruism]]></title>
			<description><![CDATA[<p>YT version (with references): https://www.youtube.com/watch?v=lxaTinmKxs0</p>
<p>Support us! https://www.patreon.com/mlst</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p><br></p>
<p>Carla Cremer and Igor Krawczuk argue that AI risk should be understood as an old problem of politics, power and control with known solutions, and that threat models should be driven by empirical work. The interaction between FTX and the Effective Altruism community has sparked a lot of discussion about the dangers of optimization, and Carla's Vox article highlights the need for an institutional turn when taking on a responsibility like risk management for humanity.</p>
<p><br></p>
<p>Carla's “Democratizing Risk” paper found that certain types of risks fall through the cracks if they are just categorized into climate change or biological risks. Deliberative democracy has been found to be a better way to make decisions, and AI tools can be used to scale this type of democracy and be used for good, but the transparency of these algorithms to the citizens using the platform must be taken into consideration.</p>
<p><br></p>
<p>Aggregating people’s diverse ways of thinking about a problem and creating a risk-averse procedure gives a likely, highly probable outcome for having converged on the best policy. There needs to be a good reason to trust one organization with the risk management of humanity and all the different ways of thinking about risk must be taken into account. AI tools can help to scale this type of deliberative democracy, but the transparency of these algorithms must be taken into consideration.</p>
<p><br></p>
<p>The ambition of the EA community and Altruism Inc. is to protect and do risk management for the whole of humanity and this requires an institutional turn in order to do it effectively. The dangers of optimization are real, and it is essential to ensure that the risk management of humanity is done properly and ethically. By understanding the importance of aggregating people’s diverse ways of thinking about a problem, and creating a risk-averse procedure, it is possible to create a likely, highly probable outcome for having converged on the best policy.</p>
<p><br></p>
<p>Carla Zoe Cremer</p>
<p>https://carlacremer.github.io/</p>
<p><br></p>
<p>Igor Krawczuk</p>
<p>https://krawczuk.eu/</p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction: Vox article and effective altruism / FTX</p>
<p>[00:11:12] Luciano Floridi on Governance and Risk</p>
<p>[00:15:50] Connor Leahy on alignment</p>
<p>[00:21:08] Ethan Caballero on scaling</p>
<p>[00:23:23] Alignment, Values and politics</p>
<p>[00:30:50] Singularitarians vs AI-thiests</p>
<p>[00:41:56] Consequentialism</p>
<p>[00:46:44] Does scale make a difference?</p>
<p>[00:51:53] Carla's Democratising risk paper</p>
<p>[01:04:03] Vox article - How effective altruists ignored risk</p>
<p>[01:20:18] Does diversity breed complexity?</p>
<p>[01:29:50] Collective rationality</p>
<p>[01:35:16] Closing statements</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/99---CARLA-CREMER--IGOR-KRAWCZUK---X-Risk--Governance--Effective-Altruism-e1ugl0o</link>
			<guid isPermaLink="false">aac6c836-894b-4a7d-8e1b-6239844f100c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 05 Feb 2023 20:53:52 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64557528/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-5%2F4481f8e5-0aac-cc52-c7a5-c150602cf74f.mp3" length="143652672" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;YT version (with references): https://www.youtube.com/watch?v=lxaTinmKxs0&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Carla Cremer and Igor Krawczuk argue that AI risk should be understood as an old problem of politics, power and control with known solutions, and that threat models should be driven by empirical work. The interaction between FTX and the Effective Altruism community has sparked a lot of discussion about the dangers of optimization, and Carla&apos;s Vox article highlights the need for an institutional turn when taking on a responsibility like risk management for humanity.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Carla&apos;s “Democratizing Risk” paper found that certain types of risks fall through the cracks if they are just categorized into climate change or biological risks. Deliberative democracy has been found to be a better way to make decisions, and AI tools can be used to scale this type of democracy and be used for good, but the transparency of these algorithms to the citizens using the platform must be taken into consideration.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Aggregating people’s diverse ways of thinking about a problem and creating a risk-averse procedure gives a likely, highly probable outcome for having converged on the best policy. There needs to be a good reason to trust one organization with the risk management of humanity and all the different ways of thinking about risk must be taken into account. AI tools can help to scale this type of deliberative democracy, but the transparency of these algorithms must be taken into consideration.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The ambition of the EA community and Altruism Inc. is to protect and do risk management for the whole of humanity and this requires an institutional turn in order to do it effectively. The dangers of optimization are real, and it is essential to ensure that the risk management of humanity is done properly and ethically. By understanding the importance of aggregating people’s diverse ways of thinking about a problem, and creating a risk-averse procedure, it is possible to create a likely, highly probable outcome for having converged on the best policy.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Carla Zoe Cremer&lt;/p&gt;
&lt;p&gt;https://carlacremer.github.io/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Igor Krawczuk&lt;/p&gt;
&lt;p&gt;https://krawczuk.eu/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction: Vox article and effective altruism / FTX&lt;/p&gt;
&lt;p&gt;[00:11:12] Luciano Floridi on Governance and Risk&lt;/p&gt;
&lt;p&gt;[00:15:50] Connor Leahy on alignment&lt;/p&gt;
&lt;p&gt;[00:21:08] Ethan Caballero on scaling&lt;/p&gt;
&lt;p&gt;[00:23:23] Alignment, Values and politics&lt;/p&gt;
&lt;p&gt;[00:30:50] Singularitarians vs AI-thiests&lt;/p&gt;
&lt;p&gt;[00:41:56] Consequentialism&lt;/p&gt;
&lt;p&gt;[00:46:44] Does scale make a difference?&lt;/p&gt;
&lt;p&gt;[00:51:53] Carla&apos;s Democratising risk paper&lt;/p&gt;
&lt;p&gt;[01:04:03] Vox article - How effective altruists ignored risk&lt;/p&gt;
&lt;p&gt;[01:20:18] Does diversity breed complexity?&lt;/p&gt;
&lt;p&gt;[01:29:50] Collective rationality&lt;/p&gt;
&lt;p&gt;[01:35:16] Closing statements&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:39:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1675630400824-7211fdde4d57e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>99</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[[NO MUSIC] #98 - Prof. LUCIANO FLORIDI - ChatGPT, Singularitarians, Ethics, Philosophy of Information]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT version: https://youtu.be/YLNGvvgq3eg</p>
<p><br></p>
<p>We are living in an age of rapid technological advancement, and with this growth comes a digital divide. Professor Luciano Floridi of the Oxford Internet Institute / Oxford University believes that this divide not only affects our understanding of the implications of this new age, but also the organization of a fair society.&nbsp;</p>
<p>The Information Revolution has been transforming the global economy, with the majority of global GDP now relying on intangible goods, such as information-related services. This in turn has led to the generation of immense amounts of data, more than humanity has ever seen in its history. With 95% of this data being generated by the current generation, Professor Floridi believes that we are becoming overwhelmed by this data, and that our agency as humans is being eroded as a result.&nbsp;</p>
<p>According to Professor Floridi, the digital divide has caused a lack of balance between technological growth and our understanding of this growth. He believes that the infosphere is becoming polluted and the manifold of the infosphere is increasingly determined by technology and AI. Identifying, anticipating and resolving these problems has become essential, and Professor Floridi has dedicated his research to the Philosophy of Information, Philosophy of Technology and Digital Ethics.&nbsp;</p>
<p>We must equip ourselves with a viable philosophy of information to help us better understand and address the risks of this new information age. Professor Floridi is leading the charge, and his research on Digital Ethics, the Philosophy of Information and the Philosophy of Technology is helping us to better anticipate, identify and resolve problems caused by the digital divide.</p>
<p>TOC:</p>
<p>[00:00:00] Introduction to Luciano and his ideas</p>
<p>[00:14:00] Chat GPT / language models</p>
<p>[00:28:45] AI risk / "Singularitarians"&nbsp;</p>
<p>[00:37:15] Forms of governance</p>
<p>[00:43:56] Re-ontologising the world</p>
<p>[00:55:56] It from bit and Computationalism and philosophy without purpose</p>
<p>[01:03:05] Getting into Digital Ethics</p>
<p><br></p>
<p>Interviewer: Dr. Tim Scarfe</p>
<p><br></p>
<p>References:</p>
<p>GPT‐3: Its Nature, Scope, Limits, and Consequences [Floridi]</p>
<p>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3827044</p>
<p><br></p>
<p>Ultraintelligent Machines, Singularity, and Other Sci-fi Distractions about AI [Floridi]</p>
<p>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4222347</p>
<p><br></p>
<p>The Philosophy of Information [Floridi]</p>
<p>https://www.amazon.co.uk/Philosophy-Information-Luciano-Floridi/dp/0199232393</p>
<p><br></p>
<p>Information: A Very Short Introduction [Floridi]</p>
<p>https://www.amazon.co.uk/Information-Very-Short-Introduction-Introductions/dp/0199551375</p>
<p><br></p>
<p>https://en.wikipedia.org/wiki/Luciano_Floridi</p>
<p>https://www.philosophyofinformation.net/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/NO-MUSIC-98---Prof--LUCIANO-FLORIDI---ChatGPT--Singularitarians--Ethics--Philosophy-of-Information-e1udbq2</link>
			<guid isPermaLink="false">a023843f-299e-43fa-8951-3bd7e8f62a3b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 03 Feb 2023 11:08:10 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64449794/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-3%2Ff88b042f-4471-aeea-ac51-c0e3b8ab9cb3.mp3" length="158918400" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/YLNGvvgq3eg&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We are living in an age of rapid technological advancement, and with this growth comes a digital divide. Professor Luciano Floridi of the Oxford Internet Institute / Oxford University believes that this divide not only affects our understanding of the implications of this new age, but also the organization of a fair society.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The Information Revolution has been transforming the global economy, with the majority of global GDP now relying on intangible goods, such as information-related services. This in turn has led to the generation of immense amounts of data, more than humanity has ever seen in its history. With 95% of this data being generated by the current generation, Professor Floridi believes that we are becoming overwhelmed by this data, and that our agency as humans is being eroded as a result.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;According to Professor Floridi, the digital divide has caused a lack of balance between technological growth and our understanding of this growth. He believes that the infosphere is becoming polluted and the manifold of the infosphere is increasingly determined by technology and AI. Identifying, anticipating and resolving these problems has become essential, and Professor Floridi has dedicated his research to the Philosophy of Information, Philosophy of Technology and Digital Ethics.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We must equip ourselves with a viable philosophy of information to help us better understand and address the risks of this new information age. Professor Floridi is leading the charge, and his research on Digital Ethics, the Philosophy of Information and the Philosophy of Technology is helping us to better anticipate, identify and resolve problems caused by the digital divide.&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction to Luciano and his ideas&lt;/p&gt;
&lt;p&gt;[00:14:00] Chat GPT / language models&lt;/p&gt;
&lt;p&gt;[00:28:45] AI risk / &quot;Singularitarians&quot;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:37:15] Forms of governance&lt;/p&gt;
&lt;p&gt;[00:43:56] Re-ontologising the world&lt;/p&gt;
&lt;p&gt;[00:55:56] It from bit and Computationalism and philosophy without purpose&lt;/p&gt;
&lt;p&gt;[01:03:05] Getting into Digital Ethics&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interviewer: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;GPT‐3: Its Nature, Scope, Limits, and Consequences [Floridi]&lt;/p&gt;
&lt;p&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3827044&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ultraintelligent Machines, Singularity, and Other Sci-fi Distractions about AI [Floridi]&lt;/p&gt;
&lt;p&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4222347&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Philosophy of Information [Floridi]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Philosophy-Information-Luciano-Floridi/dp/0199232393&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Information: A Very Short Introduction [Floridi]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Information-Very-Short-Introduction-Introductions/dp/0199551375&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Luciano_Floridi&lt;/p&gt;
&lt;p&gt;https://www.philosophyofinformation.net/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:06:12</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1675422463491-fa738f9611d3a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>98</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#98 - Prof. LUCIANO FLORIDI - ChatGPT, Superintelligence, Ethics, Philosophy of Information]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT version: https://youtu.be/YLNGvvgq3eg</p>
<p>(If music annoying, skip to main interview @ 14:14)</p>
<p>We are living in an age of rapid technological advancement, and with this growth comes a digital divide. Professor Luciano Floridi of the Oxford Internet Institute / Oxford University believes that this divide not only affects our understanding of the implications of this new age, but also the organization of a fair society.&nbsp;</p>
<p>The Information Revolution has been transforming the global economy, with the majority of global GDP now relying on intangible goods, such as information-related services. This in turn has led to the generation of immense amounts of data, more than humanity has ever seen in its history. With 95% of this data being generated by the current generation, Professor Floridi believes that we are becoming overwhelmed by this data, and that our agency as humans is being eroded as a result.&nbsp;</p>
<p>According to Professor Floridi, the digital divide has caused a lack of balance between technological growth and our understanding of this growth. He believes that the infosphere is becoming polluted and the manifold of the infosphere is increasingly determined by technology and AI. Identifying, anticipating and resolving these problems has become essential, and Professor Floridi has dedicated his research to the Philosophy of Information, Philosophy of Technology and Digital Ethics.&nbsp;</p>
<p>We must equip ourselves with a viable philosophy of information to help us better understand and address the risks of this new information age. Professor Floridi is leading the charge, and his research on Digital Ethics, the Philosophy of Information and the Philosophy of Technology is helping us to better anticipate, identify and resolve problems caused by the digital divide.</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction to Luciano and his ideas</p>
<p>[00:14:40] Chat GPT / language models</p>
<p>[00:29:24] AI risk / "Singularitarians"&nbsp;</p>
<p>[00:30:34] Re-ontologising the world</p>
<p>[00:56:35] It from bit and Computationalism and philosophy without purpose</p>
<p>[01:03:43] Getting into Digital Ethics</p>
<p><br></p>
<p>References:</p>
<p>GPT‐3: Its Nature, Scope, Limits, and Consequences [Floridi]</p>
<p>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3827044</p>
<p><br></p>
<p>Ultraintelligent Machines, Singularity, and Other Sci-fi Distractions about AI [Floridi]</p>
<p>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4222347</p>
<p><br></p>
<p>The Philosophy of Information [Floridi]</p>
<p>https://www.amazon.co.uk/Philosophy-Information-Luciano-Floridi/dp/0199232393</p>
<p><br></p>
<p>Information: A Very Short Introduction [Floridi]</p>
<p>https://www.amazon.co.uk/Information-Very-Short-Introduction-Introductions/dp/0199551375</p>
<p><br></p>
<p>https://en.wikipedia.org/wiki/Luciano_Floridi</p>
<p>https://www.philosophyofinformation.net/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/98---Prof--LUCIANO-FLORIDI---ChatGPT--Superintelligence--Ethics--Philosophy-of-Information-e1ucs52</link>
			<guid isPermaLink="false">dfc9a2aa-9f01-437a-93fe-144bd3a79e1c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 03 Feb 2023 02:26:19 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64433762/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-1-3%2F21b72512-f79e-b583-7335-cb0479b32a0f.mp3" length="160470720" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/YLNGvvgq3eg&lt;/p&gt;
&lt;p&gt;(If music annoying, skip to main interview @ 14:14)&lt;/p&gt;
&lt;p&gt;We are living in an age of rapid technological advancement, and with this growth comes a digital divide. Professor Luciano Floridi of the Oxford Internet Institute / Oxford University believes that this divide not only affects our understanding of the implications of this new age, but also the organization of a fair society.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The Information Revolution has been transforming the global economy, with the majority of global GDP now relying on intangible goods, such as information-related services. This in turn has led to the generation of immense amounts of data, more than humanity has ever seen in its history. With 95% of this data being generated by the current generation, Professor Floridi believes that we are becoming overwhelmed by this data, and that our agency as humans is being eroded as a result.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;According to Professor Floridi, the digital divide has caused a lack of balance between technological growth and our understanding of this growth. He believes that the infosphere is becoming polluted and the manifold of the infosphere is increasingly determined by technology and AI. Identifying, anticipating and resolving these problems has become essential, and Professor Floridi has dedicated his research to the Philosophy of Information, Philosophy of Technology and Digital Ethics.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We must equip ourselves with a viable philosophy of information to help us better understand and address the risks of this new information age. Professor Floridi is leading the charge, and his research on Digital Ethics, the Philosophy of Information and the Philosophy of Technology is helping us to better anticipate, identify and resolve problems caused by the digital divide.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction to Luciano and his ideas&lt;/p&gt;
&lt;p&gt;[00:14:40] Chat GPT / language models&lt;/p&gt;
&lt;p&gt;[00:29:24] AI risk / &quot;Singularitarians&quot;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:30:34] Re-ontologising the world&lt;/p&gt;
&lt;p&gt;[00:56:35] It from bit and Computationalism and philosophy without purpose&lt;/p&gt;
&lt;p&gt;[01:03:43] Getting into Digital Ethics&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;GPT‐3: Its Nature, Scope, Limits, and Consequences [Floridi]&lt;/p&gt;
&lt;p&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3827044&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ultraintelligent Machines, Singularity, and Other Sci-fi Distractions about AI [Floridi]&lt;/p&gt;
&lt;p&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4222347&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Philosophy of Information [Floridi]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Philosophy-Information-Luciano-Floridi/dp/0199232393&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Information: A Very Short Introduction [Floridi]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Information-Very-Short-Introduction-Introductions/dp/0199551375&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Luciano_Floridi&lt;/p&gt;
&lt;p&gt;https://www.philosophyofinformation.net/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:06:51</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1675391134941-1d721b3c0cb9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>98</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#97 SREEJAN KUMAR - Human Inductive Biases in Machines from Language]]></title>
			<description><![CDATA[<p>Research has shown that humans possess strong inductive biases which enable them to quickly learn and generalize. In order to instill these same useful human inductive biases into machines, a paper was presented by Sreejan Kumar at the NeurIPS conference which won the Outstanding Paper of the Year award. The paper is called Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines.</p>
<p>This paper focuses on using a controlled stimulus space of two-dimensional binary grids to define the space of abstract concepts that humans have and a feedback loop of collaboration between humans and machines to understand the differences in human and machine inductive biases.&nbsp;</p>
<p>It is important to make machines more human-like to collaborate with them and understand their behavior. Synthesised discrete programs running on a turing machine computational model instead of a neural network substrate offers promise for the future of artificial intelligence. Neural networks and program induction should both be explored to get a well-rounded view of intelligence which works in multiple domains, computational substrates and which can acquire a diverse set of capabilities.</p>
<p>Natural language understanding in models can also be improved by instilling human language biases and programs into AI models. Sreejan used an experimental framework consisting of two dual task distributions, one generated from human priors and one from machine priors, to understand the differences in human and machine inductive biases. Furthermore, he demonstrated that compressive abstractions can be used to capture the essential structure of the environment for more human-like behavior. This means that emergent language-based inductive priors can be distilled into artificial neural networks, and AI &nbsp;models can be aligned to the us, world and indeed, our values.</p>
<p>Humans possess strong inductive biases which enable them to quickly learn to perform various tasks. This is in contrast to neural networks, which lack the same inductive biases and struggle to learn them empirically from observational data, thus, they have difficulty generalizing to novel environments due to their lack of prior knowledge.&nbsp;</p>
<p>Sreejan's results showed that when guided with representations from language and programs, the meta-learning agent not only improved performance on task distributions humans are adept at, but also decreased performa on control task distributions where humans perform poorly. This indicates that the abstraction supported by these representations, in the substrate of language or indeed, a program, is key in the development of aligned artificial agents with human-like generalization, capabilities, aligned values and behaviour.</p>
<p><br></p>
<p>References</p>
<p>Using natural language and program abstractions to instill human inductive biases in machines [Kumar et al/NEURIPS]</p>
<p>https://openreview.net/pdf?id=buXZ7nIqiwE</p>
<p><br></p>
<p>Core Knowledge [Elizabeth S. Spelke / Harvard]</p>
<p>https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf</p>
<p><br></p>
<p>The Debate Over Understanding in AI's Large Language Models [Melanie Mitchell]</p>
<p>https://arxiv.org/abs/2210.13966</p>
<p><br></p>
<p>On the Measure of Intelligence [Francois Chollet]</p>
<p>https://arxiv.org/abs/1911.01547</p>
<p><br></p>
<p>ARC challenge [Chollet]</p>
<p>https://github.com/fchollet/ARC</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/97-SREEJAN-KUMAR---Human-Inductive-Biases-in-Machines-from-Language-e1u50ps</link>
			<guid isPermaLink="false">8aa6fb24-4d96-459e-8545-dea43c8fac59</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 28 Jan 2023 18:35:51 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/64176380/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2023-0-28%2F56774d90-b570-185c-0845-14d2cc8faeb7.mp3" length="59929920" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Research has shown that humans possess strong inductive biases which enable them to quickly learn and generalize. In order to instill these same useful human inductive biases into machines, a paper was presented by Sreejan Kumar at the NeurIPS conference which won the Outstanding Paper of the Year award. The paper is called Using Natural Language and Program Abstractions to Instill Human Inductive Biases in Machines.&lt;/p&gt;
&lt;p&gt;This paper focuses on using a controlled stimulus space of two-dimensional binary grids to define the space of abstract concepts that humans have and a feedback loop of collaboration between humans and machines to understand the differences in human and machine inductive biases.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;It is important to make machines more human-like to collaborate with them and understand their behavior. Synthesised discrete programs running on a turing machine computational model instead of a neural network substrate offers promise for the future of artificial intelligence. Neural networks and program induction should both be explored to get a well-rounded view of intelligence which works in multiple domains, computational substrates and which can acquire a diverse set of capabilities.&lt;/p&gt;
&lt;p&gt;Natural language understanding in models can also be improved by instilling human language biases and programs into AI models. Sreejan used an experimental framework consisting of two dual task distributions, one generated from human priors and one from machine priors, to understand the differences in human and machine inductive biases. Furthermore, he demonstrated that compressive abstractions can be used to capture the essential structure of the environment for more human-like behavior. This means that emergent language-based inductive priors can be distilled into artificial neural networks, and AI &amp;nbsp;models can be aligned to the us, world and indeed, our values.&lt;/p&gt;
&lt;p&gt;Humans possess strong inductive biases which enable them to quickly learn to perform various tasks. This is in contrast to neural networks, which lack the same inductive biases and struggle to learn them empirically from observational data, thus, they have difficulty generalizing to novel environments due to their lack of prior knowledge.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Sreejan&apos;s results showed that when guided with representations from language and programs, the meta-learning agent not only improved performance on task distributions humans are adept at, but also decreased performa on control task distributions where humans perform poorly. This indicates that the abstraction supported by these representations, in the substrate of language or indeed, a program, is key in the development of aligned artificial agents with human-like generalization, capabilities, aligned values and behaviour.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;p&gt;Using natural language and program abstractions to instill human inductive biases in machines [Kumar et al/NEURIPS]&lt;/p&gt;
&lt;p&gt;https://openreview.net/pdf?id=buXZ7nIqiwE&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Core Knowledge [Elizabeth S. Spelke / Harvard]&lt;/p&gt;
&lt;p&gt;https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Debate Over Understanding in AI&apos;s Large Language Models [Melanie Mitchell]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.13966&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;On the Measure of Intelligence [Francois Chollet]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;ARC challenge [Chollet]&lt;/p&gt;
&lt;p&gt;https://github.com/fchollet/ARC&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:24:58</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1674930938468-63289daa1a79a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>97</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#96 Prof. PEDRO DOMINGOS - There are no infinities, utility functions, neurosymbolic]]></title>
			<description><![CDATA[<p>Pedro Domingos, Professor Emeritus of Computer Science and Engineering at the University of Washington, is renowned for his research in machine learning, particularly for his work on Markov logic networks that allow for uncertain inference. He is also the author of the acclaimed book "The Master Algorithm".</p>
<p><br></p>
<p>Panel: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction</p>
<p>[00:01:34] Galaxtica / misinformation / gatekeeping</p>
<p>[00:12:31] Is there a master algorithm?</p>
<p>[00:16:29] Limits of our understanding&nbsp;</p>
<p>[00:21:57] Intentionality, Agency, Creativity</p>
<p>[00:27:56] Compositionality&nbsp;</p>
<p>[00:29:30] Digital Physics / It from bit / Wolfram&nbsp;</p>
<p>[00:35:17] Alignment / Utility functions</p>
<p>[00:43:36] Meritocracy &nbsp;</p>
<p>[00:45:53] Game theory&nbsp;</p>
<p>[01:00:00] EA/consequentialism/Utility</p>
<p>[01:11:09] Emergence / relationalism&nbsp;</p>
<p>[01:19:26] Markov logic&nbsp;</p>
<p>[01:25:38] Moving away from anthropocentrism&nbsp;</p>
<p>[01:28:57] Neurosymbolic / infinity / tensor algerbra</p>
<p>[01:53:45] Abstraction</p>
<p>[01:57:26] Symmetries / Geometric DL</p>
<p>[02:02:46] Bias variance trade off&nbsp;</p>
<p>[02:05:49] What seen at neurips</p>
<p>[02:12:58] Chalmers talk on LLMs</p>
<p>[02:28:32] Definition of intelligence</p>
<p>[02:32:40] LLMs&nbsp;</p>
<p>[02:35:14] On experts in different fields</p>
<p>[02:40:15] Back to intelligence</p>
<p>[02:41:37] Spline theory / extrapolation</p>
<p><br></p>
<p>YT version:&nbsp; https://www.youtube.com/watch?v=C9BH3F2c0vQ</p>
<p><br></p>
<p>References;</p>
<p><br></p>
<p>The Master Algorithm [Domingos]</p>
<p>https://www.amazon.co.uk/s?k=master+algorithm&amp;i=stripbooks&amp;crid=3CJ67DCY96DE8&amp;sprefix=master+algorith%2Cstripbooks%2C82&amp;ref=nb_sb_noss_2</p>
<p><br></p>
<p>INFORMATION, PHYSICS, QUANTUM: THE SEARCH FOR LINKS [John Wheeler/It from Bit]</p>
<p>https://philpapers.org/archive/WHEIPQ.pdf</p>
<p><br></p>
<p>A New Kind Of Science [Wolfram]</p>
<p>https://www.amazon.co.uk/New-Kind-Science-Stephen-Wolfram/dp/1579550088</p>
<p><br></p>
<p>The Rationalist's Guide to the Galaxy: Superintelligent AI and the Geeks Who Are Trying to Save Humanity's Future [Tom Chivers]</p>
<p>https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795</p>
<p><br></p>
<p>The Status Game: On Social Position and How We Use It [Will Storr]</p>
<p>https://www.goodreads.com/book/show/60598238-the-status-game</p>
<p><br></p>
<p>Newcomb's paradox</p>
<p>https://en.wikipedia.org/wiki/Newcomb%27s_paradox</p>
<p><br></p>
<p>The Case for Strong Emergence [Sabine Hossenfelder]</p>
<p>https://philpapers.org/rec/HOSTCF-3</p>
<p><br></p>
<p>Markov Logic: An Interface Layer for Artificial Intelligence [Domingos]</p>
<p>https://www.morganclaypool.com/doi/abs/10.2200/S00206ED1V01Y200907AIM007</p>
<p><br></p>
<p>Note; Pedro discussed “Tensor Logic” - I was not able to find a reference</p>
<p><br></p>
<p>Neural Networks and the Chomsky Hierarchy [Grégoire Delétang/DeepMind]</p>
<p>https://arxiv.org/abs/2207.02098</p>
<p><br></p>
<p>Connectionism and Cognitive Architecture: A Critical Analysis [Jerry A. Fodor and Zenon W. Pylyshyn]</p>
<p>https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf</p>
<p><br></p>
<p>Every Model Learned by Gradient Descent Is Approximately a Kernel Machine [Pedro Domingos]</p>
<p>https://arxiv.org/abs/2012.00152</p>
<p><br></p>
<p>A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27 [LeCun]</p>
<p>https://openreview.net/pdf?id=BZ5a1r-kVsf</p>
<p><br></p>
<p>Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges [Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veličković]</p>
<p>https://arxiv.org/abs/2104.13478</p>
<p><br></p>
<p>The Algebraic Mind: Integrating Connectionism and Cognitive Science [Gary Marcus]</p>
<p>https://www.amazon.co.uk/Algebraic-Mind-Integrating-Connectionism-D
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/96-Prof--PEDRO-DOMINGOS---There-are-no-infinities--utility-functions--neurosymbolic-e1st1d3</link>
			<guid isPermaLink="false">433f389f-391a-46f5-9633-95a28bd72c22</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 30 Dec 2022 12:18:50 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62866275/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-30%2Fc324daaf-01f8-b193-73ee-4063db376493.mp3" length="243697536" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Pedro Domingos, Professor Emeritus of Computer Science and Engineering at the University of Washington, is renowned for his research in machine learning, particularly for his work on Markov logic networks that allow for uncertain inference. He is also the author of the acclaimed book &quot;The Master Algorithm&quot;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:01:34] Galaxtica / misinformation / gatekeeping&lt;/p&gt;
&lt;p&gt;[00:12:31] Is there a master algorithm?&lt;/p&gt;
&lt;p&gt;[00:16:29] Limits of our understanding&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:21:57] Intentionality, Agency, Creativity&lt;/p&gt;
&lt;p&gt;[00:27:56] Compositionality&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:29:30] Digital Physics / It from bit / Wolfram&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:35:17] Alignment / Utility functions&lt;/p&gt;
&lt;p&gt;[00:43:36] Meritocracy &amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:45:53] Game theory&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:00:00] EA/consequentialism/Utility&lt;/p&gt;
&lt;p&gt;[01:11:09] Emergence / relationalism&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:19:26] Markov logic&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:25:38] Moving away from anthropocentrism&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:28:57] Neurosymbolic / infinity / tensor algerbra&lt;/p&gt;
&lt;p&gt;[01:53:45] Abstraction&lt;/p&gt;
&lt;p&gt;[01:57:26] Symmetries / Geometric DL&lt;/p&gt;
&lt;p&gt;[02:02:46] Bias variance trade off&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[02:05:49] What seen at neurips&lt;/p&gt;
&lt;p&gt;[02:12:58] Chalmers talk on LLMs&lt;/p&gt;
&lt;p&gt;[02:28:32] Definition of intelligence&lt;/p&gt;
&lt;p&gt;[02:32:40] LLMs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[02:35:14] On experts in different fields&lt;/p&gt;
&lt;p&gt;[02:40:15] Back to intelligence&lt;/p&gt;
&lt;p&gt;[02:41:37] Spline theory / extrapolation&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version:&amp;nbsp; https://www.youtube.com/watch?v=C9BH3F2c0vQ&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Master Algorithm [Domingos]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/s?k=master+algorithm&amp;amp;i=stripbooks&amp;amp;crid=3CJ67DCY96DE8&amp;amp;sprefix=master+algorith%2Cstripbooks%2C82&amp;amp;ref=nb_sb_noss_2&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;INFORMATION, PHYSICS, QUANTUM: THE SEARCH FOR LINKS [John Wheeler/It from Bit]&lt;/p&gt;
&lt;p&gt;https://philpapers.org/archive/WHEIPQ.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A New Kind Of Science [Wolfram]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/New-Kind-Science-Stephen-Wolfram/dp/1579550088&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Rationalist&apos;s Guide to the Galaxy: Superintelligent AI and the Geeks Who Are Trying to Save Humanity&apos;s Future [Tom Chivers]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Status Game: On Social Position and How We Use It [Will Storr]&lt;/p&gt;
&lt;p&gt;https://www.goodreads.com/book/show/60598238-the-status-game&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Newcomb&apos;s paradox&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Newcomb%27s_paradox&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Case for Strong Emergence [Sabine Hossenfelder]&lt;/p&gt;
&lt;p&gt;https://philpapers.org/rec/HOSTCF-3&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Markov Logic: An Interface Layer for Artificial Intelligence [Domingos]&lt;/p&gt;
&lt;p&gt;https://www.morganclaypool.com/doi/abs/10.2200/S00206ED1V01Y200907AIM007&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Note; Pedro discussed “Tensor Logic” - I was not able to find a reference&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Neural Networks and the Chomsky Hierarchy [Grégoire Delétang/DeepMind]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2207.02098&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Connectionism and Cognitive Architecture: A Critical Analysis [Jerry A. Fodor and Zenon W. Pylyshyn]&lt;/p&gt;
&lt;p&gt;https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Every Model Learned by Gradient Descent Is Approximately a Kernel Machine [Pedro Domingos]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2012.00152&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27 [LeCun]&lt;/p&gt;
&lt;p&gt;https://openreview.net/pdf?id=BZ5a1r-kVsf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges [Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veličković]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2104.13478&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Algebraic Mind: Integrating Connectionism and Cognitive Science [Gary Marcus]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Algebraic-Mind-Integrating-Connectionism-D
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:49:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1672402669872-6113483545481.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>96</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#95 - Prof. IRINA RISH - AGI, Complex Systems, Transhumanism]]></title>
			<description><![CDATA[<p>Canadian Excellence Research Chair in Autonomous AI. Irina holds an MSc and PhD in AI from the University of California, Irvine as well as an MSc in Applied Mathematics from the Moscow Gubkin Institute. Her research focuses on machine learning, neural data analysis, and neuroscience-inspired AI. In particular, she is exploring continual lifelong learning, optimization algorithms for deep neural networks, sparse modelling and probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis. Prof. Rish holds 64 patents, has published over 80 research papers, several book chapters, three edited books, and a monograph on Sparse Modelling. She has served as a Senior Area Chair for NeurIPS and ICML. &nbsp;&nbsp;Irina's research is focussed on taking us closer to the holy grail of Artificial General Intelligence. &nbsp;She continues to push the boundaries of machine learning, continually striving to make advancements in neuroscience-inspired AI.</p>
<p>In a conversation about artificial intelligence (AI), Irina and Tim discussed the idea of transhumanism and the potential for AI to improve human flourishing. Irina suggested that instead of looking at AI as something to be controlled and regulated, people should view it as a tool to augment human capabilities. She argued that attempting to create an AI that is smarter than humans is not the best approach, and that a hybrid of human and AI intelligence is much more beneficial. As an example, she mentioned how technology can be used as an extension of the human mind, to track mental states and improve self-understanding. Ultimately, Irina concluded that transhumanism is about having a symbiotic relationship with technology, which can have a positive effect on both parties.</p>
<p>Tim then discussed the contrasting types of intelligence and how this could lead to something interesting emerging from the combination. He brought up the Trolley Problem and how difficult moral quandaries could be programmed into an AI. Irina then referenced The Garden of Forking Paths, a story which explores the idea of how different paths in life can be taken and how decisions from the past can have an effect on the present.</p>
<p>To better understand AI and intelligence, Irina suggested looking at it from multiple perspectives and understanding the importance of complex systems science in programming and understanding dynamical systems. She discussed the work of Michael Levin, who is looking into reprogramming biological computers with chemical interventions, and Tim mentioned Alex Mordvinsev, who is looking into the self-healing and repair of these systems. Ultimately, Irina argued that the key to understanding AI and intelligence is to recognize the complexity of the systems and to create hybrid models of human and AI intelligence.</p>
<p>Find Irina;</p>
<p>https://mila.quebec/en/person/irina-rish/</p>
<p>https://twitter.com/irinarish</p>
<p><br></p>
<p>YT version: https://youtu.be/8-ilcF0R7mI&nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p><br></p>
<p>References;</p>
<p>The Garden of Forking Paths: Jorge Luis Borges [Jorge Luis Borges]</p>
<p>https://www.amazon.co.uk/Garden-Forking-Paths-Penguin-Modern/dp/0241339057</p>
<p>The Brain from Inside Out [György Buzsáki]</p>
<p>https://www.amazon.co.uk/Brain-Inside-Out-Gy%C3%B6rgy-Buzs%C3%A1ki/dp/0190905387</p>
<p>Growing Isotropic Neural Cellular Automata [Alexander Mordvintsev]</p>
<p>https://arxiv.org/abs/2205.01681</p>
<p>The Extended Mind [Andy Clark and David Chalmers]</p>
<p>https://www.jstor.org/stable/3328150</p>
<p>The Gentle Seduction [Marc Stiegler]</p>
<p>https://www.amazon.co.uk/Gentle-Seduction-Marc-Stiegler/dp/0671698877</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/95---Prof--IRINA-RISH---AGI--Complex-Systems--Transhumanism-e1so8lh</link>
			<guid isPermaLink="false">5a5a2a48-d3b5-4091-8f96-b501dea2fc8e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 26 Dec 2022 19:29:14 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62709873/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-26%2Fd096c2eb-f32f-c82a-dde1-ef7efcce9425.mp3" length="94100160" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Canadian Excellence Research Chair in Autonomous AI. Irina holds an MSc and PhD in AI from the University of California, Irvine as well as an MSc in Applied Mathematics from the Moscow Gubkin Institute. Her research focuses on machine learning, neural data analysis, and neuroscience-inspired AI. In particular, she is exploring continual lifelong learning, optimization algorithms for deep neural networks, sparse modelling and probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis. Prof. Rish holds 64 patents, has published over 80 research papers, several book chapters, three edited books, and a monograph on Sparse Modelling. She has served as a Senior Area Chair for NeurIPS and ICML. &amp;nbsp;&amp;nbsp;Irina&apos;s research is focussed on taking us closer to the holy grail of Artificial General Intelligence. &amp;nbsp;She continues to push the boundaries of machine learning, continually striving to make advancements in neuroscience-inspired AI.&lt;/p&gt;
&lt;p&gt;In a conversation about artificial intelligence (AI), Irina and Tim discussed the idea of transhumanism and the potential for AI to improve human flourishing. Irina suggested that instead of looking at AI as something to be controlled and regulated, people should view it as a tool to augment human capabilities. She argued that attempting to create an AI that is smarter than humans is not the best approach, and that a hybrid of human and AI intelligence is much more beneficial. As an example, she mentioned how technology can be used as an extension of the human mind, to track mental states and improve self-understanding. Ultimately, Irina concluded that transhumanism is about having a symbiotic relationship with technology, which can have a positive effect on both parties.&lt;/p&gt;
&lt;p&gt;Tim then discussed the contrasting types of intelligence and how this could lead to something interesting emerging from the combination. He brought up the Trolley Problem and how difficult moral quandaries could be programmed into an AI. Irina then referenced The Garden of Forking Paths, a story which explores the idea of how different paths in life can be taken and how decisions from the past can have an effect on the present.&lt;/p&gt;
&lt;p&gt;To better understand AI and intelligence, Irina suggested looking at it from multiple perspectives and understanding the importance of complex systems science in programming and understanding dynamical systems. She discussed the work of Michael Levin, who is looking into reprogramming biological computers with chemical interventions, and Tim mentioned Alex Mordvinsev, who is looking into the self-healing and repair of these systems. Ultimately, Irina argued that the key to understanding AI and intelligence is to recognize the complexity of the systems and to create hybrid models of human and AI intelligence.&lt;/p&gt;
&lt;p&gt;Find Irina;&lt;/p&gt;
&lt;p&gt;https://mila.quebec/en/person/irina-rish/&lt;/p&gt;
&lt;p&gt;https://twitter.com/irinarish&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/8-ilcF0R7mI&amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;The Garden of Forking Paths: Jorge Luis Borges [Jorge Luis Borges]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Garden-Forking-Paths-Penguin-Modern/dp/0241339057&lt;/p&gt;
&lt;p&gt;The Brain from Inside Out [György Buzsáki]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Brain-Inside-Out-Gy%C3%B6rgy-Buzs%C3%A1ki/dp/0190905387&lt;/p&gt;
&lt;p&gt;Growing Isotropic Neural Cellular Automata [Alexander Mordvintsev]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2205.01681&lt;/p&gt;
&lt;p&gt;The Extended Mind [Andy Clark and David Chalmers]&lt;/p&gt;
&lt;p&gt;https://www.jstor.org/stable/3328150&lt;/p&gt;
&lt;p&gt;The Gentle Seduction [Marc Stiegler]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Gentle-Seduction-Marc-Stiegler/dp/0671698877&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:39:12</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1672082943213-1909da397872d.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>95</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#94 - ALAN CHAN - AI Alignment and Governance #NEURIPS]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>Alan Chan is a PhD student at Mila, the Montreal Institute for Learning Algorithms, supervised by Nicolas Le Roux. Before joining Mila, Alan was a Masters student at the Alberta Machine Intelligence Institute and the University of Alberta, where he worked with Martha White. Alan's expertise and research interests encompass value alignment and AI governance. He is currently exploring the measurement of harms from language models and the incentives that agents have to impact the world. Alan's research focuses on understanding and controlling the values expressed by machine learning models. His projects have examined the regulation of explainability in algorithmic systems, scoring rules for performative binary prediction, the effects of global exclusion in AI development, and the role of a graduate student in approaching ethical impacts in AI research. In addition, Alan has conducted research into inverse policy evaluation for value-based sequential decision-making, and the concept of "normal accidents" and AI systems. Alan's research is motivated by the need to align AI systems with human values, and his passion for scientific and governance work in this field. Alan's energy and enthusiasm for his field is infectious.&nbsp;</p>
<p>This was a discussion at NeurIPS. It was in quite a loud environment so the audio quality could have been better.&nbsp;</p>
<p>References:</p>
<p><br></p>
<p>The Rationalist's Guide to the Galaxy: Superintelligent AI and the Geeks Who Are Trying to Save Humanity's Future [Tim Chivers]</p>
<p>https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795</p>
<p><br></p>
<p>The implausibility of intelligence explosion [Chollet]</p>
<p>https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec</p>
<p><br></p>
<p>Superintelligence: Paths, Dangers, Strategies [Bostrom]</p>
<p>https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111</p>
<p><br></p>
<p>A Theory of Universal Artificial Intelligence based on Algorithmic Complexity [Hutter]</p>
<p>https://arxiv.org/abs/cs/0004001</p>
<p><br></p>
<p>YT version: https://youtu.be/XBMnOsv9_pk&nbsp;</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/94---ALAN-CHAN---AI-Alignment-and-Governance-NEURIPS-e1sntud</link>
			<guid isPermaLink="false">d4c0ff4a-a6e7-419d-9e16-eb474a162d1a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 26 Dec 2022 13:39:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62698893/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-26%2F4605a493-e90e-e35d-f934-f0809c40a24d.mp3" length="32593920" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Alan Chan is a PhD student at Mila, the Montreal Institute for Learning Algorithms, supervised by Nicolas Le Roux. Before joining Mila, Alan was a Masters student at the Alberta Machine Intelligence Institute and the University of Alberta, where he worked with Martha White. Alan&apos;s expertise and research interests encompass value alignment and AI governance. He is currently exploring the measurement of harms from language models and the incentives that agents have to impact the world. Alan&apos;s research focuses on understanding and controlling the values expressed by machine learning models. His projects have examined the regulation of explainability in algorithmic systems, scoring rules for performative binary prediction, the effects of global exclusion in AI development, and the role of a graduate student in approaching ethical impacts in AI research. In addition, Alan has conducted research into inverse policy evaluation for value-based sequential decision-making, and the concept of &quot;normal accidents&quot; and AI systems. Alan&apos;s research is motivated by the need to align AI systems with human values, and his passion for scientific and governance work in this field. Alan&apos;s energy and enthusiasm for his field is infectious.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This was a discussion at NeurIPS. It was in quite a loud environment so the audio quality could have been better.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Rationalist&apos;s Guide to the Galaxy: Superintelligent AI and the Geeks Who Are Trying to Save Humanity&apos;s Future [Tim Chivers]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Does-Not-Hate-You-Superintelligence/dp/1474608795&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The implausibility of intelligence explosion [Chollet]&lt;/p&gt;
&lt;p&gt;https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Superintelligence: Paths, Dangers, Strategies [Bostrom]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Theory of Universal Artificial Intelligence based on Algorithmic Complexity [Hutter]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/cs/0004001&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/XBMnOsv9_pk&amp;nbsp;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:13:34</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1672061931777-b33fdbcda79eb.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>94</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#93 Prof. MURRAY SHANAHAN - Consciousness, Embodiment, Language Models]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p><br></p>
<p>Professor <strong>Murray Shanahan</strong> is a renowned researcher on sophisticated cognition and its implications for artificial intelligence. His 2016 article ‘Conscious Exotica’ explores the Space of Possible Minds, a concept first proposed by philosopher Aaron Sloman in 1984, which includes all the different forms of minds from those of other animals to those of artificial intelligence. Shanahan rejects the idea of an impenetrable realm of subjective experience and argues that the majority of the space of possible minds may be occupied by non-natural variants, such as the ‘conscious exotica’ of which he speaks. &nbsp;In his paper ‘Talking About Large Language Models’, Shanahan discusses the capabilities and limitations of large language models (LLMs). He argues that prompt engineering is a key element for advanced AI systems, as it involves exploiting prompt prefixes to adjust LLMs to various tasks. However, Shanahan cautions against ascribing human-like characteristics to these systems, as they are fundamentally different and lack a shared comprehension with humans. Even though LLMs can be integrated into embodied systems, it does not mean that they possess human-like language abilities. Ultimately, Shanahan concludes that although LLMs are formidable and versatile, we must be wary of over-simplifying their capacities and limitations.</p>
<p>YT version: https://youtu.be/BqkWpP3uMMU</p>
<p>Full references on the YT description.&nbsp;</p>
<p><br></p>
<p>[00:00:00] Introduction</p>
<p>[00:08:51] Consciousness and &nbsp;Consciousness Exotica</p>
<p>[00:34:59] Slightly Consciousness LLMs</p>
<p>[00:38:05] Embodiment</p>
<p>[00:51:32] Symbol Grounding&nbsp;</p>
<p>[00:54:13] Emergence</p>
<p>[00:57:09] Reasoning</p>
<p>[01:03:16] Intentional Stance</p>
<p>[01:07:06] Digression on Chomsky show and Andrew Lampinen</p>
<p>[01:10:31] Prompt Engineering</p>
<p><br></p>
<p>Find Murray online:</p>
<p>https://www.doc.ic.ac.uk/~mpsha/</p>
<p>https://twitter.com/mpshanahan?lang=en</p>
<p>https://scholar.google.co.uk/citations?user=00bnGpAAAAAJ&amp;hl=en</p>
<p><br></p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/93-Prof--MURRAY-SHANAHAN---Consciousness--Embodiment--Language-Models-e1sm6k6</link>
			<guid isPermaLink="false">8fefd42a-e85f-440b-b23d-720cd0c4d8e9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 24 Dec 2022 18:26:48 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62642246/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-24%2F1c7f0dc8-f752-0f66-bcbb-9a055c8467c3.mp3" length="115546176" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Professor &lt;strong&gt;Murray Shanahan&lt;/strong&gt; is a renowned researcher on sophisticated cognition and its implications for artificial intelligence. His 2016 article ‘Conscious Exotica’ explores the Space of Possible Minds, a concept first proposed by philosopher Aaron Sloman in 1984, which includes all the different forms of minds from those of other animals to those of artificial intelligence. Shanahan rejects the idea of an impenetrable realm of subjective experience and argues that the majority of the space of possible minds may be occupied by non-natural variants, such as the ‘conscious exotica’ of which he speaks. &amp;nbsp;In his paper ‘Talking About Large Language Models’, Shanahan discusses the capabilities and limitations of large language models (LLMs). He argues that prompt engineering is a key element for advanced AI systems, as it involves exploiting prompt prefixes to adjust LLMs to various tasks. However, Shanahan cautions against ascribing human-like characteristics to these systems, as they are fundamentally different and lack a shared comprehension with humans. Even though LLMs can be integrated into embodied systems, it does not mean that they possess human-like language abilities. Ultimately, Shanahan concludes that although LLMs are formidable and versatile, we must be wary of over-simplifying their capacities and limitations.&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/BqkWpP3uMMU&lt;/p&gt;
&lt;p&gt;Full references on the YT description.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:08:51] Consciousness and &amp;nbsp;Consciousness Exotica&lt;/p&gt;
&lt;p&gt;[00:34:59] Slightly Consciousness LLMs&lt;/p&gt;
&lt;p&gt;[00:38:05] Embodiment&lt;/p&gt;
&lt;p&gt;[00:51:32] Symbol Grounding&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:54:13] Emergence&lt;/p&gt;
&lt;p&gt;[00:57:09] Reasoning&lt;/p&gt;
&lt;p&gt;[01:03:16] Intentional Stance&lt;/p&gt;
&lt;p&gt;[01:07:06] Digression on Chomsky show and Andrew Lampinen&lt;/p&gt;
&lt;p&gt;[01:10:31] Prompt Engineering&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Find Murray online:&lt;/p&gt;
&lt;p&gt;https://www.doc.ic.ac.uk/~mpsha/&lt;/p&gt;
&lt;p&gt;https://twitter.com/mpshanahan?lang=en&lt;/p&gt;
&lt;p&gt;https://scholar.google.co.uk/citations?user=00bnGpAAAAAJ&amp;amp;hl=en&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:20:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1671896801021-769540cbc0a98.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>93</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#92 - SARA HOOKER - Fairness, Interpretability, Language Models]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>Sara Hooker is an exceptionally talented and accomplished leader and research scientist in the field of machine learning. She is the founder of Cohere For AI, a non-profit research lab that seeks to solve complex machine learning problems. She is passionate about creating more points of entry into machine learning research and has dedicated her efforts to understanding how progress in this field can be translated into reliable and accessible machine learning in the real-world.</p>
<p>Sara is also the co-founder of the Trustworthy ML Initiative, a forum and seminar series related to Trustworthy ML. She is on the advisory board of Patterns and is an active member of the MLC research group, which has a focus on making participation in machine learning research more accessible.</p>
<p>Before starting Cohere For AI, Sara worked as a research scientist at Google Brain. She has written several influential research papers, including "The Hardware Lottery", "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation", "Moving Beyond “Algorithmic Bias is a Data Problem”" and "Characterizing and Mitigating Bias in Compact Models".&nbsp;</p>
<p>In addition to her research work, Sara is also the founder of the local Bay Area non-profit Delta Analytics, which works with non-profits and communities all over the world to build technical capacity and empower others to use data. She regularly gives tutorials on machine learning fundamentals, interpretability, model compression and deep neural networks and is dedicated to collaborating with independent researchers around the world.</p>
<p>Sara Hooker is famous for writing a paper introducing the concept of the 'hardware lottery', in which the success of a research idea is determined not by its inherent superiority, but by its compatibility with available software and hardware. She argued that choices about software and hardware have had a substantial impact in deciding the outcomes of early computer science history, and that with the increasing heterogeneity of the hardware landscape, gains from advances in computing may become increasingly disparate. Sara proposed that an interim goal should be to create better feedback mechanisms for researchers to understand how their algorithms interact with the hardware they use. She suggested that domain-specific languages, auto-tuning of algorithmic parameters, and better profiling tools may help to alleviate this issue, as well as provide researchers with more informed opinions about how hardware and software should progress. Ultimately, Sara encouraged researchers to be mindful of the implications of the hardware lottery, as it could mean that progress on some research directions is further obstructed. If you want to learn more about that paper, watch our previous interview with Sara.</p>
<p>YT version: https://youtu.be/7oJui4eSCoY</p>
<p>MLST Discord: https://discord.gg/aNPkGUQtc5</p>
<p>TOC:</p>
<p>[00:00:00] Intro</p>
<p>[00:02:53] Interpretability / Fairness</p>
<p>[00:35:29] LLMs</p>
<p><br></p>
<p>Find Sara:</p>
<p>https://www.sarahooker.me/</p>
<p>https://twitter.com/sarahookr</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/92---SARA-HOOKER---Fairness--Interpretability--Language-Models-e1skba2</link>
			<guid isPermaLink="false">5d53f325-5748-4518-ac54-d43b69c8c599</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 23 Dec 2022 01:32:15 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62581506/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-23%2Fe7589c5f-a1ef-d1e5-8af8-196ef6030af2.mp3" length="123651840" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Sara Hooker is an exceptionally talented and accomplished leader and research scientist in the field of machine learning. She is the founder of Cohere For AI, a non-profit research lab that seeks to solve complex machine learning problems. She is passionate about creating more points of entry into machine learning research and has dedicated her efforts to understanding how progress in this field can be translated into reliable and accessible machine learning in the real-world.&lt;/p&gt;
&lt;p&gt;Sara is also the co-founder of the Trustworthy ML Initiative, a forum and seminar series related to Trustworthy ML. She is on the advisory board of Patterns and is an active member of the MLC research group, which has a focus on making participation in machine learning research more accessible.&lt;/p&gt;
&lt;p&gt;Before starting Cohere For AI, Sara worked as a research scientist at Google Brain. She has written several influential research papers, including &quot;The Hardware Lottery&quot;, &quot;The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation&quot;, &quot;Moving Beyond “Algorithmic Bias is a Data Problem”&quot; and &quot;Characterizing and Mitigating Bias in Compact Models&quot;.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In addition to her research work, Sara is also the founder of the local Bay Area non-profit Delta Analytics, which works with non-profits and communities all over the world to build technical capacity and empower others to use data. She regularly gives tutorials on machine learning fundamentals, interpretability, model compression and deep neural networks and is dedicated to collaborating with independent researchers around the world.&lt;/p&gt;
&lt;p&gt;Sara Hooker is famous for writing a paper introducing the concept of the &apos;hardware lottery&apos;, in which the success of a research idea is determined not by its inherent superiority, but by its compatibility with available software and hardware. She argued that choices about software and hardware have had a substantial impact in deciding the outcomes of early computer science history, and that with the increasing heterogeneity of the hardware landscape, gains from advances in computing may become increasingly disparate. Sara proposed that an interim goal should be to create better feedback mechanisms for researchers to understand how their algorithms interact with the hardware they use. She suggested that domain-specific languages, auto-tuning of algorithmic parameters, and better profiling tools may help to alleviate this issue, as well as provide researchers with more informed opinions about how hardware and software should progress. Ultimately, Sara encouraged researchers to be mindful of the implications of the hardware lottery, as it could mean that progress on some research directions is further obstructed. If you want to learn more about that paper, watch our previous interview with Sara.&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/7oJui4eSCoY&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro&lt;/p&gt;
&lt;p&gt;[00:02:53] Interpretability / Fairness&lt;/p&gt;
&lt;p&gt;[00:35:29] LLMs&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Find Sara:&lt;/p&gt;
&lt;p&gt;https://www.sarahooker.me/&lt;/p&gt;
&lt;p&gt;https://twitter.com/sarahookr&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1671759099566-baf13a989db9e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>92</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#91 - HATTIE ZHOU - Teaching Algorithmic Reasoning via In-context Learning #NeurIPS]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p><br></p>
<p>Hattie Zhou, a PhD student at Université de Montréal and Mila, has set out to understand and explain the performance of modern neural networks, believing it a key factor in building better, more trusted models. Having previously worked as a data scientist at Uber, a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research, she has recently released a paper in collaboration with the Google Brain team, titled ‘Teaching Algorithmic Reasoning via In-context Learning’. In this work, Hattie identifies and examines four key stages for successfully teaching algorithmic reasoning to large language models (LLMs): formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to combine skills, and teaching how to use skills as tools. Through the application of algorithmic prompting, Hattie has achieved remarkable results, with an order of magnitude error reduction on some tasks compared to the best available baselines. This breakthrough demonstrates algorithmic prompting’s viability as an approach for teaching algorithmic reasoning to LLMs, and may have implications for other tasks requiring similar reasoning capabilities.</p>
<p><br></p>
<p>TOC</p>
<p>[00:00:00] Hattie Zhou</p>
<p>[00:19:49] Markus Rabe [Google Brain]</p>
<p><br></p>
<p>Hattie's Twitter - https://twitter.com/oh_that_hat</p>
<p>Website - http://hattiezhou.com/</p>
<p><br></p>
<p>Teaching Algorithmic Reasoning via In-context Learning [Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi]</p>
<p>https://arxiv.org/pdf/2211.09066.pdf</p>
<p><br></p>
<p>Markus Rabe [Google Brain]:</p>
<p>https://twitter.com/markusnrabe</p>
<p>https://research.google/people/106335/</p>
<p>https://www.linkedin.com/in/markusnrabe</p>
<p><br></p>
<p>Autoformalization with Large Language Models [Albert Jiang Charles Edgar Staats Christian Szegedy Markus Rabe Mateja Jamnik Wenda Li Yuhuai Tony Wu]</p>
<p>https://research.google/pubs/pub51691/</p>
<p><br></p>
<p>Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT: https://youtu.be/80i6D2TJdQ4</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/91---HATTIE-ZHOU---Teaching-Algorithmic-Reasoning-via-In-context-Learning-NeurIPS-e1sgvh9</link>
			<guid isPermaLink="false">a41b3a0c-d5ca-4269-a23b-33f47cdc08fc</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 20 Dec 2022 17:04:50 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62471145/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-20%2F583c8109-d89f-19c1-8546-79e6540a4469.mp3" length="50996160" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Hattie Zhou, a PhD student at Université de Montréal and Mila, has set out to understand and explain the performance of modern neural networks, believing it a key factor in building better, more trusted models. Having previously worked as a data scientist at Uber, a private equity analyst at Radar Capital, and an economic consultant at Cornerstone Research, she has recently released a paper in collaboration with the Google Brain team, titled ‘Teaching Algorithmic Reasoning via In-context Learning’. In this work, Hattie identifies and examines four key stages for successfully teaching algorithmic reasoning to large language models (LLMs): formulating algorithms as skills, teaching multiple skills simultaneously, teaching how to combine skills, and teaching how to use skills as tools. Through the application of algorithmic prompting, Hattie has achieved remarkable results, with an order of magnitude error reduction on some tasks compared to the best available baselines. This breakthrough demonstrates algorithmic prompting’s viability as an approach for teaching algorithmic reasoning to LLMs, and may have implications for other tasks requiring similar reasoning capabilities.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC&lt;/p&gt;
&lt;p&gt;[00:00:00] Hattie Zhou&lt;/p&gt;
&lt;p&gt;[00:19:49] Markus Rabe [Google Brain]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Hattie&apos;s Twitter - https://twitter.com/oh_that_hat&lt;/p&gt;
&lt;p&gt;Website - http://hattiezhou.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Teaching Algorithmic Reasoning via In-context Learning [Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2211.09066.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Markus Rabe [Google Brain]:&lt;/p&gt;
&lt;p&gt;https://twitter.com/markusnrabe&lt;/p&gt;
&lt;p&gt;https://research.google/people/106335/&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/markusnrabe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Autoformalization with Large Language Models [Albert Jiang Charles Edgar Staats Christian Szegedy Markus Rabe Mateja Jamnik Wenda Li Yuhuai Tony Wu]&lt;/p&gt;
&lt;p&gt;https://research.google/pubs/pub51691/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/80i6D2TJdQ4&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:21:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1671555876747-14f5d12cd5f5a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>91</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[(Music Removed) #90 - Prof. DAVID CHALMERS - Consciousness in LLMs [Special Edition]]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>(On the main version we released; the music was a tiny bit too loud in places, and some pieces had percussion which was a bit distracting -- here is a version with all music removed so you have the option! )</p>
<p>David Chalmers is a professor of philosophy and neural science at New York University, and an honorary professor of philosophy at the Australian National University. He is the co-director of the Center for Mind, Brain, and Consciousness, as well as the PhilPapers Foundation. His research focuses on the philosophy of mind, especially consciousness, and its connection to fields such as cognitive science, physics, and technology. He also investigates areas such as the philosophy of language, metaphysics, and epistemology. With his impressive breadth of knowledge and experience, David Chalmers is a leader in the philosophical community.</p>
<p><br></p>
<p>The central challenge for consciousness studies is to explain how something immaterial, subjective, and personal can arise out of something material, objective, and impersonal. This is illustrated by the example of a bat, whose sensory experience is much different from ours, making it difficult to imagine what it's like to be one. Thomas Nagel's "inconceivability argument" has its advantages and disadvantages, but ultimately it is impossible to solve the mind-body problem due to the subjective nature of experience. This is further explored by examining the concept of philosophical zombies, which are physically and behaviorally indistinguishable from conscious humans yet lack conscious experience. This has implications for the Hard Problem of Consciousness, which is the attempt to explain how mental states are linked to neurophysiological activity. The Chinese Room Argument is used as a thought experiment to explain why physicality may be insufficient to be the source of the subjective, coherent experience we call consciousness. Despite much debate, the Hard Problem of Consciousness remains unsolved. Chalmers has been working on a functional approach to decide whether large language models are, or could be conscious.&nbsp;</p>
<p><br></p>
<p>Filmed at #neurips22</p>
<p><br></p>
<p>Discord: https://discord.gg/aNPkGUQtc5</p>
<p>Pod: https://anchor.fm/machinelearningstreettalk/episodes/90---Prof--DAVID-CHALMERS---Slightly-Conscious-LLMs-e1sej50</p>
<p><br></p>
<p>TOC;</p>
<p>[00:00:00] Introduction</p>
<p>[00:00:40] LLMs consciousness pitch</p>
<p>[00:06:33] Philosophical Zombies</p>
<p>[00:09:26] The hard problem of consciousness</p>
<p>[00:11:40] Nagal's bat and intelligibility&nbsp;</p>
<p>[00:21:04] LLM intro clip from NeurIPS</p>
<p>[00:22:55] Connor Leahy on self-awareness in LLMs</p>
<p>[00:23:30] Sneak peek from unreleased show - could consciousness be a submodule?</p>
<p>[00:33:44] SeppH</p>
<p>[00:36:15] Tim interviews David at NeurIPS (functionalism / panpsychism / Searle)</p>
<p>[00:45:20] Peter Hase interviews Chalmers (focus on interpretability/safety)</p>
<p><br></p>
<p>Panel:</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p><br></p>
<p>Contact David;</p>
<p>https://mobile.twitter.com/davidchalmers42</p>
<p>https://consc.net/</p>
<p><br></p>
<p>References;</p>
<p><br></p>
<p>Could a Large Language Model Be Conscious? [Chalmers NeurIPS22 talk]</p>
<p>https://nips.cc/media/neurips-2022/Slides/55867.pdf</p>
<p><br></p>
<p>What Is It Like to Be a Bat? [Nagel]</p>
<p>https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf</p>
<p><br></p>
<p>Zombies</p>
<p>https://plato.stanford.edu/entries/zombies/</p>
<p><br></p>
<p>zombies on the web [Chalmers]</p>
<p>https://consc.net/zombies-on-the-web/</p>
<p><br></p>
<p>The hard problem of consciousness [Chalmers]</p>
<p>https://psycnet.apa.org/record/2007-00485-017</p>
<p><br></p>
<p>David Chalmers, "Are Large Language Models Sentient?" [NYU talk, same as at NeurIPS]</p>
<p>https://www.youtube.com/watch?v=-BcuCmf00_Y</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Music-Removed-90---Prof--DAVID-CHALMERS---Consciousness-in-LLMs-Special-Edition-e1sf1l7</link>
			<guid isPermaLink="false">19aad36d-57c1-4f0a-b303-8aacc91bf965</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 19 Dec 2022 11:10:43 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62407783/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-19%2Faeed01f9-97eb-4232-5f12-4a81e6bf34c5.mp3" length="129116160" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;(On the main version we released; the music was a tiny bit too loud in places, and some pieces had percussion which was a bit distracting -- here is a version with all music removed so you have the option! )&lt;/p&gt;
&lt;p&gt;David Chalmers is a professor of philosophy and neural science at New York University, and an honorary professor of philosophy at the Australian National University. He is the co-director of the Center for Mind, Brain, and Consciousness, as well as the PhilPapers Foundation. His research focuses on the philosophy of mind, especially consciousness, and its connection to fields such as cognitive science, physics, and technology. He also investigates areas such as the philosophy of language, metaphysics, and epistemology. With his impressive breadth of knowledge and experience, David Chalmers is a leader in the philosophical community.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The central challenge for consciousness studies is to explain how something immaterial, subjective, and personal can arise out of something material, objective, and impersonal. This is illustrated by the example of a bat, whose sensory experience is much different from ours, making it difficult to imagine what it&apos;s like to be one. Thomas Nagel&apos;s &quot;inconceivability argument&quot; has its advantages and disadvantages, but ultimately it is impossible to solve the mind-body problem due to the subjective nature of experience. This is further explored by examining the concept of philosophical zombies, which are physically and behaviorally indistinguishable from conscious humans yet lack conscious experience. This has implications for the Hard Problem of Consciousness, which is the attempt to explain how mental states are linked to neurophysiological activity. The Chinese Room Argument is used as a thought experiment to explain why physicality may be insufficient to be the source of the subjective, coherent experience we call consciousness. Despite much debate, the Hard Problem of Consciousness remains unsolved. Chalmers has been working on a functional approach to decide whether large language models are, or could be conscious.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Filmed at #neurips22&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;Pod: https://anchor.fm/machinelearningstreettalk/episodes/90---Prof--DAVID-CHALMERS---Slightly-Conscious-LLMs-e1sej50&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC;&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:00:40] LLMs consciousness pitch&lt;/p&gt;
&lt;p&gt;[00:06:33] Philosophical Zombies&lt;/p&gt;
&lt;p&gt;[00:09:26] The hard problem of consciousness&lt;/p&gt;
&lt;p&gt;[00:11:40] Nagal&apos;s bat and intelligibility&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:21:04] LLM intro clip from NeurIPS&lt;/p&gt;
&lt;p&gt;[00:22:55] Connor Leahy on self-awareness in LLMs&lt;/p&gt;
&lt;p&gt;[00:23:30] Sneak peek from unreleased show - could consciousness be a submodule?&lt;/p&gt;
&lt;p&gt;[00:33:44] SeppH&lt;/p&gt;
&lt;p&gt;[00:36:15] Tim interviews David at NeurIPS (functionalism / panpsychism / Searle)&lt;/p&gt;
&lt;p&gt;[00:45:20] Peter Hase interviews Chalmers (focus on interpretability/safety)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel:&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Contact David;&lt;/p&gt;
&lt;p&gt;https://mobile.twitter.com/davidchalmers42&lt;/p&gt;
&lt;p&gt;https://consc.net/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Could a Large Language Model Be Conscious? [Chalmers NeurIPS22 talk]&lt;/p&gt;
&lt;p&gt;https://nips.cc/media/neurips-2022/Slides/55867.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What Is It Like to Be a Bat? [Nagel]&lt;/p&gt;
&lt;p&gt;https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Zombies&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/zombies/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;zombies on the web [Chalmers]&lt;/p&gt;
&lt;p&gt;https://consc.net/zombies-on-the-web/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The hard problem of consciousness [Chalmers]&lt;/p&gt;
&lt;p&gt;https://psycnet.apa.org/record/2007-00485-017&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;David Chalmers, &quot;Are Large Language Models Sentient?&quot; [NYU talk, same as at NeurIPS]&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=-BcuCmf00_Y&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1671448219215-006a9ea45f7c4.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>90</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#90 - Prof. DAVID CHALMERS - Consciousness in LLMs [Special Edition]]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>David Chalmers is a professor of philosophy and neural science at New York University, and an honorary professor of philosophy at the Australian National University. He is the co-director of the Center for Mind, Brain, and Consciousness, as well as the PhilPapers Foundation. His research focuses on the philosophy of mind, especially consciousness, and its connection to fields such as cognitive science, physics, and technology. He also investigates areas such as the philosophy of language, metaphysics, and epistemology. With his impressive breadth of knowledge and experience, David Chalmers is a leader in the philosophical community.</p>
<p><br></p>
<p>The central challenge for consciousness studies is to explain how something immaterial, subjective, and personal can arise out of something material, objective, and impersonal. This is illustrated by the example of a bat, whose sensory experience is much different from ours, making it difficult to imagine what it's like to be one. Thomas Nagel's "inconceivability argument" has its advantages and disadvantages, but ultimately it is impossible to solve the mind-body problem due to the subjective nature of experience. This is further explored by examining the concept of philosophical zombies, which are physically and behaviorally indistinguishable from conscious humans yet lack conscious experience. This has implications for the Hard Problem of Consciousness, which is the attempt to explain how mental states are linked to neurophysiological activity. The Chinese Room Argument is used as a thought experiment to explain why physicality may be insufficient to be the source of the subjective, coherent experience we call consciousness. Despite much debate, the Hard Problem of Consciousness remains unsolved. Chalmers has been working on a functional approach to decide whether large language models are, or could be conscious.&nbsp;</p>
<p><br></p>
<p>Filmed at #neurips22</p>
<p><br></p>
<p>Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT: https://youtu.be/T7aIxncLuWk</p>
<p><br></p>
<p>TOC;</p>
<p>[00:00:00] Introduction</p>
<p>[00:00:40] LLMs consciousness pitch</p>
<p>[00:06:33] Philosophical Zombies</p>
<p>[00:09:26] The hard problem of consciousness</p>
<p>[<a href="https://www.youtube.com/watch?v=T7aIxncLuWk&amp;t=700s">00:11:40</a>] Nagal's bat and intelligibility</p>
<p>[00:21:04] LLM intro clip from NeurIPS</p>
<p>[00:22:55] Connor Leahy on self-awareness in LLMs</p>
<p>[00:23:30] Sneak peek from unreleased show - could consciousness be a submodule?</p>
<p>[00:33:44] SeppH</p>
<p>[00:36:15] Tim interviews David at NeurIPS (functionalism / panpsychism / Searle)</p>
<p>[00:45:20] Peter Hase interviews Chalmers (focus on interpretability/safety)</p>
<p><br></p>
<p>Panel:</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p><br></p>
<p>Contact David;</p>
<p>https://mobile.twitter.com/davidchalmers42</p>
<p>https://consc.net/</p>
<p><br></p>
<p>References;</p>
<p><br></p>
<p>Could a Large Language Model Be Conscious? [Chalmers NeurIPS22 talk]&nbsp;</p>
<p>https://nips.cc/media/neurips-2022/Slides/55867.pdf</p>
<p><br></p>
<p>What Is It Like to Be a Bat? [Nagel]</p>
<p>https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf</p>
<p><br></p>
<p>Zombies</p>
<p>https://plato.stanford.edu/entries/zombies/</p>
<p><br></p>
<p>zombies on the web [Chalmers]</p>
<p>https://consc.net/zombies-on-the-web/</p>
<p><br></p>
<p>The hard problem of consciousness [Chalmers]</p>
<p>https://psycnet.apa.org/record/2007-00485-017</p>
<p><br></p>
<p>David Chalmers, "Are Large Language Models Sentient?" [NYU talk, same as at NeurIPS]</p>
<p>https://www.youtube.com/watch?v=-BcuCmf00_Y</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/90---Prof--DAVID-CHALMERS---Consciousness-in-LLMs-Special-Edition-e1sej50</link>
			<guid isPermaLink="false">886902b3-7e8c-4678-9f13-e0e23f383640</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 19 Dec 2022 01:23:06 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62392928/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-19%2F292febaa-6bad-4785-baf5-8b4eef6b373f.mp3" length="129116160" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;David Chalmers is a professor of philosophy and neural science at New York University, and an honorary professor of philosophy at the Australian National University. He is the co-director of the Center for Mind, Brain, and Consciousness, as well as the PhilPapers Foundation. His research focuses on the philosophy of mind, especially consciousness, and its connection to fields such as cognitive science, physics, and technology. He also investigates areas such as the philosophy of language, metaphysics, and epistemology. With his impressive breadth of knowledge and experience, David Chalmers is a leader in the philosophical community.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The central challenge for consciousness studies is to explain how something immaterial, subjective, and personal can arise out of something material, objective, and impersonal. This is illustrated by the example of a bat, whose sensory experience is much different from ours, making it difficult to imagine what it&apos;s like to be one. Thomas Nagel&apos;s &quot;inconceivability argument&quot; has its advantages and disadvantages, but ultimately it is impossible to solve the mind-body problem due to the subjective nature of experience. This is further explored by examining the concept of philosophical zombies, which are physically and behaviorally indistinguishable from conscious humans yet lack conscious experience. This has implications for the Hard Problem of Consciousness, which is the attempt to explain how mental states are linked to neurophysiological activity. The Chinese Room Argument is used as a thought experiment to explain why physicality may be insufficient to be the source of the subjective, coherent experience we call consciousness. Despite much debate, the Hard Problem of Consciousness remains unsolved. Chalmers has been working on a functional approach to decide whether large language models are, or could be conscious.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Filmed at #neurips22&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/T7aIxncLuWk&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC;&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:00:40] LLMs consciousness pitch&lt;/p&gt;
&lt;p&gt;[00:06:33] Philosophical Zombies&lt;/p&gt;
&lt;p&gt;[00:09:26] The hard problem of consciousness&lt;/p&gt;
&lt;p&gt;[&lt;a href=&quot;https://www.youtube.com/watch?v=T7aIxncLuWk&amp;amp;t=700s&quot;&gt;00:11:40&lt;/a&gt;] Nagal&apos;s bat and intelligibility&lt;/p&gt;
&lt;p&gt;[00:21:04] LLM intro clip from NeurIPS&lt;/p&gt;
&lt;p&gt;[00:22:55] Connor Leahy on self-awareness in LLMs&lt;/p&gt;
&lt;p&gt;[00:23:30] Sneak peek from unreleased show - could consciousness be a submodule?&lt;/p&gt;
&lt;p&gt;[00:33:44] SeppH&lt;/p&gt;
&lt;p&gt;[00:36:15] Tim interviews David at NeurIPS (functionalism / panpsychism / Searle)&lt;/p&gt;
&lt;p&gt;[00:45:20] Peter Hase interviews Chalmers (focus on interpretability/safety)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel:&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Contact David;&lt;/p&gt;
&lt;p&gt;https://mobile.twitter.com/davidchalmers42&lt;/p&gt;
&lt;p&gt;https://consc.net/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Could a Large Language Model Be Conscious? [Chalmers NeurIPS22 talk]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://nips.cc/media/neurips-2022/Slides/55867.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What Is It Like to Be a Bat? [Nagel]&lt;/p&gt;
&lt;p&gt;https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Zombies&lt;/p&gt;
&lt;p&gt;https://plato.stanford.edu/entries/zombies/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;zombies on the web [Chalmers]&lt;/p&gt;
&lt;p&gt;https://consc.net/zombies-on-the-web/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The hard problem of consciousness [Chalmers]&lt;/p&gt;
&lt;p&gt;https://psycnet.apa.org/record/2007-00485-017&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;David Chalmers, &quot;Are Large Language Models Sentient?&quot; [NYU talk, same as at NeurIPS]&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=-BcuCmf00_Y&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:53:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1671416181447-a16f6e5403572.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>90</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#88 Dr. WALID SABA - Why machines will never rule the world [UNPLUGGED]]]></title>
			<description><![CDATA[<p>Support us! https://www.patreon.com/mlst</p>
<p>Dr. Walid Saba recently reviewed the book Machines Will Never Rule The World, which argues that strong AI is impossible. He acknowledges the complexity of modeling mental processes and language, as well as interactive dialogues, and questions the authors' use of "never." Despite his skepticism, he is impressed with recent developments in large language models, though he questions the extent of their success.</p>
<p>We then discussed the successes of cognitive science. Walid believes that something has been achieved which many cognitive scientists would never accept, namely the ability to learn from data empirically. Keith agrees that this is a huge step, but notes that there is still much work to be done to get to the "other 5%" of accuracy. They both agree that the current models are too brittle and require much more data and parameters to get to the desired level of accuracy.</p>
<p>Walid then expresses admiration for deep learning systems' ability to learn non-trivial aspects of language from ingesting text only. He argues that this is an "existential proof" of language competency and that it would be impossible for a group of luminaries such as Montague, Marvin Minsky, John McCarthy, and a thousand other bright engineers to replicate the same level of competency as we have now with LLMs. He then discusses the problem of semantics and pragmatics, as well as symbol grounding, and expresses skepticism about grounded meaning and embodiment. He believes that artificial intelligence should be used to solve real-world problems which require human intelligence but not believe that robots should be built to understand love or other subjective feelings.</p>
<p>We discussed the unique properties of natural human language. Walid believes that the core unique property is the ability to do abductive reasoning, which is the process of reasoning to the best explanation or understanding. Keith adds that there are two types of abduction - one for generating hypotheses and one for justifying them. In both cases, abductive reasoning involves choosing from a set of plausible possibilities.</p>
<p>Finally, we discussed the book "Machines Will Never Rule The World" and its argument that the current mathematics and technology is not enough to model complex systems. Walid agrees with the book's argument but is still optimistic that a new mathematics can be discovered. Keith suggests the possibility of an AGI discovering the mathematics to create itself. They also discussed how the book could serve as a reminder to temper the hype surrounding AI and to focus on exploration, creativity, and daring ideas. Walid ended by stressing the importance of science, noting that engineers should play within the Venn diagrams drawn by scientists, rather than trying to hack their way through it.</p>
<p>Transcript: https://share.descript.com/view/BFQb5iaegJC</p>
<p>Discord: https://discord.gg/aNPkGUQtc5</p>
<p>YT: https://youtu.be/IMnWAuoucjo</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Intro</p>
<p>[00:06:52] Walid's change of heart on DL/LLMs and on the skeptics like Gary Marcus</p>
<p>[00:22:52] Symbol Grounding</p>
<p>[00:32:26] On Montague</p>
<p>[00:40:41] On Abduction</p>
<p>[00:50:54] Language of thought</p>
<p>[00:56:08] Why machines will never rule the world book review</p>
<p>[01:20:06] Engineers should play in the scientists Venn Diagram!</p>
<p>Panel;</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p>Mark Mcguill</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/88-Dr--WALID-SABA---Why-machines-will-never-rule-the-world-UNPLUGGED-e1sb1di</link>
			<guid isPermaLink="false">6c22257a-b16e-40f3-9d32-35103da10739</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 16 Dec 2022 02:23:04 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62276466/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-16%2F848d26cf-34f8-7d8d-fe1f-775d3bc965ea.mp3" length="118075886" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Dr. Walid Saba recently reviewed the book Machines Will Never Rule The World, which argues that strong AI is impossible. He acknowledges the complexity of modeling mental processes and language, as well as interactive dialogues, and questions the authors&apos; use of &quot;never.&quot; Despite his skepticism, he is impressed with recent developments in large language models, though he questions the extent of their success.&lt;/p&gt;
&lt;p&gt;We then discussed the successes of cognitive science. Walid believes that something has been achieved which many cognitive scientists would never accept, namely the ability to learn from data empirically. Keith agrees that this is a huge step, but notes that there is still much work to be done to get to the &quot;other 5%&quot; of accuracy. They both agree that the current models are too brittle and require much more data and parameters to get to the desired level of accuracy.&lt;/p&gt;
&lt;p&gt;Walid then expresses admiration for deep learning systems&apos; ability to learn non-trivial aspects of language from ingesting text only. He argues that this is an &quot;existential proof&quot; of language competency and that it would be impossible for a group of luminaries such as Montague, Marvin Minsky, John McCarthy, and a thousand other bright engineers to replicate the same level of competency as we have now with LLMs. He then discusses the problem of semantics and pragmatics, as well as symbol grounding, and expresses skepticism about grounded meaning and embodiment. He believes that artificial intelligence should be used to solve real-world problems which require human intelligence but not believe that robots should be built to understand love or other subjective feelings.&lt;/p&gt;
&lt;p&gt;We discussed the unique properties of natural human language. Walid believes that the core unique property is the ability to do abductive reasoning, which is the process of reasoning to the best explanation or understanding. Keith adds that there are two types of abduction - one for generating hypotheses and one for justifying them. In both cases, abductive reasoning involves choosing from a set of plausible possibilities.&lt;/p&gt;
&lt;p&gt;Finally, we discussed the book &quot;Machines Will Never Rule The World&quot; and its argument that the current mathematics and technology is not enough to model complex systems. Walid agrees with the book&apos;s argument but is still optimistic that a new mathematics can be discovered. Keith suggests the possibility of an AGI discovering the mathematics to create itself. They also discussed how the book could serve as a reminder to temper the hype surrounding AI and to focus on exploration, creativity, and daring ideas. Walid ended by stressing the importance of science, noting that engineers should play within the Venn diagrams drawn by scientists, rather than trying to hack their way through it.&lt;/p&gt;
&lt;p&gt;Transcript: https://share.descript.com/view/BFQb5iaegJC&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/aNPkGUQtc5&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/IMnWAuoucjo&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro&lt;/p&gt;
&lt;p&gt;[00:06:52] Walid&apos;s change of heart on DL/LLMs and on the skeptics like Gary Marcus&lt;/p&gt;
&lt;p&gt;[00:22:52] Symbol Grounding&lt;/p&gt;
&lt;p&gt;[00:32:26] On Montague&lt;/p&gt;
&lt;p&gt;[00:40:41] On Abduction&lt;/p&gt;
&lt;p&gt;[00:50:54] Language of thought&lt;/p&gt;
&lt;p&gt;[00:56:08] Why machines will never rule the world book review&lt;/p&gt;
&lt;p&gt;[01:20:06] Engineers should play in the scientists Venn Diagram!&lt;/p&gt;
&lt;p&gt;Panel;&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Mark Mcguill&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:21:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1671157367284-7975c2d5eedaa.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>88</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#86 - Prof. YANN LECUN and Dr. RANDALL BALESTRIERO - SSL, Data Augmentation, Reward isn't enough [NEURIPS2022] ]]></title>
			<description><![CDATA[<p>Yann LeCun is a French computer scientist known for his pioneering work on convolutional neural networks, optical character recognition and computer vision. He is a Silver Professor at New York University and Vice President, Chief AI Scientist at Meta. Along with Yoshua Bengio and Geoffrey Hinton, he was awarded the 2018 Turing Award for their work on deep learning, earning them the nickname of the "Godfathers of Deep Learning".</p>
<p><br></p>
<p>Dr. Randall Balestriero has been researching learnable signal processing since 2013, with a focus on learnable parametrized wavelets and deep wavelet transforms. His research has been used by NASA, leading to applications such as Marsquake detection. During his PhD at Rice University, Randall explored deep networks from a theoretical perspective and improved state-of-the-art methods such as batch-normalization and generative networks. Later, when joining Meta AI Research (FAIR) as a postdoc with Prof. Yann LeCun, Randall further broadened his research interests to include self-supervised learning and the biases emerging from data-augmentation and regularization, resulting in numerous publications.</p>
<p><br></p>
<p>Episode recorded live at NeurIPS.&nbsp;</p>
<p><br></p>
<p>YT: https://youtu.be/9dLd6n9yT8U (references are there)</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst&nbsp;</p>
<p>Host: Dr. Tim Scarfe</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] LeCun interview</p>
<p>[00:18:25] Randall Balestriero interview (mostly on spectral SSL paper, first ref)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/86---Prof--YANN-LECUN-and-Dr--RANDALL-BALESTRIERO---SSL--Data-Augmentation--Reward-isnt-enough-NEURIPS2022-e1s2kkk</link>
			<guid isPermaLink="false">6b251bdc-dd72-4587-bf92-e3aa74f46437</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 11 Dec 2022 00:41:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/62001236/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-11%2F31ada0f7-f4f4-38b1-bd5c-7d6c7a699e8f.mp3" length="73139520" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Yann LeCun is a French computer scientist known for his pioneering work on convolutional neural networks, optical character recognition and computer vision. He is a Silver Professor at New York University and Vice President, Chief AI Scientist at Meta. Along with Yoshua Bengio and Geoffrey Hinton, he was awarded the 2018 Turing Award for their work on deep learning, earning them the nickname of the &quot;Godfathers of Deep Learning&quot;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Randall Balestriero has been researching learnable signal processing since 2013, with a focus on learnable parametrized wavelets and deep wavelet transforms. His research has been used by NASA, leading to applications such as Marsquake detection. During his PhD at Rice University, Randall explored deep networks from a theoretical perspective and improved state-of-the-art methods such as batch-normalization and generative networks. Later, when joining Meta AI Research (FAIR) as a postdoc with Prof. Yann LeCun, Randall further broadened his research interests to include self-supervised learning and the biases emerging from data-augmentation and regularization, resulting in numerous publications.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Episode recorded live at NeurIPS.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/9dLd6n9yT8U (references are there)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Host: Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] LeCun interview&lt;/p&gt;
&lt;p&gt;[00:18:25] Randall Balestriero interview (mostly on spectral SSL paper, first ref)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:30:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1670719246292-1752f40a37d24.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>86</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#85 Dr. Petar Veličković (Deepmind) - Categories, Graphs, Reasoning [NEURIPS22 UNPLUGGED]]]></title>
			<description><![CDATA[<p>Dr. Petar Veličković &nbsp;is a Staff Research Scientist at DeepMind, he has firmly established himself as one of the most significant up and coming researchers in the deep learning space. He invented Graph Attention Networks in 2017 and has been a leading light in the field ever since pioneering research in Graph Neural Networks, Geometric Deep Learning and also Neural Algorithmic reasoning. If you haven’t already, you should check out our video on the Geometric Deep learning blueprint, featuring Petar. I caught up with him last week at NeurIPS. In this show, from NeurIPS 2022 we discussed his recent work on category theory and graph neural networks.</p>
<p><br></p>
<p>https://petar-v.com/</p>
<p>https://twitter.com/PetarV_93/</p>
<p><br></p>
<p>TOC:</p>
<p>Categories &nbsp;(Cats for AI) [00:00:00]</p>
<p>Reasoning [00:14:44]</p>
<p>Extrapolation [00:19:09]</p>
<p>Ishan Misra Skit [00:27:50]</p>
<p>Graphs (Expander Graph Propagation) [00:29:18]</p>
<p><br></p>
<p>YT: https://youtu.be/1lkdWduuN14</p>
<p>MLST Discord: https://discord.gg/V25vQeFwhS</p>
<p><br></p>
<p>Support us! https://www.patreon.com/mlst</p>
<p><br></p>
<p>References on YT description, lots of them!&nbsp;</p>
<p><br></p>
<p>Host: Dr. Tim Scarfe</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/85-Dr--Petar-Velikovi-Deepmind---Categories--Graphs--Reasoning-NEURIPS22-UNPLUGGED-e1rvtha</link>
			<guid isPermaLink="false">225b5310-9d5d-41ad-ae02-aac8f9111736</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 08 Dec 2022 23:45:34 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/61912042/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-9%2F301976398-44100-2-92ee0219bd9a2.m4a" length="35836861" type="audio/x-m4a"/>
			<itunes:summary>&lt;p&gt;Dr. Petar Veličković &amp;nbsp;is a Staff Research Scientist at DeepMind, he has firmly established himself as one of the most significant up and coming researchers in the deep learning space. He invented Graph Attention Networks in 2017 and has been a leading light in the field ever since pioneering research in Graph Neural Networks, Geometric Deep Learning and also Neural Algorithmic reasoning. If you haven’t already, you should check out our video on the Geometric Deep learning blueprint, featuring Petar. I caught up with him last week at NeurIPS. In this show, from NeurIPS 2022 we discussed his recent work on category theory and graph neural networks.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://petar-v.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/PetarV_93/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;Categories &amp;nbsp;(Cats for AI) [00:00:00]&lt;/p&gt;
&lt;p&gt;Reasoning [00:14:44]&lt;/p&gt;
&lt;p&gt;Extrapolation [00:19:09]&lt;/p&gt;
&lt;p&gt;Ishan Misra Skit [00:27:50]&lt;/p&gt;
&lt;p&gt;Graphs (Expander Graph Propagation) [00:29:18]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/1lkdWduuN14&lt;/p&gt;
&lt;p&gt;MLST Discord: https://discord.gg/V25vQeFwhS&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us! https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References on YT description, lots of them!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Host: Dr. Tim Scarfe&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:36:55</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1670543128016-7566908c2a092.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>85</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#84 LAURA RUIS - Large language models are not zero-shot communicators [NEURIPS UNPLUGGED]]]></title>
			<description><![CDATA[<p>In this NeurIPSs interview, we speak with Laura Ruis about her research on the ability of language models to interpret language in context. She has designed a simple task to evaluate the performance of widely used state-of-the-art language models and has found that they struggle to make pragmatic inferences (implicatures). Tune in to learn more about her findings and what they mean for the future of conversational AI.</p>
<p><br></p>
<p>Laura Ruis</p>
<p>https://www.lauraruis.com/</p>
<p>https://twitter.com/LauraRuis</p>
<p><br></p>
<p>BLOOM</p>
<p>https://bigscience.huggingface.co/blog/bloom</p>
<p><br></p>
<p>Large language models are not zero-shot communicators [Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette]</p>
<p>https://arxiv.org/abs/2210.14986</p>
<p><br></p>
<p>[Zhang et al] OPT: Open Pre-trained Transformer Language Models</p>
<p>https://arxiv.org/pdf/2205.01068.pdf</p>
<p><br></p>
<p>[Lampinen] Can language models handle recursively nested grammatical structures? A case study on comparing models and humans</p>
<p>https://arxiv.org/pdf/2210.15303.pdf</p>
<p><br></p>
<p>[Gary Marcus] Horse rides astronaut</p>
<p>https://garymarcus.substack.com/p/horse-rides-astronaut</p>
<p><br></p>
<p>[Gary Marcus] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about</p>
<p>https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/</p>
<p><br></p>
<p>[Bender et al] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</p>
<p>https://dl.acm.org/doi/10.1145/3442188.3445922&nbsp;</p>
<p><br></p>
<p>[janus] Simulators (Less Wrong)</p>
<p>https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/84-LAURA-RUIS---Large-language-models-are-not-zero-shot-communicators-NEURIPS-UNPLUGGED-e1rri6k</link>
			<guid isPermaLink="false">e6e0bec4-f28e-4553-a573-33502348f5b2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 06 Dec 2022 17:36:04 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/61769364/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-6%2F0b5fdbf7-c39d-a59e-34ae-68a8b363175a.mp3" length="66714240" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this NeurIPSs interview, we speak with Laura Ruis about her research on the ability of language models to interpret language in context. She has designed a simple task to evaluate the performance of widely used state-of-the-art language models and has found that they struggle to make pragmatic inferences (implicatures). Tune in to learn more about her findings and what they mean for the future of conversational AI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Laura Ruis&lt;/p&gt;
&lt;p&gt;https://www.lauraruis.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/LauraRuis&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;BLOOM&lt;/p&gt;
&lt;p&gt;https://bigscience.huggingface.co/blog/bloom&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Large language models are not zero-shot communicators [Laura Ruis, Akbir Khan, Stella Biderman, Sara Hooker, Tim Rocktäschel, Edward Grefenstette]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.14986&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Zhang et al] OPT: Open Pre-trained Transformer Language Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2205.01068.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Lampinen] Can language models handle recursively nested grammatical structures? A case study on comparing models and humans&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2210.15303.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Gary Marcus] Horse rides astronaut&lt;/p&gt;
&lt;p&gt;https://garymarcus.substack.com/p/horse-rides-astronaut&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Gary Marcus] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about&lt;/p&gt;
&lt;p&gt;https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Bender et al] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?&lt;/p&gt;
&lt;p&gt;https://dl.acm.org/doi/10.1145/3442188.3445922&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[janus] Simulators (Less Wrong)&lt;/p&gt;
&lt;p&gt;https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:27:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1670348154861-7ed5929059803.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>84</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#83 Dr. ANDREW LAMPINEN (Deepmind) - Natural Language, Symbols and Grounding [NEURIPS2022 UNPLUGGED]]]></title>
			<description><![CDATA[<p>First in our unplugged series live from #NeurIPS2022</p>
<p>We discuss natural language understanding, symbol meaning and grounding and Chomsky with Dr. Andrew Lampinen from DeepMind.&nbsp;</p>
<p>We recorded a LOT of material from NeurIPS, keep an eye out for the uploads.&nbsp;</p>
<p><br></p>
<p>YT version: https://youtu.be/46A-BcBbMnA</p>
<p><br></p>
<p>References</p>
<p>[Paul Cisek] Beyond the computer metaphor: Behaviour as interaction</p>
<p>https://philpapers.org/rec/CISBTC</p>
<p><br></p>
<p>Linguistic Competence (Chomsky reference)</p>
<p>https://en.wikipedia.org/wiki/Linguistic_competence</p>
<p><br></p>
<p>[Andrew Lampinen] Can language models handle recursively nested grammatical structures? A case study on comparing models and humans</p>
<p>https://arxiv.org/abs/2210.15303</p>
<p><br></p>
<p>[Fodor et al] Connectionism and Cognitive Architecture: A Critical Analysis</p>
<p>https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf</p>
<p><br></p>
<p>[Melanie Mitchell et al] The Debate Over Understanding in AI's Large Language Models</p>
<p>https://arxiv.org/abs/2210.13966</p>
<p><br></p>
<p>[Gary Marcus] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about</p>
<p>https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/</p>
<p><br></p>
<p>[Bender et al] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</p>
<p>https://dl.acm.org/doi/10.1145/3442188.3445922</p>
<p><br></p>
<p>[Adam Santoro, Andrew Lampinen et al] Symbolic Behaviour in Artificial Intelligence</p>
<p>https://arxiv.org/abs/2102.03406</p>
<p><br></p>
<p>[Ishita Dasgupta, Lampinen et al] Language models show human-like content effects on reasoning</p>
<p>https://arxiv.org/abs/2207.07051</p>
<p><br></p>
<p>REACT - Synergizing Reasoning and Acting in Language Models</p>
<p>https://arxiv.org/pdf/2210.03629.pdf</p>
<p>https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html</p>
<p><br></p>
<p>[Fabian Paischer] HELM - History Compression via Language Models in Reinforcement Learning</p>
<p>https://ml-jku.github.io/blog/2022/helm/</p>
<p>https://arxiv.org/abs/2205.12258</p>
<p><br></p>
<p>[Laura Ruis] Large language models are not zero-shot communicators</p>
<p>https://arxiv.org/pdf/2210.14986.pdf</p>
<p><br></p>
<p>[Kumar] Using natural language and program abstractions to instill human inductive biases in machines</p>
<p>https://arxiv.org/pdf/2205.11558.pdf</p>
<p><br></p>
<p>Juho Kim</p>
<p>https://juhokim.com/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/83-Dr--ANDREW-LAMPINEN-Deepmind---Natural-Language--Symbols-and-Grounding-NEURIPS2022-UNPLUGGED-e1rne53</link>
			<guid isPermaLink="false">9e222cf7-61a7-438a-a673-d8014fd767f2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 04 Dec 2022 07:51:26 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/61634147/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-11-4%2Fc72faf62-61e3-adf3-f536-eb12931f875b.mp3" length="49479360" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;First in our unplugged series live from #NeurIPS2022&lt;/p&gt;
&lt;p&gt;We discuss natural language understanding, symbol meaning and grounding and Chomsky with Dr. Andrew Lampinen from DeepMind.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We recorded a LOT of material from NeurIPS, keep an eye out for the uploads.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/46A-BcBbMnA&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References&lt;/p&gt;
&lt;p&gt;[Paul Cisek] Beyond the computer metaphor: Behaviour as interaction&lt;/p&gt;
&lt;p&gt;https://philpapers.org/rec/CISBTC&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Linguistic Competence (Chomsky reference)&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Linguistic_competence&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Andrew Lampinen] Can language models handle recursively nested grammatical structures? A case study on comparing models and humans&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.15303&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Fodor et al] Connectionism and Cognitive Architecture: A Critical Analysis&lt;/p&gt;
&lt;p&gt;https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Melanie Mitchell et al] The Debate Over Understanding in AI&apos;s Large Language Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.13966&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Gary Marcus] GPT-3, Bloviator: OpenAI’s language generator has no idea what it’s talking about&lt;/p&gt;
&lt;p&gt;https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Bender et al] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?&lt;/p&gt;
&lt;p&gt;https://dl.acm.org/doi/10.1145/3442188.3445922&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Adam Santoro, Andrew Lampinen et al] Symbolic Behaviour in Artificial Intelligence&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2102.03406&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Ishita Dasgupta, Lampinen et al] Language models show human-like content effects on reasoning&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2207.07051&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;REACT - Synergizing Reasoning and Acting in Language Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2210.03629.pdf&lt;/p&gt;
&lt;p&gt;https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Fabian Paischer] HELM - History Compression via Language Models in Reinforcement Learning&lt;/p&gt;
&lt;p&gt;https://ml-jku.github.io/blog/2022/helm/&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2205.12258&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Laura Ruis] Large language models are not zero-shot communicators&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2210.14986.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Kumar] Using natural language and program abstractions to instill human inductive biases in machines&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2205.11558.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Juho Kim&lt;/p&gt;
&lt;p&gt;https://juhokim.com/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:20:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1670140246070-fe1857eec35dd.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>83</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#82 - Dr. JOSCHA BACH - Digital Physics, DL and Consciousness [UNPLUGGED]]]></title>
			<description><![CDATA[<p>AI Helps Ukraine - Charity Conference</p>
<p>A charity conference on AI to raise funds for medical and humanitarian aid for Ukraine</p>
<p>https://aihelpsukraine.cc/</p>
<p><br></p>
<p>YT version: https://youtu.be/LgwjcqhkOA4</p>
<p><br></p>
<p>Support us!</p>
<p>https://www.patreon.com/mlst&nbsp;</p>
<p><br></p>
<p>Dr. Joscha Bach (born 1973 in Weimar, Germany) is a German artificial intelligence researcher and cognitive scientist focusing on cognitive architectures, mental representation, emotion, social modelling, and multi-agent systems.&nbsp;</p>
<p>http://bach.ai/</p>
<p>https://twitter.com/plinz</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Ukraine Charity Conference and NeurIPS 2022</p>
<p>[00:03:40] Theory of computation, Godel, Penrose</p>
<p>[00:11:44] Modelling physical reality</p>
<p>[00:15:19] Is our universe infinite?</p>
<p>[00:24:30] Large language models, and on DL / is Gary Marcus hitting a wall?</p>
<p>[00:45:17] Generative models / Codex / Language of thought</p>
<p>[00:58:46] Consciousness (with Friston references)</p>
<p><br></p>
<p>References:</p>
<p><br></p>
<p>Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?) [Friston]</p>
<p>https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full</p>
<p><br></p>
<p>Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Yasaman Razeghi]</p>
<p>https://arxiv.org/abs/2202.07206</p>
<p><br></p>
<p>Deep Learning Is Hitting a Wall [Gary Marcus]</p>
<p>https://nautil.us/deep-learning-is-hitting-a-wall-238440/</p>
<p><br></p>
<p>Turing machines</p>
<p>https://en.wikipedia.org/wiki/Turing_machine</p>
<p>Lambda Calculus</p>
<p>https://en.wikipedia.org/wiki/Lambda_calculus</p>
<p>Godel's incompletness theorem</p>
<p>https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems</p>
<p>Oracle machine</p>
<p>https://en.wikipedia.org/wiki/Oracle_machine</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/82---Dr--JOSCHA-BACH---Digital-Physics--DL-and-Consciousness-UNPLUGGED-e1rctlc</link>
			<guid isPermaLink="false">d68ffd66-9ee4-494a-95b7-6a9e1993d4e5</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 27 Nov 2022 20:31:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/61289580/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-27%2F6acae91c-c188-3292-6e06-252781f05e02.mp3" length="108447552" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;AI Helps Ukraine - Charity Conference&lt;/p&gt;
&lt;p&gt;A charity conference on AI to raise funds for medical and humanitarian aid for Ukraine&lt;/p&gt;
&lt;p&gt;https://aihelpsukraine.cc/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/LgwjcqhkOA4&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Support us!&lt;/p&gt;
&lt;p&gt;https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Joscha Bach (born 1973 in Weimar, Germany) is a German artificial intelligence researcher and cognitive scientist focusing on cognitive architectures, mental representation, emotion, social modelling, and multi-agent systems.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;http://bach.ai/&lt;/p&gt;
&lt;p&gt;https://twitter.com/plinz&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Ukraine Charity Conference and NeurIPS 2022&lt;/p&gt;
&lt;p&gt;[00:03:40] Theory of computation, Godel, Penrose&lt;/p&gt;
&lt;p&gt;[00:11:44] Modelling physical reality&lt;/p&gt;
&lt;p&gt;[00:15:19] Is our universe infinite?&lt;/p&gt;
&lt;p&gt;[00:24:30] Large language models, and on DL / is Gary Marcus hitting a wall?&lt;/p&gt;
&lt;p&gt;[00:45:17] Generative models / Codex / Language of thought&lt;/p&gt;
&lt;p&gt;[00:58:46] Consciousness (with Friston references)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?) [Friston]&lt;/p&gt;
&lt;p&gt;https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Yasaman Razeghi]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2202.07206&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Deep Learning Is Hitting a Wall [Gary Marcus]&lt;/p&gt;
&lt;p&gt;https://nautil.us/deep-learning-is-hitting-a-wall-238440/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Turing machines&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Turing_machine&lt;/p&gt;
&lt;p&gt;Lambda Calculus&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Lambda_calculus&lt;/p&gt;
&lt;p&gt;Godel&apos;s incompletness theorem&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems&lt;/p&gt;
&lt;p&gt;Oracle machine&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Oracle_machine&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:15:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1669581057569-f0d53e15c2f2d.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>82</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#81 JULIAN TOGELIUS, Prof. KEN STANLEY - AGI, Games, Diversity & Creativity [UNPLUGGED]]]></title>
			<description><![CDATA[<p>Support us (and please rate on podcast app)</p>
<p>https://www.patreon.com/mlst&nbsp;</p>
<p><br></p>
<p>In this show tonight with Prof. Julian Togelius (NYU) and Prof. Ken Stanley we discuss open-endedness, AGI, game AI and reinforcement learning. &nbsp;</p>
<p><br></p>
<p>[Prof Julian Togelius]</p>
<p>https://engineering.nyu.edu/faculty/julian-togelius</p>
<p>https://twitter.com/togelius</p>
<p><br></p>
<p>[Prof Ken Stanley]</p>
<p>https://www.cs.ucf.edu/~kstanley/</p>
<p>https://twitter.com/kenneth0stanley</p>
<p><br></p>
<p>TOC:</p>
<p>[00:00:00] Introduction</p>
<p>[00:01:07] AI and computer games</p>
<p>[00:12:23] Intelligence</p>
<p>[00:21:27] Intelligence Explosion</p>
<p>[00:25:37] What should we be aspiring towards?</p>
<p>[00:29:14] Should AI contribute to culture?</p>
<p>[00:32:12] On creativity and open-endedness</p>
<p>[00:36:11] RL overfitting</p>
<p>[00:44:02] Diversity preservation</p>
<p>[00:51:18] Empiricism vs rationalism , in gradient descent the data pushes you around</p>
<p>[00:55:49] Creativity and interestingness (does complexity / information increase)</p>
<p>[01:03:20] What does a population give us?</p>
<p>[01:05:58] Emergence / generalisation snobbery</p>
<p><br></p>
<p>References;</p>
<p>[Hutter/Legg] Universal Intelligence: A Definition of Machine Intelligence</p>
<p>https://arxiv.org/abs/0712.3329</p>
<p><br></p>
<p>https://en.wikipedia.org/wiki/Artificial_general_intelligence</p>
<p>https://en.wikipedia.org/wiki/I._J._Good</p>
<p>https://en.wikipedia.org/wiki/G%C3%B6del_machine</p>
<p><br></p>
<p>[Chollet] Impossibility of intelligence explosion</p>
<p>https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec</p>
<p><br></p>
<p>[Alex Irpan] - RL is hard</p>
<p>https://www.alexirpan.com/2018/02/14/rl-hard.html</p>
<p>https://nethackchallenge.com/</p>
<p>Map elites</p>
<p>https://arxiv.org/abs/1504.04909</p>
<p><br></p>
<p>Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space</p>
<p>https://arxiv.org/abs/1912.02400</p>
<p><br></p>
<p>[Stanley] - Why greatness cannot be planned</p>
<p>https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237</p>
<p><br></p>
<p>[Lehman/Stanley] Abandoning Objectives: Evolution through the Search for Novelty Alone</p>
<p>https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/81-JULIAN-TOGELIUS--Prof--KEN-STANLEY---AGI--Games--Diversity--Creativity-UNPLUGGED-e1r1jhu</link>
			<guid isPermaLink="false">71bd8c87-6c62-4e64-a2dc-8e32651baf91</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 20 Nov 2022 04:05:21 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/60918782/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-20%2F02b7ed72-9d32-982d-f87c-b46408a6410d.mp3" length="100468061" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Support us (and please rate on podcast app)&lt;/p&gt;
&lt;p&gt;https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this show tonight with Prof. Julian Togelius (NYU) and Prof. Ken Stanley we discuss open-endedness, AGI, game AI and reinforcement learning. &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Prof Julian Togelius]&lt;/p&gt;
&lt;p&gt;https://engineering.nyu.edu/faculty/julian-togelius&lt;/p&gt;
&lt;p&gt;https://twitter.com/togelius&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Prof Ken Stanley]&lt;/p&gt;
&lt;p&gt;https://www.cs.ucf.edu/~kstanley/&lt;/p&gt;
&lt;p&gt;https://twitter.com/kenneth0stanley&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction&lt;/p&gt;
&lt;p&gt;[00:01:07] AI and computer games&lt;/p&gt;
&lt;p&gt;[00:12:23] Intelligence&lt;/p&gt;
&lt;p&gt;[00:21:27] Intelligence Explosion&lt;/p&gt;
&lt;p&gt;[00:25:37] What should we be aspiring towards?&lt;/p&gt;
&lt;p&gt;[00:29:14] Should AI contribute to culture?&lt;/p&gt;
&lt;p&gt;[00:32:12] On creativity and open-endedness&lt;/p&gt;
&lt;p&gt;[00:36:11] RL overfitting&lt;/p&gt;
&lt;p&gt;[00:44:02] Diversity preservation&lt;/p&gt;
&lt;p&gt;[00:51:18] Empiricism vs rationalism , in gradient descent the data pushes you around&lt;/p&gt;
&lt;p&gt;[00:55:49] Creativity and interestingness (does complexity / information increase)&lt;/p&gt;
&lt;p&gt;[01:03:20] What does a population give us?&lt;/p&gt;
&lt;p&gt;[01:05:58] Emergence / generalisation snobbery&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;[Hutter/Legg] Universal Intelligence: A Definition of Machine Intelligence&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/0712.3329&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Artificial_general_intelligence&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/I._J._Good&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/G%C3%B6del_machine&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Chollet] Impossibility of intelligence explosion&lt;/p&gt;
&lt;p&gt;https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Alex Irpan] - RL is hard&lt;/p&gt;
&lt;p&gt;https://www.alexirpan.com/2018/02/14/rl-hard.html&lt;/p&gt;
&lt;p&gt;https://nethackchallenge.com/&lt;/p&gt;
&lt;p&gt;Map elites&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1504.04909&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Covariance Matrix Adaptation for the Rapid Illumination of Behavior Space&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1912.02400&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Stanley] - Why greatness cannot be planned&lt;/p&gt;
&lt;p&gt;https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[Lehman/Stanley] Abandoning Objectives: Evolution through the Search for Novelty Alone&lt;/p&gt;
&lt;p&gt;https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:09:46</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1668917100295-9fc7146a897a4.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>81</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#80 AIDAN GOMEZ [CEO Cohere] - Language as Software]]></title>
			<description><![CDATA[<p>We had a conversation with Aidan Gomez, the CEO of language-based AI platform Cohere. Cohere is a startup which uses artificial intelligence to help users build the next generation of language-based applications. It's headquartered in Toronto. The company has raised $175 million in funding so far.</p>
<p>Language may well become a key new substrate for software building, both in its representation and how we build the software. It may democratise software building so that more people can build software, and we can build new types of software. Aidan and I discuss this in detail in this episode of MLST.</p>
<p>Check out Cohere -- https://dashboard.cohere.ai/welcome/register?utm_source=influencer&amp;utm_medium=social&amp;utm_campaign=mlst</p>
<p>Support us!</p>
<p>https://www.patreon.com/mlst&nbsp;</p>
<p>YT version: https://youtu.be/ooBt_di8DLs</p>
<p>TOC:</p>
<p>[00:00:00] Aidan Gomez intro</p>
<p>[00:02:12] What's it like being a CEO?</p>
<p>[00:02:52] Transformers</p>
<p>[00:09:33] Deepmind Chomsky Hierarchy</p>
<p>[00:14:58] Cohere roadmap</p>
<p>[00:18:18] Friction using LLMs for startups</p>
<p>[00:25:31] How different from OpenAI / GPT-3</p>
<p>[00:29:31] Engineering questions on Cohere</p>
<p>[00:35:13] Francois Chollet says that LLMs are like databases</p>
<p>[00:38:34] Next frontier of language models</p>
<p>[00:42:04] Different modes of understanding in LLMs</p>
<p>[00:47:04] LLMs are the new extended mind</p>
<p>[00:50:03] Is language the next interface, and why might that be bad?</p>
<p>References:</p>
<p>[Balestriero] Spine theory of NNs</p>
<p>https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf</p>
<p>[Delétang et al] Neural Networks and the Chomsky Hierarchy</p>
<p>https://arxiv.org/abs/2207.02098</p>
<p>[Fodor, Pylyshyn] Connectionism and Cognitive Architecture: A Critical Analysis</p>
<p>https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/docs/jaf.pdf</p>
<p>[Chalmers, Clark] The extended mind</p>
<p>https://icds.uoregon.edu/wp-content/uploads/2014/06/Clark-and-Chalmers-The-Extended-Mind.pdf</p>
<p>[Melanie Mitchell et al] The Debate Over Understanding in AI's Large Language Models</p>
<p>https://arxiv.org/abs/2210.13966</p>
<p>[Jay Alammar]</p>
<p>Illustrated stable diffusion</p>
<p>https://jalammar.github.io/illustrated-stable-diffusion/</p>
<p>Illustrated transformer</p>
<p>https://jalammar.github.io/illustrated-transformer/</p>
<p>https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ</p>
<p>[Sandra Kublik] (works at Cohere!)</p>
<p>https://www.youtube.com/channel/UCjG6QzmabZrBEeGh3vi-wDQ</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/80-AIDAN-GOMEZ-CEO-Cohere---Language-as-Software-e1qp5vd</link>
			<guid isPermaLink="false">60a7f9f6-6e94-4e1d-9265-6e9b4e2126b7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 15 Nov 2022 01:03:48 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/60642733/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-15%2Ffb3d3060-e938-33bb-e82d-a488ca4f5e41.mp3" length="74648956" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We had a conversation with Aidan Gomez, the CEO of language-based AI platform Cohere. Cohere is a startup which uses artificial intelligence to help users build the next generation of language-based applications. It&apos;s headquartered in Toronto. The company has raised $175 million in funding so far.&lt;/p&gt;
&lt;p&gt;Language may well become a key new substrate for software building, both in its representation and how we build the software. It may democratise software building so that more people can build software, and we can build new types of software. Aidan and I discuss this in detail in this episode of MLST.&lt;/p&gt;
&lt;p&gt;Check out Cohere -- https://dashboard.cohere.ai/welcome/register?utm_source=influencer&amp;amp;utm_medium=social&amp;amp;utm_campaign=mlst&lt;/p&gt;
&lt;p&gt;Support us!&lt;/p&gt;
&lt;p&gt;https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/ooBt_di8DLs&lt;/p&gt;
&lt;p&gt;TOC:&lt;/p&gt;
&lt;p&gt;[00:00:00] Aidan Gomez intro&lt;/p&gt;
&lt;p&gt;[00:02:12] What&apos;s it like being a CEO?&lt;/p&gt;
&lt;p&gt;[00:02:52] Transformers&lt;/p&gt;
&lt;p&gt;[00:09:33] Deepmind Chomsky Hierarchy&lt;/p&gt;
&lt;p&gt;[00:14:58] Cohere roadmap&lt;/p&gt;
&lt;p&gt;[00:18:18] Friction using LLMs for startups&lt;/p&gt;
&lt;p&gt;[00:25:31] How different from OpenAI / GPT-3&lt;/p&gt;
&lt;p&gt;[00:29:31] Engineering questions on Cohere&lt;/p&gt;
&lt;p&gt;[00:35:13] Francois Chollet says that LLMs are like databases&lt;/p&gt;
&lt;p&gt;[00:38:34] Next frontier of language models&lt;/p&gt;
&lt;p&gt;[00:42:04] Different modes of understanding in LLMs&lt;/p&gt;
&lt;p&gt;[00:47:04] LLMs are the new extended mind&lt;/p&gt;
&lt;p&gt;[00:50:03] Is language the next interface, and why might that be bad?&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;[Balestriero] Spine theory of NNs&lt;/p&gt;
&lt;p&gt;https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf&lt;/p&gt;
&lt;p&gt;[Delétang et al] Neural Networks and the Chomsky Hierarchy&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2207.02098&lt;/p&gt;
&lt;p&gt;[Fodor, Pylyshyn] Connectionism and Cognitive Architecture: A Critical Analysis&lt;/p&gt;
&lt;p&gt;https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/docs/jaf.pdf&lt;/p&gt;
&lt;p&gt;[Chalmers, Clark] The extended mind&lt;/p&gt;
&lt;p&gt;https://icds.uoregon.edu/wp-content/uploads/2014/06/Clark-and-Chalmers-The-Extended-Mind.pdf&lt;/p&gt;
&lt;p&gt;[Melanie Mitchell et al] The Debate Over Understanding in AI&apos;s Large Language Models&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2210.13966&lt;/p&gt;
&lt;p&gt;[Jay Alammar]&lt;/p&gt;
&lt;p&gt;Illustrated stable diffusion&lt;/p&gt;
&lt;p&gt;https://jalammar.github.io/illustrated-stable-diffusion/&lt;/p&gt;
&lt;p&gt;Illustrated transformer&lt;/p&gt;
&lt;p&gt;https://jalammar.github.io/illustrated-transformer/&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/channel/UCmOwsoHty5PrmE-3QhUBfPQ&lt;/p&gt;
&lt;p&gt;[Sandra Kublik] (works at Cohere!)&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/channel/UCjG6QzmabZrBEeGh3vi-wDQ&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1668474213159-b57a28c4ca3cd.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>80</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#79 Consciousness and the Chinese Room [Special Edition] (CHOLLET, BISHOP, CHALMERS, BACH)]]></title>
			<description><![CDATA[<p>This video is demonetised on music copyright so we would appreciate support on our Patreon! https://www.patreon.com/mlst&nbsp;</p>
<p>We would also appreciate it if you rated us on your podcast platform.&nbsp;</p>
<p>YT: https://youtu.be/_KVAzAzO5HU</p>
<p>Panel: Dr. Tim Scarfe, Dr. Keith Duggar</p>
<p>Guests: Prof. J. Mark Bishop, Francois Chollet, Prof. David Chalmers, Dr. Joscha Bach, Prof. Karl Friston, Alexander Mattick, Sam Roffey</p>
<p>The Chinese Room Argument was first proposed by philosopher John Searle in 1980. It is an argument against the possibility of artificial intelligence (AI) – that is, the idea that a machine could ever be truly intelligent, as opposed to just imitating intelligence.</p>
<p>The argument goes like this:</p>
<p>Imagine a room in which a person sits at a desk, with a book of rules in front of them. This person does not understand Chinese.</p>
<p>Someone outside the room passes a piece of paper through a slot in the door. On this paper is a Chinese character. The person in the room consults the book of rules and, following these rules, writes down another Chinese character and passes it back out through the slot.</p>
<p>To someone outside the room, it appears that the person in the room is engaging in a conversation in Chinese. In reality, they have no idea what they are doing – they are just following the rules in the book.</p>
<p>The Chinese Room Argument is an argument against the idea that a machine could ever be truly intelligent. It is based on the idea that intelligence requires understanding, and that following rules is not the same as understanding.</p>
<p>in this detailed investigation into the Chinese Room, Consciousness and Syntax vs Semantics, we interview luminaries J.Mark Bishop and Francois Chollet and use unreleased footage from our interviews with David Chalmers, Joscha Bach and Karl Friston. We also cover material from Walid Saba and interview Alex Mattick from Yannic's Discord.&nbsp;</p>
<p>This is probably my favourite ever episode of MLST. I hope you enjoy it! &nbsp;With Keith Duggar.&nbsp;</p>
<p>Note that we are using clips from our unreleased interviews from David Chalmers and Joscha Bach -- we will release those shows properly in the coming weeks. We apologise for delay releasing our backlog, we have been busy building a startup company in the background.</p>
<p><br></p>
<p>TOC:&nbsp;</p>
<p>[00:00:00] Kick off</p>
<p>[00:00:46] Searle</p>
<p>[00:05:09] Bishop introduces CRA</p>
<p>[00:00:00] Stevan Hardad take on CRA&nbsp;</p>
<p>[00:14:03] Francois Chollet dissects CRA</p>
<p>[00:34:16] Chalmers on consciousness</p>
<p>[00:36:27] Joscha Bach on consciousness</p>
<p>[00:42:01] Bishop introduction</p>
<p>[00:51:51] Karl Friston on consciousness</p>
<p>[00:55:19] Bishop on consciousness and comments on Chalmers&nbsp;</p>
<p>[01:21:37] Private language games (including clip with Sam Roffey)</p>
<p>[01:27:27] Dr. Walid Saba on the chinese room (gofai/systematicity take)</p>
<p>[00:34:36] Bishop: on agency / teleology</p>
<p>[01:36:38] Bishop: back to CRA</p>
<p>[01:40:53] Noam Chomsky on mysteries&nbsp;</p>
<p>[01:45:56] Eric Curiel on math does not represent</p>
<p>[01:48:14] Alexander Mattick on syntax vs semantics</p>
<p><br></p>
<p>Thanks to: Mark MC on Discord for stimulating conversation, Alexander Mattick, Dr. Keith Duggar, Sam Roffey. Sam's YouTube channel is https://www.youtube.com/channel/UCjRNMsglFYFwNsnOWIOgt1Q</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/79-Consciousness-and-the-Chinese-Room-Special-Edition-CHOLLET--BISHOP--CHALMERS--BACH-e1qet8o</link>
			<guid isPermaLink="false">955eecc7-7834-4ee8-a488-dda079d68732</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 08 Nov 2022 19:44:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/60306136/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-10-8%2F9a558340-8a0f-5977-5fc5-c04960c7bd26.mp3" length="186593472" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This video is demonetised on music copyright so we would appreciate support on our Patreon! https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We would also appreciate it if you rated us on your podcast platform.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/_KVAzAzO5HU&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe, Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Guests: Prof. J. Mark Bishop, Francois Chollet, Prof. David Chalmers, Dr. Joscha Bach, Prof. Karl Friston, Alexander Mattick, Sam Roffey&lt;/p&gt;
&lt;p&gt;The Chinese Room Argument was first proposed by philosopher John Searle in 1980. It is an argument against the possibility of artificial intelligence (AI) – that is, the idea that a machine could ever be truly intelligent, as opposed to just imitating intelligence.&lt;/p&gt;
&lt;p&gt;The argument goes like this:&lt;/p&gt;
&lt;p&gt;Imagine a room in which a person sits at a desk, with a book of rules in front of them. This person does not understand Chinese.&lt;/p&gt;
&lt;p&gt;Someone outside the room passes a piece of paper through a slot in the door. On this paper is a Chinese character. The person in the room consults the book of rules and, following these rules, writes down another Chinese character and passes it back out through the slot.&lt;/p&gt;
&lt;p&gt;To someone outside the room, it appears that the person in the room is engaging in a conversation in Chinese. In reality, they have no idea what they are doing – they are just following the rules in the book.&lt;/p&gt;
&lt;p&gt;The Chinese Room Argument is an argument against the idea that a machine could ever be truly intelligent. It is based on the idea that intelligence requires understanding, and that following rules is not the same as understanding.&lt;/p&gt;
&lt;p&gt;in this detailed investigation into the Chinese Room, Consciousness and Syntax vs Semantics, we interview luminaries J.Mark Bishop and Francois Chollet and use unreleased footage from our interviews with David Chalmers, Joscha Bach and Karl Friston. We also cover material from Walid Saba and interview Alex Mattick from Yannic&apos;s Discord.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This is probably my favourite ever episode of MLST. I hope you enjoy it! &amp;nbsp;With Keith Duggar.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Note that we are using clips from our unreleased interviews from David Chalmers and Joscha Bach -- we will release those shows properly in the coming weeks. We apologise for delay releasing our backlog, we have been busy building a startup company in the background.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;TOC:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:00:00] Kick off&lt;/p&gt;
&lt;p&gt;[00:00:46] Searle&lt;/p&gt;
&lt;p&gt;[00:05:09] Bishop introduces CRA&lt;/p&gt;
&lt;p&gt;[00:00:00] Stevan Hardad take on CRA&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:14:03] Francois Chollet dissects CRA&lt;/p&gt;
&lt;p&gt;[00:34:16] Chalmers on consciousness&lt;/p&gt;
&lt;p&gt;[00:36:27] Joscha Bach on consciousness&lt;/p&gt;
&lt;p&gt;[00:42:01] Bishop introduction&lt;/p&gt;
&lt;p&gt;[00:51:51] Karl Friston on consciousness&lt;/p&gt;
&lt;p&gt;[00:55:19] Bishop on consciousness and comments on Chalmers&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:21:37] Private language games (including clip with Sam Roffey)&lt;/p&gt;
&lt;p&gt;[01:27:27] Dr. Walid Saba on the chinese room (gofai/systematicity take)&lt;/p&gt;
&lt;p&gt;[00:34:36] Bishop: on agency / teleology&lt;/p&gt;
&lt;p&gt;[01:36:38] Bishop: back to CRA&lt;/p&gt;
&lt;p&gt;[01:40:53] Noam Chomsky on mysteries&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:45:56] Eric Curiel on math does not represent&lt;/p&gt;
&lt;p&gt;[01:48:14] Alexander Mattick on syntax vs semantics&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thanks to: Mark MC on Discord for stimulating conversation, Alexander Mattick, Dr. Keith Duggar, Sam Roffey. Sam&apos;s YouTube channel is https://www.youtube.com/channel/UCjRNMsglFYFwNsnOWIOgt1Q&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:09:34</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1667936645845-bbbddea12e152.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>79</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[MLST #78 - Prof. NOAM CHOMSKY (Special Edition)]]></title>
			<description><![CDATA[<p>Patreon: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbDlvTFQwdWppUFFlQzBLcHJqekpCNmNGOU9Wd3xBQ3Jtc0ttYUNyMkJ5djItcG9ZRE00Mm1nSnV5MVpWMTI4M2dqTzNPQ3ptMkU1RUUzaWc1cXdjZ2ItS0xqbzVDZmVRdVM4bFpMSEtpN0pnYlR1bGQ3X3JsdkhwaGJjam5nZGJXUlZyV0dXS3dBS2RWWW9CbWV6bw&amp;q=https%3A%2F%2Fwww.patreon.com%2Fmlst&amp;v=MDt2e8XtUcA" rel="nofollow" target="_blank">https://www.patreon.com/mlst</a></p>
<p>Discord: <a href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqa0NuSFpoUmtJSnVqRFhpTVJieVNaQnVHQUN4UXxBQ3Jtc0tuM3VaMGFJLTdwOEdzQVJBdUREeTVCX3gyc3ZiczZYUVZxUjkwX3ZrczUxNUE3Y1BQaER3Q04wN1RUVnFYUzJ4bzdqMTBzZGdaM195aF9XUkNBenV0azU5ZWI2Vm1YREVUV05KYXZ1RVlhS1FpR2pINA&amp;q=https%3A%2F%2Fdiscord.gg%2FESrGqhf5CB&amp;v=MDt2e8XtUcA" rel="nofollow" target="_blank">https://discord.gg/ESrGqhf5CB</a></p>
<p>In this special edition episode, we have a conversation with Prof. Noam Chomsky, the father of modern linguistics and the most important intellectual of the 20th century.&nbsp;</p>
<p>With a career spanning the better part of a century, we took the chance to ask Prof. Chomsky his thoughts not only on the progress of linguistics and cognitive science but also the deepest enduring mysteries of science and philosophy as a whole - exploring what may lie beyond our limits of understanding. We also discuss the rise of connectionism and large language models, our quest to discover an intelligible world, and the boundaries between silicon and biology.</p>
<p>We explore some of the profound misunderstandings of linguistics in general and Chomsky’s own work specifically which have persisted, at the highest levels of academia for over sixty years. &nbsp;</p>
<p>We have produced a significant introduction section where we discuss in detail Yann LeCun’s recent position paper on AGI, a recent paper on emergence in LLMs, empiricism related to cognitive science, cognitive templates, “the ghost in the machine” and language.&nbsp;</p>
<p><br></p>
<p>Panel:&nbsp;</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p>Dr. Walid Saba&nbsp;</p>
<p><br></p>
<p>YT version: https://youtu.be/-9I4SgkHpcA</p>
<p><br></p>
<p>00:00:00 Kick off</p>
<p>00:02:24 C1: LeCun's recent position paper on AI, JEPA, Schmidhuber, EBMs</p>
<p>00:48:38 C2: Emergent abilities in LLMs paper</p>
<p>00:51:32 C3: Empiricism</p>
<p>01:25:33 C4: Cognitive Templates</p>
<p>01:35:47 C5: The Ghost in the Machine</p>
<p>01:59:21 C6: Connectionism and Cognitive Architecture: A Critical Analysis by Fodor and Pylyshyn</p>
<p>02:19:25 C7: We deep-faked Chomsky</p>
<p>02:29:11 C8: Language</p>
<p>02:34:41 C9: Chomsky interview kick-off!</p>
<p>02:35:39 Large Language Models such as GPT-3</p>
<p>02:39:14 Connectionism and radical empiricism</p>
<p>02:44:44 Hybrid systems such as neurosymbolic</p>
<p>02:48:47 Computationalism silicon vs biological</p>
<p>02:53:28 Limits of human understanding</p>
<p>03:00:46 Semantics state-of-the-art</p>
<p>03:06:43 Universal grammar, I-Language, and language of thought</p>
<p>03:16:27 Profound and enduring misunderstandings</p>
<p>03:25:41 Greatest remaining mysteries science and philosophy</p>
<p>03:33:10 Debrief and 'Chuckles' from Chomsky</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/MLST-78---Prof--NOAM-CHOMSKY-Special-Edition-e1l0760</link>
			<guid isPermaLink="false">e505dbb5-f742-4352-bd38-e558e49d192a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 08 Jul 2022 22:16:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/54581888/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-6-8%2Fbea2dff0-649d-224a-7398-5d3411bf031e.mp3" length="208351248" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbDlvTFQwdWppUFFlQzBLcHJqekpCNmNGOU9Wd3xBQ3Jtc0ttYUNyMkJ5djItcG9ZRE00Mm1nSnV5MVpWMTI4M2dqTzNPQ3ptMkU1RUUzaWc1cXdjZ2ItS0xqbzVDZmVRdVM4bFpMSEtpN0pnYlR1bGQ3X3JsdkhwaGJjam5nZGJXUlZyV0dXS3dBS2RWWW9CbWV6bw&amp;amp;q=https%3A%2F%2Fwww.patreon.com%2Fmlst&amp;amp;v=MDt2e8XtUcA&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://www.patreon.com/mlst&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Discord: &lt;a href=&quot;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqa0NuSFpoUmtJSnVqRFhpTVJieVNaQnVHQUN4UXxBQ3Jtc0tuM3VaMGFJLTdwOEdzQVJBdUREeTVCX3gyc3ZiczZYUVZxUjkwX3ZrczUxNUE3Y1BQaER3Q04wN1RUVnFYUzJ4bzdqMTBzZGdaM195aF9XUkNBenV0azU5ZWI2Vm1YREVUV05KYXZ1RVlhS1FpR2pINA&amp;amp;q=https%3A%2F%2Fdiscord.gg%2FESrGqhf5CB&amp;amp;v=MDt2e8XtUcA&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot;&gt;https://discord.gg/ESrGqhf5CB&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this special edition episode, we have a conversation with Prof. Noam Chomsky, the father of modern linguistics and the most important intellectual of the 20th century.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;With a career spanning the better part of a century, we took the chance to ask Prof. Chomsky his thoughts not only on the progress of linguistics and cognitive science but also the deepest enduring mysteries of science and philosophy as a whole - exploring what may lie beyond our limits of understanding. We also discuss the rise of connectionism and large language models, our quest to discover an intelligible world, and the boundaries between silicon and biology.&lt;/p&gt;
&lt;p&gt;We explore some of the profound misunderstandings of linguistics in general and Chomsky’s own work specifically which have persisted, at the highest levels of academia for over sixty years. &amp;nbsp;&lt;/p&gt;
&lt;p&gt;We have produced a significant introduction section where we discuss in detail Yann LeCun’s recent position paper on AGI, a recent paper on emergence in LLMs, empiricism related to cognitive science, cognitive templates, “the ghost in the machine” and language.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Dr. Walid Saba&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/-9I4SgkHpcA&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Kick off&lt;/p&gt;
&lt;p&gt;00:02:24 C1: LeCun&apos;s recent position paper on AI, JEPA, Schmidhuber, EBMs&lt;/p&gt;
&lt;p&gt;00:48:38 C2: Emergent abilities in LLMs paper&lt;/p&gt;
&lt;p&gt;00:51:32 C3: Empiricism&lt;/p&gt;
&lt;p&gt;01:25:33 C4: Cognitive Templates&lt;/p&gt;
&lt;p&gt;01:35:47 C5: The Ghost in the Machine&lt;/p&gt;
&lt;p&gt;01:59:21 C6: Connectionism and Cognitive Architecture: A Critical Analysis by Fodor and Pylyshyn&lt;/p&gt;
&lt;p&gt;02:19:25 C7: We deep-faked Chomsky&lt;/p&gt;
&lt;p&gt;02:29:11 C8: Language&lt;/p&gt;
&lt;p&gt;02:34:41 C9: Chomsky interview kick-off!&lt;/p&gt;
&lt;p&gt;02:35:39 Large Language Models such as GPT-3&lt;/p&gt;
&lt;p&gt;02:39:14 Connectionism and radical empiricism&lt;/p&gt;
&lt;p&gt;02:44:44 Hybrid systems such as neurosymbolic&lt;/p&gt;
&lt;p&gt;02:48:47 Computationalism silicon vs biological&lt;/p&gt;
&lt;p&gt;02:53:28 Limits of human understanding&lt;/p&gt;
&lt;p&gt;03:00:46 Semantics state-of-the-art&lt;/p&gt;
&lt;p&gt;03:06:43 Universal grammar, I-Language, and language of thought&lt;/p&gt;
&lt;p&gt;03:16:27 Profound and enduring misunderstandings&lt;/p&gt;
&lt;p&gt;03:25:41 Greatest remaining mysteries science and philosophy&lt;/p&gt;
&lt;p&gt;03:33:10 Debrief and &apos;Chuckles&apos; from Chomsky&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>03:37:01</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1657318532166-8d89da9ddce5a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>78</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#77 - Vitaliy Chiley (Cerebras)]]></title>
			<description><![CDATA[<p>Vitaliy Chiley &nbsp;is a Machine Learning Research Engineer at the next-generation computing hardware company Cerebras Systems. We spoke about how DL workloads including sparse workloads can run faster on Cerebras hardware.</p>
<p><br></p>
<p>[00:00:00] Housekeeping</p>
<p>[00:01:08] Preamble</p>
<p>[00:01:50] Vitaliy Chiley Introduction</p>
<p>[00:03:11] Cerebrus architecture</p>
<p>[00:08:12] Memory management and FLOP utilisation</p>
<p>[00:18:01] Centralised vs decentralised compute architecture</p>
<p>[00:21:12] Sparsity</p>
<p>[00:23:47] Does Sparse NN imply Heterogeneous compute?</p>
<p>[00:29:21] Cost of distributed memory stores?</p>
<p>[00:31:01] Activation vs weight sparsity</p>
<p>[00:37:52] What constitutes a dead weight to be pruned?</p>
<p>[00:39:02] Is it still a saving if we have to choose between weight and activation sparsity?</p>
<p>[00:41:02] Cerebras is a cool place to work</p>
<p>[00:44:05] What is sparsity? Why do we need to start dense?&nbsp;</p>
<p>[00:46:36] Evolutionary algorithms on Cerebras?</p>
<p>[00:47:57] How can we start sparse? Google RIGL</p>
<p>[00:51:44] Inductive priors, why do we need them if we can start sparse?</p>
<p>[00:56:02] Why anthropomorphise inductive priors?</p>
<p>[01:02:13] Could Cerebras run a cyclic computational graph?</p>
<p>[01:03:16] Are NNs locality sensitive hashing tables?</p>
<p><br></p>
<p>References;</p>
<p>Rigging the Lottery: Making All Tickets Winners [RIGL]</p>
<p>https://arxiv.org/pdf/1911.11134.pdf</p>
<p><br></p>
<p>[D] DanNet, the CUDA CNN of Dan Ciresan in Jurgen Schmidhuber's team, won 4 image recognition challenges prior to AlexNet</p>
<p>https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/&nbsp;</p>
<p><br></p>
<p>A Spline Theory of Deep Learning [Balestriero]</p>
<p>https://proceedings.mlr.press/v80/balestriero18b.html&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/77---Vitaliy-Chiley-Cerebras-e1k1hvu</link>
			<guid isPermaLink="false">c7767be9-ca00-404b-82d9-df98a7c746c5</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 16 Jun 2022 14:27:25 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/53577150/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-5-16%2F657fdf4b-2e57-c09b-67c2-826794d27185.mp3" length="97291395" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Vitaliy Chiley &amp;nbsp;is a Machine Learning Research Engineer at the next-generation computing hardware company Cerebras Systems. We spoke about how DL workloads including sparse workloads can run faster on Cerebras hardware.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Housekeeping&lt;/p&gt;
&lt;p&gt;[00:01:08] Preamble&lt;/p&gt;
&lt;p&gt;[00:01:50] Vitaliy Chiley Introduction&lt;/p&gt;
&lt;p&gt;[00:03:11] Cerebrus architecture&lt;/p&gt;
&lt;p&gt;[00:08:12] Memory management and FLOP utilisation&lt;/p&gt;
&lt;p&gt;[00:18:01] Centralised vs decentralised compute architecture&lt;/p&gt;
&lt;p&gt;[00:21:12] Sparsity&lt;/p&gt;
&lt;p&gt;[00:23:47] Does Sparse NN imply Heterogeneous compute?&lt;/p&gt;
&lt;p&gt;[00:29:21] Cost of distributed memory stores?&lt;/p&gt;
&lt;p&gt;[00:31:01] Activation vs weight sparsity&lt;/p&gt;
&lt;p&gt;[00:37:52] What constitutes a dead weight to be pruned?&lt;/p&gt;
&lt;p&gt;[00:39:02] Is it still a saving if we have to choose between weight and activation sparsity?&lt;/p&gt;
&lt;p&gt;[00:41:02] Cerebras is a cool place to work&lt;/p&gt;
&lt;p&gt;[00:44:05] What is sparsity? Why do we need to start dense?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:46:36] Evolutionary algorithms on Cerebras?&lt;/p&gt;
&lt;p&gt;[00:47:57] How can we start sparse? Google RIGL&lt;/p&gt;
&lt;p&gt;[00:51:44] Inductive priors, why do we need them if we can start sparse?&lt;/p&gt;
&lt;p&gt;[00:56:02] Why anthropomorphise inductive priors?&lt;/p&gt;
&lt;p&gt;[01:02:13] Could Cerebras run a cyclic computational graph?&lt;/p&gt;
&lt;p&gt;[01:03:16] Are NNs locality sensitive hashing tables?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;Rigging the Lottery: Making All Tickets Winners [RIGL]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1911.11134.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[D] DanNet, the CUDA CNN of Dan Ciresan in Jurgen Schmidhuber&apos;s team, won 4 image recognition challenges prior to AlexNet&lt;/p&gt;
&lt;p&gt;https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Spline Theory of Deep Learning [Balestriero]&lt;/p&gt;
&lt;p&gt;https://proceedings.mlr.press/v80/balestriero18b.html&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1655389639366-a5fefebb22022.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>77</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#76 - LUKAS BIEWALD (Weights and Biases CEO)]]></title>
			<description><![CDATA[<p>Check out Weights and Biases here!&nbsp;</p>
<p>https://wandb.me/MLST</p>
<p>Lukas Biewald is an entrepreneur living in San Francisco. He was the founder and CEO of Figure Eight an Internet company that collects training data for machine learning. &nbsp;In 2018, he founded Weights and Biases, a company that creates developer tools for machine learning. Recently WandB got a cash injection of 15 million dollars in its second funding round.&nbsp;</p>
<p>Lukas has a bachelors and masters in mathematics and computer science respectively from Stanford university. &nbsp;He was a research student under the tutelage of the legendary Daphne Koller.&nbsp;</p>
<p>Lukas Biewald</p>
<p>https://twitter.com/l2k</p>
<p>[00:00:00] Preamble</p>
<p>[00:01:27] Intro to Lukas</p>
<p>[00:02:46] How did Lukas build 2 sucessful startups?</p>
<p>[00:05:49] Rebalancing games with ML</p>
<p>[00:08:14] Elevator pitch for WandB</p>
<p>[00:10:38] Science vs Engineering divide in ML DevOps</p>
<p>[00:14:11] Too much focus on the minutiae?</p>
<p>[00:18:03] Vertical information sharing in large enterprises (metrics)</p>
<p>[00:20:37] Centralised vs Decentralised topology</p>
<p>[00:24:02] Generalisation vs specialisation</p>
<p>[00:28:59] Enhancing explainability</p>
<p>[00:33:14] Should we try and understand "the machine" or is testing / behaviourism enough?</p>
<p>[00:36:55] WandB roadmap</p>
<p>[00:39:06] WandB / ML Ops competitor space?</p>
<p>[00:44:10] How is WandB differentiated over Sagemaker / AzureML</p>
<p>[00:46:02] WandB Sponsorship of ML YT channels</p>
<p>[00:48:43] Alternatives to deep learning?</p>
<p>[00:53:47] How to build a business like WandB</p>
<p><br></p>
<p>Panel: Tim Scarfe Ph.D and Keith Duggar Ph.D</p>
<p><br></p>
<p>Note we didn't get paid by Weights and Biases to conduct this interview.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/76---LUKAS-BIEWALD-Weights-and-Biases-CEO-e1jmvd5</link>
			<guid isPermaLink="false">8c909458-d218-4a71-ae8e-8b7aeb476932</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 09 Jun 2022 00:02:00 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/53230437/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-5-8%2F6bfa5a0d-90fc-a4e0-f2e0-4c1cb4d7dc21.mp3" length="82968129" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Check out Weights and Biases here!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://wandb.me/MLST&lt;/p&gt;
&lt;p&gt;Lukas Biewald is an entrepreneur living in San Francisco. He was the founder and CEO of Figure Eight an Internet company that collects training data for machine learning. &amp;nbsp;In 2018, he founded Weights and Biases, a company that creates developer tools for machine learning. Recently WandB got a cash injection of 15 million dollars in its second funding round.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Lukas has a bachelors and masters in mathematics and computer science respectively from Stanford university. &amp;nbsp;He was a research student under the tutelage of the legendary Daphne Koller.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Lukas Biewald&lt;/p&gt;
&lt;p&gt;https://twitter.com/l2k&lt;/p&gt;
&lt;p&gt;[00:00:00] Preamble&lt;/p&gt;
&lt;p&gt;[00:01:27] Intro to Lukas&lt;/p&gt;
&lt;p&gt;[00:02:46] How did Lukas build 2 sucessful startups?&lt;/p&gt;
&lt;p&gt;[00:05:49] Rebalancing games with ML&lt;/p&gt;
&lt;p&gt;[00:08:14] Elevator pitch for WandB&lt;/p&gt;
&lt;p&gt;[00:10:38] Science vs Engineering divide in ML DevOps&lt;/p&gt;
&lt;p&gt;[00:14:11] Too much focus on the minutiae?&lt;/p&gt;
&lt;p&gt;[00:18:03] Vertical information sharing in large enterprises (metrics)&lt;/p&gt;
&lt;p&gt;[00:20:37] Centralised vs Decentralised topology&lt;/p&gt;
&lt;p&gt;[00:24:02] Generalisation vs specialisation&lt;/p&gt;
&lt;p&gt;[00:28:59] Enhancing explainability&lt;/p&gt;
&lt;p&gt;[00:33:14] Should we try and understand &quot;the machine&quot; or is testing / behaviourism enough?&lt;/p&gt;
&lt;p&gt;[00:36:55] WandB roadmap&lt;/p&gt;
&lt;p&gt;[00:39:06] WandB / ML Ops competitor space?&lt;/p&gt;
&lt;p&gt;[00:44:10] How is WandB differentiated over Sagemaker / AzureML&lt;/p&gt;
&lt;p&gt;[00:46:02] WandB Sponsorship of ML YT channels&lt;/p&gt;
&lt;p&gt;[00:48:43] Alternatives to deep learning?&lt;/p&gt;
&lt;p&gt;[00:53:47] How to build a business like WandB&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Tim Scarfe Ph.D and Keith Duggar Ph.D&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Note we didn&apos;t get paid by Weights and Biases to conduct this interview.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:57:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1654732912438-2bb448e5a64a7.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>76</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#75 - Emergence [Special Edition] with Dr. DANIELE GRATTAROLA]]></title>
			<description><![CDATA[<p>An emergent behavior or emergent property can appear when a number of simple&nbsp;entities operate in an environment, forming more complex behaviours as a collective. If emergence happens over disparate size scales, then the reason is usually a causal relation across different scales. Weak emergence describes new properties arising in systems as a result of the low-level interactions, these might be interactions between components of the system or the components and their environment.&nbsp;</p>
<p>In our epic introduction we focus a lot on the concept of self-organisation, complex systems, cellular automata and strong vs weak emergence. In the main show we discuss this more in detail with Dr. Daniele Grattarola and cover his recent NeurIPS paper on learning graph cellular automata.&nbsp;</p>
<p>YT version: https://youtu.be/MDt2e8XtUcA</p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p><br></p>
<p>Featuring;</p>
<p>Dr. Daniele Grattarola</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p>Prof. David Chalmers</p>
<p>Prof. Ken Stanley</p>
<p>Prof. Julian Togelius</p>
<p>Dr. Joscha Bach</p>
<p>David Ha</p>
<p>Dr. Pei Wang</p>
<p><br></p>
<p>[00:00:00] Special Edition Intro: Emergence and Cellular Automata</p>
<p>[00:49:02] Intro to Daniele and CAs</p>
<p>[00:57:23] Numerical analysis link with CA (PDEs)</p>
<p>[00:59:50] The representational dichotomy of discrete and continuous at different scales</p>
<p>[01:05:21] Universal computation in CAs</p>
<p>[01:10:27] Computational irreducibility&nbsp;</p>
<p>[01:16:33] Is the universe discrete?</p>
<p>[01:20:49] Emergence but with the same computational principle</p>
<p>[01:23:10] How do you formalise the emergent phenomenon&nbsp;</p>
<p>[01:25:44] Growing cellular automata</p>
<p>[01:33:53] Openeded and unbounded computation is required for this kind of behaviour</p>
<p>[01:37:31] Graph cellula automata</p>
<p>[01:43:40] Connection to protein folding</p>
<p>[01:46:24] Are CAs the best tool for the job?</p>
<p>[01:49:37] Where to go to find more information</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/75---Emergence-Special-Edition-with-Dr--DANIELE-GRATTAROLA-e1hrb23</link>
			<guid isPermaLink="false">f5f9f96e-f13f-4d93-a668-8d4a64f1a24b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 29 Apr 2022 12:17:05 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/51276291/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-3-29%2F36ec7f84-e3ba-1b8e-6e39-94e296d58819.mp3" length="166299406" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;An emergent behavior or emergent property can appear when a number of simple&amp;nbsp;entities operate in an environment, forming more complex behaviours as a collective. If emergence happens over disparate size scales, then the reason is usually a causal relation across different scales. Weak emergence describes new properties arising in systems as a result of the low-level interactions, these might be interactions between components of the system or the components and their environment.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In our epic introduction we focus a lot on the concept of self-organisation, complex systems, cellular automata and strong vs weak emergence. In the main show we discuss this more in detail with Dr. Daniele Grattarola and cover his recent NeurIPS paper on learning graph cellular automata.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/MDt2e8XtUcA&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Featuring;&lt;/p&gt;
&lt;p&gt;Dr. Daniele Grattarola&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Prof. David Chalmers&lt;/p&gt;
&lt;p&gt;Prof. Ken Stanley&lt;/p&gt;
&lt;p&gt;Prof. Julian Togelius&lt;/p&gt;
&lt;p&gt;Dr. Joscha Bach&lt;/p&gt;
&lt;p&gt;David Ha&lt;/p&gt;
&lt;p&gt;Dr. Pei Wang&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Special Edition Intro: Emergence and Cellular Automata&lt;/p&gt;
&lt;p&gt;[00:49:02] Intro to Daniele and CAs&lt;/p&gt;
&lt;p&gt;[00:57:23] Numerical analysis link with CA (PDEs)&lt;/p&gt;
&lt;p&gt;[00:59:50] The representational dichotomy of discrete and continuous at different scales&lt;/p&gt;
&lt;p&gt;[01:05:21] Universal computation in CAs&lt;/p&gt;
&lt;p&gt;[01:10:27] Computational irreducibility&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:16:33] Is the universe discrete?&lt;/p&gt;
&lt;p&gt;[01:20:49] Emergence but with the same computational principle&lt;/p&gt;
&lt;p&gt;[01:23:10] How do you formalise the emergent phenomenon&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:25:44] Growing cellular automata&lt;/p&gt;
&lt;p&gt;[01:33:53] Openeded and unbounded computation is required for this kind of behaviour&lt;/p&gt;
&lt;p&gt;[01:37:31] Graph cellula automata&lt;/p&gt;
&lt;p&gt;[01:43:40] Connection to protein folding&lt;/p&gt;
&lt;p&gt;[01:46:24] Are CAs the best tool for the job?&lt;/p&gt;
&lt;p&gt;[01:49:37] Where to go to find more information&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:55:28</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1651234620976-a069c0ab87908.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>75</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#74 Dr. ANDREW LAMPINEN - Symbolic behaviour in AI [UNPLUGGED]]]></title>
			<description><![CDATA[<p>Please note that in this interview Dr. Lampinen was expressing his personal opinions and they do not necessarily represent those of DeepMind.&nbsp;</p>
<p><br></p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p>YT version: https://youtu.be/yPMtSXXn4OY</p>
<p><br></p>
<p>&nbsp;Dr. Andrew Lampinen is a Senior Research Scientist at DeepMind, and he thinks that symbols are subjective in the relativistic sense. Dr. Lampinen completed his PhD in Cognitive Psychology at Stanford University. His background is in mathematics, physics, and machine learning. Andrew has said that his research interests are in cognitive flexibililty and generalization, and how these abilities are enabled by factors like language, memory, and embodiment. &nbsp;Andrew with his coauthors has just released a paper called symbolic behaviour in artificial intelligence. Andrew lead in the paper by saying the human ability to use symbols has yet to be replicated in machines. He thinks that one of the key areas to bridge the gap here is considering how symbol meaning is established, and he strongly believes it is the symbol users themselves who agree upon the symbol meaning, And that the use of symbols entails behaviours which coalesce agreements about their meaning. Which in plain English means that symbols are defined by behaviours rather than their content.</p>
<p><br></p>
<p>[00:00:00] Intro to Andrew and Symbolic Behaviour paper</p>
<p>[00:07:01] Semantics underpins the unreasonable effectiveness of symbols</p>
<p>[00:12:56] The Depth of Subjectivity</p>
<p>[00:21:03] Walid Saba - universal cognitive templates</p>
<p>[00:27:47] Insufficiently Darwinian&nbsp;</p>
<p>[00:30:52] Discovered vs invented</p>
<p>[00:34:19] Does language have primacy</p>
<p>[00:35:59] Research directions</p>
<p>[00:39:43] Comparison to BenG OpenCog and human compatible AI</p>
<p>[00:42:53] Aligning AI with our culture</p>
<p>[00:47:55] Do we need to model the worst aspects of human behaviour?&nbsp;</p>
<p>[00:50:57] Fairness</p>
<p>[00:54:24] Memorisatation on LLMs</p>
<p>[01:00:38] Wason selection task</p>
<p>[01:03:45] Would an Andrew hashtable robot be intelligent?</p>
<p><br></p>
<p>Dr. Andrew Lampinen</p>
<p>https://lampinen.github.io/</p>
<p>https://twitter.com/AndrewLampinen</p>
<p><br></p>
<p>Symbolic Behaviour in Artificial Intelligence</p>
<p>https://arxiv.org/abs/2102.03406</p>
<p><br></p>
<p>Imitating Interactive Intelligence</p>
<p>https://arxiv.org/abs/2012.05672</p>
<p>https://www.deepmind.com/publications/imitating-interactive-intelligence</p>
<p><br></p>
<p>Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Yasaman Razeghi]</p>
<p>https://arxiv.org/abs/2202.07206</p>
<p><br></p>
<p>Big bench dataset</p>
<p>https://github.com/google/BIG-bench</p>
<p><br></p>
<p>Teaching Autoregressive Language Models Complex Tasks By Demonstration [Recchia]</p>
<p>https://arxiv.org/pdf/2109.02102.pdf</p>
<p><br></p>
<p>Wason selection task</p>
<p>https://en.wikipedia.org/wiki/Wason_selection_task</p>
<p><br></p>
<p>Gary Lupyan</p>
<p>https://psych.wisc.edu/staff/lupyan-gary/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/74-Dr--ANDREW-LAMPINEN---Symbolic-behaviour-in-AI-UNPLUGGED-e1h6far</link>
			<guid isPermaLink="false">c3199dcc-0788-4844-a71a-0c8f5c4d8fa6</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 14 Apr 2022 17:20:24 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/50592539/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-3-14%2F9316a669-8858-5ef2-5fe8-a0b9dc7d09b7.mp3" length="94520448" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Please note that in this interview Dr. Lampinen was expressing his personal opinions and they do not necessarily represent those of DeepMind.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/yPMtSXXn4OY&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;Dr. Andrew Lampinen is a Senior Research Scientist at DeepMind, and he thinks that symbols are subjective in the relativistic sense. Dr. Lampinen completed his PhD in Cognitive Psychology at Stanford University. His background is in mathematics, physics, and machine learning. Andrew has said that his research interests are in cognitive flexibililty and generalization, and how these abilities are enabled by factors like language, memory, and embodiment. &amp;nbsp;Andrew with his coauthors has just released a paper called symbolic behaviour in artificial intelligence. Andrew lead in the paper by saying the human ability to use symbols has yet to be replicated in machines. He thinks that one of the key areas to bridge the gap here is considering how symbol meaning is established, and he strongly believes it is the symbol users themselves who agree upon the symbol meaning, And that the use of symbols entails behaviours which coalesce agreements about their meaning. Which in plain English means that symbols are defined by behaviours rather than their content.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro to Andrew and Symbolic Behaviour paper&lt;/p&gt;
&lt;p&gt;[00:07:01] Semantics underpins the unreasonable effectiveness of symbols&lt;/p&gt;
&lt;p&gt;[00:12:56] The Depth of Subjectivity&lt;/p&gt;
&lt;p&gt;[00:21:03] Walid Saba - universal cognitive templates&lt;/p&gt;
&lt;p&gt;[00:27:47] Insufficiently Darwinian&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:30:52] Discovered vs invented&lt;/p&gt;
&lt;p&gt;[00:34:19] Does language have primacy&lt;/p&gt;
&lt;p&gt;[00:35:59] Research directions&lt;/p&gt;
&lt;p&gt;[00:39:43] Comparison to BenG OpenCog and human compatible AI&lt;/p&gt;
&lt;p&gt;[00:42:53] Aligning AI with our culture&lt;/p&gt;
&lt;p&gt;[00:47:55] Do we need to model the worst aspects of human behaviour?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:50:57] Fairness&lt;/p&gt;
&lt;p&gt;[00:54:24] Memorisatation on LLMs&lt;/p&gt;
&lt;p&gt;[01:00:38] Wason selection task&lt;/p&gt;
&lt;p&gt;[01:03:45] Would an Andrew hashtable robot be intelligent?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Andrew Lampinen&lt;/p&gt;
&lt;p&gt;https://lampinen.github.io/&lt;/p&gt;
&lt;p&gt;https://twitter.com/AndrewLampinen&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Symbolic Behaviour in Artificial Intelligence&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2102.03406&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Imitating Interactive Intelligence&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2012.05672&lt;/p&gt;
&lt;p&gt;https://www.deepmind.com/publications/imitating-interactive-intelligence&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Yasaman Razeghi]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2202.07206&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Big bench dataset&lt;/p&gt;
&lt;p&gt;https://github.com/google/BIG-bench&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Teaching Autoregressive Language Models Complex Tasks By Demonstration [Recchia]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2109.02102.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Wason selection task&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Wason_selection_task&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gary Lupyan&lt;/p&gt;
&lt;p&gt;https://psych.wisc.edu/staff/lupyan-gary/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:05:38</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1649956815848-5a417b9d43494.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>74</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#73 - YASAMAN RAZEGHI & Prof. SAMEER SINGH - NLP benchmarks]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p>YT version: https://youtu.be/RzGaI7vXrkk</p>
<p>This week we speak with Yasaman Razeghi and Prof. Sameer Singh from UC Urvine. Yasaman recently published a paper called Impact of Pretraining Term Frequencies on Few-Shot Reasoning where she demonstrated comprehensively that large language models only perform well on reasoning tasks because they memorise the dataset. For the first time she showed the accuracy was linearly correlated to the occurance rate in the training corpus, something which OpenAI should have done in the first place!&nbsp;</p>
<p>We also speak with Sameer who has been a pioneering force in the area of machine learning interpretability for many years now, he created LIME with Marco Riberio and also had his hands all over the famous Checklist paper and many others.&nbsp;</p>
<p>We also get into the metric obsession in the NLP world and whether metrics are one of the principle reasons why we are failing to make any progress in NLU.&nbsp;</p>
<p>[00:00:00] Impact of Pretraining Term Frequencies on Few-Shot Reasoning</p>
<p>[00:14:59] Metrics</p>
<p>[00:18:55] Definition of reasoning</p>
<p>[00:25:12] Metrics (again)</p>
<p>[00:28:52] On true believers&nbsp;</p>
<p>[00:33:04] Sameers work on model explainability / LIME&nbsp;</p>
<p>[00:36:58] Computational irreducability&nbsp;</p>
<p>[00:41:07] ML DevOps and Checklist</p>
<p>[00:45:58] Future of ML devops</p>
<p>[00:49:34] Thinking about future</p>
<p><br></p>
<p>Prof. Sameer Singh</p>
<p>https://sameersingh.org/</p>
<p><br></p>
<p>Yasaman Razeghi</p>
<p>https://yasamanrazeghi.com/</p>
<p><br></p>
<p>References;</p>
<p><br></p>
<p>Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Razeghi et al with Singh]</p>
<p>https://arxiv.org/pdf/2202.07206.pdf</p>
<p><br></p>
<p>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [Riberio et al with Singh]</p>
<p>https://arxiv.org/pdf/2005.04118.pdf</p>
<p><br></p>
<p>“Why Should I Trust You?” Explaining the Predictions of Any Classifier (LIME) [Riberio et al with Singh]</p>
<p>https://arxiv.org/abs/1602.04938</p>
<p><br></p>
<p>Tim interviewing LIME Creator Marco Ribeiro in 2019</p>
<p>https://www.youtube.com/watch?v=6aUU-Ob4a8I</p>
<p><br></p>
<p>Tim video on LIME/SHAP on his other channel</p>
<p>https://www.youtube.com/watch?v=jhopjN08lTM</p>
<p><br></p>
<p>Our interview with Christoph Molar</p>
<p>https://www.youtube.com/watch?v=0LIACHcxpHU</p>
<p><br></p>
<p>Interpretable Machine Learning book @ChristophMolnar</p>
<p>https://christophm.github.io/interpretable-ml-book/</p>
<p><br></p>
<p>Machine Teaching: A New Paradigm for Building Machine Learning Systems [Simard]</p>
<p>https://arxiv.org/abs/1707.06742</p>
<p><br></p>
<p>Whimsical notes on machine teaching</p>
<p>https://whimsical.com/machine-teaching-Ntke9EHHSR25yHnsypHnth</p>
<p><br></p>
<p>Gopher paper (Deepmind)</p>
<p>https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval</p>
<p>https://arxiv.org/pdf/2112.11446.pdf</p>
<p><br></p>
<p>EleutherAI</p>
<p>https://www.eleuther.ai/</p>
<p>https://github.com/kingoflolz/mesh-transformer-jax/</p>
<p>https://pile.eleuther.ai/</p>
<p><br></p>
<p>A Theory of Universal Artificial Intelligence based on Algorithmic Complexity [Hutter]</p>
<p>https://arxiv.org/pdf/cs/0004001.pdf</p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/73---YASAMAN-RAZEGHI--Prof--SAMEER-SINGH---NLP-benchmarks-e1grvjj</link>
			<guid isPermaLink="false">e659b75d-b40b-4b46-aa7e-d842025688a6</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 07 Apr 2022 11:56:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/50248755/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-3-7%2Fb423170b-1bdc-6466-f8e6-42a2f27545e9.mp3" length="80483458" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/RzGaI7vXrkk&lt;/p&gt;
&lt;p&gt;This week we speak with Yasaman Razeghi and Prof. Sameer Singh from UC Urvine. Yasaman recently published a paper called Impact of Pretraining Term Frequencies on Few-Shot Reasoning where she demonstrated comprehensively that large language models only perform well on reasoning tasks because they memorise the dataset. For the first time she showed the accuracy was linearly correlated to the occurance rate in the training corpus, something which OpenAI should have done in the first place!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We also speak with Sameer who has been a pioneering force in the area of machine learning interpretability for many years now, he created LIME with Marco Riberio and also had his hands all over the famous Checklist paper and many others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We also get into the metric obsession in the NLP world and whether metrics are one of the principle reasons why we are failing to make any progress in NLU.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:00:00] Impact of Pretraining Term Frequencies on Few-Shot Reasoning&lt;/p&gt;
&lt;p&gt;[00:14:59] Metrics&lt;/p&gt;
&lt;p&gt;[00:18:55] Definition of reasoning&lt;/p&gt;
&lt;p&gt;[00:25:12] Metrics (again)&lt;/p&gt;
&lt;p&gt;[00:28:52] On true believers&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:33:04] Sameers work on model explainability / LIME&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:36:58] Computational irreducability&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:41:07] ML DevOps and Checklist&lt;/p&gt;
&lt;p&gt;[00:45:58] Future of ML devops&lt;/p&gt;
&lt;p&gt;[00:49:34] Thinking about future&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. Sameer Singh&lt;/p&gt;
&lt;p&gt;https://sameersingh.org/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Yasaman Razeghi&lt;/p&gt;
&lt;p&gt;https://yasamanrazeghi.com/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Impact of Pretraining Term Frequencies on Few-Shot Reasoning [Razeghi et al with Singh]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2202.07206.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [Riberio et al with Singh]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2005.04118.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;“Why Should I Trust You?” Explaining the Predictions of Any Classifier (LIME) [Riberio et al with Singh]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1602.04938&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tim interviewing LIME Creator Marco Ribeiro in 2019&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=6aUU-Ob4a8I&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tim video on LIME/SHAP on his other channel&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=jhopjN08lTM&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Our interview with Christoph Molar&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=0LIACHcxpHU&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interpretable Machine Learning book @ChristophMolnar&lt;/p&gt;
&lt;p&gt;https://christophm.github.io/interpretable-ml-book/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Machine Teaching: A New Paradigm for Building Machine Learning Systems [Simard]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1707.06742&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Whimsical notes on machine teaching&lt;/p&gt;
&lt;p&gt;https://whimsical.com/machine-teaching-Ntke9EHHSR25yHnsypHnth&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gopher paper (Deepmind)&lt;/p&gt;
&lt;p&gt;https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2112.11446.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;EleutherAI&lt;/p&gt;
&lt;p&gt;https://www.eleuther.ai/&lt;/p&gt;
&lt;p&gt;https://github.com/kingoflolz/mesh-transformer-jax/&lt;/p&gt;
&lt;p&gt;https://pile.eleuther.ai/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Theory of Universal Artificial Intelligence based on Algorithmic Complexity [Hutter]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/cs/0004001.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:55:53</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1649332607965-f5b2bb086cbc2.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>73</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#72 Prof. KEN STANLEY 2.0 - On Art and Subjectivity [UNPLUGGED]]]></title>
			<description><![CDATA[<p>YT version: https://youtu.be/DxBZORM9F-8</p>
<p>Patreon: https://www.patreon.com/mlst&nbsp;</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p><br></p>
<p>Prof. Ken Stanley argued in his book that our world has become saturated with objectives. The process of setting an objective, attempting to achieve it, and measuring progress along the way has become the primary route to achievement in our culture. He’s not saying that objectives are bad per se, especially if they’re modest, but he thinks that when goals are ambitious then the search space becomes deceptive.</p>
<p><br></p>
<p>Is the key to artificial intelligence really related to intelligence? Does taking a job with a higher salary really bring you closer to being a millionaire? The problem is that the stepping stones which lead to ambitious objectives tend to be pretty strange, they don't resemble the final end state at all. Vaccum tubes led to computers for example and Youtube started as a dating website.&nbsp;</p>
<p><br></p>
<p>What fascinated us about this conversation with Ken is that we got a much deeper understanding of his philosophy. He lead by saying that he thought it's worth questioning whether artificial intelligence is even a science or not. Ken thinks that the secret to future progress is for us to embrace more subjectivity.&nbsp;</p>
<p><br></p>
<p>[00:00:00] Tim Intro</p>
<p>[00:12:54] Intro</p>
<p>[00:17:08] Seeing ideas everywhere - AI and art are highly connected</p>
<p>[00:28:40] Creativity in Mathematics</p>
<p>[00:30:14] Where is the intelligence in art?</p>
<p>[00:38:49] Is AI disappointingly simple to mechanise?</p>
<p>[00:42:48] Slightly conscious</p>
<p>[00:46:27] Do we have subjective experience?</p>
<p>[00:50:23] Fear of the unknown</p>
<p>[00:51:48] Free Will</p>
<p>[00:54:22] Chalmers</p>
<p>[00:55:08] What's happening now in open-endedness</p>
<p>[00:58:31] Generalisation</p>
<p>[01:06:34] Representation primitives and what it means to understand</p>
<p>[01:12:37] Appeal to definitions, knowledge itself blocks discovery</p>
<p><br></p>
<p>Make sure you buy Kenneth's book!</p>
<p><br></p>
<p>Why Greatness Cannot Be Planned: The Myth of the Objective [Stanley, Lehman]</p>
<p>https://www.amazon.co.uk/Why-Greatness-Cannot-Planned-Objective/dp/3319155237</p>
<p><br></p>
<p>Abandoning Objectives: Evolution through the</p>
<p>Search for Novelty Alone [Lehman, Stanley]</p>
<p>https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf</p>
<p><br></p>
<p>Twitter</p>
<p>https://twitter.com/kenneth0stanley</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/72-Prof--KEN-STANLEY-2-0---On-Art-and-Subjectivity-UNPLUGGED-e1gelqt</link>
			<guid isPermaLink="false">85f6d54d-a675-40d2-8636-1362d84c5597</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 29 Mar 2022 21:31:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/49812765/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-2-29%2F7c22738e-43be-8bb3-9019-94f4a936a946.mp3" length="122131150" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;YT version: https://youtu.be/DxBZORM9F-8&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Prof. Ken Stanley argued in his book that our world has become saturated with objectives. The process of setting an objective, attempting to achieve it, and measuring progress along the way has become the primary route to achievement in our culture. He’s not saying that objectives are bad per se, especially if they’re modest, but he thinks that when goals are ambitious then the search space becomes deceptive.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Is the key to artificial intelligence really related to intelligence? Does taking a job with a higher salary really bring you closer to being a millionaire? The problem is that the stepping stones which lead to ambitious objectives tend to be pretty strange, they don&apos;t resemble the final end state at all. Vaccum tubes led to computers for example and Youtube started as a dating website.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What fascinated us about this conversation with Ken is that we got a much deeper understanding of his philosophy. He lead by saying that he thought it&apos;s worth questioning whether artificial intelligence is even a science or not. Ken thinks that the secret to future progress is for us to embrace more subjectivity.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Tim Intro&lt;/p&gt;
&lt;p&gt;[00:12:54] Intro&lt;/p&gt;
&lt;p&gt;[00:17:08] Seeing ideas everywhere - AI and art are highly connected&lt;/p&gt;
&lt;p&gt;[00:28:40] Creativity in Mathematics&lt;/p&gt;
&lt;p&gt;[00:30:14] Where is the intelligence in art?&lt;/p&gt;
&lt;p&gt;[00:38:49] Is AI disappointingly simple to mechanise?&lt;/p&gt;
&lt;p&gt;[00:42:48] Slightly conscious&lt;/p&gt;
&lt;p&gt;[00:46:27] Do we have subjective experience?&lt;/p&gt;
&lt;p&gt;[00:50:23] Fear of the unknown&lt;/p&gt;
&lt;p&gt;[00:51:48] Free Will&lt;/p&gt;
&lt;p&gt;[00:54:22] Chalmers&lt;/p&gt;
&lt;p&gt;[00:55:08] What&apos;s happening now in open-endedness&lt;/p&gt;
&lt;p&gt;[00:58:31] Generalisation&lt;/p&gt;
&lt;p&gt;[01:06:34] Representation primitives and what it means to understand&lt;/p&gt;
&lt;p&gt;[01:12:37] Appeal to definitions, knowledge itself blocks discovery&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Make sure you buy Kenneth&apos;s book!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Why Greatness Cannot Be Planned: The Myth of the Objective [Stanley, Lehman]&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Why-Greatness-Cannot-Planned-Objective/dp/3319155237&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Abandoning Objectives: Evolution through the&lt;/p&gt;
&lt;p&gt;Search for Novelty Alone [Lehman, Stanley]&lt;/p&gt;
&lt;p&gt;https://www.cs.swarthmore.edu/~meeden/DevelopmentalRobotics/lehman_ecj11.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Twitter&lt;/p&gt;
&lt;p&gt;https://twitter.com/kenneth0stanley&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:24:48</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1648589503968-d74566789148a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>72</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#71 - ZAK JOST (Graph Neural Networks + Geometric DL) [UNPLUGGED]]]></title>
			<description><![CDATA[<p>Special discount link for Zak's GNN course - https://bit.ly/3uqmYVq</p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p>YT version: https://youtu.be/jAGIuobLp60 (there are lots of helper graphics there, recommended if poss)</p>
<p><br></p>
<p>Want to sponsor MLST!? Let us know on Linkedin / Twitter.&nbsp;</p>
<p><br></p>
<p>[00:00:00] Preamble</p>
<p>[00:03:12] Geometric deep learning</p>
<p>[00:10:04] Message passing</p>
<p>[00:20:42] Top down vs bottom up</p>
<p>[00:24:59] All NN architectures are different forms of information diffusion processes (squashing and smoothing problem)</p>
<p>[00:29:51] Graph rewiring</p>
<p>[00:31:38] Back to information diffusion&nbsp;</p>
<p>[00:42:43] Transformers vs GNNs</p>
<p>[00:47:10] Equivariant subgraph aggregation networks + WL test</p>
<p>[00:55:36] Do equivariant layers aggregate too?</p>
<p>[00:57:49] Zak's GNN course</p>
<p><br></p>
<p>Exhaustive list of references on the YT show URL (https://youtu.be/jAGIuobLp60)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/71---ZAK-JOST-Graph-Neural-Networks--Geometric-DL-UNPLUGGED-e1g8dvr</link>
			<guid isPermaLink="false">26bb5cb0-1e78-482f-948d-a54b985e8fbe</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 25 Mar 2022 18:10:05 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/49608123/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-2-25%2F469e352d-1565-1e37-7d4f-93d9240b57fb.mp3" length="90140372" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Special discount link for Zak&apos;s GNN course - https://bit.ly/3uqmYVq&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/jAGIuobLp60 (there are lots of helper graphics there, recommended if poss)&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Want to sponsor MLST!? Let us know on Linkedin / Twitter.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Preamble&lt;/p&gt;
&lt;p&gt;[00:03:12] Geometric deep learning&lt;/p&gt;
&lt;p&gt;[00:10:04] Message passing&lt;/p&gt;
&lt;p&gt;[00:20:42] Top down vs bottom up&lt;/p&gt;
&lt;p&gt;[00:24:59] All NN architectures are different forms of information diffusion processes (squashing and smoothing problem)&lt;/p&gt;
&lt;p&gt;[00:29:51] Graph rewiring&lt;/p&gt;
&lt;p&gt;[00:31:38] Back to information diffusion&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:42:43] Transformers vs GNNs&lt;/p&gt;
&lt;p&gt;[00:47:10] Equivariant subgraph aggregation networks + WL test&lt;/p&gt;
&lt;p&gt;[00:55:36] Do equivariant layers aggregate too?&lt;/p&gt;
&lt;p&gt;[00:57:49] Zak&apos;s GNN course&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Exhaustive list of references on the YT show URL (https://youtu.be/jAGIuobLp60)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:35</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1648231797927-382fd1e27ef0f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>71</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#70 - LETITIA PARCALABESCU - Symbolics, Linguistics [UNPLUGGED]]]></title>
			<description><![CDATA[<p>Today we are having a discussion with Letitia Parcalabescu from the AI Coffee Break youtube channel! We discuss linguistics, symbolic AI and our respective Youtube channels. Make sure you subscribe to her channel! In the first 15 minutes Tim dissects the recent article from Gary Marcus "Deep learning has hit a wall".&nbsp;</p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/ESrGqhf5CB</p>
<p>YT: https://youtu.be/p2D2duT-R2E</p>
<p><br></p>
<p>[00:00:00] Comments on Gary Marcus Article / Symbolic AI</p>
<p>[00:14:57] Greetings</p>
<p>[00:17:40] Introduction</p>
<p>[00:18:48] A shared journey towards computation</p>
<p>[00:22:10] A linguistics outsider</p>
<p>[00:24:11] Is computational linguistics AI?</p>
<p>[00:28:23] swinging pendulums of dogma and resource allocation</p>
<p>[00:31:16] the road less travelled</p>
<p>[00:34:35] pitching grants with multimodality ... and then the truth</p>
<p>[00:40:50] some aspects of language are statistically learnable</p>
<p>[00:44:58] ... and some aspects of language are dimensionally cursed</p>
<p>[00:48:24] it's good to have both approaches to machine intelligence</p>
<p>[00:51:14] the world runs on symbols</p>
<p>[00:54:28] there is much more to learn biology</p>
<p>[00:59:26] Letitia's creation process</p>
<p>[01:02:23] don't overfit content, instead publish and iterate</p>
<p>[01:07:48] merging the big picture arrow from the small direction arrows</p>
<p>[01:11:02] use passion to drive through failure to success</p>
<p>[01:12:56] stay positive</p>
<p>[01:16:02] closing remarks</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/70---LETITIA-PARCALABESCU---Symbolics--Linguistics-UNPLUGGED-e1fuqas</link>
			<guid isPermaLink="false">46ea9658-aefa-436e-a75b-429a1ec39b2f</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 19 Mar 2022 14:24:37 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/49293084/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-2-19%2F7cadb348-563c-bb7b-4a8b-9a74e8cc4422.mp3" length="113055082" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today we are having a discussion with Letitia Parcalabescu from the AI Coffee Break youtube channel! We discuss linguistics, symbolic AI and our respective Youtube channels. Make sure you subscribe to her channel! In the first 15 minutes Tim dissects the recent article from Gary Marcus &quot;Deep learning has hit a wall&quot;.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/ESrGqhf5CB&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/p2D2duT-R2E&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Comments on Gary Marcus Article / Symbolic AI&lt;/p&gt;
&lt;p&gt;[00:14:57] Greetings&lt;/p&gt;
&lt;p&gt;[00:17:40] Introduction&lt;/p&gt;
&lt;p&gt;[00:18:48] A shared journey towards computation&lt;/p&gt;
&lt;p&gt;[00:22:10] A linguistics outsider&lt;/p&gt;
&lt;p&gt;[00:24:11] Is computational linguistics AI?&lt;/p&gt;
&lt;p&gt;[00:28:23] swinging pendulums of dogma and resource allocation&lt;/p&gt;
&lt;p&gt;[00:31:16] the road less travelled&lt;/p&gt;
&lt;p&gt;[00:34:35] pitching grants with multimodality ... and then the truth&lt;/p&gt;
&lt;p&gt;[00:40:50] some aspects of language are statistically learnable&lt;/p&gt;
&lt;p&gt;[00:44:58] ... and some aspects of language are dimensionally cursed&lt;/p&gt;
&lt;p&gt;[00:48:24] it&apos;s good to have both approaches to machine intelligence&lt;/p&gt;
&lt;p&gt;[00:51:14] the world runs on symbols&lt;/p&gt;
&lt;p&gt;[00:54:28] there is much more to learn biology&lt;/p&gt;
&lt;p&gt;[00:59:26] Letitia&apos;s creation process&lt;/p&gt;
&lt;p&gt;[01:02:23] don&apos;t overfit content, instead publish and iterate&lt;/p&gt;
&lt;p&gt;[01:07:48] merging the big picture arrow from the small direction arrows&lt;/p&gt;
&lt;p&gt;[01:11:02] use passion to drive through failure to success&lt;/p&gt;
&lt;p&gt;[01:12:56] stay positive&lt;/p&gt;
&lt;p&gt;[01:16:02] closing remarks&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:18:30</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1647699870634-7c0104a04f637.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>70</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#69 DR. THOMAS LUX - Interpolation of Sparse High-Dimensional Data]]></title>
			<description><![CDATA[<p>Today we are speaking with Dr. Thomas Lux, a research scientist at Meta in Silicon Valley.&nbsp;</p>
<p><br></p>
<p>In some sense, all of supervised machine learning can be framed through the lens of geometry. All training data exists as points in euclidean space, and we want to predict the value of a function at all those points. Neural networks appear to be the modus operandi these days for many domains of prediction. In that light; we might ask ourselves — what makes neural networks better than classical techniques like K nearest neighbour from a geometric perspective. Our guest today has done research on exactly that problem, trying to define error bounds for approximations in terms of directions, distances, and derivatives. &nbsp;</p>
<p><br></p>
<p>The insights from Thomas's work point at why neural networks are so good at problems which everything else fails at, like image recognition. The key is in their ability to ignore parts of the input space, do nonlinear dimension reduction, and concentrate their approximation power on important parts of the function.&nbsp;</p>
<p><br></p>
<p>[00:00:00] Intro to Show</p>
<p>[00:04:11] Intro to Thomas (Main show kick off)</p>
<p>[00:04:56] Interpolation of Sparse High-Dimensional Data</p>
<p>[00:12:19] Where does one place the basis functions to partition the space, the perennial question</p>
<p>[00:16:20] The sampling phenomenon -- where did all those dimensions come from?</p>
<p>[00:17:40] The placement of the MLP basis functions, they are not where you think they are</p>
<p>[00:23:15] NNs only extrapolate when given explicit priors to do so, CNNs in the translation domain</p>
<p>[00:25:31] Transformers extrapolate in the permutation domain</p>
<p>[00:28:26] NN priors work by creating space junk everywhere</p>
<p>[00:36:44] Are vector spaces the way to go? On discrete problems</p>
<p>[00:40:23] Activation functioms</p>
<p>[00:45:57] What can we prove about NNs? Gradients without backprop</p>
<p><br></p>
<p>Interpolation of Sparse High-Dimensional Data [Lux]</p>
<p>https://tchlux.github.io/papers/tchlux-2020-NUMA.pdf</p>
<p><br></p>
<p>A Spline Theory of Deep Learning [_Balestriero_]</p>
<p>https://proceedings.mlr.press/v80/balestriero18b.html</p>
<p><br></p>
<p>Gradients without Backpropagation ‘22</p>
<p>https://arxiv.org/pdf/2202.08587.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/69-DR--THOMAS-LUX---Interpolation-of-Sparse-High-Dimensional-Data-e1fk3am</link>
			<guid isPermaLink="false">b48cc804-6bad-41b3-8d8c-e740f09a3736</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 12 Mar 2022 14:13:07 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48941846/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-2-12%2Fb0de86de-c053-2048-f079-396dec019876.mp3" length="72929770" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today we are speaking with Dr. Thomas Lux, a research scientist at Meta in Silicon Valley.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In some sense, all of supervised machine learning can be framed through the lens of geometry. All training data exists as points in euclidean space, and we want to predict the value of a function at all those points. Neural networks appear to be the modus operandi these days for many domains of prediction. In that light; we might ask ourselves — what makes neural networks better than classical techniques like K nearest neighbour from a geometric perspective. Our guest today has done research on exactly that problem, trying to define error bounds for approximations in terms of directions, distances, and derivatives. &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The insights from Thomas&apos;s work point at why neural networks are so good at problems which everything else fails at, like image recognition. The key is in their ability to ignore parts of the input space, do nonlinear dimension reduction, and concentrate their approximation power on important parts of the function.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro to Show&lt;/p&gt;
&lt;p&gt;[00:04:11] Intro to Thomas (Main show kick off)&lt;/p&gt;
&lt;p&gt;[00:04:56] Interpolation of Sparse High-Dimensional Data&lt;/p&gt;
&lt;p&gt;[00:12:19] Where does one place the basis functions to partition the space, the perennial question&lt;/p&gt;
&lt;p&gt;[00:16:20] The sampling phenomenon -- where did all those dimensions come from?&lt;/p&gt;
&lt;p&gt;[00:17:40] The placement of the MLP basis functions, they are not where you think they are&lt;/p&gt;
&lt;p&gt;[00:23:15] NNs only extrapolate when given explicit priors to do so, CNNs in the translation domain&lt;/p&gt;
&lt;p&gt;[00:25:31] Transformers extrapolate in the permutation domain&lt;/p&gt;
&lt;p&gt;[00:28:26] NN priors work by creating space junk everywhere&lt;/p&gt;
&lt;p&gt;[00:36:44] Are vector spaces the way to go? On discrete problems&lt;/p&gt;
&lt;p&gt;[00:40:23] Activation functioms&lt;/p&gt;
&lt;p&gt;[00:45:57] What can we prove about NNs? Gradients without backprop&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interpolation of Sparse High-Dimensional Data [Lux]&lt;/p&gt;
&lt;p&gt;https://tchlux.github.io/papers/tchlux-2020-NUMA.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Spline Theory of Deep Learning [_Balestriero_]&lt;/p&gt;
&lt;p&gt;https://proceedings.mlr.press/v80/balestriero18b.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gradients without Backpropagation ‘22&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2202.08587.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:50:38</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1647094379475-c0df953c57859.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>69</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#68 DR. WALID SABA 2.0 - Natural Language Understanding [UNPLUGGED]]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/HNnAwSduud</p>
<p>YT version: https://youtu.be/pMtk-iUaEuQ</p>
<p><br></p>
<p>Dr. Walid Saba is an old-school polymath. He has a background in cognitive &nbsp;psychology, linguistics, philosophy, computer science and logic and he’s is now a Senior Scientist at Sorcero.</p>
<p><br></p>
<p>Walid is perhaps the most outspoken critic of BERTOLOGY, which is to say trying to solve the problem of natural language understanding with application of large statistical language models. Walid thinks this approach is cursed to failure because it’s analogous to memorising infinity with a large hashtable. Walid thinks that the various appeals to infinity by some deep learning researchers are risible.</p>
<p><br></p>
<p>[00:00:00] MLST Housekeeping</p>
<p>[00:08:03] Dr. Walid Saba Intro</p>
<p>[00:11:56] AI Cannot Ignore Symbolic Logic, and Here’s Why</p>
<p>[00:23:39] Main show - Proposition: Statistical learning doesn't work</p>
<p>[01:04:44] Discovering a sorting algorithm bottom-up is hard</p>
<p>[01:17:36] The axioms of nature (universal cognitive templates)</p>
<p>[01:31:06] MLPs are locality sensitive hashing tables</p>
<p><br></p>
<p>References;</p>
<p>The Missing Text Phenomenon, Again: the case of Compound Nominals</p>
<p>https://ontologik.medium.com/the-missing-text-phenomenon-again-the-case-of-compound-nominals-abb6ece3e205</p>
<p><br></p>
<p>A Spline Theory of Deep Networks</p>
<p>https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf</p>
<p><br></p>
<p>The Defeat of the Winograd Schema Challenge</p>
<p>https://arxiv.org/pdf/2201.02387.pdf</p>
<p><br></p>
<p>Impact of Pretraining Term Frequencies on Few-Shot Reasoning</p>
<p>https://twitter.com/yasaman_razeghi/status/1495112604854882304?s=21</p>
<p>https://arxiv.org/abs/2202.07206</p>
<p><br></p>
<p>AI Cannot Ignore Symbolic Logic, and Here’s Why</p>
<p>https://medium.com/ontologik/ai-cannot-ignore-symbolic-logic-and-heres-why-1f896713525b</p>
<p><br></p>
<p>Learnability can be undecidable</p>
<p>http://gtts.ehu.es/German/Docencia/1819/AC/extras/s42256-018-0002-3.pdf</p>
<p><br></p>
<p>Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</p>
<p>https://arxiv.org/pdf/2112.11446.pdf</p>
<p><br></p>
<p>DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning</p>
<p>https://arxiv.org/abs/2006.08381</p>
<p><br></p>
<p>On the Measure of Intelligence [Chollet]</p>
<p>https://arxiv.org/abs/1911.01547</p>
<p><br></p>
<p>A Formal Theory of Commonsense Psychology: How People Think People Think</p>
<p>https://www.amazon.co.uk/Formal-Theory-Commonsense-Psychology-People/dp/1107151007</p>
<p><br></p>
<p>Continuum hypothesis</p>
<p>https://en.wikipedia.org/wiki/Continuum_hypothesis</p>
<p><br></p>
<p>Gödel numbering + completness theorems</p>
<p>https://en.wikipedia.org/wiki/G%C3%B6del_numbering</p>
<p>https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems</p>
<p><br></p>
<p>Concepts: Where Cognitive Science Went Wrong [Jerry A. Fodor]</p>
<p>https://oxford.universitypressscholarship.com/view/10.1093/0198236360.001.0001/acprof-9780198236368</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/68-DR--WALID-SABA-2-0---Natural-Language-Understanding-UNPLUGGED-e1fbsdp</link>
			<guid isPermaLink="false">ef843a67-85e6-40f4-b5c5-a8ef3e135b5b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 07 Mar 2022 13:25:57 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48672633/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-2-7%2F252293775-44100-2-c7ff5cdd5f40b.m4a" length="99215427" type="audio/x-m4a"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/HNnAwSduud&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/pMtk-iUaEuQ&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Walid Saba is an old-school polymath. He has a background in cognitive &amp;nbsp;psychology, linguistics, philosophy, computer science and logic and he’s is now a Senior Scientist at Sorcero.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Walid is perhaps the most outspoken critic of BERTOLOGY, which is to say trying to solve the problem of natural language understanding with application of large statistical language models. Walid thinks this approach is cursed to failure because it’s analogous to memorising infinity with a large hashtable. Walid thinks that the various appeals to infinity by some deep learning researchers are risible.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] MLST Housekeeping&lt;/p&gt;
&lt;p&gt;[00:08:03] Dr. Walid Saba Intro&lt;/p&gt;
&lt;p&gt;[00:11:56] AI Cannot Ignore Symbolic Logic, and Here’s Why&lt;/p&gt;
&lt;p&gt;[00:23:39] Main show - Proposition: Statistical learning doesn&apos;t work&lt;/p&gt;
&lt;p&gt;[01:04:44] Discovering a sorting algorithm bottom-up is hard&lt;/p&gt;
&lt;p&gt;[01:17:36] The axioms of nature (universal cognitive templates)&lt;/p&gt;
&lt;p&gt;[01:31:06] MLPs are locality sensitive hashing tables&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;The Missing Text Phenomenon, Again: the case of Compound Nominals&lt;/p&gt;
&lt;p&gt;https://ontologik.medium.com/the-missing-text-phenomenon-again-the-case-of-compound-nominals-abb6ece3e205&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Spline Theory of Deep Networks&lt;/p&gt;
&lt;p&gt;https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The Defeat of the Winograd Schema Challenge&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2201.02387.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Impact of Pretraining Term Frequencies on Few-Shot Reasoning&lt;/p&gt;
&lt;p&gt;https://twitter.com/yasaman_razeghi/status/1495112604854882304?s=21&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2202.07206&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;AI Cannot Ignore Symbolic Logic, and Here’s Why&lt;/p&gt;
&lt;p&gt;https://medium.com/ontologik/ai-cannot-ignore-symbolic-logic-and-heres-why-1f896713525b&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Learnability can be undecidable&lt;/p&gt;
&lt;p&gt;http://gtts.ehu.es/German/Docencia/1819/AC/extras/s42256-018-0002-3.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2112.11446.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2006.08381&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;On the Measure of Intelligence [Chollet]&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;A Formal Theory of Commonsense Psychology: How People Think People Think&lt;/p&gt;
&lt;p&gt;https://www.amazon.co.uk/Formal-Theory-Commonsense-Psychology-People/dp/1107151007&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Continuum hypothesis&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/Continuum_hypothesis&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gödel numbering + completness theorems&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/G%C3%B6del_numbering&lt;/p&gt;
&lt;p&gt;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Concepts: Where Cognitive Science Went Wrong [Jerry A. Fodor]&lt;/p&gt;
&lt;p&gt;https://oxford.universitypressscholarship.com/view/10.1093/0198236360.001.0001/acprof-9780198236368&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:42:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1646659547702-d0e844241844f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>68</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#67 Prof. KARL FRISTON 2.0]]></title>
			<description><![CDATA[<p>We engage in a bit of epistemic foraging with Prof. Karl Friston! In this show; we discuss the free energy principle in detail, also emergence, cognition, consciousness and Karl's burden of knowledge!</p>
<p><br></p>
<p>YT: https://youtu.be/xKQ-F2-o8uM</p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/HNnAwSduud</p>
<p><br></p>
<p>[00:00:00] Introduction to FEP/Friston</p>
<p>[00:06:53] Cheers to Epistemic Foraging!</p>
<p>[00:09:17] The Burden of Knowledge Across Disciplines</p>
<p>[00:12:55] On-show introduction to Friston</p>
<p>[00:14:23] Simple does NOT mean Easy</p>
<p>[00:21:25] Searching for a Mathematics of Cognition</p>
<p>[00:26:44] The Low Road and The High Road to the Principle</p>
<p>[00:28:27] What's changed for the FEP in the last year</p>
<p>[00:39:36] FEP as stochastic systems with a pullback attractor</p>
<p>[00:44:03] An attracting set at multiple time scales and time infinity</p>
<p>[00:53:56] What about fuzzy Markov boundaries?</p>
<p>[00:59:17] Is reality densely or sparsely coupled?</p>
<p>[01:07:00] Is a Strong and Weak Emergence distinction useful?</p>
<p>[01:13:25] a Philosopher, a Zombie, and a Sentient Consciousness walk into a bar ...&nbsp;</p>
<p>[01:24:28] Can we recreate consciousness in silico? Will it have qualia?</p>
<p>[01:28:29] Subjectivity and building hypotheses</p>
<p>[01:34:17] Subject specific realizations to minimize free energy</p>
<p>[01:37:21] Free will in a deterministic Universe</p>
<p><br></p>
<p>The free energy principle made simpler but not too simple</p>
<p>https://arxiv.org/abs/2201.06387</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/67-Prof--KARL-FRISTON-2-0-e1f4ck3</link>
			<guid isPermaLink="false">c0374dd4-5857-416c-bbba-c94538315d46</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 02 Mar 2022 10:01:46 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48427075/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-2-2%2F04dcf002-3cef-4c37-ac83-b68c43a69e4f.mp3" length="147136264" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We engage in a bit of epistemic foraging with Prof. Karl Friston! In this show; we discuss the free energy principle in detail, also emergence, cognition, consciousness and Karl&apos;s burden of knowledge!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;YT: https://youtu.be/xKQ-F2-o8uM&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/HNnAwSduud&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction to FEP/Friston&lt;/p&gt;
&lt;p&gt;[00:06:53] Cheers to Epistemic Foraging!&lt;/p&gt;
&lt;p&gt;[00:09:17] The Burden of Knowledge Across Disciplines&lt;/p&gt;
&lt;p&gt;[00:12:55] On-show introduction to Friston&lt;/p&gt;
&lt;p&gt;[00:14:23] Simple does NOT mean Easy&lt;/p&gt;
&lt;p&gt;[00:21:25] Searching for a Mathematics of Cognition&lt;/p&gt;
&lt;p&gt;[00:26:44] The Low Road and The High Road to the Principle&lt;/p&gt;
&lt;p&gt;[00:28:27] What&apos;s changed for the FEP in the last year&lt;/p&gt;
&lt;p&gt;[00:39:36] FEP as stochastic systems with a pullback attractor&lt;/p&gt;
&lt;p&gt;[00:44:03] An attracting set at multiple time scales and time infinity&lt;/p&gt;
&lt;p&gt;[00:53:56] What about fuzzy Markov boundaries?&lt;/p&gt;
&lt;p&gt;[00:59:17] Is reality densely or sparsely coupled?&lt;/p&gt;
&lt;p&gt;[01:07:00] Is a Strong and Weak Emergence distinction useful?&lt;/p&gt;
&lt;p&gt;[01:13:25] a Philosopher, a Zombie, and a Sentient Consciousness walk into a bar ...&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[01:24:28] Can we recreate consciousness in silico? Will it have qualia?&lt;/p&gt;
&lt;p&gt;[01:28:29] Subjectivity and building hypotheses&lt;/p&gt;
&lt;p&gt;[01:34:17] Subject specific realizations to minimize free energy&lt;/p&gt;
&lt;p&gt;[01:37:21] Free will in a deterministic Universe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The free energy principle made simpler but not too simple&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2201.06387&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:42:10</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1646215296248-453f81759752f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>67</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#66 ALEXANDER MATTICK - [Unplugged / Community Edition]]]></title>
			<description><![CDATA[<p>We have a chat with Alexander Mattick aka ZickZack from Yannic's Discord community. Alex is one of the leading voices in that community and has an impressive technical depth. Don't forget MLST has now started it's own Discord server too, come and join us! We are going to run regular events, our first big event on Wednesday 9th 1700-1900 UK time.&nbsp;</p>
<p><br></p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/HNnAwSduud</p>
<p>YT version: https://youtu.be/rGOOLC8cIO4</p>
<p><br></p>
<p>[00:00:00] Introduction to Alex&nbsp;</p>
<p>[00:02:16] Spline theory of NNs&nbsp;</p>
<p>[00:05:19] Do NNs abstract?&nbsp;</p>
<p>[00:08:27] Tim's exposition of spline theory of NNs</p>
<p>[00:11:11] Semantics in NNs&nbsp;</p>
<p>[00:13:37] Continuous vs discrete&nbsp;</p>
<p>[00:19:00] Open-ended Search</p>
<p>[00:22:54] Inductive logic programming</p>
<p>[00:25:00] Control to gain knowledge and knowledge to gain control</p>
<p>[00:30:22] Being a generalist with a breadth of knowledge and knowledge transfer</p>
<p>[00:36:29] Causality</p>
<p>[00:43:14] Discrete program synthesis + theorem solvers</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/66-ALEXANDER-MATTICK---Unplugged--Community-Edition-e1f12n9</link>
			<guid isPermaLink="false">1bc202a1-47c2-4150-8919-f66a7c8accbc</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 28 Feb 2022 08:09:10 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48318633/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-1-28%2F59d17419-d570-9ae7-3e29-b1b367645f10.mp3" length="72763539" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We have a chat with Alexander Mattick aka ZickZack from Yannic&apos;s Discord community. Alex is one of the leading voices in that community and has an impressive technical depth. Don&apos;t forget MLST has now started it&apos;s own Discord server too, come and join us! We are going to run regular events, our first big event on Wednesday 9th 1700-1900 UK time.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/HNnAwSduud&lt;/p&gt;
&lt;p&gt;YT version: https://youtu.be/rGOOLC8cIO4&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Introduction to Alex&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:02:16] Spline theory of NNs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:05:19] Do NNs abstract?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:08:27] Tim&apos;s exposition of spline theory of NNs&lt;/p&gt;
&lt;p&gt;[00:11:11] Semantics in NNs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:13:37] Continuous vs discrete&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:19:00] Open-ended Search&lt;/p&gt;
&lt;p&gt;[00:22:54] Inductive logic programming&lt;/p&gt;
&lt;p&gt;[00:25:00] Control to gain knowledge and knowledge to gain control&lt;/p&gt;
&lt;p&gt;[00:30:22] Being a generalist with a breadth of knowledge and knowledge transfer&lt;/p&gt;
&lt;p&gt;[00:36:29] Causality&lt;/p&gt;
&lt;p&gt;[00:43:14] Discrete program synthesis + theorem solvers&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:50:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1646035815801-d17722f374c1b.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>66</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#65 Prof. PEDRO DOMINGOS [Unplugged]]]></title>
			<description><![CDATA[<p>Note: there are no politics discussed in this show and please do not interpret this show as any kind of a political statement from us. &nbsp;We have decided not to discuss politics on MLST anymore due to its divisive nature.&nbsp;</p>
<p><br></p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/HNnAwSduud</p>
<p><br></p>
<p>[00:00:00] Intro</p>
<p>[00:01:36] What we all need to understand about machine learning</p>
<p>[00:06:05] The Master Algorithm Target Audience</p>
<p>[00:09:50] Deeply Connected Algorithms seen from Divergent Frames of Reference</p>
<p>[00:12:49] There is a Master Algorithm; and it's mine!</p>
<p>[00:14:59] The Tribe of Evolution</p>
<p>[00:17:17] Biological Inspirations and Predictive Coding</p>
<p>[00:22:09] Shoe-Horning Gradient Descent</p>
<p>[00:27:12] Sparsity at Training Time vs Prediction Time</p>
<p>[00:30:00] World Models and Predictive Coding</p>
<p>[00:33:24] The Cartoons of System 1 and System 2</p>
<p>[00:40:37] AlphaGo Searching vs Learning</p>
<p>[00:45:56] Discriminative Models evolve into Generative Models</p>
<p>[00:50:36] Generative Models, Predictive Coding, GFlowNets</p>
<p>[00:55:50] Sympathy for a Thousand Brains</p>
<p>[00:59:05] A Spectrum of Tribes</p>
<p>[01:04:29] Causal Structure and Modelling</p>
<p>[01:09:39] Entropy and The Duality of Past vs Future, Knowledge vs Control</p>
<p>[01:16:14] A Discrete Universe?</p>
<p>[01:19:49] And yet continuous models work so well</p>
<p>[01:23:31] Finding a Discretised Theory of Everything</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/65-Prof--PEDRO-DOMINGOS-Unplugged-e1eu8pp</link>
			<guid isPermaLink="false">e0d20e0a-15b8-4d74-a816-f17f69d9a85a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 26 Feb 2022 00:27:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48226553/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-1-26%2F14f8fb2f-d937-5d27-3241-b3dff1801ad3.mp3" length="127393342" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Note: there are no politics discussed in this show and please do not interpret this show as any kind of a political statement from us. &amp;nbsp;We have decided not to discuss politics on MLST anymore due to its divisive nature.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/HNnAwSduud&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Intro&lt;/p&gt;
&lt;p&gt;[00:01:36] What we all need to understand about machine learning&lt;/p&gt;
&lt;p&gt;[00:06:05] The Master Algorithm Target Audience&lt;/p&gt;
&lt;p&gt;[00:09:50] Deeply Connected Algorithms seen from Divergent Frames of Reference&lt;/p&gt;
&lt;p&gt;[00:12:49] There is a Master Algorithm; and it&apos;s mine!&lt;/p&gt;
&lt;p&gt;[00:14:59] The Tribe of Evolution&lt;/p&gt;
&lt;p&gt;[00:17:17] Biological Inspirations and Predictive Coding&lt;/p&gt;
&lt;p&gt;[00:22:09] Shoe-Horning Gradient Descent&lt;/p&gt;
&lt;p&gt;[00:27:12] Sparsity at Training Time vs Prediction Time&lt;/p&gt;
&lt;p&gt;[00:30:00] World Models and Predictive Coding&lt;/p&gt;
&lt;p&gt;[00:33:24] The Cartoons of System 1 and System 2&lt;/p&gt;
&lt;p&gt;[00:40:37] AlphaGo Searching vs Learning&lt;/p&gt;
&lt;p&gt;[00:45:56] Discriminative Models evolve into Generative Models&lt;/p&gt;
&lt;p&gt;[00:50:36] Generative Models, Predictive Coding, GFlowNets&lt;/p&gt;
&lt;p&gt;[00:55:50] Sympathy for a Thousand Brains&lt;/p&gt;
&lt;p&gt;[00:59:05] A Spectrum of Tribes&lt;/p&gt;
&lt;p&gt;[01:04:29] Causal Structure and Modelling&lt;/p&gt;
&lt;p&gt;[01:09:39] Entropy and The Duality of Past vs Future, Knowledge vs Control&lt;/p&gt;
&lt;p&gt;[01:16:14] A Discrete Universe?&lt;/p&gt;
&lt;p&gt;[01:19:49] And yet continuous models work so well&lt;/p&gt;
&lt;p&gt;[01:23:31] Finding a Discretised Theory of Everything&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:28:27</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1645835257100-69c6dac30d819.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>65</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#64 Prof. Gary Marcus 3.0]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>Discord: https://discord.gg/HNnAwSduud</p>
<p>YT: https://www.youtube.com/watch?v=ZDY2nhkPZxw</p>
<p>We have a chat with Prof. Gary Marcus about everything which is currently top of mind for him, consciousness&nbsp;</p>
<p><br></p>
<p>[00:00:00] Gary intro</p>
<p>[00:01:25] Slightly conscious</p>
<p>[00:24:59] Abstract, compositional models</p>
<p>[00:32:46] Spline theory of NNs</p>
<p>[00:36:17] Self driving cars / algebraic reasoning&nbsp;</p>
<p>[00:39:43] Extrapolation</p>
<p>[00:44:15] Scaling laws</p>
<p>[00:49:50] Maximum likelihood estimation</p>
<p><br></p>
<p>References:</p>
<p>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</p>
<p>https://arxiv.org/abs/2201.02177</p>
<p><br></p>
<p>DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT</p>
<p>https://arxiv.org/pdf/1912.02292.pdf</p>
<p><br></p>
<p>Bayesian Deep Learning and a Probabilistic Perspective of Generalization</p>
<p>https://arxiv.org/pdf/2002.08791.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/64-Prof--Gary-Marcus-3-0-e1es2gc</link>
			<guid isPermaLink="false">56e13bd1-0861-4c66-93b5-35b6ea3c9520</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 24 Feb 2022 15:44:12 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48154572/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-1-24%2Ff7efaa32-dd48-9e03-aa3b-0687dde9e928.mp3" length="74585669" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Discord: https://discord.gg/HNnAwSduud&lt;/p&gt;
&lt;p&gt;YT: https://www.youtube.com/watch?v=ZDY2nhkPZxw&lt;/p&gt;
&lt;p&gt;We have a chat with Prof. Gary Marcus about everything which is currently top of mind for him, consciousness&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;[00:00:00] Gary intro&lt;/p&gt;
&lt;p&gt;[00:01:25] Slightly conscious&lt;/p&gt;
&lt;p&gt;[00:24:59] Abstract, compositional models&lt;/p&gt;
&lt;p&gt;[00:32:46] Spline theory of NNs&lt;/p&gt;
&lt;p&gt;[00:36:17] Self driving cars / algebraic reasoning&amp;nbsp;&lt;/p&gt;
&lt;p&gt;[00:39:43] Extrapolation&lt;/p&gt;
&lt;p&gt;[00:44:15] Scaling laws&lt;/p&gt;
&lt;p&gt;[00:49:50] Maximum likelihood estimation&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2201.02177&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1912.02292.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Bayesian Deep Learning and a Probabilistic Perspective of Generalization&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2002.08791.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:51:47</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1645717441360-7fe39c15343c4.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>64</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#063 - Prof. YOSHUA BENGIO - GFlowNets, Consciousness & Causality]]></title>
			<description><![CDATA[<p>We are now sponsored by Weights and Biases! Please visit our sponsor link: http://wandb.me/MLST</p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>For Yoshua Bengio, GFlowNets are the most exciting thing on the horizon of Machine Learning today. He believes they can solve previously intractable problems and hold the key to unlocking machine abstract reasoning itself. This discussion explores the promise of GFlowNets and the personal journey Prof. Bengio traveled to reach them.</p>
<p>Panel:</p>
<p>Dr. Tim Scarfe</p>
<p>Dr. Keith Duggar</p>
<p>Dr. Yannic Kilcher</p>
<p><br></p>
<p>Our special thanks to:&nbsp;</p>
<p>- Alexander Mattick (Zickzack)</p>
<p>References:</p>
<p>Yoshua Bengio @ MILA (https://mila.quebec/en/person/bengio-yoshua/)</p>
<p>GFlowNet Foundations (https://arxiv.org/pdf/2111.09266.pdf)</p>
<p>Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation (https://arxiv.org/pdf/2106.04399.pdf)</p>
<p>Interpolation Consistency Training for Semi-Supervised Learning (https://arxiv.org/pdf/1903.03825.pdf)</p>
<p>Towards Causal Representation Learning (https://arxiv.org/pdf/2102.11107.pdf)</p>
<p>Causal inference using invariant prediction: identification and confidence intervals (https://arxiv.org/pdf/1501.01332.pdf)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/063---Prof--YOSHUA-BENGIO---GFlowNets--Consciousness--Causality-e1enlor</link>
			<guid isPermaLink="false">0625feba-36d1-4cfc-adc9-3b5ed8b3e6c3</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 22 Feb 2022 00:07:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/48010459/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-1-21%2F8ff3e9bb-1aff-6fef-2c2e-140852498452.mp3" length="134102567" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We are now sponsored by Weights and Biases! Please visit our sponsor link: http://wandb.me/MLST&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;For Yoshua Bengio, GFlowNets are the most exciting thing on the horizon of Machine Learning today. He believes they can solve previously intractable problems and hold the key to unlocking machine abstract reasoning itself. This discussion explores the promise of GFlowNets and the personal journey Prof. Bengio traveled to reach them.&lt;/p&gt;
&lt;p&gt;Panel:&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Dr. Yannic Kilcher&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Our special thanks to:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;- Alexander Mattick (Zickzack)&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;Yoshua Bengio @ MILA (https://mila.quebec/en/person/bengio-yoshua/)&lt;/p&gt;
&lt;p&gt;GFlowNet Foundations (https://arxiv.org/pdf/2111.09266.pdf)&lt;/p&gt;
&lt;p&gt;Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation (https://arxiv.org/pdf/2106.04399.pdf)&lt;/p&gt;
&lt;p&gt;Interpolation Consistency Training for Semi-Supervised Learning (https://arxiv.org/pdf/1903.03825.pdf)&lt;/p&gt;
&lt;p&gt;Towards Causal Representation Learning (https://arxiv.org/pdf/2102.11107.pdf)&lt;/p&gt;
&lt;p&gt;Causal inference using invariant prediction: identification and confidence intervals (https://arxiv.org/pdf/1501.01332.pdf)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:33:07</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1645488375361-30b01938b5df7.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>63</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#062 - Dr. Guy Emerson - Linguistics, Distributional Semantics]]></title>
			<description><![CDATA[<p>Dr. Guy Emerson is a computational linguist and obtained<strong> </strong>his Ph.D from Cambridge university where he is now a research fellow and lecturer. On panel we also have myself, Dr. Tim Scarfe, as well as Dr. Keith Duggar and the veritable Dr. Walid Saba. We dive into distributional semantics, probability theory, fuzzy logic, grounding, vagueness and the grammar/cognition connection.</p>
<p>The aim of distributional semantics is to design computational techniques that can automatically learn the meanings of words from a body of text. The twin challenges are: how do we represent meaning, and how do we learn these representations? We want to learn the meanings of words from a corpus by exploiting the fact that the context of a word tells us something about its meaning. This is known as the distributional hypothesis. In his Ph.D thesis, Dr. Guy Emerson presented a distributional model which can learn truth-conditional semantics which are grounded by objects in the real world.</p>
<p>Hope you enjoy the show!</p>
<p><a href="https://www.cai.cam.ac.uk/people/dr-guy-emerson">https://www.cai.cam.ac.uk/people/dr-guy-emerson</a></p>
<p><a href="https://www.repository.cam.ac.uk/handle/1810/284882?show=full">https://www.repository.cam.ac.uk/handle/1810/284882?show=full</a></p>
<p><a href="https://www.semanticscholar.org/paper/Computational-linguistics-and-grammar-engineering-Bender-Emerson/bbd6f3b92a0f1ea8212f383cc4719bfe86b3588c">https://www.semanticscholar.org/paper/Computational-linguistics-and-grammar-engineering-Bender-Emerson/bbd6f3b92a0f1ea8212f383cc4719bfe86b3588c</a></p>
<p><br></p>
<p>Patreon: https://www.patreon.com/mlst</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/062---Dr--Guy-Emerson---Linguistics--Distributional-Semantics-e1ds64t</link>
			<guid isPermaLink="false">9acd8525-4034-48ac-83a9-cf2bb20af75f</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 03 Feb 2022 12:41:59 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/47109725/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-1-3%2F26941690-35c1-33ee-e47c-4979d30a79ea.mp3" length="172480138" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Guy Emerson is a computational linguist and obtained&lt;strong&gt; &lt;/strong&gt;his Ph.D from Cambridge university where he is now a research fellow and lecturer. On panel we also have myself, Dr. Tim Scarfe, as well as Dr. Keith Duggar and the veritable Dr. Walid Saba. We dive into distributional semantics, probability theory, fuzzy logic, grounding, vagueness and the grammar/cognition connection.&lt;/p&gt;
&lt;p&gt;The aim of distributional semantics is to design computational techniques that can automatically learn the meanings of words from a body of text. The twin challenges are: how do we represent meaning, and how do we learn these representations? We want to learn the meanings of words from a corpus by exploiting the fact that the context of a word tells us something about its meaning. This is known as the distributional hypothesis. In his Ph.D thesis, Dr. Guy Emerson presented a distributional model which can learn truth-conditional semantics which are grounded by objects in the real world.&lt;/p&gt;
&lt;p&gt;Hope you enjoy the show!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cai.cam.ac.uk/people/dr-guy-emerson&quot;&gt;https://www.cai.cam.ac.uk/people/dr-guy-emerson&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.repository.cam.ac.uk/handle/1810/284882?show=full&quot;&gt;https://www.repository.cam.ac.uk/handle/1810/284882?show=full&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.semanticscholar.org/paper/Computational-linguistics-and-grammar-engineering-Bender-Emerson/bbd6f3b92a0f1ea8212f383cc4719bfe86b3588c&quot;&gt;https://www.semanticscholar.org/paper/Computational-linguistics-and-grammar-engineering-Bender-Emerson/bbd6f3b92a0f1ea8212f383cc4719bfe86b3588c&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:29:50</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1643891923394-b96bf902c6e7f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>62</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[061: Interpolation, Extrapolation and Linearisation (Prof. Yann LeCun, Dr. Randall Balestriero)]]></title>
			<description><![CDATA[<p>We are now sponsored by Weights and Biases! Please visit our sponsor link: http://wandb.me/MLST</p>
<p>Patreon: https://www.patreon.com/mlst</p>
<p>Yann LeCun thinks that it's specious to say neural network models are interpolating because in high dimensions, everything is extrapolation. Recently Dr. <em>Randall Balestriero</em>, Dr. Jerome Pesente and prof. Yann LeCun released their paper learning in high dimensions always amounts to extrapolation. This discussion has completely changed how we think about neural networks and their behaviour.</p>
<p>[00:00:00] Pre-intro</p>
<p>[00:11:58] Intro Part 1: On linearisation in NNs</p>
<p>[00:28:17] Intro Part 2: On interpolation in NNs</p>
<p>[00:47:45] Intro Part 3: On the curse</p>
<p>[00:48:19] LeCun</p>
<p>[01:40:51] Randall B</p>
<p>YouTube version: https://youtu.be/86ib0sfdFtw</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/061-Interpolation--Extrapolation-and-Linearisation-Prof--Yann-LeCun--Dr--Randall-Balestriero-e1cgdr0</link>
			<guid isPermaLink="false">198d3ee2-efda-4f3a-9dfe-e80b78b08a23</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 04 Jan 2022 12:59:49 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/45675808/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2022-0-4%2F2aa9693f-f8f0-ebad-2737-3a1e1c40a352.mp3" length="191742825" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We are now sponsored by Weights and Biases! Please visit our sponsor link: http://wandb.me/MLST&lt;/p&gt;
&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;Yann LeCun thinks that it&apos;s specious to say neural network models are interpolating because in high dimensions, everything is extrapolation. Recently Dr. &lt;em&gt;Randall Balestriero&lt;/em&gt;, Dr. Jerome Pesente and prof. Yann LeCun released their paper learning in high dimensions always amounts to extrapolation. This discussion has completely changed how we think about neural networks and their behaviour.&lt;/p&gt;
&lt;p&gt;[00:00:00] Pre-intro&lt;/p&gt;
&lt;p&gt;[00:11:58] Intro Part 1: On linearisation in NNs&lt;/p&gt;
&lt;p&gt;[00:28:17] Intro Part 2: On interpolation in NNs&lt;/p&gt;
&lt;p&gt;[00:47:45] Intro Part 3: On the curse&lt;/p&gt;
&lt;p&gt;[00:48:19] LeCun&lt;/p&gt;
&lt;p&gt;[01:40:51] Randall B&lt;/p&gt;
&lt;p&gt;YouTube version: https://youtu.be/86ib0sfdFtw&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>03:19:43</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1641301222836-f5a2899f5b3fd.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>61</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#60 Geometric Deep Learning Blueprint (Special Edition)]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact tractable given enough computational horsepower. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning and second, learning by local gradient-descent type methods, typically implemented as backpropagation.</p>
<p>While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not uniform and have strong repeating patterns as a result of the low-dimensionality and structure of the physical world.</p>
<p>Geometric Deep Learning unifies a broad class of ML problems from the perspectives of <strong>symmetry </strong>and <strong>invariance</strong>. These principles not only underlie the breakthrough performance of convolutional neural networks and the recent success of graph neural networks but also provide a principled way to construct new types of problem-specific inductive biases.</p>
<p>This week we spoke with Professor Michael Bronstein (head of graph ML at Twitter) and Dr.</p>
<p>Petar Veličković (Senior Research Scientist at DeepMind), and Dr. Taco Cohen and Prof. Joan Bruna about their new proto-book Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.</p>
<p>See the table of contents for this (long) show at https://youtu.be/bIZB1hIJ4u8&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/60-Geometric-Deep-Learning-Blueprint-Special-Edition-e17i495</link>
			<guid isPermaLink="false">f83c2af5-c105-4695-bfed-e079ed9e9640</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 19 Sep 2021 01:29:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/40488677/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-8-19%2F449d199e-c75f-722f-e76a-1ae28213dd6c.mp3" length="205055555" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact tractable given enough computational horsepower. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning and second, learning by local gradient-descent type methods, typically implemented as backpropagation.&lt;/p&gt;
&lt;p&gt;While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not uniform and have strong repeating patterns as a result of the low-dimensionality and structure of the physical world.&lt;/p&gt;
&lt;p&gt;Geometric Deep Learning unifies a broad class of ML problems from the perspectives of &lt;strong&gt;symmetry &lt;/strong&gt;and &lt;strong&gt;invariance&lt;/strong&gt;. These principles not only underlie the breakthrough performance of convolutional neural networks and the recent success of graph neural networks but also provide a principled way to construct new types of problem-specific inductive biases.&lt;/p&gt;
&lt;p&gt;This week we spoke with Professor Michael Bronstein (head of graph ML at Twitter) and Dr.&lt;/p&gt;
&lt;p&gt;Petar Veličković (Senior Research Scientist at DeepMind), and Dr. Taco Cohen and Prof. Joan Bruna about their new proto-book Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.&lt;/p&gt;
&lt;p&gt;See the table of contents for this (long) show at https://youtu.be/bIZB1hIJ4u8&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>03:33:22</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1632014934296-81e58a7cb8e06.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>60</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ #59 - Jeff Hawkins (Thousand Brains Theory)]]></title>
			<description><![CDATA[<p>Patreon: https://www.patreon.com/mlst</p>
<p>The ultimate goal of neuroscience is to learn how the human brain gives rise to human intelligence and what it means to be intelligent. Understanding how the brain works is considered one of humanity’s greatest challenges.&nbsp;</p>
<p>Jeff Hawkins thinks that the reality we perceive is a kind of simulation, a hallucination, a confabulation. He thinks that our brains are a model reality based on thousands of information streams originating from the sensors in our body. &nbsp;Critically - Hawkins doesn’t think there is just one model but rather; thousands.&nbsp;</p>
<p>Jeff has just released his new book, A thousand brains: a new theory of intelligence. It’s an inspiring and well-written book and I hope after watching this show; you will be inspired to read it too.&nbsp;</p>
<p>https://numenta.com/a-thousand-brains-by-jeff-hawkins/</p>
<p>https://numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/</p>
<p>Panel:</p>
<p>Dr. Keith Duggar https://twitter.com/DoctorDuggar</p>
<p>Connor Leahy https://twitter.com/npcollapse</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/59---Jeff-Hawkins-Thousand-Brains-Theory-e16sb64</link>
			<guid isPermaLink="false">5604cbf9-a551-406e-b89f-f4a06e16a8ab</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 03 Sep 2021 18:09:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/39774852/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-8-3%2F4d48d916-4cd7-7398-69f6-83aded175656.mp3" length="223232085" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Patreon: https://www.patreon.com/mlst&lt;/p&gt;
&lt;p&gt;The ultimate goal of neuroscience is to learn how the human brain gives rise to human intelligence and what it means to be intelligent. Understanding how the brain works is considered one of humanity’s greatest challenges.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Jeff Hawkins thinks that the reality we perceive is a kind of simulation, a hallucination, a confabulation. He thinks that our brains are a model reality based on thousands of information streams originating from the sensors in our body. &amp;nbsp;Critically - Hawkins doesn’t think there is just one model but rather; thousands.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Jeff has just released his new book, A thousand brains: a new theory of intelligence. It’s an inspiring and well-written book and I hope after watching this show; you will be inspired to read it too.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://numenta.com/a-thousand-brains-by-jeff-hawkins/&lt;/p&gt;
&lt;p&gt;https://numenta.com/blog/2019/01/16/the-thousand-brains-theory-of-intelligence/&lt;/p&gt;
&lt;p&gt;Panel:&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar https://twitter.com/DoctorDuggar&lt;/p&gt;
&lt;p&gt;Connor Leahy https://twitter.com/npcollapse&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:34:51</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode400/4981699/4981699-1630691001435-b6e7c95fb7b8c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>59</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#58 Dr. Ben Goertzel - Artificial General Intelligence]]></title>
			<description><![CDATA[<p>The field of Artificial Intelligence was founded in the mid 1950s with the aim of constructing “thinking machines” - that is to say, computer systems with human-like general intelligence. Think of humanoid robots that not only look but act and think with intelligence equal to and ultimately greater than that of human beings. But in the intervening years, the field has drifted far from its ambitious old-fashioned roots.</p>
<p>Dr. Ben Goertzel is an artificial intelligence researcher, CEO and founder of SingularityNET. A project combining artificial intelligence and blockchain to democratize access to artificial intelligence. Ben seeks to fulfil the original ambitions of the field. &nbsp;Ben graduated with a PhD in Mathematics from Temple University in 1990. Ben’s approach to AGI over many decades now has been inspired by many disciplines, but in particular from human cognitive psychology and computer science perspective. To date Ben’s work has been mostly theoretically-driven. Ben thinks that most of the deep learning approaches to AGI today try to model the brain. They may have a loose analogy to human neuroscience but they have not tried to derive the details of an AGI architecture from an overall conception of what a mind is. Ben thinks that what matters for creating human-level (or greater) intelligence is having the right information processing architecture, not the underlying mechanics via which the architecture is implemented.</p>
<p>Ben thinks that there is a certain set of key cognitive processes and interactions that AGI systems must implement explicitly such as; working and long-term memory, deliberative and reactive processing, perc biological systems tend to be messy, complex and integrative; searching for a single “algorithm of general intelligence” is an inappropriate attempt to project the aesthetics of physics or theoretical computer science into a qualitatively different domain.</p>
<p>TOC is on the YT show description https://www.youtube.com/watch?v=sw8IE3MX1SY</p>
<p>Panel: Dr. Tim Scarfe, Dr. Yannic Kilcher, Dr. Keith Duggar</p>
<p>Artificial General Intelligence: Concept, State of the Art, and Future Prospects</p>
<p>https://sciendo.com/abstract/journals...</p>
<p>The General Theory of General Intelligence: A Pragmatic Patternist Perspective</p>
<p>https://arxiv.org/abs/2103.15100</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/58-Dr--Ben-Goertzel---Artificial-General-Intelligence-e15p20i</link>
			<guid isPermaLink="false">0fcb7894-4556-410c-8903-47b5eefd090c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 11 Aug 2021 14:05:22 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/38618578/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-7-11%2F3eea790a-e615-bc66-a8d3-7faf1f0593bb.mp3" length="143222564" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;The field of Artificial Intelligence was founded in the mid 1950s with the aim of constructing “thinking machines” - that is to say, computer systems with human-like general intelligence. Think of humanoid robots that not only look but act and think with intelligence equal to and ultimately greater than that of human beings. But in the intervening years, the field has drifted far from its ambitious old-fashioned roots.&lt;/p&gt;
&lt;p&gt;Dr. Ben Goertzel is an artificial intelligence researcher, CEO and founder of SingularityNET. A project combining artificial intelligence and blockchain to democratize access to artificial intelligence. Ben seeks to fulfil the original ambitions of the field. &amp;nbsp;Ben graduated with a PhD in Mathematics from Temple University in 1990. Ben’s approach to AGI over many decades now has been inspired by many disciplines, but in particular from human cognitive psychology and computer science perspective. To date Ben’s work has been mostly theoretically-driven. Ben thinks that most of the deep learning approaches to AGI today try to model the brain. They may have a loose analogy to human neuroscience but they have not tried to derive the details of an AGI architecture from an overall conception of what a mind is. Ben thinks that what matters for creating human-level (or greater) intelligence is having the right information processing architecture, not the underlying mechanics via which the architecture is implemented.&lt;/p&gt;
&lt;p&gt;Ben thinks that there is a certain set of key cognitive processes and interactions that AGI systems must implement explicitly such as; working and long-term memory, deliberative and reactive processing, perc biological systems tend to be messy, complex and integrative; searching for a single “algorithm of general intelligence” is an inappropriate attempt to project the aesthetics of physics or theoretical computer science into a qualitatively different domain.&lt;/p&gt;
&lt;p&gt;TOC is on the YT show description https://www.youtube.com/watch?v=sw8IE3MX1SY&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe, Dr. Yannic Kilcher, Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Artificial General Intelligence: Concept, State of the Art, and Future Prospects&lt;/p&gt;
&lt;p&gt;https://sciendo.com/abstract/journals...&lt;/p&gt;
&lt;p&gt;The General Theory of General Intelligence: A Pragmatic Patternist Perspective&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2103.15100&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:28:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1628690729970-8a32d141e2b0c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>58</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#57 - Prof. Melanie Mitchell - Why AI is harder than we think]]></title>
			<description><![CDATA[<p>Since its beginning in the 1950s, the field of artificial intelligence has vacillated between periods of optimistic predictions and massive investment and periods of disappointment, loss of confidence, and reduced funding. Even with today’s seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. &nbsp;Professor Melanie Mitchell thinks one reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself.</p>
<p>YT vid- https://www.youtube.com/watch?v=A8m1Oqz2HKc</p>
<p>Main show kick off [<a href="https://www.youtube.com/watch?v=A8m1Oqz2HKc&amp;t=1611s">00:26:51</a>]</p>
<p><br></p>
<p>Panel: Dr. Tim Scarfe, Dr. Keith Duggar, Letitia Parcalabescu (https://www.youtube.com/c/AICoffeeBreak/)</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/57---Prof--Melanie-Mitchell---Why-AI-is-harder-than-we-think-e1502td</link>
			<guid isPermaLink="false">ce219ec4-4862-4f7f-a511-cf6a321eb785</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 25 Jul 2021 15:40:20 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/37800301/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-6-25%2F136f8d4a-706e-128d-80e2-10db09f28fc9.mp3" length="145305828" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Since its beginning in the 1950s, the field of artificial intelligence has vacillated between periods of optimistic predictions and massive investment and periods of disappointment, loss of confidence, and reduced funding. Even with today’s seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. &amp;nbsp;Professor Melanie Mitchell thinks one reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself.&lt;/p&gt;
&lt;p&gt;YT vid- https://www.youtube.com/watch?v=A8m1Oqz2HKc&lt;/p&gt;
&lt;p&gt;Main show kick off [&lt;a href=&quot;https://www.youtube.com/watch?v=A8m1Oqz2HKc&amp;amp;t=1611s&quot;&gt;00:26:51&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe, Dr. Keith Duggar, Letitia Parcalabescu (https://www.youtube.com/c/AICoffeeBreak/)&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:31:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1627227628969-01e932f02c13a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>57</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#56 - Dr. Walid Saba, Gadi Singer, Prof. J. Mark Bishop (Panel discussion)]]></title>
			<description><![CDATA[<p>It has been over three decades since the statistical revolution overtook AI by a storm and over two &nbsp;decades since deep learning (DL) helped usher the latest resurgence of artificial intelligence (AI). However, the disappointing progress in conversational agents, NLU, and self-driving cars, has made it clear that progress has not lived up to the promise of these empirical and data-driven methods. DARPA has suggested that it is time for a third wave in AI, one that would be characterized by hybrid models – models that combine knowledge-based approaches with data-driven machine learning techniques.&nbsp;</p>
<p>Joining us on this panel discussion is polymath and linguist Walid Saba - Co-founder ONTOLOGIK.AI, Gadi Singer - VP &amp; Director, Cognitive Computing Research, Intel Labs and J. Mark Bishop - Professor of Cognitive Computing (Emeritus), Goldsmiths, University of London and Scientific Adviser to FACT360.</p>
<p>Moderated by Dr. Keith Duggar and Dr. Tim Scarfe</p>
<p>https://www.linkedin.com/in/gadi-singer/</p>
<p>https://www.linkedin.com/in/walidsaba/</p>
<p>https://www.linkedin.com/in/profjmarkbishop/</p>
<p>#machinelearning #artificialintelligence</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/56---Dr--Walid-Saba--Gadi-Singer--Prof--J--Mark-Bishop-Panel-discussion-e145bt0</link>
			<guid isPermaLink="false">15a18fe5-7f45-44c6-8bfc-fd372510a470</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 08 Jul 2021 21:31:16 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/36924768/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-6-8%2F2fc7931c-492f-a5cb-1aaf-17cc4b9c7675.mp3" length="136904175" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;It has been over three decades since the statistical revolution overtook AI by a storm and over two &amp;nbsp;decades since deep learning (DL) helped usher the latest resurgence of artificial intelligence (AI). However, the disappointing progress in conversational agents, NLU, and self-driving cars, has made it clear that progress has not lived up to the promise of these empirical and data-driven methods. DARPA has suggested that it is time for a third wave in AI, one that would be characterized by hybrid models – models that combine knowledge-based approaches with data-driven machine learning techniques.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Joining us on this panel discussion is polymath and linguist Walid Saba - Co-founder ONTOLOGIK.AI, Gadi Singer - VP &amp;amp; Director, Cognitive Computing Research, Intel Labs and J. Mark Bishop - Professor of Cognitive Computing (Emeritus), Goldsmiths, University of London and Scientific Adviser to FACT360.&lt;/p&gt;
&lt;p&gt;Moderated by Dr. Keith Duggar and Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/gadi-singer/&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/walidsaba/&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/profjmarkbishop/&lt;/p&gt;
&lt;p&gt;#machinelearning #artificialintelligence&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:11:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1625779885441-fbd9e688e9427.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>56</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#55 Self-Supervised Vision Models (Dr. Ishan Misra - FAIR).]]></title>
			<description><![CDATA[<p>Dr. Ishan Misra is a Research Scientist at Facebook AI Research where he works on Computer Vision and Machine Learning. His main research interest is reducing the need for human supervision, and indeed, human knowledge in visual learning systems. He finished his PhD at the Robotics Institute at Carnegie Mellon. He has done stints at Microsoft Research, INRIA and Yale. His bachelors is in computer science where he achieved the highest GPA in his cohort.&nbsp;</p>
<p><br></p>
<p>Ishan is fast becoming a prolific scientist, already with more than 3000 citations under his belt and co-authoring with Yann LeCun; the godfather of deep learning. &nbsp;Today though we will be focusing an exciting cluster of recent papers around unsupervised representation learning for computer vision released from FAIR. These are; DINO: Emerging Properties in Self-Supervised Vision Transformers, BARLOW TWINS: Self-Supervised Learning via Redundancy Reduction and PAWS: Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with</p>
<p>Support Samples. All of these papers are hot off the press, just being officially released in the last month or so. Many of you will remember PIRL: Self-Supervised Learning of Pretext-Invariant Representations which Ishan was the primary author of in 2019.</p>
<p><br></p>
<p>References;</p>
<p><br></p>
<p>Shuffle and Learn - https://arxiv.org/abs/1603.08561</p>
<p>DepthContrast - https://arxiv.org/abs/2101.02691</p>
<p>DINO - https://arxiv.org/abs/2104.14294</p>
<p>Barlow Twins - https://arxiv.org/abs/2103.03230</p>
<p>SwAV - https://arxiv.org/abs/2006.09882</p>
<p>PIRL - https://arxiv.org/abs/1912.01991</p>
<p>AVID - https://arxiv.org/abs/2004.12943 (best paper candidate at CVPR'21 (just announced over the weekend) - http://cvpr2021.thecvf.com/node/290)</p>
<p>&nbsp;</p>
<p>Alexei (Alyosha) Efros</p>
<p>http://people.eecs.berkeley.edu/~efros/</p>
<p>http://www.cs.cmu.edu/~tmalisie/projects/nips09/</p>
<p>&nbsp;</p>
<p>Exemplar networks</p>
<p>https://arxiv.org/abs/1406.6909</p>
<p>&nbsp;</p>
<p>The bitter lesson - Rich Sutton</p>
<p>http://www.incompleteideas.net/IncIdeas/BitterLesson.html</p>
<p>&nbsp;</p>
<p>Machine Teaching: A New Paradigm for Building Machine Learning Systems</p>
<p>https://arxiv.org/abs/1707.06742</p>
<p>&nbsp;</p>
<p>POET</p>
<p>https://arxiv.org/pdf/1901.01753.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/55-Self-Supervised-Vision-Models-Dr--Ishan-Misra---FAIR-e1355js</link>
			<guid isPermaLink="false">4a6c5359-97c5-4269-b95f-420780b7747b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 21 Jun 2021 01:21:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/35869756/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-5-21%2F6f9782df-d4f0-9fa1-1fe8-b04a7444e7d1.mp3" length="92555457" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Ishan Misra is a Research Scientist at Facebook AI Research where he works on Computer Vision and Machine Learning. His main research interest is reducing the need for human supervision, and indeed, human knowledge in visual learning systems. He finished his PhD at the Robotics Institute at Carnegie Mellon. He has done stints at Microsoft Research, INRIA and Yale. His bachelors is in computer science where he achieved the highest GPA in his cohort.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ishan is fast becoming a prolific scientist, already with more than 3000 citations under his belt and co-authoring with Yann LeCun; the godfather of deep learning. &amp;nbsp;Today though we will be focusing an exciting cluster of recent papers around unsupervised representation learning for computer vision released from FAIR. These are; DINO: Emerging Properties in Self-Supervised Vision Transformers, BARLOW TWINS: Self-Supervised Learning via Redundancy Reduction and PAWS: Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with&lt;/p&gt;
&lt;p&gt;Support Samples. All of these papers are hot off the press, just being officially released in the last month or so. Many of you will remember PIRL: Self-Supervised Learning of Pretext-Invariant Representations which Ishan was the primary author of in 2019.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;References;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Shuffle and Learn - https://arxiv.org/abs/1603.08561&lt;/p&gt;
&lt;p&gt;DepthContrast - https://arxiv.org/abs/2101.02691&lt;/p&gt;
&lt;p&gt;DINO - https://arxiv.org/abs/2104.14294&lt;/p&gt;
&lt;p&gt;Barlow Twins - https://arxiv.org/abs/2103.03230&lt;/p&gt;
&lt;p&gt;SwAV - https://arxiv.org/abs/2006.09882&lt;/p&gt;
&lt;p&gt;PIRL - https://arxiv.org/abs/1912.01991&lt;/p&gt;
&lt;p&gt;AVID - https://arxiv.org/abs/2004.12943 (best paper candidate at CVPR&apos;21 (just announced over the weekend) - http://cvpr2021.thecvf.com/node/290)&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Alexei (Alyosha) Efros&lt;/p&gt;
&lt;p&gt;http://people.eecs.berkeley.edu/~efros/&lt;/p&gt;
&lt;p&gt;http://www.cs.cmu.edu/~tmalisie/projects/nips09/&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Exemplar networks&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1406.6909&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The bitter lesson - Rich Sutton&lt;/p&gt;
&lt;p&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Machine Teaching: A New Paradigm for Building Machine Learning Systems&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1707.06742&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;POET&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1901.01753.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:36:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>55</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#54 Gary Marcus and Luis Lamb - Neurosymbolic models]]></title>
			<description><![CDATA[<p>Professor Gary Marcus is a scientist, best-selling author, and entrepreneur. He is Founder and CEO of&nbsp;Robust.AI, and was Founder and CEO of Geometric Intelligence, a machine learning company acquired by Uber in 2016. Gary said in his recent next decade paper that — without us, or other creatures like us, the world would continue to exist, but it would not be described, distilled, or understood. &nbsp;Human lives are filled with abstraction and causal description. This is so powerful. Francois Chollet the other week said that intelligence is literally sensitivity to abstract analogies, and that is all there is to it. It's almost as if one of the most important features of intelligence is to be able to abstract knowledge, this drives the generalisation which will allow you to mine previous experience to make sense of many future novel situations. &nbsp;&nbsp;Also joining us today is Professor Luis Lamb — Secretary of Innovation for Science and Technology of the State of Rio Grande do Sul, Brazil. His Research Interests are Machine Learning and Reasoning, Neuro-Symbolic Computing, Logic in Computation and Artificial Intelligence, Cognitive and Neural Computation and also AI Ethics and Social Computing. Luis released his new paper Neurosymbolic AI: the third wave at the end of last year. It beautifully articulated the key ingredients needed in the next generation of AI systems, integrating type 1 and type 2 approaches to AI and it summarises all the of the achievements of the last 20 years of research. &nbsp;&nbsp;We cover a lot of ground in today's show. Explaining the limitations of deep learning, Rich Sutton's the bitter lesson and "reward is enough", and the semantic foundation which is required for us to build robust AI.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/54-Gary-Marcus-and-Luis-Lamb---Neurosymbolic-models-e125495</link>
			<guid isPermaLink="false">fd1736df-6b5a-4687-b36c-4f280b031620</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 04 Jun 2021 08:41:29 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/34819813/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-5-4%2F7e2e44b8-15e5-33eb-e154-02b5e1033bea.mp3" length="138538826" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Gary Marcus is a scientist, best-selling author, and entrepreneur. He is Founder and CEO of&amp;nbsp;Robust.AI, and was Founder and CEO of Geometric Intelligence, a machine learning company acquired by Uber in 2016. Gary said in his recent next decade paper that — without us, or other creatures like us, the world would continue to exist, but it would not be described, distilled, or understood. &amp;nbsp;Human lives are filled with abstraction and causal description. This is so powerful. Francois Chollet the other week said that intelligence is literally sensitivity to abstract analogies, and that is all there is to it. It&apos;s almost as if one of the most important features of intelligence is to be able to abstract knowledge, this drives the generalisation which will allow you to mine previous experience to make sense of many future novel situations. &amp;nbsp;&amp;nbsp;Also joining us today is Professor Luis Lamb — Secretary of Innovation for Science and Technology of the State of Rio Grande do Sul, Brazil. His Research Interests are Machine Learning and Reasoning, Neuro-Symbolic Computing, Logic in Computation and Artificial Intelligence, Cognitive and Neural Computation and also AI Ethics and Social Computing. Luis released his new paper Neurosymbolic AI: the third wave at the end of last year. It beautifully articulated the key ingredients needed in the next generation of AI systems, integrating type 1 and type 2 approaches to AI and it summarises all the of the achievements of the last 20 years of research. &amp;nbsp;&amp;nbsp;We cover a lot of ground in today&apos;s show. Explaining the limitations of deep learning, Rich Sutton&apos;s the bitter lesson and &quot;reward is enough&quot;, and the semantic foundation which is required for us to build robust AI.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:24:12</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>54</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#53 Quantum Natural Language Processing - Prof. Bob Coecke (Oxford)]]></title>
			<description><![CDATA[<p>Bob Coercke is a celebrated physicist, he's been a Physics and Quantum professor at Oxford University for the last 20 years. He is particularly interested in Structure which is to say, Logic, Order, and Category Theory. He is well known for work involving compositional distributional models of natural language meaning and he is also fascinated with understanding how our brains work. Bob was recently appointed as the Chief Scientist at Cambridge Quantum Computing.</p>
<p><br></p>
<p>Bob thinks that interactions between systems in Quantum Mechanics carries naturally over to how word meanings interact in natural language. Bob argues that this interaction embodies the phenomenon of quantum teleportation.</p>
<p><br></p>
<p>Bob invented ZX-calculus, a graphical calculus for revealing the compositional structure inside quantum circuits - to show entanglement states and protocols in a visually succinct but logically complete way. Von Neumann himself didn't even like his own original symbolic formalism of quantum theory, despite it being widely used!</p>
<p><br></p>
<p>We hope you enjoy this fascinating conversation which might give you a lot of insight into natural language processing.&nbsp;</p>
<p><br></p>
<p><br></p>
<p>Tim Intro [00:00:00]</p>
<p>The topological brain (Post-record button skit) [00:13:22]</p>
<p>Show kick off [00:19:31]</p>
<p>Bob introduction [00:22:37]</p>
<p>Changing culture in universities [00:24:51]</p>
<p>Machine Learning is like electricity [00:31:50]</p>
<p>NLP -- what is Bob's Quantum conception? [00:34:50]</p>
<p>The missing text problem [00:52:59]</p>
<p>Can statistical induction be trusted? [00:59:49]</p>
<p>On pragmatism and hybrid systems [01:04:42]</p>
<p>Parlour tricks, parsing and information flows [01:07:43]</p>
<p>How much human input is required with Bob's method? [01:11:29]</p>
<p>Reality, meaning, structure and language [01:14:42]</p>
<p>Replacing complexity with quantum entanglement, emergent complexity [01:17:45]</p>
<p>Loading quantum data requires machine learning [01:19:49]&nbsp;</p>
<p>QC is happy math coincidence for NLP [01:22:30]</p>
<p>The Theory of English (ToE) [01:28:23]&nbsp;</p>
<p>... or can we learn the ToE? [01:29:56]&nbsp;</p>
<p>How did diagrammatic quantum calculus come about? [01:31:04</p>
<p>The state of quantum computing today [01:37:49]&nbsp;</p>
<p>NLP on QC might be doable even in the NISQ era [01:40:48]&nbsp;</p>
<p>Hype and private investment are driving progress [01:48:34]&nbsp;</p>
<p>Crypto discussion (moved to post-show) [01:50:38]&nbsp;</p>
<p>Kilcher is in a startup (moved to post show) [01:53:40</p>
<p>Debrief [01:55:26]&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/53-Quantum-Natural-Language-Processing---Prof--Bob-Coecke-Oxford-e1164i5</link>
			<guid isPermaLink="false">335950a9-47a2-4292-93c6-1689541aefd0</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 19 May 2021 11:07:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/33804293/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-4-19%2F7ed9e6cc-97aa-9c19-9dfe-4b1a50547e08.mp3" length="198359565" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Bob Coercke is a celebrated physicist, he&apos;s been a Physics and Quantum professor at Oxford University for the last 20 years. He is particularly interested in Structure which is to say, Logic, Order, and Category Theory. He is well known for work involving compositional distributional models of natural language meaning and he is also fascinated with understanding how our brains work. Bob was recently appointed as the Chief Scientist at Cambridge Quantum Computing.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Bob thinks that interactions between systems in Quantum Mechanics carries naturally over to how word meanings interact in natural language. Bob argues that this interaction embodies the phenomenon of quantum teleportation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Bob invented ZX-calculus, a graphical calculus for revealing the compositional structure inside quantum circuits - to show entanglement states and protocols in a visually succinct but logically complete way. Von Neumann himself didn&apos;t even like his own original symbolic formalism of quantum theory, despite it being widely used!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We hope you enjoy this fascinating conversation which might give you a lot of insight into natural language processing.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Tim Intro [00:00:00]&lt;/p&gt;
&lt;p&gt;The topological brain (Post-record button skit) [00:13:22]&lt;/p&gt;
&lt;p&gt;Show kick off [00:19:31]&lt;/p&gt;
&lt;p&gt;Bob introduction [00:22:37]&lt;/p&gt;
&lt;p&gt;Changing culture in universities [00:24:51]&lt;/p&gt;
&lt;p&gt;Machine Learning is like electricity [00:31:50]&lt;/p&gt;
&lt;p&gt;NLP -- what is Bob&apos;s Quantum conception? [00:34:50]&lt;/p&gt;
&lt;p&gt;The missing text problem [00:52:59]&lt;/p&gt;
&lt;p&gt;Can statistical induction be trusted? [00:59:49]&lt;/p&gt;
&lt;p&gt;On pragmatism and hybrid systems [01:04:42]&lt;/p&gt;
&lt;p&gt;Parlour tricks, parsing and information flows [01:07:43]&lt;/p&gt;
&lt;p&gt;How much human input is required with Bob&apos;s method? [01:11:29]&lt;/p&gt;
&lt;p&gt;Reality, meaning, structure and language [01:14:42]&lt;/p&gt;
&lt;p&gt;Replacing complexity with quantum entanglement, emergent complexity [01:17:45]&lt;/p&gt;
&lt;p&gt;Loading quantum data requires machine learning [01:19:49]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;QC is happy math coincidence for NLP [01:22:30]&lt;/p&gt;
&lt;p&gt;The Theory of English (ToE) [01:28:23]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;... or can we learn the ToE? [01:29:56]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;How did diagrammatic quantum calculus come about? [01:31:04&lt;/p&gt;
&lt;p&gt;The state of quantum computing today [01:37:49]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;NLP on QC might be doable even in the NISQ era [01:40:48]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Hype and private investment are driving progress [01:48:34]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Crypto discussion (moved to post-show) [01:50:38]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Kilcher is in a startup (moved to post show) [01:53:40&lt;/p&gt;
&lt;p&gt;Debrief [01:55:26]&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:17:39</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>53</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#52 - Unadversarial Examples (Hadi Salman, MIT)]]></title>
			<description><![CDATA[<p>Performing reliably on unseen or shifting data distributions is a difficult challenge for modern vision systems, even slight corruptions or transformations of images are enough to slash the accuracy of state-of-the-art classifiers. When an adversary is allowed to modify an input image directly, models can be manipulated into predicting anything even when there is no perceptible change, this is known an adversarial example. The ideal definition of an adversarial example is when humans consistently say two pictures are the same but a machine disagrees. Hadi Salman, a Ph.D student at MIT (ex-Uber and Microsoft Research) started thinking about how adversarial robustness &nbsp;could be leveraged beyond security.</p>
<p><br></p>
<p>He realised that the phenomenon of adversarial examples could actually be turned upside down to lead to more robust models instead of breaking them. Hadi actually utilized the brittleness of neural networks to design unadversarial examples or robust objects which_ are objects designed specifically to be robustly recognized by neural networks.&nbsp;</p>
<p><br></p>
<p>Introduction [00:00:00]</p>
<p>DR KILCHER'S PHD HAT [00:11:18]</p>
<p>Main Introduction [00:11:38]</p>
<p>Hadi's Introduction [00:14:43]</p>
<p>More robust models == transfer better [00:46:41]</p>
<p>Features not bugs paper [00:49:13]</p>
<p>Manifolds [00:55:51]</p>
<p>Robustness and Transferability [00:58:00]</p>
<p>Do non-robust features generalize worse than robust? [00:59:52]</p>
<p>The unreasonable predicament of entangled features [01:01:57]</p>
<p>We can only find adversarial examples in the vicinity [01:09:30]</p>
<p>Certifiability of models for robustness [01:13:55]</p>
<p>Carlini is coming for you! And we are screwed [01:23:21]</p>
<p>Distribution shift and corruptions are a bigger problem than adversarial examples [01:25:34]</p>
<p>All roads lead to generalization [01:26:47]</p>
<p>Unadversarial examples [01:27:26]</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/52---Unadversarial-Examples-Hadi-Salman--MIT-e1015k2</link>
			<guid isPermaLink="false">66f08345-0c35-4133-b578-50877b5b9afb</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 01 May 2021 01:02:47 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/32592962/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-4-1%2F5f3cf278-f85b-807f-e28d-89e906266727.mp3" length="156124433" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Performing reliably on unseen or shifting data distributions is a difficult challenge for modern vision systems, even slight corruptions or transformations of images are enough to slash the accuracy of state-of-the-art classifiers. When an adversary is allowed to modify an input image directly, models can be manipulated into predicting anything even when there is no perceptible change, this is known an adversarial example. The ideal definition of an adversarial example is when humans consistently say two pictures are the same but a machine disagrees. Hadi Salman, a Ph.D student at MIT (ex-Uber and Microsoft Research) started thinking about how adversarial robustness &amp;nbsp;could be leveraged beyond security.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;He realised that the phenomenon of adversarial examples could actually be turned upside down to lead to more robust models instead of breaking them. Hadi actually utilized the brittleness of neural networks to design unadversarial examples or robust objects which_ are objects designed specifically to be robustly recognized by neural networks.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Introduction [00:00:00]&lt;/p&gt;
&lt;p&gt;DR KILCHER&apos;S PHD HAT [00:11:18]&lt;/p&gt;
&lt;p&gt;Main Introduction [00:11:38]&lt;/p&gt;
&lt;p&gt;Hadi&apos;s Introduction [00:14:43]&lt;/p&gt;
&lt;p&gt;More robust models == transfer better [00:46:41]&lt;/p&gt;
&lt;p&gt;Features not bugs paper [00:49:13]&lt;/p&gt;
&lt;p&gt;Manifolds [00:55:51]&lt;/p&gt;
&lt;p&gt;Robustness and Transferability [00:58:00]&lt;/p&gt;
&lt;p&gt;Do non-robust features generalize worse than robust? [00:59:52]&lt;/p&gt;
&lt;p&gt;The unreasonable predicament of entangled features [01:01:57]&lt;/p&gt;
&lt;p&gt;We can only find adversarial examples in the vicinity [01:09:30]&lt;/p&gt;
&lt;p&gt;Certifiability of models for robustness [01:13:55]&lt;/p&gt;
&lt;p&gt;Carlini is coming for you! And we are screwed [01:23:21]&lt;/p&gt;
&lt;p&gt;Distribution shift and corruptions are a bigger problem than adversarial examples [01:25:34]&lt;/p&gt;
&lt;p&gt;All roads lead to generalization [01:26:47]&lt;/p&gt;
&lt;p&gt;Unadversarial examples [01:27:26]&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:48:16</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>52</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#51 Francois Chollet - Intelligence and Generalisation]]></title>
			<description><![CDATA[<p>In today's show we are joined by Francois Chollet, I have been inspired by Francois ever since I read his Deep Learning with Python book and started using the Keras library which he invented many, many years ago. Francois has a clarity of thought that I've never seen in any other human being! He has extremely interesting views on intelligence as generalisation, abstraction and an information conversation ratio. He wrote on the measure of intelligence at the end of 2019 and it had a huge impact on my thinking. He thinks that NNs can only model continuous problems, which have a smooth learnable manifold and that many "type 2" problems which involve reasoning and/or planning are not suitable for NNs. He thinks that many problems have type 1 and type 2 enmeshed together. He thinks that the future of AI must include program synthesis to allow us to generalise broadly from a few examples, but the search could be guided by neural networks because the search space is interpolative to some extent.</p>
<p>https://youtu.be/J0p_thJJnoo</p>
<p>Tim's Whimsical notes; https://whimsical.com/chollet-show-QQ2atZUoRR3yFDsxKVzCbj</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/51-Francois-Chollet---Intelligence-and-Generalisation-ev1i79</link>
			<guid isPermaLink="false">abfbbc14-6472-49ae-8b25-301e068faa76</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 16 Apr 2021 13:11:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/31557289/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-3-16%2F61075915-627b-3084-1f75-f74708a6f28e.mp3" length="175463743" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In today&apos;s show we are joined by Francois Chollet, I have been inspired by Francois ever since I read his Deep Learning with Python book and started using the Keras library which he invented many, many years ago. Francois has a clarity of thought that I&apos;ve never seen in any other human being! He has extremely interesting views on intelligence as generalisation, abstraction and an information conversation ratio. He wrote on the measure of intelligence at the end of 2019 and it had a huge impact on my thinking. He thinks that NNs can only model continuous problems, which have a smooth learnable manifold and that many &quot;type 2&quot; problems which involve reasoning and/or planning are not suitable for NNs. He thinks that many problems have type 1 and type 2 enmeshed together. He thinks that the future of AI must include program synthesis to allow us to generalise broadly from a few examples, but the search could be guided by neural networks because the search space is interpolative to some extent.&lt;/p&gt;
&lt;p&gt;https://youtu.be/J0p_thJJnoo&lt;/p&gt;
&lt;p&gt;Tim&apos;s Whimsical notes; https://whimsical.com/chollet-show-QQ2atZUoRR3yFDsxKVzCbj&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:01:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>51</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#50 Christian Szegedy - Formal Reasoning, Program Synthesis]]></title>
			<description><![CDATA[<p>Dr. Christian Szegedy from Google Research is a deep learning heavyweight. He invented adversarial examples, one of the first object detection algorithms, the inceptionnet architecture, and co-invented batchnorm. He thinks that if you bet on computers and software in 1990 you would have been as right as if you bet on AI now. But he thinks that we have been programming computers the same way since the 1950s and there has been a huge stagnation ever since. Mathematics is the process of taking a fuzzy thought and formalising it. But could we automate that? Could we create a system which will act like a super human mathematician but you can talk to it in natural language? This is what Christian calls autoformalisation. Christian thinks that automating many of the things we do in mathematics is the first step towards software synthesis and building human-level AGI. Mathematics ability is the litmus test for general reasoning ability. Christian has a fascinating take on transformers too.&nbsp;</p>
<p>With Yannic Lightspeed Kilcher and Dr. Mathew Salvaris</p>
<p>Whimsical Canvas with Tim's Notes:</p>
<p>https://whimsical.com/mar-26th-christian-szegedy-CpgGhnEYDBrDMFoATU6XYC</p>
<p>YouTube version (with detailed table of contents) https://youtu.be/ehNGGYFO6ms</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/50-Christian-Szegedy---Formal-Reasoning--Program-Synthesis-eu643d</link>
			<guid isPermaLink="false">24216e78-b376-402a-95e0-9fafe914a2c9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 04 Apr 2021 01:12:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/30658093/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-3-4%2Ff5f8420a-442b-ef4b-140e-b8cea06a1bbe.mp3" length="179776392" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Christian Szegedy from Google Research is a deep learning heavyweight. He invented adversarial examples, one of the first object detection algorithms, the inceptionnet architecture, and co-invented batchnorm. He thinks that if you bet on computers and software in 1990 you would have been as right as if you bet on AI now. But he thinks that we have been programming computers the same way since the 1950s and there has been a huge stagnation ever since. Mathematics is the process of taking a fuzzy thought and formalising it. But could we automate that? Could we create a system which will act like a super human mathematician but you can talk to it in natural language? This is what Christian calls autoformalisation. Christian thinks that automating many of the things we do in mathematics is the first step towards software synthesis and building human-level AGI. Mathematics ability is the litmus test for general reasoning ability. Christian has a fascinating take on transformers too.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;With Yannic Lightspeed Kilcher and Dr. Mathew Salvaris&lt;/p&gt;
&lt;p&gt;Whimsical Canvas with Tim&apos;s Notes:&lt;/p&gt;
&lt;p&gt;https://whimsical.com/mar-26th-christian-szegedy-CpgGhnEYDBrDMFoATU6XYC&lt;/p&gt;
&lt;p&gt;YouTube version (with detailed table of contents) https://youtu.be/ehNGGYFO6ms&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:33:22</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>50</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#49 - Meta-Gradients in RL - Dr. Tom Zahavy (DeepMind) ]]></title>
			<description><![CDATA[<p>The race is on, we are on a collective mission to understand and create artificial general intelligence. Dr. Tom Zahavy, a Research Scientist at DeepMind thinks that reinforcement learning is the most general learning framework that we have today, and in his opinion it could lead to artificial general intelligence. He thinks there are no tasks which could not be solved by simply maximising a reward.&nbsp;</p>
<p><br></p>
<p>Back in 2012 when Tom was an undergraduate, before the deep learning revolution he attended an online lecture on how CNNs automatically discover representations. This was an epiphany for Tom. He decided in that very moment that he was going to become an ML researcher. &nbsp;Tom's view is that the ability to recognise patterns and discover structure is the most important aspect of intelligence. This has been his quest ever since. He is particularly focused on using diversity preservation and metagradients to discover this structure.&nbsp;</p>
<p><br></p>
<p>In this discussion we dive deep into meta gradients in reinforcement learning.&nbsp;</p>
<p><br></p>
<p>Video version and TOC @ &nbsp;https://www.youtube.com/watch?v=hfaZwgk_iS0</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/49---Meta-Gradients-in-RL---Dr--Tom-Zahavy-DeepMind-etbcr9</link>
			<guid isPermaLink="false">6cd9ceab-7103-4b10-8410-8e6a84be564e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 23 Mar 2021 21:36:02 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/29782313/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-2-23%2Fbb101b9c-5c88-aab8-9727-6c8d2f54d580.mp3" length="164608217" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;The race is on, we are on a collective mission to understand and create artificial general intelligence. Dr. Tom Zahavy, a Research Scientist at DeepMind thinks that reinforcement learning is the most general learning framework that we have today, and in his opinion it could lead to artificial general intelligence. He thinks there are no tasks which could not be solved by simply maximising a reward.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Back in 2012 when Tom was an undergraduate, before the deep learning revolution he attended an online lecture on how CNNs automatically discover representations. This was an epiphany for Tom. He decided in that very moment that he was going to become an ML researcher. &amp;nbsp;Tom&apos;s view is that the ability to recognise patterns and discover structure is the most important aspect of intelligence. This has been his quest ever since. He is particularly focused on using diversity preservation and metagradients to discover this structure.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In this discussion we dive deep into meta gradients in reinforcement learning.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Video version and TOC @ &amp;nbsp;https://www.youtube.com/watch?v=hfaZwgk_iS0&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:25:13</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo400/4981699/4981699-1615918015913-f78f18744aff1.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>49</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#48 Machine Learning Security - Andy Smith]]></title>
			<description><![CDATA[<p>First episode in a series we are doing on ML DevOps. Starting with the thing which nobody seems to be talking about enough, security! We chat with cyber security expert Andy Smith about threat modelling and trust boundaries for an ML DevOps system.&nbsp;</p>
<p>Intro [00:00:00]</p>
<p>ML DevOps - a security perspective [00:00:50]</p>
<p>Threat Modelling [00:03:03]</p>
<p>Adversarial examples? [00:11:27]</p>
<p>Nobody understands the whole stack [00:13:53]</p>
<p>On the size of the state space, the element of unpredictability [00:18:32]</p>
<p>Threat modelling in more detail [00:21:17]</p>
<p>Trust boundaries for an ML DevOps system [00:25:45]</p>
<p><br></p>
<p>Andy has a YouTube channel on cyber security! Check it out @&nbsp;</p>
<p>https://www.youtube.com/channel/UCywP24ly6h6NTusX88TQKTQ</p>
<p>https://www.linkedin.com/in/andysmith-uk/</p>
<p><br></p>
<p>Video version:</p>
<p>https://youtu.be/7Tz-3S4lypI</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/48-Machine-Learning-Security---Andy-Smith-esp0jf</link>
			<guid isPermaLink="false">9592439a-b16e-4afa-8289-f757e5863f10</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 16 Mar 2021 22:35:09 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/29179951/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-2-16%2Fd84b8eff-cf37-dbe5-270b-07b5d1966bad.mp3" length="53975241" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;First episode in a series we are doing on ML DevOps. Starting with the thing which nobody seems to be talking about enough, security! We chat with cyber security expert Andy Smith about threat modelling and trust boundaries for an ML DevOps system.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Intro [00:00:00]&lt;/p&gt;
&lt;p&gt;ML DevOps - a security perspective [00:00:50]&lt;/p&gt;
&lt;p&gt;Threat Modelling [00:03:03]&lt;/p&gt;
&lt;p&gt;Adversarial examples? [00:11:27]&lt;/p&gt;
&lt;p&gt;Nobody understands the whole stack [00:13:53]&lt;/p&gt;
&lt;p&gt;On the size of the state space, the element of unpredictability [00:18:32]&lt;/p&gt;
&lt;p&gt;Threat modelling in more detail [00:21:17]&lt;/p&gt;
&lt;p&gt;Trust boundaries for an ML DevOps system [00:25:45]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Andy has a YouTube channel on cyber security! Check it out @&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/channel/UCywP24ly6h6NTusX88TQKTQ&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/andysmith-uk/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Video version:&lt;/p&gt;
&lt;p&gt;https://youtu.be/7Tz-3S4lypI&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:37:27</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>48</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[047 Interpretable Machine Learning - Christoph Molnar]]></title>
			<description><![CDATA[<p>Christoph Molnar is one of the main people to know in the space of interpretable ML. In 2018 he released the first version of his incredible online book, interpretable machine learning. Interpretability is often a deciding factor when a machine learning (ML) model is used in a product, a decision process, or in research. Interpretability methods can be used to discover knowledge, to debug or justify the model and its predictions, and to control and improve the model, reason about potential bias in models as well as increase the social acceptance of models. But Interpretability methods can also be quite esoteric, add an additional layer of complexity and potential pitfalls and requires expert knowledge to understand. Is it even possible to understand complex models or even humans for that matter in any &nbsp;meaningful way?&nbsp;</p>
<p><br></p>
<p>Introduction to IML [00:00:00]</p>
<p>Show Kickoff [00:13:28]</p>
<p>What makes a good explanation? [00:15:51]</p>
<p>Quantification of how good an explanation is [00:19:59]</p>
<p>Knowledge of the pitfalls of IML [00:22:14]</p>
<p>Are linear models even interpretable? [00:24:26]</p>
<p>Complex Math models to explain Complex Math models? [00:27:04]</p>
<p>Saliency maps are glorified edge detectors [00:28:35]</p>
<p>Challenge on IML -- feature dependence [00:36:46]</p>
<p>Don't leap to using a complex model! Surrogate models can be too dumb [00:40:52]</p>
<p>On airplane pilots. Seeking to understand vs testing [00:44:09]</p>
<p>IML Could help us make better models or lead a better life [00:51:53]</p>
<p>Lack of statistical rigor and quantification of uncertainty [00:55:35]</p>
<p>On Causality [01:01:09]</p>
<p>Broadening out the discussion to the process or institutional level [01:08:53]</p>
<p>No focus on fairness / ethics? [01:11:44]</p>
<p>Is it possible to condition ML model training on IML metrics ? [01:15:27]</p>
<p>Where is IML going? Some of the esoterica of the IML methods [01:18:35]</p>
<p>You can't compress information without common knowledge, the latter becomes the bottleneck [01:23:25]</p>
<p>IML methods used non-interactively? Making IML an engineering discipline [01:31:10]</p>
<p>Tim Postscript -- on the lack of effective corporate operating models for IML, security, engineering and ethics [01:36:34]</p>
<p><br></p>
<p>Explanation in Artificial Intelligence: Insights from the Social Sciences (Tim Miller 2018)</p>
<p>https://arxiv.org/pdf/1706.07269.pdf</p>
<p><br></p>
<p>Seven Myths in Machine Learning Research (Chang 19)&nbsp;</p>
<p>Myth 7: Saliency maps are robust ways to interpret neural networks</p>
<p>https://arxiv.org/pdf/1902.06789.pdf</p>
<p><br></p>
<p>Sanity Checks for Saliency Maps (Adebayo 2020)</p>
<p>https://arxiv.org/pdf/1810.03292.pdf</p>
<p><br></p>
<p>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.</p>
<p>https://christophm.github.io/interpretable-ml-book/</p>
<p><br></p>
<p>Christoph Molnar:</p>
<p>https://www.linkedin.com/in/christoph-molnar-63777189/</p>
<p>https://machine-master.blogspot.com/</p>
<p>https://twitter.com/ChristophMolnar</p>
<p><br></p>
<p>Please show your appreciation and buy Christoph's book here;</p>
<p>https://www.lulu.com/shop/christoph-molnar/interpretable-machine-learning/paperback/product-24449081.html?page=1&amp;pageSize=4</p>
<p><br></p>
<p>Panel:&nbsp;</p>
<p>Connor Tann https://www.linkedin.com/in/connor-tann-a92906a1/</p>
<p>Dr. Tim Scarfe&nbsp;</p>
<p>Dr. Keith Duggar</p>
<p><br></p>
<p>Video version:</p>
<p>https://youtu.be/0LIACHcxpHU</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/047-Interpretable-Machine-Learning---Christoph-Molnar-eshafn</link>
			<guid isPermaLink="false">6ae4269b-6a5a-4151-b8aa-a54095c735d7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 14 Mar 2021 12:34:18 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/28927927/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-2-14%2Fa356cd09-f89f-7d67-4ac3-b0b0d8cd9a4f.mp3" length="148855327" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Christoph Molnar is one of the main people to know in the space of interpretable ML. In 2018 he released the first version of his incredible online book, interpretable machine learning. Interpretability is often a deciding factor when a machine learning (ML) model is used in a product, a decision process, or in research. Interpretability methods can be used to discover knowledge, to debug or justify the model and its predictions, and to control and improve the model, reason about potential bias in models as well as increase the social acceptance of models. But Interpretability methods can also be quite esoteric, add an additional layer of complexity and potential pitfalls and requires expert knowledge to understand. Is it even possible to understand complex models or even humans for that matter in any &amp;nbsp;meaningful way?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Introduction to IML [00:00:00]&lt;/p&gt;
&lt;p&gt;Show Kickoff [00:13:28]&lt;/p&gt;
&lt;p&gt;What makes a good explanation? [00:15:51]&lt;/p&gt;
&lt;p&gt;Quantification of how good an explanation is [00:19:59]&lt;/p&gt;
&lt;p&gt;Knowledge of the pitfalls of IML [00:22:14]&lt;/p&gt;
&lt;p&gt;Are linear models even interpretable? [00:24:26]&lt;/p&gt;
&lt;p&gt;Complex Math models to explain Complex Math models? [00:27:04]&lt;/p&gt;
&lt;p&gt;Saliency maps are glorified edge detectors [00:28:35]&lt;/p&gt;
&lt;p&gt;Challenge on IML -- feature dependence [00:36:46]&lt;/p&gt;
&lt;p&gt;Don&apos;t leap to using a complex model! Surrogate models can be too dumb [00:40:52]&lt;/p&gt;
&lt;p&gt;On airplane pilots. Seeking to understand vs testing [00:44:09]&lt;/p&gt;
&lt;p&gt;IML Could help us make better models or lead a better life [00:51:53]&lt;/p&gt;
&lt;p&gt;Lack of statistical rigor and quantification of uncertainty [00:55:35]&lt;/p&gt;
&lt;p&gt;On Causality [01:01:09]&lt;/p&gt;
&lt;p&gt;Broadening out the discussion to the process or institutional level [01:08:53]&lt;/p&gt;
&lt;p&gt;No focus on fairness / ethics? [01:11:44]&lt;/p&gt;
&lt;p&gt;Is it possible to condition ML model training on IML metrics ? [01:15:27]&lt;/p&gt;
&lt;p&gt;Where is IML going? Some of the esoterica of the IML methods [01:18:35]&lt;/p&gt;
&lt;p&gt;You can&apos;t compress information without common knowledge, the latter becomes the bottleneck [01:23:25]&lt;/p&gt;
&lt;p&gt;IML methods used non-interactively? Making IML an engineering discipline [01:31:10]&lt;/p&gt;
&lt;p&gt;Tim Postscript -- on the lack of effective corporate operating models for IML, security, engineering and ethics [01:36:34]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Explanation in Artificial Intelligence: Insights from the Social Sciences (Tim Miller 2018)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1706.07269.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Seven Myths in Machine Learning Research (Chang 19)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Myth 7: Saliency maps are robust ways to interpret neural networks&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1902.06789.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sanity Checks for Saliency Maps (Adebayo 2020)&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/1810.03292.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.&lt;/p&gt;
&lt;p&gt;https://christophm.github.io/interpretable-ml-book/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Christoph Molnar:&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/christoph-molnar-63777189/&lt;/p&gt;
&lt;p&gt;https://machine-master.blogspot.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/ChristophMolnar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Please show your appreciation and buy Christoph&apos;s book here;&lt;/p&gt;
&lt;p&gt;https://www.lulu.com/shop/christoph-molnar/interpretable-machine-learning/paperback/product-24449081.html?page=1&amp;amp;pageSize=4&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Connor Tann https://www.linkedin.com/in/connor-tann-a92906a1/&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Video version:&lt;/p&gt;
&lt;p&gt;https://youtu.be/0LIACHcxpHU&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:40:12</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>47</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#046 The Great ML Stagnation (Mark Saroufim and Dr. Mathew Salvaris)]]></title>
			<description><![CDATA[<p>Academics think of themselves as trailblazers, explorers — seekers of the truth.</p>
<p>Any fundamental discovery involves a significant degree of risk. If an idea is guaranteed to work then it moves from the realm of research to engineering. Unfortunately, this also means that most research careers will invariably be failures at least if failures are measured via “objective” metrics like citations. Today we discuss the recent article from Mark Saroufim called Machine Learning: the great stagnation. We discuss the rise of gentleman scientists, fake rigor, incentives in ML, SOTA-chasing, "graduate student descent", distribution of talent in ML and how to learn effectively. &nbsp;</p>
<p><br></p>
<p>With special guest interviewer Mat Salvaris.&nbsp;</p>
<p><br></p>
<p>Machine learning: the great stagnation [00:00:00]</p>
<p>Main show kick off [00:16:30]</p>
<p>Great stagnation article / Bad incentive systems in academia [00:18:24]</p>
<p>OpenAI is a media business [00:19:48]</p>
<p>Incentive structures in academia [00:22:13]</p>
<p>SOTA chasing [00:24:47]</p>
<p>F You Money [00:28:53]</p>
<p>Research grants and gentlemen scientists [00:29:13]</p>
<p>Following your own gradient of interest and making a contribution [00:33:27]</p>
<p>Marketing yourself to be successful [00:37:07]</p>
<p>Tech companies create the bad incentives [00:42:20]</p>
<p>GPT3 was sota chasing but it seemed really... "good"? Scaling laws? [00:51:09]</p>
<p>Dota / game AI [00:58:39]</p>
<p>Hard to go it alone? [01:02:08]</p>
<p>Reaching out to people [01:09:21]</p>
<p>Willingness to be wrong [01:13:14]</p>
<p>Distribution of talent / tech interviews [01:18:30]</p>
<p>What should you read online and how to learn? Sharing your stuff online and finding your niece [01:25:52]</p>
<p><br></p>
<p>Mark Saroufim:</p>
<p><br></p>
<p>https://marksaroufim.substack.com/</p>
<p>http://robotoverlordmanual.com/</p>
<p>https://twitter.com/marksaroufim</p>
<p>https://www.youtube.com/marksaroufim</p>
<p><br></p>
<p>Dr. Mathew Salvaris:</p>
<p><br></p>
<p>https://www.linkedin.com/in/drmathewsalvaris/</p>
<p>https://twitter.com/MSalvaris</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/046-The-Great-ML-Stagnation-Mark-Saroufim-and-Dr--Mathew-Salvaris-erqdv2</link>
			<guid isPermaLink="false">5c8cf397-751f-4481-b0d3-693f83d1385c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 06 Mar 2021 19:47:15 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/28177826/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-2-6%2Fd8589288-c049-87fc-20b6-5558fdda3d2e.mp3" length="144259141" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Academics think of themselves as trailblazers, explorers — seekers of the truth.&lt;/p&gt;
&lt;p&gt;Any fundamental discovery involves a significant degree of risk. If an idea is guaranteed to work then it moves from the realm of research to engineering. Unfortunately, this also means that most research careers will invariably be failures at least if failures are measured via “objective” metrics like citations. Today we discuss the recent article from Mark Saroufim called Machine Learning: the great stagnation. We discuss the rise of gentleman scientists, fake rigor, incentives in ML, SOTA-chasing, &quot;graduate student descent&quot;, distribution of talent in ML and how to learn effectively. &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;With special guest interviewer Mat Salvaris.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Machine learning: the great stagnation [00:00:00]&lt;/p&gt;
&lt;p&gt;Main show kick off [00:16:30]&lt;/p&gt;
&lt;p&gt;Great stagnation article / Bad incentive systems in academia [00:18:24]&lt;/p&gt;
&lt;p&gt;OpenAI is a media business [00:19:48]&lt;/p&gt;
&lt;p&gt;Incentive structures in academia [00:22:13]&lt;/p&gt;
&lt;p&gt;SOTA chasing [00:24:47]&lt;/p&gt;
&lt;p&gt;F You Money [00:28:53]&lt;/p&gt;
&lt;p&gt;Research grants and gentlemen scientists [00:29:13]&lt;/p&gt;
&lt;p&gt;Following your own gradient of interest and making a contribution [00:33:27]&lt;/p&gt;
&lt;p&gt;Marketing yourself to be successful [00:37:07]&lt;/p&gt;
&lt;p&gt;Tech companies create the bad incentives [00:42:20]&lt;/p&gt;
&lt;p&gt;GPT3 was sota chasing but it seemed really... &quot;good&quot;? Scaling laws? [00:51:09]&lt;/p&gt;
&lt;p&gt;Dota / game AI [00:58:39]&lt;/p&gt;
&lt;p&gt;Hard to go it alone? [01:02:08]&lt;/p&gt;
&lt;p&gt;Reaching out to people [01:09:21]&lt;/p&gt;
&lt;p&gt;Willingness to be wrong [01:13:14]&lt;/p&gt;
&lt;p&gt;Distribution of talent / tech interviews [01:18:30]&lt;/p&gt;
&lt;p&gt;What should you read online and how to learn? Sharing your stuff online and finding your niece [01:25:52]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Mark Saroufim:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://marksaroufim.substack.com/&lt;/p&gt;
&lt;p&gt;http://robotoverlordmanual.com/&lt;/p&gt;
&lt;p&gt;https://twitter.com/marksaroufim&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/marksaroufim&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Mathew Salvaris:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/drmathewsalvaris/&lt;/p&gt;
&lt;p&gt;https://twitter.com/MSalvaris&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:39:57</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>46</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#045 Microsoft's Platform for Reinforcement Learning (Bonsai)]]></title>
			<description><![CDATA[<p>Microsoft has an interesting strategy with their new “autonomous systems” technology also known as Project Bonsai. They want to create an interface to abstract away the complexity and esoterica of deep reinforcement learning. They want to fuse together expert knowledge and artificial intelligence all on one platform, so that complex problems can be decomposed into simpler ones. They want to take machine learning Ph.Ds out of the equation and make autonomous systems engineering look more like a traditional software engineering process. It is an ambitious undertaking, but interesting. Reinforcement learning is extremely difficult (as I cover in the video), and if you don’t have a team of RL Ph.Ds with tech industry experience, you shouldn’t even consider doing it yourself. This is our take on it!</p>
<p><br></p>
<p>There are 3 chapters in this video;</p>
<p><br></p>
<p>Chapter 1: Tim's intro and take on RL being hard, intro to Bonsai and machine teaching&nbsp;</p>
<p>Chapter 2: Interview with Scott Stanfield [recorded Jan 2020] 00:56:41	</p>
<p>Chapter 3: Traditional street talk episode [recorded Dec 2020] 01:38:13</p>
<p><br></p>
<p>This is *not* an official communication from Microsoft, all personal opinions. There is no MS-confidential information in this video.&nbsp;</p>
<p><br></p>
<p>With:</p>
<p>Scott Stanfield</p>
<p>https://twitter.com/seesharp</p>
<p><br></p>
<p>Megan Bloemsma</p>
<p>https://twitter.com/BloemsmaMegan</p>
<p><br></p>
<p>Gurdeep Pall (he has not validated anything we have said in this video or been involved in the creation of it)</p>
<p>https://www.linkedin.com/in/gurdeep-pall-0aa639bb/</p>
<p><br></p>
<p>Panel:&nbsp;</p>
<p>Dr. Keith Duggar</p>
<p>Dr. Tim Scarfe</p>
<p>Yannic Kilcher</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/045-Microsofts-Platform-for-Reinforcement-Learning-Bonsai-er84to</link>
			<guid isPermaLink="false">12b79026-d63b-48ef-9dd8-82e96d1ea3ef</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 28 Feb 2021 22:59:48 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/27578744/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-1-28%2Fea35702e-96d9-d007-0be5-37a16985a893.mp3" length="216588314" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Microsoft has an interesting strategy with their new “autonomous systems” technology also known as Project Bonsai. They want to create an interface to abstract away the complexity and esoterica of deep reinforcement learning. They want to fuse together expert knowledge and artificial intelligence all on one platform, so that complex problems can be decomposed into simpler ones. They want to take machine learning Ph.Ds out of the equation and make autonomous systems engineering look more like a traditional software engineering process. It is an ambitious undertaking, but interesting. Reinforcement learning is extremely difficult (as I cover in the video), and if you don’t have a team of RL Ph.Ds with tech industry experience, you shouldn’t even consider doing it yourself. This is our take on it!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;There are 3 chapters in this video;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Chapter 1: Tim&apos;s intro and take on RL being hard, intro to Bonsai and machine teaching&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Chapter 2: Interview with Scott Stanfield [recorded Jan 2020] 00:56:41	&lt;/p&gt;
&lt;p&gt;Chapter 3: Traditional street talk episode [recorded Dec 2020] 01:38:13&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This is *not* an official communication from Microsoft, all personal opinions. There is no MS-confidential information in this video.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;With:&lt;/p&gt;
&lt;p&gt;Scott Stanfield&lt;/p&gt;
&lt;p&gt;https://twitter.com/seesharp&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Megan Bloemsma&lt;/p&gt;
&lt;p&gt;https://twitter.com/BloemsmaMegan&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Gurdeep Pall (he has not validated anything we have said in this video or been involved in the creation of it)&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/gurdeep-pall-0aa639bb/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;Yannic Kilcher&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:30:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>45</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#044 - Data-efficient Image Transformers (Hugo Touvron)]]></title>
			<description><![CDATA[<p>Today we are going to talk about the *Data-efficient image Transformers paper or (DeiT) which Hugo is the primary author of. One of the recipes of success for vision models since the DL revolution began has been the availability of large training sets. CNNs have been optimized for almost a decade now, including through extensive architecture search which is prone to overfitting. Motivated by the success of transformers-based models</p>
<p>in Natural Language Processing there has been increasing attention in applying these approaches to vision models. Hugo and his collaborators used a different training strategy and a new distillation token to get a massive increase in sample efficiency with image transformers.&nbsp;</p>
<p><br></p>
<p>00:00:00 Introduction</p>
<p>00:06:33 Data augmentation is all you need</p>
<p>00:09:53 Now the image patches are the convolutions though?</p>
<p>00:12:16 Where are those inductive biases hiding?</p>
<p>00:15:46 Distillation token</p>
<p>00:21:01 Why different resolutions on training</p>
<p>00:24:14 How data efficient can we get?</p>
<p>00:26:47 Out of domain generalisation</p>
<p>00:28:22 Why are transformers data efficient at all? Learning invariances</p>
<p>00:32:04 Is data augmentation cheating?</p>
<p>00:33:25 Distillation strategies - matching the intermediatae teacher representation as well as output</p>
<p>00:35:49 Do ML models learn the same thing for a problem?</p>
<p>00:39:01 How is it like at Facebook AI?</p>
<p>00:41:17 How long is the PhD programme?</p>
<p>00:42:03 Other interests outside of transformers?</p>
<p>00:43:18 Transformers for Vision and Language</p>
<p>00:47:40 Could we improve transformers models? (Hybrid models)</p>
<p>00:49:03 Biggest challenges in AI?</p>
<p>00:50:52 How far can we go with data driven approach?</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/044---Data-efficient-Image-Transformers-Hugo-Touvron-er35ak</link>
			<guid isPermaLink="false">bab65645-93ef-4444-bf08-11dba466af08</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 25 Feb 2021 22:39:23 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/27415316/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-1-25%2F39508938-c3cf-0d25-39b3-e34a56bb29b5.mp3" length="100561049" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today we are going to talk about the *Data-efficient image Transformers paper or (DeiT) which Hugo is the primary author of. One of the recipes of success for vision models since the DL revolution began has been the availability of large training sets. CNNs have been optimized for almost a decade now, including through extensive architecture search which is prone to overfitting. Motivated by the success of transformers-based models&lt;/p&gt;
&lt;p&gt;in Natural Language Processing there has been increasing attention in applying these approaches to vision models. Hugo and his collaborators used a different training strategy and a new distillation token to get a massive increase in sample efficiency with image transformers.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&lt;/p&gt;
&lt;p&gt;00:06:33 Data augmentation is all you need&lt;/p&gt;
&lt;p&gt;00:09:53 Now the image patches are the convolutions though?&lt;/p&gt;
&lt;p&gt;00:12:16 Where are those inductive biases hiding?&lt;/p&gt;
&lt;p&gt;00:15:46 Distillation token&lt;/p&gt;
&lt;p&gt;00:21:01 Why different resolutions on training&lt;/p&gt;
&lt;p&gt;00:24:14 How data efficient can we get?&lt;/p&gt;
&lt;p&gt;00:26:47 Out of domain generalisation&lt;/p&gt;
&lt;p&gt;00:28:22 Why are transformers data efficient at all? Learning invariances&lt;/p&gt;
&lt;p&gt;00:32:04 Is data augmentation cheating?&lt;/p&gt;
&lt;p&gt;00:33:25 Distillation strategies - matching the intermediatae teacher representation as well as output&lt;/p&gt;
&lt;p&gt;00:35:49 Do ML models learn the same thing for a problem?&lt;/p&gt;
&lt;p&gt;00:39:01 How is it like at Facebook AI?&lt;/p&gt;
&lt;p&gt;00:41:17 How long is the PhD programme?&lt;/p&gt;
&lt;p&gt;00:42:03 Other interests outside of transformers?&lt;/p&gt;
&lt;p&gt;00:43:18 Transformers for Vision and Language&lt;/p&gt;
&lt;p&gt;00:47:40 Could we improve transformers models? (Hybrid models)&lt;/p&gt;
&lt;p&gt;00:49:03 Biggest challenges in AI?&lt;/p&gt;
&lt;p&gt;00:50:52 How far can we go with data driven approach?&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>00:52:22</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>44</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#043 Prof J. Mark Bishop - Artificial Intelligence Is Stupid and Causal Reasoning won't fix it.]]></title>
			<description><![CDATA[<p>Professor Mark Bishop does not think that computers can be conscious or have phenomenological states of consciousness unless we are willing to accept panpsychism which is idea that mentality is fundamental and ubiquitous in the natural world, or put simply, that your goldfish and everything else for that matter has a mind. Panpsychism postulates that distinctions between intelligences are largely arbitrary.</p>
<p>Mark’s work in the ‘philosophy of AI’ led to an influential critique of computational approaches to Artificial Intelligence through a thorough examination of John Searle's 'Chinese Room Argument'</p>
<p>Mark just published a paper called artificial intelligence is stupid and causal reasoning wont fix it. He makes it clear in this paper that in his opinion computers will never be able to compute everything, understand anything, or feel anything.&nbsp;</p>
<p><br></p>
<p>00:00:00​ Tim Intro</p>
<p>00:15:04​ Intro&nbsp;</p>
<p>00:18:49​ Introduction to Marks ideas&nbsp;</p>
<p>00:25:49​ Some problems are not computable&nbsp;</p>
<p>00:29:57​ the dancing was Pixies fallacy&nbsp;</p>
<p>00:32:36​ The observer relative problem, and its all in the mapping&nbsp;</p>
<p>00:43:03​ Conscious Experience&nbsp;</p>
<p>00:53:30​ Intelligence without representation, consciousness is something that we do&nbsp;</p>
<p>01:02:36​ Consciousness helps us to act autonomously&nbsp;</p>
<p>01:05:13​ The Chinese room argument&nbsp;</p>
<p>01:14:58​ Simulation argument and computation doesn't have phenomenal consciousness&nbsp;</p>
<p>01:17:44​ Language informs our colour perception&nbsp;</p>
<p>01:23:11​ We have our own distinct ontologies&nbsp;</p>
<p>01:27:12​ Kurt Gödel, Turing and Penrose and the implications of their work&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/043-Prof-J--Mark-Bishop---Artificial-Intelligence-Is-Stupid-and-Causal-Reasoning-wont-fix-it-eqkkrd</link>
			<guid isPermaLink="false">d95d40c2-773c-4473-809a-d1eed711ee27</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 19 Feb 2021 11:04:08 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/26939693/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-1-19%2F4c6a08f3-efc3-d628-8f70-ec9a9f6eaa03.mp3" length="137449185" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Mark Bishop does not think that computers can be conscious or have phenomenological states of consciousness unless we are willing to accept panpsychism which is idea that mentality is fundamental and ubiquitous in the natural world, or put simply, that your goldfish and everything else for that matter has a mind. Panpsychism postulates that distinctions between intelligences are largely arbitrary.&lt;/p&gt;
&lt;p&gt;Mark’s work in the ‘philosophy of AI’ led to an influential critique of computational approaches to Artificial Intelligence through a thorough examination of John Searle&apos;s &apos;Chinese Room Argument&apos;&lt;/p&gt;
&lt;p&gt;Mark just published a paper called artificial intelligence is stupid and causal reasoning wont fix it. He makes it clear in this paper that in his opinion computers will never be able to compute everything, understand anything, or feel anything.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00​ Tim Intro&lt;/p&gt;
&lt;p&gt;00:15:04​ Intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:18:49​ Introduction to Marks ideas&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:25:49​ Some problems are not computable&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:29:57​ the dancing was Pixies fallacy&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:32:36​ The observer relative problem, and its all in the mapping&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:43:03​ Conscious Experience&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:53:30​ Intelligence without representation, consciousness is something that we do&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:02:36​ Consciousness helps us to act autonomously&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:05:13​ The Chinese room argument&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:14:58​ Simulation argument and computation doesn&apos;t have phenomenal consciousness&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:17:44​ Language informs our colour perception&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:23:11​ We have our own distinct ontologies&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:27:12​ Kurt Gödel, Turing and Penrose and the implications of their work&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:35:14</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>43</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#042 - Pedro Domingos - Ethics and Cancel Culture]]></title>
			<description><![CDATA[<p>Today we have professor Pedro Domingos and we are going to talk about activism in machine learning, cancel culture, AI ethics and kernels. In Pedro's book the master algorithm, he segmented the AI community into 5 distinct tribes with 5 unique identities (and before you ask, no the irony of an anti-identitarian doing do was not lost on us!). Pedro recently published an article in Quillette called Beating Back Cancel Culture: A Case Study from the Field of Artificial Intelligence. Domingos has railed against political activism in the machine learning community and cancel culture. Recently Pedro was involved in a controversy where he asserted the NeurIPS broader impact statements are an ideological filter mechanism.</p>
<p><br></p>
<p>Important Disclaimer: All views expressed are personal opinions.</p>
<p><br></p>
<p>00:00:00 Caveating</p>
<p>00:04:08 Main intro</p>
<p>00:07:44 Cancelling culture is a culture and intellectual weakness&nbsp;</p>
<p>00:12:26 Is cancel culture a post-modern religion?&nbsp;</p>
<p>00:24:46 Should we have gateways and gatekeepers?&nbsp;</p>
<p>00:29:30 Does everything require broader impact statements?&nbsp;</p>
<p>00:33:55 We are stifling diversity (of thought) not promoting it.&nbsp;</p>
<p>00:39:09 What is fair and how to do fair?</p>
<p>00:45:11 Models can introduce biases by compressing away minority data&nbsp;</p>
<p>00:48:36 Accurate but unequal soap dispensers&nbsp;</p>
<p>00:53:55 Agendas are not even self-consistent&nbsp;</p>
<p>00:56:42 Is vs Ought: all variables should be used for Is&nbsp;</p>
<p>01:00:38 Fighting back cancellation with cancellation?</p>
<p>01:10:01 Intent and degree matter in right vs wrong.&nbsp;</p>
<p>01:11:08 Limiting principles matter&nbsp;</p>
<p>01:15:10 Gradient descent and kernels&nbsp;</p>
<p>01:20:16 Training Journey matter more than Destination&nbsp;</p>
<p>01:24:36 Can training paths teach us about symmetry?</p>
<p>01:28:37 What is the most promising path to AGI?&nbsp;</p>
<p>01:31:29 Intelligence will lose its mystery</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/042---Pedro-Domingos---Ethics-and-Cancel-Culture-eq864v</link>
			<guid isPermaLink="false">0b98de97-d8bb-4e7f-8314-ead34f6e71f9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Thu, 11 Feb 2021 01:08:33 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/26531423/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-1-11%2Fb69088f0-80c9-c51b-d0cb-3b8408e9ec94.mp3" length="135337318" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today we have professor Pedro Domingos and we are going to talk about activism in machine learning, cancel culture, AI ethics and kernels. In Pedro&apos;s book the master algorithm, he segmented the AI community into 5 distinct tribes with 5 unique identities (and before you ask, no the irony of an anti-identitarian doing do was not lost on us!). Pedro recently published an article in Quillette called Beating Back Cancel Culture: A Case Study from the Field of Artificial Intelligence. Domingos has railed against political activism in the machine learning community and cancel culture. Recently Pedro was involved in a controversy where he asserted the NeurIPS broader impact statements are an ideological filter mechanism.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Important Disclaimer: All views expressed are personal opinions.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Caveating&lt;/p&gt;
&lt;p&gt;00:04:08 Main intro&lt;/p&gt;
&lt;p&gt;00:07:44 Cancelling culture is a culture and intellectual weakness&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:12:26 Is cancel culture a post-modern religion?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:24:46 Should we have gateways and gatekeepers?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:29:30 Does everything require broader impact statements?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:33:55 We are stifling diversity (of thought) not promoting it.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:39:09 What is fair and how to do fair?&lt;/p&gt;
&lt;p&gt;00:45:11 Models can introduce biases by compressing away minority data&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:48:36 Accurate but unequal soap dispensers&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:53:55 Agendas are not even self-consistent&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:56:42 Is vs Ought: all variables should be used for Is&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:00:38 Fighting back cancellation with cancellation?&lt;/p&gt;
&lt;p&gt;01:10:01 Intent and degree matter in right vs wrong.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:11:08 Limiting principles matter&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:15:10 Gradient descent and kernels&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:20:16 Training Journey matter more than Destination&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:24:36 Can training paths teach us about symmetry?&lt;/p&gt;
&lt;p&gt;01:28:37 What is the most promising path to AGI?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:31:29 Intelligence will lose its mystery&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:33:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>42</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#041 - Biologically Plausible Neural Networks - Dr. Simon Stringer]]></title>
			<description><![CDATA[<p>Dr. Simon Stringer. Obtained his Ph.D in mathematical state space control theory and has been a Senior Research Fellow at Oxford University for over 27 years. Simon is the director of the the Oxford Centre for Theoretical Neuroscience and Artificial Intelligence, which is based within the Oxford University Department of Experimental Psychology. His department covers vision, spatial processing, motor function, language and consciousness -- in particular -- how the primate visual system learns to make sense of complex natural scenes. Dr. Stringers laboratory houses a team of theoreticians, who are developing computer models of a range of different aspects of brain function. Simon's lab is investigating the neural and synaptic dynamics that underpin brain function. An important matter here is the The feature-binding problem which concerns how the visual system represents the hierarchical relationships between features. the visual system must represent hierarchical binding relations across the entire visual field at every spatial scale and level in the hierarchy of visual primitives.</p>
<p><br></p>
<p>We discuss the emergence of self-organised behaviour, complex information processing, invariant sensory representations and hierarchical feature binding which emerges when you build biologically plausible neural networks with temporal spiking dynamics.&nbsp;</p>
<p><br></p>
<p><br></p>
<p>00:00:09 Tim Intro&nbsp;</p>
<p>00:09:31 Show kickoff&nbsp;</p>
<p>00:14:37 Hierarchical Feature binding and timing of action potentials&nbsp;</p>
<p>00:30:16 Hebb to Spike-timing-dependent plasticity (STDP)&nbsp;</p>
<p>00:35:27 Encoding of shape primitives&nbsp;</p>
<p>00:38:50 Is imagination working in the same place in the brain&nbsp;</p>
<p>00:41:12 Compare to supervised CNNs&nbsp;</p>
<p>00:45:59 Speech recognition, motor system, learning mazes&nbsp;</p>
<p>00:49:28 How practical are these spiking NNs&nbsp;</p>
<p>00:50:19 Why simulate the human brain&nbsp;</p>
<p>00:52:46 How much computational power do you gain from differential timings&nbsp;</p>
<p>00:55:08 Adversarial inputs&nbsp;</p>
<p>00:59:41 Generative / causal component needed?&nbsp;</p>
<p>01:01:46 Modalities of processing i.e. language&nbsp;</p>
<p>01:03:42 Understanding&nbsp;</p>
<p>01:04:37 Human hardware&nbsp;</p>
<p>01:06:19 Roadmap of NNs?&nbsp;</p>
<p>01:10:36 Intepretability methods for these new models&nbsp;</p>
<p>01:13:03 Won't GPT just scale and do this anyway?&nbsp;</p>
<p>01:15:51 What about trace learning and transformation learning&nbsp;</p>
<p>01:18:50 Categories of invariance&nbsp;</p>
<p>01:19:47 Biological plausibility&nbsp;</p>
<p><br></p>
<p>https://www.youtube.com/watch?v=aisgNLypUKs</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/041---Biologically-Plausible-Neural-Networks---Dr--Simon-Stringer-ept4db</link>
			<guid isPermaLink="false">fcca829f-a6e2-4eeb-9ae4-d35536be7777</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 03 Feb 2021 20:39:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/26169195/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-1-3%2Fd9a6bd7b-ccd6-0588-e57a-340dcc59fd46.mp3" length="125190941" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Simon Stringer. Obtained his Ph.D in mathematical state space control theory and has been a Senior Research Fellow at Oxford University for over 27 years. Simon is the director of the the Oxford Centre for Theoretical Neuroscience and Artificial Intelligence, which is based within the Oxford University Department of Experimental Psychology. His department covers vision, spatial processing, motor function, language and consciousness -- in particular -- how the primate visual system learns to make sense of complex natural scenes. Dr. Stringers laboratory houses a team of theoreticians, who are developing computer models of a range of different aspects of brain function. Simon&apos;s lab is investigating the neural and synaptic dynamics that underpin brain function. An important matter here is the The feature-binding problem which concerns how the visual system represents the hierarchical relationships between features. the visual system must represent hierarchical binding relations across the entire visual field at every spatial scale and level in the hierarchy of visual primitives.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We discuss the emergence of self-organised behaviour, complex information processing, invariant sensory representations and hierarchical feature binding which emerges when you build biologically plausible neural networks with temporal spiking dynamics.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:09 Tim Intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:09:31 Show kickoff&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:14:37 Hierarchical Feature binding and timing of action potentials&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:30:16 Hebb to Spike-timing-dependent plasticity (STDP)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:35:27 Encoding of shape primitives&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:38:50 Is imagination working in the same place in the brain&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:41:12 Compare to supervised CNNs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:45:59 Speech recognition, motor system, learning mazes&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:49:28 How practical are these spiking NNs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:50:19 Why simulate the human brain&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:52:46 How much computational power do you gain from differential timings&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:55:08 Adversarial inputs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:59:41 Generative / causal component needed?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:01:46 Modalities of processing i.e. language&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:03:42 Understanding&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:04:37 Human hardware&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:06:19 Roadmap of NNs?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:10:36 Intepretability methods for these new models&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:13:03 Won&apos;t GPT just scale and do this anyway?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:15:51 What about trace learning and transformation learning&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:18:50 Categories of invariance&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:19:47 Biological plausibility&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=aisgNLypUKs&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:26:56</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>41</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#040 - Adversarial Examples (Dr. Nicholas Carlini, Dr. Wieland Brendel, Florian Tramèr)]]></title>
			<description><![CDATA[<p>Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. there's good reason to believe neural networks look at very different features than we would have expected. &nbsp;As articulated in the 2019 "features not bugs" paper Adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans.&nbsp;</p>
<p><br></p>
<p>Adversarial examples don't just affect deep learning models. A cottage industry has sprung up around Threat Modeling in AI and ML Systems and their dependencies. Joining us this evening are some of currently leading researchers in adversarial examples;</p>
<p><br></p>
<p>Florian Tramèr - A fifth year PhD student in Computer Science at Stanford University</p>
<p>https://floriantramer.com/​</p>
<p>https://twitter.com/florian_tramer​</p>
<p><br></p>
<p>Dr. Wieland Brendel - Machine Learning Researcher at the University of Tübingen &amp; Co-Founder of layer7.ai</p>
<p>https://medium.com/@wielandbr​</p>
<p>https://twitter.com/wielandbr​</p>
<p><br></p>
<p><br></p>
<p>Dr. Nicholas Carlini - Research scientist at Google Brain working in that exciting space between machine learning and computer security.&nbsp;</p>
<p>https://nicholas.carlini.com/​</p>
<p><br></p>
<p>We really hope you enjoy the conversation, remember to subscribe!&nbsp;</p>
<p><br></p>
<p>Yannic Intro [00:00:00​]</p>
<p>Tim Intro [00:04:07​]</p>
<p>Threat Taxonomy [00:09:00​]&nbsp;</p>
<p>Main show intro [00:11:30​]</p>
<p>Whats wrong with Neural Networks? [00:14:52​]</p>
<p>The role of memorization [00:19:51​]</p>
<p>Anthropomorphization of models [00:22:42​]</p>
<p>Whats the harm really though / focusing on actual ML security risks [00:27:03​]</p>
<p>Shortcut learning / OOD generalization [00:36:18​]</p>
<p>Human generalization [00:40:11​]</p>
<p>An existential problem in DL getting the models to learn what we want? [00:41:39​]</p>
<p>Defenses to adversarial examples [00:47:15​]</p>
<p>What if we had all the data and the labels? Still problems? [00:54:28​]</p>
<p>Defenses are easily broken [01:00:24​]</p>
<p>Self deception in academia [01:06:46​]</p>
<p>ML Security [01:28:15​]</p>
<p><br></p>
<p>https://www.youtube.com/watch?v=2PenK06tvE4</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/040---Adversarial-Examples-Dr--Nicholas-Carlini--Dr--Wieland-Brendel--Florian-Tramr-epo5qr</link>
			<guid isPermaLink="false">be58067a-292d-4cf5-821c-cce661322bf2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 31 Jan 2021 19:46:10 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/26006811/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-31%2Fd8ba3656-aa4d-3289-ee77-578db5c4f8b5.mp3" length="138601161" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. there&apos;s good reason to believe neural networks look at very different features than we would have expected. &amp;nbsp;As articulated in the 2019 &quot;features not bugs&quot; paper Adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Adversarial examples don&apos;t just affect deep learning models. A cottage industry has sprung up around Threat Modeling in AI and ML Systems and their dependencies. Joining us this evening are some of currently leading researchers in adversarial examples;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Florian Tramèr - A fifth year PhD student in Computer Science at Stanford University&lt;/p&gt;
&lt;p&gt;https://floriantramer.com/​&lt;/p&gt;
&lt;p&gt;https://twitter.com/florian_tramer​&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Wieland Brendel - Machine Learning Researcher at the University of Tübingen &amp;amp; Co-Founder of layer7.ai&lt;/p&gt;
&lt;p&gt;https://medium.com/@wielandbr​&lt;/p&gt;
&lt;p&gt;https://twitter.com/wielandbr​&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Dr. Nicholas Carlini - Research scientist at Google Brain working in that exciting space between machine learning and computer security.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://nicholas.carlini.com/​&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We really hope you enjoy the conversation, remember to subscribe!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Yannic Intro [00:00:00​]&lt;/p&gt;
&lt;p&gt;Tim Intro [00:04:07​]&lt;/p&gt;
&lt;p&gt;Threat Taxonomy [00:09:00​]&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Main show intro [00:11:30​]&lt;/p&gt;
&lt;p&gt;Whats wrong with Neural Networks? [00:14:52​]&lt;/p&gt;
&lt;p&gt;The role of memorization [00:19:51​]&lt;/p&gt;
&lt;p&gt;Anthropomorphization of models [00:22:42​]&lt;/p&gt;
&lt;p&gt;Whats the harm really though / focusing on actual ML security risks [00:27:03​]&lt;/p&gt;
&lt;p&gt;Shortcut learning / OOD generalization [00:36:18​]&lt;/p&gt;
&lt;p&gt;Human generalization [00:40:11​]&lt;/p&gt;
&lt;p&gt;An existential problem in DL getting the models to learn what we want? [00:41:39​]&lt;/p&gt;
&lt;p&gt;Defenses to adversarial examples [00:47:15​]&lt;/p&gt;
&lt;p&gt;What if we had all the data and the labels? Still problems? [00:54:28​]&lt;/p&gt;
&lt;p&gt;Defenses are easily broken [01:00:24​]&lt;/p&gt;
&lt;p&gt;Self deception in academia [01:06:46​]&lt;/p&gt;
&lt;p&gt;ML Security [01:28:15​]&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=2PenK06tvE4&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:36:15</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>40</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#039 - Lena Voita - NLP]]></title>
			<description><![CDATA[<p>ena Voita is a Ph.D. student at the University of Edinburgh and University of Amsterdam. Previously, She was a research scientist at Yandex Research and worked closely with the Yandex Translate team. She still teaches NLP at the Yandex School of Data Analysis. She has created an exciting new NLP course on her website lena-voita.github.io which you folks need to check out! She has one of the most well presented blogs we have ever seen, where she discusses her research in an easily digestable manner. Lena has been investigating many fascinating topics in machine learning and NLP. Today we are going to talk about three of her papers and corresponding blog articles;</p>
<p><br></p>
<p>Source and Target Contributions to NMT Predictions -- Where she talks about the influential dichotomy between the source and the prefix of neural translation models.</p>
<p>https://arxiv.org/pdf/2010.10907.pdf</p>
<p>https://lena-voita.github.io/posts/source_target_contributions_to_nmt.html</p>
<p><br></p>
<p>Information-Theoretic Probing with MDL -- Where Lena proposes a technique of evaluating a model using the minimum description length or Kolmogorov complexity of labels given representations rather than something basic like accuracy</p>
<p>https://arxiv.org/pdf/2003.12298.pdf</p>
<p>https://lena-voita.github.io/posts/mdl_probes.html</p>
<p><br></p>
<p>Evolution of Representations in the Transformer - Lena investigates the evolution of representations of individual tokens in Transformers -- trained with different training objectives (MT, LM, MLM)&nbsp;</p>
<p>https://arxiv.org/abs/1909.01380</p>
<p>https://lena-voita.github.io/posts/emnlp19_evolution.html</p>
<p><br></p>
<p>Panel Dr. Tim Scarfe, Yannic Kilcher, Sayak Paul</p>
<p><br></p>
<p>00:00:00 Kenneth Stanley / Greatness can not be planned house keeping</p>
<p>00:21:09 Kilcher intro</p>
<p>00:28:54 Hello Lena</p>
<p>00:29:21 Tim - Lenas NMT paper</p>
<p>00:35:26 Tim - Minimum Description Length / Probe paper</p>
<p>00:40:12 Tim - Evolution of representations</p>
<p>00:46:40 Lenas NLP course</p>
<p>00:49:18 The peppermint tea situation&nbsp;</p>
<p>00:49:28 Main Show Kick Off&nbsp;</p>
<p>00:50:22 Hallucination vs exposure bias&nbsp;</p>
<p>00:53:04 Lenas focus on explaining the models not SOTA chasing</p>
<p>00:56:34 Probes paper and NLP intepretability</p>
<p>01:02:18 Why standard probing doesnt work</p>
<p>01:12:12 Evolutions of representations paper</p>
<p>01:23:53 BERTScore &nbsp;and BERT Rediscovers the Classical NLP Pipeline paper</p>
<p>01:25:10 Is the shifting encoding context because of BERT bidirectionality</p>
<p>01:26:43 Objective defines which information we lose on input</p>
<p>01:27:59 How influential is the dataset?</p>
<p>01:29:42 Where is the community going wrong?</p>
<p>01:31:55 Thoughts on GOFAI/Understanding in NLP?</p>
<p>01:36:38 Lena's NLP course&nbsp;</p>
<p>01:47:40 How to foster better learning / understanding</p>
<p>01:52:17 Lena's toolset and languages</p>
<p>01:54:12 Mathematics is all you need</p>
<p>01:56:03 Programming languages</p>
<p><br></p>
<p>https://lena-voita.github.io/</p>
<p>https://www.linkedin.com/in/elena-voita/</p>
<p>https://scholar.google.com/citations?user=EcN9o7kAAAAJ&amp;hl=ja</p>
<p>https://twitter.com/lena_voita</p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/039---Lena-Voita---NLP-epcj2q</link>
			<guid isPermaLink="false">e3ba72e9-cf58-4e4e-abdd-35d0463eb31e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 23 Jan 2021 23:36:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/25627162/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-23%2Fd0a06188-ce34-8b77-79bf-d0721a276fc6.mp3" length="113761345" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;ena Voita is a Ph.D. student at the University of Edinburgh and University of Amsterdam. Previously, She was a research scientist at Yandex Research and worked closely with the Yandex Translate team. She still teaches NLP at the Yandex School of Data Analysis. She has created an exciting new NLP course on her website lena-voita.github.io which you folks need to check out! She has one of the most well presented blogs we have ever seen, where she discusses her research in an easily digestable manner. Lena has been investigating many fascinating topics in machine learning and NLP. Today we are going to talk about three of her papers and corresponding blog articles;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Source and Target Contributions to NMT Predictions -- Where she talks about the influential dichotomy between the source and the prefix of neural translation models.&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2010.10907.pdf&lt;/p&gt;
&lt;p&gt;https://lena-voita.github.io/posts/source_target_contributions_to_nmt.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Information-Theoretic Probing with MDL -- Where Lena proposes a technique of evaluating a model using the minimum description length or Kolmogorov complexity of labels given representations rather than something basic like accuracy&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2003.12298.pdf&lt;/p&gt;
&lt;p&gt;https://lena-voita.github.io/posts/mdl_probes.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Evolution of Representations in the Transformer - Lena investigates the evolution of representations of individual tokens in Transformers -- trained with different training objectives (MT, LM, MLM)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1909.01380&lt;/p&gt;
&lt;p&gt;https://lena-voita.github.io/posts/emnlp19_evolution.html&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel Dr. Tim Scarfe, Yannic Kilcher, Sayak Paul&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Kenneth Stanley / Greatness can not be planned house keeping&lt;/p&gt;
&lt;p&gt;00:21:09 Kilcher intro&lt;/p&gt;
&lt;p&gt;00:28:54 Hello Lena&lt;/p&gt;
&lt;p&gt;00:29:21 Tim - Lenas NMT paper&lt;/p&gt;
&lt;p&gt;00:35:26 Tim - Minimum Description Length / Probe paper&lt;/p&gt;
&lt;p&gt;00:40:12 Tim - Evolution of representations&lt;/p&gt;
&lt;p&gt;00:46:40 Lenas NLP course&lt;/p&gt;
&lt;p&gt;00:49:18 The peppermint tea situation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:49:28 Main Show Kick Off&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:50:22 Hallucination vs exposure bias&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:53:04 Lenas focus on explaining the models not SOTA chasing&lt;/p&gt;
&lt;p&gt;00:56:34 Probes paper and NLP intepretability&lt;/p&gt;
&lt;p&gt;01:02:18 Why standard probing doesnt work&lt;/p&gt;
&lt;p&gt;01:12:12 Evolutions of representations paper&lt;/p&gt;
&lt;p&gt;01:23:53 BERTScore &amp;nbsp;and BERT Rediscovers the Classical NLP Pipeline paper&lt;/p&gt;
&lt;p&gt;01:25:10 Is the shifting encoding context because of BERT bidirectionality&lt;/p&gt;
&lt;p&gt;01:26:43 Objective defines which information we lose on input&lt;/p&gt;
&lt;p&gt;01:27:59 How influential is the dataset?&lt;/p&gt;
&lt;p&gt;01:29:42 Where is the community going wrong?&lt;/p&gt;
&lt;p&gt;01:31:55 Thoughts on GOFAI/Understanding in NLP?&lt;/p&gt;
&lt;p&gt;01:36:38 Lena&apos;s NLP course&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:47:40 How to foster better learning / understanding&lt;/p&gt;
&lt;p&gt;01:52:17 Lena&apos;s toolset and languages&lt;/p&gt;
&lt;p&gt;01:54:12 Mathematics is all you need&lt;/p&gt;
&lt;p&gt;01:56:03 Programming languages&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://lena-voita.github.io/&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/elena-voita/&lt;/p&gt;
&lt;p&gt;https://scholar.google.com/citations?user=EcN9o7kAAAAJ&amp;amp;hl=ja&lt;/p&gt;
&lt;p&gt;https://twitter.com/lena_voita&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:58:21</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>39</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#038 - Professor Kenneth Stanley - Why Greatness Cannot Be Planned]]></title>
			<description><![CDATA[<p>Professor Kenneth Stanley is currently a research science manager at OpenAI in San Fransisco. We've Been dreaming about getting Kenneth on the show since the very begininning of Machine Learning Street Talk. Some of you might recall that our first ever show was on the enhanced POET paper, of course Kenneth had his hands all over it. He's been cited over 16000 times, his most popular paper with over 3K citations was the NEAT algorithm. His interests are neuroevolution, open-endedness, NNs, artificial life, and AI. He invented the concept of novelty search with no clearly defined objective. His key idea is that there is a tyranny of objectives prevailing in every aspect of our lives, society and indeed our algorithms. Crucially, these objectives produce convergent behaviour and thinking and distract us from discovering stepping stones which will lead to greatness. He thinks that this monotonic objective obsession, this idea that we need to continue to improve benchmarks every year is dangerous. He wrote about this in detail in his recent book "greatness can not be planned" which will be the main topic of discussion in the show. We also cover his ideas on open endedness in machine learning.&nbsp;</p>
<p><br></p>
<p><br></p>
<p>00:00:00 Intro to Kenneth&nbsp;</p>
<p>00:01:16 Show structure disclaimer&nbsp;</p>
<p>00:04:16 Passionate discussion&nbsp;</p>
<p>00:06:26 WHy greatness cant be planned and the tyranny of objectives&nbsp;</p>
<p>00:14:40 Chinese Finger Trap &nbsp;</p>
<p>00:16:28 Perverse Incentives and feedback loops&nbsp;</p>
<p>00:18:17 Deception&nbsp;</p>
<p>00:23:29 Maze example&nbsp;</p>
<p>00:24:44 How can we define curiosity or interestingness&nbsp;</p>
<p>00:26:59 Open endedness&nbsp;</p>
<p>00:33:01 ICML 2019 and Yannic, POET, first MSLST&nbsp;</p>
<p>00:36:17 evolutionary algorithms++&nbsp;</p>
<p>00:43:18 POET, the first MLST &nbsp;</p>
<p>00:45:39 A lesson to GOFAI people&nbsp;</p>
<p>00:48:46 Machine Learning -- the great stagnation&nbsp;</p>
<p>00:54:34 Actual scientific successes are usually luck, and against the odds -- Biontech&nbsp;</p>
<p>00:56:21 Picbreeder and NEAT&nbsp;</p>
<p>01:10:47 How Tim applies these ideas to his life and why he runs MLST&nbsp;</p>
<p>01:14:58 Keith Skit about UCF&nbsp;</p>
<p>01:15:13 Main show kick off&nbsp;</p>
<p>01:18:02 Why does Kenneth value serindipitous exploration so much&nbsp;</p>
<p>01:24:10 Scientific support for Keneths ideas in normal life&nbsp;</p>
<p>01:27:12 We should drop objectives to achieve them. An oxymoron?&nbsp;</p>
<p>01:33:13 Isnt this just resource allocation between exploration and exploitation?&nbsp;</p>
<p>01:39:06 Are objectives merely a matter of degree?&nbsp;</p>
<p>01:42:38 How do we allocate funds for treasure hunting in society&nbsp;</p>
<p>01:47:34 A keen nose for what is interesting, and voting can be dangerous&nbsp;</p>
<p>01:53:00 Committees are the antithesis of innovation&nbsp;</p>
<p>01:56:21 Does Kenneth apply these ideas to his real life?&nbsp;</p>
<p>01:59:48 Divergence vs interestingness vs novelty vs complexity&nbsp;</p>
<p>02:08:13 Picbreeder&nbsp;</p>
<p>02:12:39 Isnt everything novel in some sense?&nbsp;</p>
<p>02:16:35 Imagine if there was no selection pressure?&nbsp;</p>
<p>02:18:31 Is innovation == environment exploitation?&nbsp;</p>
<p>02:20:37 Is it possible to take shortcuts if you already knew what the innovations were?&nbsp;</p>
<p>02:21:11 Go Explore -- does the algorithm encode the stepping stones?&nbsp;</p>
<p>02:24:41 What does it mean for things to be interestingly different?&nbsp;</p>
<p>02:26:11 behavioral characterization / diversity measure to your broad interests&nbsp;</p>
<p>02:30:54 Shaping objectives&nbsp;</p>
<p>02:32:49 Why do all ambitious objectives have deception? Picbreeder analogy&nbsp;</p>
<p>02:35:59 Exploration vs Exploitation, Science vs Engineering&nbsp;</p>
<p>02:43:18 Schools of thought in ML and could search lead to AGI&nbsp;</p>
<p>02:45:49 Official ending&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/038---Professor-Kenneth-Stanley---Why-Greatness-Cannot-Be-Planned-ep7116</link>
			<guid isPermaLink="false">0bb76105-c75f-48ff-9440-cc66b3b5a3a4</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 20 Jan 2021 01:36:13 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/25444838/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-20%2F1abe9ab2-019c-9218-1aa8-273a23b01082.mp3" length="159840716" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Professor Kenneth Stanley is currently a research science manager at OpenAI in San Fransisco. We&apos;ve Been dreaming about getting Kenneth on the show since the very begininning of Machine Learning Street Talk. Some of you might recall that our first ever show was on the enhanced POET paper, of course Kenneth had his hands all over it. He&apos;s been cited over 16000 times, his most popular paper with over 3K citations was the NEAT algorithm. His interests are neuroevolution, open-endedness, NNs, artificial life, and AI. He invented the concept of novelty search with no clearly defined objective. His key idea is that there is a tyranny of objectives prevailing in every aspect of our lives, society and indeed our algorithms. Crucially, these objectives produce convergent behaviour and thinking and distract us from discovering stepping stones which will lead to greatness. He thinks that this monotonic objective obsession, this idea that we need to continue to improve benchmarks every year is dangerous. He wrote about this in detail in his recent book &quot;greatness can not be planned&quot; which will be the main topic of discussion in the show. We also cover his ideas on open endedness in machine learning.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro to Kenneth&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:01:16 Show structure disclaimer&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:04:16 Passionate discussion&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:06:26 WHy greatness cant be planned and the tyranny of objectives&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:14:40 Chinese Finger Trap &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:16:28 Perverse Incentives and feedback loops&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:18:17 Deception&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:23:29 Maze example&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:24:44 How can we define curiosity or interestingness&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:26:59 Open endedness&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:33:01 ICML 2019 and Yannic, POET, first MSLST&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:17 evolutionary algorithms++&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:43:18 POET, the first MLST &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:45:39 A lesson to GOFAI people&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:48:46 Machine Learning -- the great stagnation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:54:34 Actual scientific successes are usually luck, and against the odds -- Biontech&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:56:21 Picbreeder and NEAT&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:10:47 How Tim applies these ideas to his life and why he runs MLST&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:14:58 Keith Skit about UCF&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:15:13 Main show kick off&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:18:02 Why does Kenneth value serindipitous exploration so much&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:24:10 Scientific support for Keneths ideas in normal life&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:27:12 We should drop objectives to achieve them. An oxymoron?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:33:13 Isnt this just resource allocation between exploration and exploitation?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:39:06 Are objectives merely a matter of degree?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:42:38 How do we allocate funds for treasure hunting in society&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:47:34 A keen nose for what is interesting, and voting can be dangerous&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:53:00 Committees are the antithesis of innovation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:56:21 Does Kenneth apply these ideas to his real life?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:59:48 Divergence vs interestingness vs novelty vs complexity&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:08:13 Picbreeder&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:12:39 Isnt everything novel in some sense?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:16:35 Imagine if there was no selection pressure?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:18:31 Is innovation == environment exploitation?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:20:37 Is it possible to take shortcuts if you already knew what the innovations were?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:21:11 Go Explore -- does the algorithm encode the stepping stones?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:24:41 What does it mean for things to be interestingly different?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:26:11 behavioral characterization / diversity measure to your broad interests&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:30:54 Shaping objectives&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:32:49 Why do all ambitious objectives have deception? Picbreeder analogy&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:35:59 Exploration vs Exploitation, Science vs Engineering&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:43:18 Schools of thought in ML and could search lead to AGI&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:45:49 Official ending&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:46:26</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/staging/podcast_uploaded_nologo/4981699/4981699-1699437574146-72eb9fca1ae9c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>38</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#037 - Tour De Bayesian with Connor Tann]]></title>
			<description><![CDATA[<p>Connor Tan is a physicist and senior data scientist working for a multinational energy company where he co-founded and leads a data science team. He holds a first-class degree in experimental and theoretical physics from Cambridge university. With a master's in particle astrophysics. He specializes in the application of machine learning models and Bayesian methods. Today we explore the history, pratical utility, and unique capabilities of Bayesian methods. We also discuss the computational difficulties inherent in Bayesian methods along with modern methods for approximate solutions such as Markov Chain Monte Carlo. Finally, we discuss how Bayesian optimization in the context of automl may one day put Data Scientists like Connor out of work.</p>
<p><br></p>
<p>Panel: Dr. Keith Duggar, Alex Stenlake, Dr. Tim Scarfe</p>
<p><br></p>
<p>00:00:00 Duggars philisophical ramblings on Bayesianism</p>
<p>00:05:10 Introduction</p>
<p>00:07:30 small datasets and prior scientific knowledge</p>
<p>00:10:37 Bayesian methods are probability theory</p>
<p>00:14:00 Bayesian methods demand hard computations</p>
<p>00:15:46 uncertainty can matter more than estimators</p>
<p>00:19:29 updating or combining knowledge is a key feature</p>
<p>00:25:39 Frequency or Reasonable Expectation as the Primary Concept&nbsp;</p>
<p>00:30:02 Gambling and coin flips</p>
<p>00:37:32 Rev. Thomas Bayes's pool table</p>
<p>00:40:37 ignorance priors are beautiful yet hard</p>
<p>00:43:49 connections between common distributions</p>
<p>00:49:13 A curious Universe, Benford's Law</p>
<p>00:55:17 choosing priors, a tale of two factories</p>
<p>01:02:19 integration, the computational Achilles heel</p>
<p>01:35:25 Bayesian social context in the ML community</p>
<p>01:10:24 frequentist methods as a first approximation</p>
<p>01:13:13 driven to Bayesian methods by small sample size</p>
<p>01:18:46 Bayesian optimization with automl, a job killer?</p>
<p>01:25:28 different approaches to hyper-parameter optimization</p>
<p>01:30:18 advice for aspiring Bayesians</p>
<p>01:33:59 who would connor interview next?</p>
<p><br></p>
<p>Connor Tann: https://www.linkedin.com/in/connor-tann-a92906a1/</p>
<p>https://twitter.com/connossor</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/037---Tour-De-Bayesian-with-Connor-Tann-eoq4a6</link>
			<guid isPermaLink="false">2efac886-cc30-4c2d-846e-1c02fb1805ff</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 11 Jan 2021 01:30:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/25022214/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-11%2Fdf974633-9a92-ac67-be02-efb5183444a7.mp3" length="91827094" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Connor Tan is a physicist and senior data scientist working for a multinational energy company where he co-founded and leads a data science team. He holds a first-class degree in experimental and theoretical physics from Cambridge university. With a master&apos;s in particle astrophysics. He specializes in the application of machine learning models and Bayesian methods. Today we explore the history, pratical utility, and unique capabilities of Bayesian methods. We also discuss the computational difficulties inherent in Bayesian methods along with modern methods for approximate solutions such as Markov Chain Monte Carlo. Finally, we discuss how Bayesian optimization in the context of automl may one day put Data Scientists like Connor out of work.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Dr. Keith Duggar, Alex Stenlake, Dr. Tim Scarfe&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Duggars philisophical ramblings on Bayesianism&lt;/p&gt;
&lt;p&gt;00:05:10 Introduction&lt;/p&gt;
&lt;p&gt;00:07:30 small datasets and prior scientific knowledge&lt;/p&gt;
&lt;p&gt;00:10:37 Bayesian methods are probability theory&lt;/p&gt;
&lt;p&gt;00:14:00 Bayesian methods demand hard computations&lt;/p&gt;
&lt;p&gt;00:15:46 uncertainty can matter more than estimators&lt;/p&gt;
&lt;p&gt;00:19:29 updating or combining knowledge is a key feature&lt;/p&gt;
&lt;p&gt;00:25:39 Frequency or Reasonable Expectation as the Primary Concept&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:30:02 Gambling and coin flips&lt;/p&gt;
&lt;p&gt;00:37:32 Rev. Thomas Bayes&apos;s pool table&lt;/p&gt;
&lt;p&gt;00:40:37 ignorance priors are beautiful yet hard&lt;/p&gt;
&lt;p&gt;00:43:49 connections between common distributions&lt;/p&gt;
&lt;p&gt;00:49:13 A curious Universe, Benford&apos;s Law&lt;/p&gt;
&lt;p&gt;00:55:17 choosing priors, a tale of two factories&lt;/p&gt;
&lt;p&gt;01:02:19 integration, the computational Achilles heel&lt;/p&gt;
&lt;p&gt;01:35:25 Bayesian social context in the ML community&lt;/p&gt;
&lt;p&gt;01:10:24 frequentist methods as a first approximation&lt;/p&gt;
&lt;p&gt;01:13:13 driven to Bayesian methods by small sample size&lt;/p&gt;
&lt;p&gt;01:18:46 Bayesian optimization with automl, a job killer?&lt;/p&gt;
&lt;p&gt;01:25:28 different approaches to hyper-parameter optimization&lt;/p&gt;
&lt;p&gt;01:30:18 advice for aspiring Bayesians&lt;/p&gt;
&lt;p&gt;01:33:59 who would connor interview next?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Connor Tann: https://www.linkedin.com/in/connor-tann-a92906a1/&lt;/p&gt;
&lt;p&gt;https://twitter.com/connossor&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:35:25</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1610328650319-48ad660666c37.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>37</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#036 - Max Welling: Quantum, Manifolds & Symmetries in ML]]></title>
			<description><![CDATA[<p>Today we had a fantastic conversation with Professor Max Welling, VP of Technology, Qualcomm Technologies Netherlands B.V.&nbsp;</p>
<p><br></p>
<p>Max is a strong believer in the power of data and computation and its relevance to artificial intelligence. There is a fundamental blank slate paradgm in machine learning, experience and data alone currently rule the roost. Max wants to build a house of domain knowledge on top of that blank slate. Max thinks there are no predictions without assumptions, no generalization without inductive bias. The bias-variance tradeoff tells us that we need to use additional human knowledge when data is insufficient.</p>
<p><br></p>
<p>Max Welling has pioneered many of the most sophistocated inductive priors in DL models developed in recent years, allowing us to use Deep Learning with non-euclidean data i.e. on graphs/topology (a field we now called "geometric deep learning") or allowing network architectures to recognise new symmetries in the data for example gauge or SE(3) equivariance. Max has also brought many other concepts from his physics playbook into ML, for example quantum and even Bayesian approaches.&nbsp;</p>
<p><br></p>
<p>This is not an episode to miss, it might be our best yet!&nbsp;</p>
<p><br></p>
<p>Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake</p>
<p><br></p>
<p>00:00:00 Show introduction&nbsp;</p>
<p>00:04:37 Protein Fold from DeepMind -- did it use SE(3) transformer?&nbsp;</p>
<p>00:09:58 How has machine learning progressed&nbsp;</p>
<p>00:19:57 Quantum Deformed Neural Networks paper&nbsp;</p>
<p>00:22:54 Probabilistic Numeric Convolutional Neural Networks paper</p>
<p>00:27:04 Ilia Karmanov from Qualcomm interview mini segment</p>
<p>00:32:04 Main Show Intro&nbsp;</p>
<p>00:35:21 How is Max known in the community?&nbsp;</p>
<p>00:36:35 How Max nurtures talent, freedom and relationship is key&nbsp;</p>
<p>00:40:30 Selecting research directions and guidance&nbsp;</p>
<p>00:43:42 Priors vs experience (bias/variance trade-off)&nbsp;</p>
<p>00:48:47 Generative models and GPT-3&nbsp;</p>
<p>00:51:57 Bias/variance trade off -- when do priors hurt us&nbsp;</p>
<p>00:54:48 Capsule networks&nbsp;</p>
<p>01:03:09 Which old ideas whould we revive&nbsp;</p>
<p>01:04:36 Hardware lottery paper&nbsp;</p>
<p>01:07:50 Greatness can't be planned (Kenneth Stanley reference)&nbsp;</p>
<p>01:09:10 A new sort of peer review and originality&nbsp;</p>
<p>01:11:57 Quantum Computing&nbsp;</p>
<p>01:14:25 Quantum deformed neural networks paper&nbsp;</p>
<p>01:21:57 Probabalistic numeric convolutional neural networks&nbsp;</p>
<p>01:26:35 Matrix exponential&nbsp;</p>
<p>01:28:44 Other ideas from physics i.e. chaos, holography, renormalisation&nbsp;</p>
<p>01:34:25 Reddit&nbsp;</p>
<p>01:37:19 Open review system in ML&nbsp;</p>
<p>01:41:43 Outro&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/036---Max-Welling-Quantum--Manifolds--Symmetries-in-ML-eogoe8</link>
			<guid isPermaLink="false">2d5dfc12-42b0-4e17-bec1-1fe8b9687665</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 03 Jan 2021 18:08:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/24715144/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2021-0-3%2Fd0f24c69-d34f-c257-6f76-4af6675487f8.mp3" length="98616761" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today we had a fantastic conversation with Professor Max Welling, VP of Technology, Qualcomm Technologies Netherlands B.V.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Max is a strong believer in the power of data and computation and its relevance to artificial intelligence. There is a fundamental blank slate paradgm in machine learning, experience and data alone currently rule the roost. Max wants to build a house of domain knowledge on top of that blank slate. Max thinks there are no predictions without assumptions, no generalization without inductive bias. The bias-variance tradeoff tells us that we need to use additional human knowledge when data is insufficient.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Max Welling has pioneered many of the most sophistocated inductive priors in DL models developed in recent years, allowing us to use Deep Learning with non-euclidean data i.e. on graphs/topology (a field we now called &quot;geometric deep learning&quot;) or allowing network architectures to recognise new symmetries in the data for example gauge or SE(3) equivariance. Max has also brought many other concepts from his physics playbook into ML, for example quantum and even Bayesian approaches.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This is not an episode to miss, it might be our best yet!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Show introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:04:37 Protein Fold from DeepMind -- did it use SE(3) transformer?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:09:58 How has machine learning progressed&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:19:57 Quantum Deformed Neural Networks paper&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:22:54 Probabilistic Numeric Convolutional Neural Networks paper&lt;/p&gt;
&lt;p&gt;00:27:04 Ilia Karmanov from Qualcomm interview mini segment&lt;/p&gt;
&lt;p&gt;00:32:04 Main Show Intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:35:21 How is Max known in the community?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:35 How Max nurtures talent, freedom and relationship is key&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:40:30 Selecting research directions and guidance&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:43:42 Priors vs experience (bias/variance trade-off)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:48:47 Generative models and GPT-3&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:51:57 Bias/variance trade off -- when do priors hurt us&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:54:48 Capsule networks&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:03:09 Which old ideas whould we revive&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:04:36 Hardware lottery paper&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:07:50 Greatness can&apos;t be planned (Kenneth Stanley reference)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:09:10 A new sort of peer review and originality&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:11:57 Quantum Computing&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:14:25 Quantum deformed neural networks paper&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:21:57 Probabalistic numeric convolutional neural networks&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:26:35 Matrix exponential&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:28:44 Other ideas from physics i.e. chaos, holography, renormalisation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:34:25 Reddit&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:37:19 Open review system in ML&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:41:43 Outro&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:42:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1609697320972-74b1dca19e938.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>36</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#035 Christmas Community Edition!]]></title>
			<description><![CDATA[<p>Welcome to the Christmas special community edition of MLST! We discuss some recent and interesting papers from Pedro Domingos (are NNs kernel machines?), Deepmind (can NNs out-reason symbolic machines?), Anna Rodgers - When BERT Plays The Lottery, All Tickets Are Winning, Prof. Mark Bishop (even causal methods won't deliver understanding), We also cover our favourite bits from the recent Montreal AI event run by Prof. Gary Marcus (including Rich Sutton, Danny Kahneman and Christof Koch). We respond to a reader mail on Capsule networks. Then we do a deep dive into Type Theory and Lambda Calculus with community member Alex Mattick. In the final hour we discuss inductive priors and label information density with another one of our discord community members. &nbsp;</p>
<p><br></p>
<p>Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake, Dr. Keith Duggar</p>
<p><br></p>
<p>Enjoy the show and don't forget to subscribe!</p>
<p><br></p>
<p>00:00:00 Welcome to Christmas Special!&nbsp;</p>
<p>00:00:44 SoTa meme&nbsp;</p>
<p>00:01:30 Happy Christmas!&nbsp;</p>
<p>00:03:11 Paper -- DeepMind - Outperforming neuro-symbolic models with NNs (Ding et al)</p>
<p>00:08:57 What does it mean to understand?&nbsp;</p>
<p>00:17:37 Paper - Prof. Mark Bishop Artificial Intelligence is stupid and causal reasoning</p>
<p>wont fix it</p>
<p>00:25:39 Paper -- Pedro Domingos - &nbsp;Every Model Learned by Gradient Descent Is Approximately a Kernel Machine</p>
<p>00:31:07 Paper - Bengio - Inductive Biases for Deep Learning of Higher-Level Cognition</p>
<p>00:32:54 Anna Rodgers - When BERT Plays The Lottery, All Tickets Are Winning</p>
<p>00:37:16 Montreal AI event - Gary Marcus on reasoning&nbsp;</p>
<p>00:40:37 Montreal AI event -- Rich Sutton on universal theory of AI</p>
<p>00:49:45 Montreal AI event -- Danny Kahneman, System 1 vs 2 and Generative Models ala free energy principle</p>
<p>01:02:57 Montreal AI event -- Christof Koch - Neuroscience is hard</p>
<p>01:10:55 Markus Carr -- reader letter on capsule networks</p>
<p>01:13:21 Alex response to Marcus Carr&nbsp;</p>
<p>01:22:06 Type theory segment -- &nbsp;with Alex Mattick from Discord</p>
<p>01:24:45 Type theory segment -- What is Type Theory&nbsp;</p>
<p>01:28:12 Type theory segment -- Difference between functional and OOP languages&nbsp;</p>
<p>01:29:03 Type theory segment -- Lambda calculus&nbsp;</p>
<p>01:30:46 Type theory segment -- Closures&nbsp;</p>
<p>01:35:05 Type theory segment -- Term rewriting (confluency and termination)&nbsp;</p>
<p>01:42:02 MType theory segment -- eta term rewritig system - Lambda Calculus &nbsp;</p>
<p>01:54:44 Type theory segment -- Types / semantics&nbsp;</p>
<p>02:06:26 Type theory segment -- Calculus of constructions&nbsp;</p>
<p>02:09:27 Type theory segment -- Homotopy type theory&nbsp;</p>
<p>02:11:02 Type theory segment -- Deep learning link&nbsp;</p>
<p>02:17:27 Jan from Discord segment -- Chrome MRU skit&nbsp;</p>
<p>02:18:56 Jan from Discord segment -- Inductive priors (with XMaster96/Jan from Discord)&nbsp;</p>
<p>02:37:59 Jan from Discord segment -- Label information density (with XMaster96/Jan from Discord)&nbsp;</p>
<p>02:55:13 Outro</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/035-Christmas-Community-Edition-eo9b2s</link>
			<guid isPermaLink="false">763a2fff-fd1d-47ca-8a77-1bf9e6a7760e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 27 Dec 2020 21:59:42 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/24472092/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-11-27%2Fcfe95ec3-515e-f86d-db35-474ca3db14bc.mp3" length="169311263" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Welcome to the Christmas special community edition of MLST! We discuss some recent and interesting papers from Pedro Domingos (are NNs kernel machines?), Deepmind (can NNs out-reason symbolic machines?), Anna Rodgers - When BERT Plays The Lottery, All Tickets Are Winning, Prof. Mark Bishop (even causal methods won&apos;t deliver understanding), We also cover our favourite bits from the recent Montreal AI event run by Prof. Gary Marcus (including Rich Sutton, Danny Kahneman and Christof Koch). We respond to a reader mail on Capsule networks. Then we do a deep dive into Type Theory and Lambda Calculus with community member Alex Mattick. In the final hour we discuss inductive priors and label information density with another one of our discord community members. &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel: Dr. Tim Scarfe, Yannic Kilcher, Alex Stenlake, Dr. Keith Duggar&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Enjoy the show and don&apos;t forget to subscribe!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Welcome to Christmas Special!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:00:44 SoTa meme&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:01:30 Happy Christmas!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:03:11 Paper -- DeepMind - Outperforming neuro-symbolic models with NNs (Ding et al)&lt;/p&gt;
&lt;p&gt;00:08:57 What does it mean to understand?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:17:37 Paper - Prof. Mark Bishop Artificial Intelligence is stupid and causal reasoning&lt;/p&gt;
&lt;p&gt;wont fix it&lt;/p&gt;
&lt;p&gt;00:25:39 Paper -- Pedro Domingos - &amp;nbsp;Every Model Learned by Gradient Descent Is Approximately a Kernel Machine&lt;/p&gt;
&lt;p&gt;00:31:07 Paper - Bengio - Inductive Biases for Deep Learning of Higher-Level Cognition&lt;/p&gt;
&lt;p&gt;00:32:54 Anna Rodgers - When BERT Plays The Lottery, All Tickets Are Winning&lt;/p&gt;
&lt;p&gt;00:37:16 Montreal AI event - Gary Marcus on reasoning&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:40:37 Montreal AI event -- Rich Sutton on universal theory of AI&lt;/p&gt;
&lt;p&gt;00:49:45 Montreal AI event -- Danny Kahneman, System 1 vs 2 and Generative Models ala free energy principle&lt;/p&gt;
&lt;p&gt;01:02:57 Montreal AI event -- Christof Koch - Neuroscience is hard&lt;/p&gt;
&lt;p&gt;01:10:55 Markus Carr -- reader letter on capsule networks&lt;/p&gt;
&lt;p&gt;01:13:21 Alex response to Marcus Carr&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:22:06 Type theory segment -- &amp;nbsp;with Alex Mattick from Discord&lt;/p&gt;
&lt;p&gt;01:24:45 Type theory segment -- What is Type Theory&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:28:12 Type theory segment -- Difference between functional and OOP languages&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:29:03 Type theory segment -- Lambda calculus&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:30:46 Type theory segment -- Closures&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:35:05 Type theory segment -- Term rewriting (confluency and termination)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:42:02 MType theory segment -- eta term rewritig system - Lambda Calculus &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:54:44 Type theory segment -- Types / semantics&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:06:26 Type theory segment -- Calculus of constructions&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:09:27 Type theory segment -- Homotopy type theory&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:11:02 Type theory segment -- Deep learning link&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:17:27 Jan from Discord segment -- Chrome MRU skit&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:18:56 Jan from Discord segment -- Inductive priors (with XMaster96/Jan from Discord)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:37:59 Jan from Discord segment -- Label information density (with XMaster96/Jan from Discord)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:55:13 Outro&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:56:03</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1609106391253-8c0bda9a6bfa3.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>35</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#034 Eray Özkural- AGI, Simulations & Safety]]></title>
			<description><![CDATA[<p>Dr. Eray Ozkural is an AGI researcher from Turkey, he is the founder of Celestial Intellect Cybernetics. Eray is extremely critical of Max Tegmark, Nick Bostrom and MIRI founder Elizier Yodokovsky and their views on AI safety. Eray thinks that these views represent a form of neoludditism and they are capturing valuable research budgets with doomsday fear-mongering and effectively want to prevent AI from being developed by those they don't agree with. Eray is also sceptical of the intelligence explosion hypothesis and the argument from simulation.</p>
<p><br></p>
<p>Panel -- Dr. Keith Duggar, Dr. Tim Scarfe, Yannic Kilcher</p>
<p><br></p>
<p>00:00:00 Show teaser intro with added nuggets and commentary</p>
<p>00:48:39 Main Show Introduction&nbsp;</p>
<p>00:53:14 Doomsaying to Control &nbsp;</p>
<p>00:56:39 Fear the Basilisk! &nbsp;</p>
<p>01:08:00 Intelligence Explosion Ethics &nbsp;</p>
<p>01:09:45 Fear the Automous Drone! ... or spam &nbsp;</p>
<p>01:11:25 Infinity Point Hypothesis &nbsp;</p>
<p>01:15:26 Meat Level Intelligence&nbsp;</p>
<p>01:21:25 Defining Intelligence ... Yet Again &nbsp;</p>
<p>01:27:34 We'll make brains and then shoot them&nbsp;</p>
<p>01:31:00 The Universe likes deep learning&nbsp;</p>
<p>01:33:16 NNs are glorified hash tables&nbsp;</p>
<p>01:38:44 Radical behaviorists &nbsp;</p>
<p>01:41:29 Omega Architecture, possible AGI? &nbsp;</p>
<p>01:53:33 Simulation hypothesis&nbsp;</p>
<p>02:09:44 No one cometh unto Simulation, but by Jesus Christ &nbsp;</p>
<p>02:16:47 Agendas, Motivations, and Mind Projections &nbsp;</p>
<p>02:23:38 A computable Universe of Bulk Automata&nbsp;</p>
<p>02:30:31 Self-Organized Post-Show Coda&nbsp;</p>
<p>02:31:29 Investigating Intelligent Agency is Science&nbsp;</p>
<p>02:36:56 Goodbye and cheers! &nbsp;</p>
<p><br></p>
<p>https://www.youtube.com/watch?v=pZsHZDA9TJU</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/034-Eray-zkural--AGI--Simulations--Safety-eo1a14</link>
			<guid isPermaLink="false">1ad36971-7b85-4c0e-904f-152a6cb3c8f8</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 20 Dec 2020 01:16:52 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/24208868/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-11-20%2Fd322e2f5-9b46-65e5-a8ec-70215cf85384.mp3" length="153832549" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Eray Ozkural is an AGI researcher from Turkey, he is the founder of Celestial Intellect Cybernetics. Eray is extremely critical of Max Tegmark, Nick Bostrom and MIRI founder Elizier Yodokovsky and their views on AI safety. Eray thinks that these views represent a form of neoludditism and they are capturing valuable research budgets with doomsday fear-mongering and effectively want to prevent AI from being developed by those they don&apos;t agree with. Eray is also sceptical of the intelligence explosion hypothesis and the argument from simulation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Panel -- Dr. Keith Duggar, Dr. Tim Scarfe, Yannic Kilcher&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Show teaser intro with added nuggets and commentary&lt;/p&gt;
&lt;p&gt;00:48:39 Main Show Introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:53:14 Doomsaying to Control &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:56:39 Fear the Basilisk! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:08:00 Intelligence Explosion Ethics &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:09:45 Fear the Automous Drone! ... or spam &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:11:25 Infinity Point Hypothesis &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:15:26 Meat Level Intelligence&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:21:25 Defining Intelligence ... Yet Again &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:27:34 We&apos;ll make brains and then shoot them&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:31:00 The Universe likes deep learning&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:33:16 NNs are glorified hash tables&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:38:44 Radical behaviorists &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:41:29 Omega Architecture, possible AGI? &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:53:33 Simulation hypothesis&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:09:44 No one cometh unto Simulation, but by Jesus Christ &amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:16:47 Agendas, Motivations, and Mind Projections &amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:23:38 A computable Universe of Bulk Automata&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:30:31 Self-Organized Post-Show Coda&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:31:29 Investigating Intelligent Agency is Science&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:36:56 Goodbye and cheers! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.youtube.com/watch?v=pZsHZDA9TJU&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:39:09</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1608427020834-68ba13e479021.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>34</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#033 Prof. Karl Friston - The Free Energy Principle]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Dr. Keith Duggar and Connor Leahy chat with Prof. Karl Friston. Professor Friston is a British neuroscientist at University College London and an authority on brain imaging. In 2016 he was ranked the most influential neuroscientist on Semantic Scholar. &nbsp;His main contribution to theoretical neurobiology is the variational Free energy principle, also known as active inference in the Bayesian brain. The FEP is a formal statement that the existential imperative for any system which survives in the changing world can be cast as an inference problem. Bayesian Brain Hypothesis states that the brain is confronted with ambiguous sensory evidence, which it interprets by making inferences about the hidden states which caused the sensory data. So is the brain an inference engine? The key concept separating Friston's idea from traditional stochastic reinforcement learning methods and even Bayesian reinforcement learning is moving away from goal-directed optimisation.</p>
<p><br></p>
<p>Remember to subscribe! Enjoy the show!</p>
<p><br></p>
<p>00:00:00 Show teaser intro&nbsp;</p>
<p>00:16:24 Main formalism for FEP&nbsp;</p>
<p>00:28:29 Path Integral&nbsp;</p>
<p>00:30:52 How did we feel talking to friston?&nbsp;</p>
<p>00:34:06 Skit - on cultures (checked, but maybe make shorter)&nbsp;</p>
<p>00:36:02 Friston joins&nbsp;</p>
<p>00:36:33 Main show introduction&nbsp;</p>
<p>00:40:51 Is prediction all it takes for intelligence?&nbsp;</p>
<p>00:48:21 balancing accuracy with flexibility&nbsp;</p>
<p>00:57:36 belief-free vs belief-based; beliefs are crucial &nbsp;</p>
<p>01:04:53 Fuzzy Markov Blankets and Wandering Sets &nbsp;</p>
<p>01:12:37 The Free Energy Principle conforms to itself &nbsp;</p>
<p>01:14:50 useful false beliefs&nbsp;</p>
<p>01:19:14 complexity minimization is the heart of free energy [01:19:14 ]Keith: &nbsp;</p>
<p>01:23:25 An Alpha to tip the scales? Absoute not! Absolutely yes! &nbsp;</p>
<p>01:28:47 FEP applied to brain anatomy &nbsp;</p>
<p>01:36:28 Are there multiple non-FEP forms in the brain?&nbsp;</p>
<p>01:43:11 a positive conneciton to backpropagation &nbsp;</p>
<p>01:47:12 The FEP does not explain the origin of FEP systems &nbsp;</p>
<p>01:49:32 Post-show banter&nbsp;</p>
<p><br></p>
<p>https://www.fil.ion.ucl.ac.uk/~karl/</p>
<p>#machinelearning</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/033-Prof--Karl-Friston---The-Free-Energy-Principle-enon6c</link>
			<guid isPermaLink="false">4a2cb3b9-9e51-4b45-8d00-2ac76d25b7db</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 13 Dec 2020 20:58:47 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/23927436/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-11-13%2F9d3bac6a-65a4-5c66-0edb-dc833671edce.mp3" length="108202383" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Dr. Keith Duggar and Connor Leahy chat with Prof. Karl Friston. Professor Friston is a British neuroscientist at University College London and an authority on brain imaging. In 2016 he was ranked the most influential neuroscientist on Semantic Scholar. &amp;nbsp;His main contribution to theoretical neurobiology is the variational Free energy principle, also known as active inference in the Bayesian brain. The FEP is a formal statement that the existential imperative for any system which survives in the changing world can be cast as an inference problem. Bayesian Brain Hypothesis states that the brain is confronted with ambiguous sensory evidence, which it interprets by making inferences about the hidden states which caused the sensory data. So is the brain an inference engine? The key concept separating Friston&apos;s idea from traditional stochastic reinforcement learning methods and even Bayesian reinforcement learning is moving away from goal-directed optimisation.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Remember to subscribe! Enjoy the show!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Show teaser intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:16:24 Main formalism for FEP&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:28:29 Path Integral&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:30:52 How did we feel talking to friston?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:34:06 Skit - on cultures (checked, but maybe make shorter)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:02 Friston joins&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:33 Main show introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:40:51 Is prediction all it takes for intelligence?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:48:21 balancing accuracy with flexibility&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:57:36 belief-free vs belief-based; beliefs are crucial &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:04:53 Fuzzy Markov Blankets and Wandering Sets &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:12:37 The Free Energy Principle conforms to itself &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:14:50 useful false beliefs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:19:14 complexity minimization is the heart of free energy [01:19:14 ]Keith: &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:23:25 An Alpha to tip the scales? Absoute not! Absolutely yes! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:28:47 FEP applied to brain anatomy &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:36:28 Are there multiple non-FEP forms in the brain?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:43:11 a positive conneciton to backpropagation &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:47:12 The FEP does not explain the origin of FEP systems &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:49:32 Post-show banter&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://www.fil.ion.ucl.ac.uk/~karl/&lt;/p&gt;
&lt;p&gt;#machinelearning&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:51:24</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1607893136054-eaf6a97cb02b.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>32</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#032- Simon Kornblith / GoogleAI - SimCLR and Paper Haul!]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Sayak Paul and Yannic Kilcher speak with Dr. Simon Kornblith from Google Brain (Ph.D from MIT). Simon is trying to understand how neural nets do what they do. Simon was the second author on the seminal Google AI SimCLR paper. We also cover "Do Wide and Deep Networks learn the same things?", "Whats in a Loss function for Image Classification?", &nbsp;and "Big Self-supervised models are strong semi-supervised learners". Simon used to be a neuroscientist and also gives us the story of his unique journey into ML.</p>
<p><br></p>
<p>00:00:00 Show Teaser / or "short version"</p>
<p>00:18:34 Show intro</p>
<p>00:22:11 Relationship between neuroscience and machine learning</p>
<p>00:29:28 Similarity analysis and evolution of representations in Neural Networks</p>
<p>00:39:55 Expressability of NNs</p>
<p>00:42:33 Whats in a loss function for image classification</p>
<p>00:46:52 Loss function implications for transfer learning</p>
<p>00:50:44 SimCLR paper&nbsp;</p>
<p>01:00:19 Contrast SimCLR to BYOL</p>
<p>01:01:43 Data augmentation</p>
<p>01:06:35 Universality of image representations</p>
<p>01:09:25 Universality of augmentations</p>
<p>01:23:04 GPT-3</p>
<p>01:25:09 GANs for data augmentation??</p>
<p>01:26:50 Julia language</p>
<p><br></p>
<p>@skornblith</p>
<p>https://www.linkedin.com/in/simon-kornblith-54b2033a/</p>
<p><br></p>
<p>https://arxiv.org/abs/2010.15327</p>
<p>Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth</p>
<p><br></p>
<p>https://arxiv.org/abs/2010.16402</p>
<p>What's in a Loss Function for Image Classification?</p>
<p><br></p>
<p>https://arxiv.org/abs/2002.05709</p>
<p>A Simple Framework for Contrastive Learning of Visual Representations</p>
<p><br></p>
<p>https://arxiv.org/abs/2006.10029</p>
<p>Big Self-Supervised Models are Strong Semi-Supervised Learners</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/032--Simon-Kornblith--GoogleAI---SimCLR-and-Paper-Haul-endpa3</link>
			<guid isPermaLink="false">2776bbeb-684c-43bd-b043-89b7bcc85cad</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 06 Dec 2020 00:43:10 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/23569155/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-11-6%2F42980d04-6f8d-9944-e9dd-afb94c0e9042.mp3" length="87333879" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Sayak Paul and Yannic Kilcher speak with Dr. Simon Kornblith from Google Brain (Ph.D from MIT). Simon is trying to understand how neural nets do what they do. Simon was the second author on the seminal Google AI SimCLR paper. We also cover &quot;Do Wide and Deep Networks learn the same things?&quot;, &quot;Whats in a Loss function for Image Classification?&quot;, &amp;nbsp;and &quot;Big Self-supervised models are strong semi-supervised learners&quot;. Simon used to be a neuroscientist and also gives us the story of his unique journey into ML.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Show Teaser / or &quot;short version&quot;&lt;/p&gt;
&lt;p&gt;00:18:34 Show intro&lt;/p&gt;
&lt;p&gt;00:22:11 Relationship between neuroscience and machine learning&lt;/p&gt;
&lt;p&gt;00:29:28 Similarity analysis and evolution of representations in Neural Networks&lt;/p&gt;
&lt;p&gt;00:39:55 Expressability of NNs&lt;/p&gt;
&lt;p&gt;00:42:33 Whats in a loss function for image classification&lt;/p&gt;
&lt;p&gt;00:46:52 Loss function implications for transfer learning&lt;/p&gt;
&lt;p&gt;00:50:44 SimCLR paper&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:00:19 Contrast SimCLR to BYOL&lt;/p&gt;
&lt;p&gt;01:01:43 Data augmentation&lt;/p&gt;
&lt;p&gt;01:06:35 Universality of image representations&lt;/p&gt;
&lt;p&gt;01:09:25 Universality of augmentations&lt;/p&gt;
&lt;p&gt;01:23:04 GPT-3&lt;/p&gt;
&lt;p&gt;01:25:09 GANs for data augmentation??&lt;/p&gt;
&lt;p&gt;01:26:50 Julia language&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;@skornblith&lt;/p&gt;
&lt;p&gt;https://www.linkedin.com/in/simon-kornblith-54b2033a/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2010.15327&lt;/p&gt;
&lt;p&gt;Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2010.16402&lt;/p&gt;
&lt;p&gt;What&apos;s in a Loss Function for Image Classification?&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2002.05709&lt;/p&gt;
&lt;p&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/2006.10029&lt;/p&gt;
&lt;p&gt;Big Self-Supervised Models are Strong Semi-Supervised Learners&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:30:29</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1607215423956-ab6c3abd0f132.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>32</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#031 WE GOT ACCESS TO GPT-3! (With Gary Marcus, Walid Saba and Connor Leahy)]]></title>
			<description><![CDATA[<p>In this special edition, Dr. Tim Scarfe, Yannic Kilcher and Keith Duggar speak with Gary Marcus and Connor Leahy about GPT-3. We have all had a significant amount of time to experiment with GPT-3 and show you demos of it in use and the considerations.</p>
<p>Note that this podcast version is significantly truncated, watch the youtube version for the TOC and experiments with GPT-3 https://www.youtube.com/watch?v=iccd86vOz3w</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/031-WE-GOT-ACCESS-TO-GPT-3--With-Gary-Marcus--Walid-Saba-and-Connor-Leahy-en2h1k</link>
			<guid isPermaLink="false">275aac9f-5a81-4159-b8d0-4a6ea44b0c78</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 28 Nov 2020 00:40:55 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/23200244/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-28%2F55ef5efc-a00d-6b0a-70b1-d39087cf4154.mp3" length="157777953" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this special edition, Dr. Tim Scarfe, Yannic Kilcher and Keith Duggar speak with Gary Marcus and Connor Leahy about GPT-3. We have all had a significant amount of time to experiment with GPT-3 and show you demos of it in use and the considerations.&lt;/p&gt;
&lt;p&gt;Note that this podcast version is significantly truncated, watch the youtube version for the TOC and experiments with GPT-3 https://www.youtube.com/watch?v=iccd86vOz3w&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:44:06</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1606524083289-05afbc4d9915a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>31</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#030 Multi-Armed Bandits and Pure-Exploration (Wouter M. Koolen)]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher discuss multi-arm bandits and pure exploration with Dr. Wouter M. Koolen, Senior Researcher, Machine Learning group, Centrum Wiskunde &amp; Informatica.</p>
<p><br></p>
<p>Wouter specialises in machine learning theory, game theory, information theory, statistics and optimisation. Wouter is currently interested in pure exploration in multi-armed bandit models, game tree search, and accelerated learning in sequential decision problems. His research has been cited 1000 times, and he has been published in NeurIPS, the number 1 ML conference 14 times as well as lots of other exciting publications.</p>
<p><br></p>
<p>Today we are going to talk about two of the most studied settings in control, decision theory, and learning in unknown environment which are the multi-armed bandit (MAB) and reinforcement learning (RL) approaches</p>
<p>- when can an agent stop learning and start exploiting using the knowledge it obtained</p>
<p>- which strategy leads to minimal learning time</p>
<p><br></p>
<p>00:00:00 What are multi-arm bandits/show trailer</p>
<p>00:12:55 Show introduction</p>
<p>00:15:50 Bandits&nbsp;</p>
<p>00:18:58 Taxonomy of decision framework approaches&nbsp;</p>
<p>00:25:46 Exploration vs Exploitation&nbsp;</p>
<p>00:31:43 the sharp divide between modes&nbsp;</p>
<p>00:34:12 bandit measures of success&nbsp;</p>
<p>00:36:44 connections to reinforcement learning&nbsp;</p>
<p>00:44:00 when to apply pure exploration in games&nbsp;</p>
<p>00:45:54 bandit lower bounds, a pure exploration renaissance&nbsp;</p>
<p>00:50:21 pure exploration compiler dreams&nbsp;</p>
<p>00:51:56 what would the PX-compiler DSL look like&nbsp;</p>
<p>00:57:13 the long arms of the bandit&nbsp;</p>
<p>01:00:21 causal models behind the curtain of arms&nbsp;</p>
<p>01:02:43 adversarial bandits, arms trying to beat you&nbsp;</p>
<p>01:05:12 bandits as an optimization problem&nbsp;</p>
<p>01:11:39 asymptotic optimality vs practical performance&nbsp;</p>
<p>01:15:38 pitfalls hiding under asymptotic cover&nbsp;</p>
<p>01:18:50 adding features to bandits&nbsp;</p>
<p>01:27:24 moderate confidence regimes &nbsp;</p>
<p>01:30:33 algorithms choice is highly sensitive to bounds&nbsp;</p>
<p>01:46:09 Post script: Keith interesting piece on n quantum&nbsp;</p>
<p><br></p>
<p>http://wouterkoolen.info</p>
<p>https://www.cwi.nl/research-groups/ma...</p>
<p><br></p>
<p>#machinelearning</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/030-Multi-Armed-Bandits-and-Pure-Exploration-Wouter-M--Koolen-emp42c</link>
			<guid isPermaLink="false">7eb7e69f-7019-4c76-883a-9afc123855a2</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 20 Nov 2020 20:36:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/22892044/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-20%2Faf6a8ad0-7465-85fb-4fa3-0eeb527206a5.mp3" length="156019259" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher discuss multi-arm bandits and pure exploration with Dr. Wouter M. Koolen, Senior Researcher, Machine Learning group, Centrum Wiskunde &amp;amp; Informatica.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Wouter specialises in machine learning theory, game theory, information theory, statistics and optimisation. Wouter is currently interested in pure exploration in multi-armed bandit models, game tree search, and accelerated learning in sequential decision problems. His research has been cited 1000 times, and he has been published in NeurIPS, the number 1 ML conference 14 times as well as lots of other exciting publications.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Today we are going to talk about two of the most studied settings in control, decision theory, and learning in unknown environment which are the multi-armed bandit (MAB) and reinforcement learning (RL) approaches&lt;/p&gt;
&lt;p&gt;- when can an agent stop learning and start exploiting using the knowledge it obtained&lt;/p&gt;
&lt;p&gt;- which strategy leads to minimal learning time&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 What are multi-arm bandits/show trailer&lt;/p&gt;
&lt;p&gt;00:12:55 Show introduction&lt;/p&gt;
&lt;p&gt;00:15:50 Bandits&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:18:58 Taxonomy of decision framework approaches&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:25:46 Exploration vs Exploitation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:31:43 the sharp divide between modes&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:34:12 bandit measures of success&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:44 connections to reinforcement learning&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:44:00 when to apply pure exploration in games&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:45:54 bandit lower bounds, a pure exploration renaissance&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:50:21 pure exploration compiler dreams&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:51:56 what would the PX-compiler DSL look like&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:57:13 the long arms of the bandit&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:00:21 causal models behind the curtain of arms&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:02:43 adversarial bandits, arms trying to beat you&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:05:12 bandits as an optimization problem&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:11:39 asymptotic optimality vs practical performance&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:15:38 pitfalls hiding under asymptotic cover&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:18:50 adding features to bandits&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:27:24 moderate confidence regimes &amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:30:33 algorithms choice is highly sensitive to bounds&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:46:09 Post script: Keith interesting piece on n quantum&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;http://wouterkoolen.info&lt;/p&gt;
&lt;p&gt;https://www.cwi.nl/research-groups/ma...&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#machinelearning&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:48:08</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1605904585908-82384e93ee94a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>30</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[#029 GPT-3, Prompt Engineering, Trading, AI Alignment, Intelligence]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Dr. Keith Duggar, Yannic Kilcher and Connor Leahy cover a broad range of topics, ranging from academia, GPT-3 and whether prompt engineering could be the next in-demand skill, markets and economics including trading and whether you can predict the stock market, AI alignment, utilitarian philosophy, randomness and intelligence and even whether the universe is infinite!&nbsp;</p>
<p><br></p>
<p>00:00:00 Show Introduction&nbsp;</p>
<p>00:12:49 Academia and doing a Ph.D&nbsp;</p>
<p>00:15:49 From academia to wall street&nbsp;</p>
<p>00:17:08 Quants -- smoke and mirrors? Tail Risk&nbsp;</p>
<p>00:19:46 Previous results dont indicate future success in markets&nbsp;</p>
<p>00:23:23 Making money from social media signals?&nbsp;</p>
<p>00:24:41 Predicting the stock market&nbsp;</p>
<p>00:27:20 Things which are and are not predictable&nbsp;</p>
<p>00:31:40 Tim postscript comment on predicting markets&nbsp;</p>
<p>00:32:37 Connor take on markets&nbsp;</p>
<p>00:35:16 As market become more efficient..&nbsp;</p>
<p>00:36:38 Snake oil in ML&nbsp;</p>
<p>00:39:20 GPT-3, we have changed our minds&nbsp;</p>
<p>00:52:34 Prompt engineering a new form of software development?&nbsp;</p>
<p>01:06:07 GPT-3 and prompt engineering&nbsp;</p>
<p>01:12:33 Emergent intelligence with increasingly weird abstractions&nbsp;</p>
<p>01:27:29 Wireheading and the economy&nbsp;</p>
<p>01:28:54 Free markets, dragon story and price vs value&nbsp;</p>
<p>01:33:59 Utilitarian philosophy and what does good look like?&nbsp;</p>
<p>01:41:39 Randomness and intelligence&nbsp;</p>
<p>01:44:55 Different schools of thought in ML&nbsp;</p>
<p>01:46:09 Is the universe infinite?&nbsp;</p>
<p><br></p>
<p>Thanks a lot for Connor Leahy for being a guest on today's show. https://twitter.com/NPCollapse -- you can join his EleutherAI community discord here: https://discord.com/invite/vtRgjbM</p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/029-GPT-3--Prompt-Engineering--Trading--AI-Alignment--Intelligence-em6pjh</link>
			<guid isPermaLink="false">0c820e3d-e1aa-41ac-a1e6-dccb6422faea</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 08 Nov 2020 18:53:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/22291505/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-8%2Fa53bf32b-d149-0f09-d591-a14551daaf5a.mp3" length="106880882" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Dr. Keith Duggar, Yannic Kilcher and Connor Leahy cover a broad range of topics, ranging from academia, GPT-3 and whether prompt engineering could be the next in-demand skill, markets and economics including trading and whether you can predict the stock market, AI alignment, utilitarian philosophy, randomness and intelligence and even whether the universe is infinite!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Show Introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:12:49 Academia and doing a Ph.D&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:15:49 From academia to wall street&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:17:08 Quants -- smoke and mirrors? Tail Risk&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:19:46 Previous results dont indicate future success in markets&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:23:23 Making money from social media signals?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:24:41 Predicting the stock market&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:27:20 Things which are and are not predictable&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:31:40 Tim postscript comment on predicting markets&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:32:37 Connor take on markets&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:35:16 As market become more efficient..&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:38 Snake oil in ML&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:39:20 GPT-3, we have changed our minds&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:52:34 Prompt engineering a new form of software development?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:06:07 GPT-3 and prompt engineering&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:12:33 Emergent intelligence with increasingly weird abstractions&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:27:29 Wireheading and the economy&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:28:54 Free markets, dragon story and price vs value&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:33:59 Utilitarian philosophy and what does good look like?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:41:39 Randomness and intelligence&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:44:55 Different schools of thought in ML&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:46:09 Is the universe infinite?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thanks a lot for Connor Leahy for being a guest on today&apos;s show. https://twitter.com/NPCollapse -- you can join his EleutherAI community discord here: https://discord.com/invite/vtRgjbM&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:50:32</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1604861627951-bbb5d979c8c3b.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>29</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[NLP is not NLU and GPT-3 - Walid Saba]]></title>
			<description><![CDATA[<p>#machinelearning</p>
<p>This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher speak with veteran NLU expert Dr. Walid Saba.&nbsp;</p>
<p>Walid is an old-school AI expert. He is a polymath, a neuroscientist, psychologist, linguist, &nbsp;philosopher, statistician, and logician. He thinks the missing information problem and lack of a typed ontology is the key issue with NLU, not sample efficiency or generalisation. He is a big critic of the deep learning movement and BERTology. We also cover GPT-3 in some detail in today's session, covering Luciano Floridi's recent article "GPT‑3: Its Nature, Scope, Limits, and Consequences" and a commentary on the incredible power of GPT-3 to perform tasks with just a few examples including the Yann LeCun commentary on Facebook and Hackernews.&nbsp;</p>
<p>Time stamps on the YouTube version</p>
<p>0:00:00 Walid intro&nbsp;</p>
<p>00:05:03 Knowledge acquisition bottleneck&nbsp;</p>
<p>00:06:11 Language is ambiguous&nbsp;</p>
<p>00:07:41 Language is not learned&nbsp;</p>
<p>00:08:32 Language is a formal language&nbsp;</p>
<p>00:08:55 Learning from data doesn’t work &nbsp;</p>
<p>00:14:01 Intelligence&nbsp;</p>
<p>00:15:07 Lack of domain knowledge these days&nbsp;</p>
<p>00:16:37 Yannic Kilcher thuglife comment&nbsp;</p>
<p>00:17:57 Deep learning assault&nbsp;</p>
<p>00:20:07 The way we evaluate language models is flawed&nbsp;</p>
<p>00:20:47 Humans do type checking&nbsp;</p>
<p>00:23:02 Ontologic&nbsp;</p>
<p>00:25:48 Comments On GPT3&nbsp;</p>
<p>00:30:54 Yann lecun and reddit&nbsp;</p>
<p>00:33:57 Minds and machines - Luciano&nbsp;</p>
<p>00:35:55 Main show introduction&nbsp;</p>
<p>00:39:02 Walid introduces himself&nbsp;</p>
<p>00:40:20 science advances one funeral at a time&nbsp;</p>
<p>00:44:58 Deep learning obsession syndrome and inception&nbsp;</p>
<p>00:46:14 BERTology / empirical methods are not NLU&nbsp;</p>
<p>00:49:55 Pattern recognition vs domain reasoning, is the knowledge in the data&nbsp;</p>
<p>00:56:04 Natural language understanding is about decoding and not compression, it's not learnable.&nbsp;</p>
<p>01:01:46 Intelligence is about not needing infinite amounts of time&nbsp;</p>
<p>01:04:23 We need an explicit ontological structure to understand anything&nbsp;</p>
<p>01:06:40 Ontological concepts&nbsp;</p>
<p>01:09:38 Word embeddings&nbsp;</p>
<p>01:12:20 There is power in structure&nbsp;</p>
<p>01:15:16 Language models are not trained on pronoun disambiguation and resolving scopes&nbsp;</p>
<p>01:17:33 The information is not in the data&nbsp;</p>
<p>01:19:03 Can we generate these rules on the fly? Rules or data?&nbsp;</p>
<p>01:20:39 The missing data problem is key&nbsp;</p>
<p>01:21:19 Problem with empirical methods and lecunn reference&nbsp;</p>
<p>01:22:45 Comparison with meatspace (brains)&nbsp;</p>
<p>01:28:16 The knowledge graph game, is knowledge constructed or discovered&nbsp;</p>
<p>01:29:41 How small can this ontology of the world be?&nbsp;</p>
<p>01:33:08 Walids taxonomy of understanding&nbsp;</p>
<p>01:38:49 The trend seems to be, less rules is better not the othe way around?&nbsp;</p>
<p>01:40:30 Testing the latest NLP models with entailment&nbsp;</p>
<p>01:42:25 Problems with the way we evaluate NLP&nbsp;</p>
<p>01:44:10 Winograd Schema challenge&nbsp;</p>
<p>01:45:56 All you need to know now is how to build neural networks, lack of rigour in ML research&nbsp;</p>
<p>01:50:47 Is everything learnable&nbsp;</p>
<p>01:53:02 &nbsp;How should we elevate language systems?&nbsp;</p>
<p>01:54:04 10 big problems in language (missing information)&nbsp;</p>
<p>01:55:59 Multiple inheritance is wrong&nbsp;</p>
<p>01:58:19 Language is ambiguous&nbsp;</p>
<p>02:01:14 How big would our world ontology need to be?&nbsp;</p>
<p>02:05:49 How to learn more about NLU&nbsp;</p>
<p>02:09:10 AlphaGo&nbsp;</p>
<p><br></p>
<p>Walid's blog: https://medium.com/@ontologik</p>
<p>LinkedIn: https://www.linkedin.com/in/walidsaba/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/NLP-is-not-NLU-and-GPT-3---Walid-Saba-em16v1</link>
			<guid isPermaLink="false">411d2f91-818b-4e4d-9d93-7a32e920ebf7</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 04 Nov 2020 19:16:32 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/22108577/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-4%2F63c66f58-056e-d167-7cb1-eca08f19b9d9.mp3" length="135582912" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;#machinelearning&lt;/p&gt;
&lt;p&gt;This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic Kilcher speak with veteran NLU expert Dr. Walid Saba.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Walid is an old-school AI expert. He is a polymath, a neuroscientist, psychologist, linguist, &amp;nbsp;philosopher, statistician, and logician. He thinks the missing information problem and lack of a typed ontology is the key issue with NLU, not sample efficiency or generalisation. He is a big critic of the deep learning movement and BERTology. We also cover GPT-3 in some detail in today&apos;s session, covering Luciano Floridi&apos;s recent article &quot;GPT‑3: Its Nature, Scope, Limits, and Consequences&quot; and a commentary on the incredible power of GPT-3 to perform tasks with just a few examples including the Yann LeCun commentary on Facebook and Hackernews.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Time stamps on the YouTube version&lt;/p&gt;
&lt;p&gt;0:00:00 Walid intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:05:03 Knowledge acquisition bottleneck&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:06:11 Language is ambiguous&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:07:41 Language is not learned&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:08:32 Language is a formal language&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:08:55 Learning from data doesn’t work &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:14:01 Intelligence&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:15:07 Lack of domain knowledge these days&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:16:37 Yannic Kilcher thuglife comment&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:17:57 Deep learning assault&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:20:07 The way we evaluate language models is flawed&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:20:47 Humans do type checking&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:23:02 Ontologic&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:25:48 Comments On GPT3&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:30:54 Yann lecun and reddit&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:33:57 Minds and machines - Luciano&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:35:55 Main show introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:39:02 Walid introduces himself&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:40:20 science advances one funeral at a time&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:44:58 Deep learning obsession syndrome and inception&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:46:14 BERTology / empirical methods are not NLU&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:49:55 Pattern recognition vs domain reasoning, is the knowledge in the data&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:56:04 Natural language understanding is about decoding and not compression, it&apos;s not learnable.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:01:46 Intelligence is about not needing infinite amounts of time&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:04:23 We need an explicit ontological structure to understand anything&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:06:40 Ontological concepts&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:09:38 Word embeddings&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:12:20 There is power in structure&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:15:16 Language models are not trained on pronoun disambiguation and resolving scopes&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:17:33 The information is not in the data&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:19:03 Can we generate these rules on the fly? Rules or data?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:20:39 The missing data problem is key&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:21:19 Problem with empirical methods and lecunn reference&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:22:45 Comparison with meatspace (brains)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:28:16 The knowledge graph game, is knowledge constructed or discovered&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:29:41 How small can this ontology of the world be?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:33:08 Walids taxonomy of understanding&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:38:49 The trend seems to be, less rules is better not the othe way around?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:40:30 Testing the latest NLP models with entailment&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:42:25 Problems with the way we evaluate NLP&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:44:10 Winograd Schema challenge&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:45:56 All you need to know now is how to build neural networks, lack of rigour in ML research&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:50:47 Is everything learnable&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:53:02 &amp;nbsp;How should we elevate language systems?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:54:04 10 big problems in language (missing information)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:55:59 Multiple inheritance is wrong&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:58:19 Language is ambiguous&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:01:14 How big would our world ontology need to be?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:05:49 How to learn more about NLU&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:09:10 AlphaGo&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Walid&apos;s blog: https://medium.com/@ontologik&lt;/p&gt;
&lt;p&gt;LinkedIn: https://www.linkedin.com/in/walidsaba/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:20:32</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1604517400598-6939939a94e26.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>28</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[AI Alignment & AGI Fire Alarm - Connor Leahy]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Alex Stenlake and Yannic Kilcher speak with AGI and AI alignment specialist Connor Leahy a machine learning engineer from Aleph Alpha and founder of EleutherAI.</p>
<p><br></p>
<p>Connor believes that AI alignment is philosophy with a deadline and that we are on the precipice, the stakes are astronomical. AI is important, and it will go wrong by default. Connor thinks that the singularity or intelligence explosion is near. Connor says that AGI is like climate change but worse, even harder problems, even shorter deadline and even worse consequences for the future. These problems are hard, and nobody knows what to do about them.</p>
<p><br></p>
<p>00:00:00 Introduction to AI alignment and AGI fire alarm&nbsp;</p>
<p>00:15:16 Main Show Intro&nbsp;</p>
<p>00:18:38 Different schools of thought on AI safety&nbsp;</p>
<p>00:24:03 What is intelligence?&nbsp;</p>
<p>00:25:48 AI Alignment&nbsp;</p>
<p>00:27:39 Humans dont have a coherent utility function&nbsp;</p>
<p>00:28:13 Newcomb's paradox and advanced decision problems&nbsp;</p>
<p>00:34:01 Incentives and behavioural economics&nbsp;</p>
<p>00:37:19 Prisoner's dilemma&nbsp;</p>
<p>00:40:24 Ayn Rand and game theory in politics and business&nbsp;</p>
<p>00:44:04 Instrumental convergence and orthogonality thesis&nbsp;</p>
<p>00:46:14 Utility functions and the Stop button problem&nbsp;</p>
<p>00:55:24 AI corrigibality - self alignment&nbsp;</p>
<p>00:56:16 Decision theory and stability / wireheading / robust delegation&nbsp;</p>
<p>00:59:30 Stop button problem&nbsp;</p>
<p>01:00:40 Making the world a better place&nbsp;</p>
<p>01:03:43 Is intelligence a search problem?&nbsp;</p>
<p>01:04:39 Mesa optimisation / humans are misaligned AI&nbsp;</p>
<p>01:06:04 Inner vs outer alignment / faulty reward functions&nbsp;</p>
<p>01:07:31 Large corporations are intelligent and have no stop function&nbsp;</p>
<p>01:10:21 Dutch booking / what is rationality / decision theory&nbsp;</p>
<p>01:16:32 Understanding very powerful AIs&nbsp;</p>
<p>01:18:03 Kolmogorov complexity&nbsp;</p>
<p>01:19:52 GPT-3 - is it intelligent, are humans even intelligent?&nbsp;</p>
<p>01:28:40 Scaling hypothesis&nbsp;</p>
<p>01:29:30 Connor thought DL was dead in 2017&nbsp;</p>
<p>01:37:54 Why is GPT-3 as intelligent as a human&nbsp;</p>
<p>01:44:43 Jeff Hawkins on intelligence as compression and the great lookup table&nbsp;</p>
<p>01:50:28 AI ethics related to AI alignment?&nbsp;</p>
<p>01:53:26 Interpretability&nbsp;</p>
<p>01:56:27 Regulation&nbsp;</p>
<p>01:57:54 Intelligence explosion&nbsp;</p>
<p><br></p>
<p><br></p>
<p>Discord: https://discord.com/invite/vtRgjbM</p>
<p>EleutherAI: https://www.eleuther.ai</p>
<p>Twitter: https://twitter.com/npcollapse</p>
<p>LinkedIn: https://www.linkedin.com/in/connor-j-leahy/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/AI-Alignment--AGI-Fire-Alarm---Connor-Leahy-elsod2</link>
			<guid isPermaLink="false">4413ec52-e4be-440e-9ae4-9046b8a3ee97</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 01 Nov 2020 20:31:52 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/21962594/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-10-1%2Fa6531d59-0540-6bc8-bdc1-f11b1f081ab4.mp3" length="179422399" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Alex Stenlake and Yannic Kilcher speak with AGI and AI alignment specialist Connor Leahy a machine learning engineer from Aleph Alpha and founder of EleutherAI.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Connor believes that AI alignment is philosophy with a deadline and that we are on the precipice, the stakes are astronomical. AI is important, and it will go wrong by default. Connor thinks that the singularity or intelligence explosion is near. Connor says that AGI is like climate change but worse, even harder problems, even shorter deadline and even worse consequences for the future. These problems are hard, and nobody knows what to do about them.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction to AI alignment and AGI fire alarm&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:15:16 Main Show Intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:18:38 Different schools of thought on AI safety&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:24:03 What is intelligence?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:25:48 AI Alignment&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:27:39 Humans dont have a coherent utility function&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:28:13 Newcomb&apos;s paradox and advanced decision problems&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:34:01 Incentives and behavioural economics&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:37:19 Prisoner&apos;s dilemma&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:40:24 Ayn Rand and game theory in politics and business&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:44:04 Instrumental convergence and orthogonality thesis&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:46:14 Utility functions and the Stop button problem&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:55:24 AI corrigibality - self alignment&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:56:16 Decision theory and stability / wireheading / robust delegation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:59:30 Stop button problem&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:00:40 Making the world a better place&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:03:43 Is intelligence a search problem?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:04:39 Mesa optimisation / humans are misaligned AI&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:06:04 Inner vs outer alignment / faulty reward functions&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:07:31 Large corporations are intelligent and have no stop function&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:10:21 Dutch booking / what is rationality / decision theory&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:16:32 Understanding very powerful AIs&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:18:03 Kolmogorov complexity&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:19:52 GPT-3 - is it intelligent, are humans even intelligent?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:28:40 Scaling hypothesis&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:29:30 Connor thought DL was dead in 2017&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:37:54 Why is GPT-3 as intelligent as a human&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:44:43 Jeff Hawkins on intelligence as compression and the great lookup table&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:50:28 AI ethics related to AI alignment?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:53:26 Interpretability&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:56:27 Regulation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:57:54 Intelligence explosion&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Discord: https://discord.com/invite/vtRgjbM&lt;/p&gt;
&lt;p&gt;EleutherAI: https://www.eleuther.ai&lt;/p&gt;
&lt;p&gt;Twitter: https://twitter.com/npcollapse&lt;/p&gt;
&lt;p&gt;LinkedIn: https://www.linkedin.com/in/connor-j-leahy/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:04:35</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1604262720453-46ac8438ef7ff.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>27</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Kaggle, ML Community / Engineering (Sanyam Bhutani)]]></title>
			<description><![CDATA[<p>Join Dr Tim Scarfe, Sayak Paul, Yannic Kilcher, and Alex Stenlake have a conversation with Mr. Chai Time Data Science; Sanyam Bhutani!</p>
<p><br></p>
<p>00:00:00 Introduction&nbsp;</p>
<p>00:03:42 Show kick off&nbsp;</p>
<p>00:06:34 How did Sanyam get started into ML&nbsp;</p>
<p>00:07:46 Being a content creator&nbsp;</p>
<p>00:09:01 Can you be self taught without a formal education in ML?&nbsp;</p>
<p>00:22:54 Kaggle&nbsp;</p>
<p>00:33:41 H20 product / job&nbsp;</p>
<p>00:40:58 Intepretability / bias / engineering skills&nbsp;</p>
<p>00:43:22 Get that first job in DS&nbsp;</p>
<p>00:46:29 AWS ML Ops architecture / ml engineering&nbsp;</p>
<p>01:14:19 Patterns&nbsp;</p>
<p>01:18:09 Testability&nbsp;</p>
<p>01:20:54 Adversarial examples&nbsp;</p>
<p><br></p>
<p>Sanyam's blog -- https://sanyambhutani.com/tag/chaitimedatascience/</p>
<p>Chai Time Data Science -- https://www.youtube.com/c/ChaiTimeDataScience</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Kaggle--ML-Community--Engineering-Sanyam-Bhutani-elm073</link>
			<guid isPermaLink="false">1c321d76-864e-44d6-8f76-eaa68a505451</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 28 Oct 2020 00:47:41 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/21741219/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-9-28%2Fa43874d0-c71e-6b77-a44c-75c83f21464b.mp3" length="84009493" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Join Dr Tim Scarfe, Sayak Paul, Yannic Kilcher, and Alex Stenlake have a conversation with Mr. Chai Time Data Science; Sanyam Bhutani!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:03:42 Show kick off&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:06:34 How did Sanyam get started into ML&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:07:46 Being a content creator&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:09:01 Can you be self taught without a formal education in ML?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:22:54 Kaggle&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:33:41 H20 product / job&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:40:58 Intepretability / bias / engineering skills&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:43:22 Get that first job in DS&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:46:29 AWS ML Ops architecture / ml engineering&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:14:19 Patterns&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:18:09 Testability&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:20:54 Adversarial examples&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Sanyam&apos;s blog -- https://sanyambhutani.com/tag/chaitimedatascience/&lt;/p&gt;
&lt;p&gt;Chai Time Data Science -- https://www.youtube.com/c/ChaiTimeDataScience&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:26:59</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1603846069525-6b1332cfe199e.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>26</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sara Hooker - The Hardware Lottery, Sparsity and Fairness]]></title>
			<description><![CDATA[<p>Dr. Tim Scarfe, Yannic Kilcher and Sayak Paul chat with Sara Hooker from the Google Brain team! We discuss her recent hardware lottery paper, pruning / sparsity, bias mitigation and intepretability.&nbsp;</p>
<p>The hardware lottery -- what causes inertia or friction in the marketplace of ideas? Is there a meritocracy of ideas or do the previous decisions we have made enslave us? Sara Hooker calls this a lottery because she feels that machine learning progress is entirely beholdant to the hardware and software landscape. Ideas succeed if they are compatible with the hardware and software at the time and also the existing inventions. The machine learning community is exceptional because the pace of innovation is fast and we operate largely in the open, this is largely because we don't build anything physical which is expensive, slow and the cost of being scooped is high. We get stuck in basins of attraction based on our technology decisions and it's expensive to jump outside of these basins. So is this story unique to hardware and AI algorithms or is it really just the story of all innovation? Every great innovation must wait for the right stepping stone to be in place before it can really happen. We are excited to bring you Sara Hooker to give her take.&nbsp;</p>
<p>YouTube version (including TOC): https://youtu.be/sQFxbQ7ade0</p>
<p>Show notes; https://drive.google.com/file/d/1S_rHnhaoVX4Nzx_8e3ESQq4uSswASNo7/view?usp=sharing</p>
<p>Sara Hooker page; https://www.sarahooker.me</p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sara-Hooker---The-Hardware-Lottery--Sparsity-and-Fairness-elbevq</link>
			<guid isPermaLink="false">7dace170-7b47-4276-84b3-6c19aaf7bf5e</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 20 Oct 2020 22:16:23 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/21395898/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-9-20%2Fe367eec0-2c05-75e6-0d3e-8581217c6113.mp3" length="87558311" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Dr. Tim Scarfe, Yannic Kilcher and Sayak Paul chat with Sara Hooker from the Google Brain team! We discuss her recent hardware lottery paper, pruning / sparsity, bias mitigation and intepretability.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The hardware lottery -- what causes inertia or friction in the marketplace of ideas? Is there a meritocracy of ideas or do the previous decisions we have made enslave us? Sara Hooker calls this a lottery because she feels that machine learning progress is entirely beholdant to the hardware and software landscape. Ideas succeed if they are compatible with the hardware and software at the time and also the existing inventions. The machine learning community is exceptional because the pace of innovation is fast and we operate largely in the open, this is largely because we don&apos;t build anything physical which is expensive, slow and the cost of being scooped is high. We get stuck in basins of attraction based on our technology decisions and it&apos;s expensive to jump outside of these basins. So is this story unique to hardware and AI algorithms or is it really just the story of all innovation? Every great innovation must wait for the right stepping stone to be in place before it can really happen. We are excited to bring you Sara Hooker to give her take.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;YouTube version (including TOC): https://youtu.be/sQFxbQ7ade0&lt;/p&gt;
&lt;p&gt;Show notes; https://drive.google.com/file/d/1S_rHnhaoVX4Nzx_8e3ESQq4uSswASNo7/view?usp=sharing&lt;/p&gt;
&lt;p&gt;Sara Hooker page; https://www.sarahooker.me&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:30:35</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1603232191850-a3fac513d82b3.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>25</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Social Dilemma Part 3 - Dr. Rebecca Roache]]></title>
			<description><![CDATA[<p>This week join Dr. Tim Scarfe, Yannic Kilcher, and Keith Duggar have a conversation with Dr. Rebecca Roache in the last of our 3-part series on the social dilemma Netflix film. Rebecca is a senior lecturer in philosophy at Royal Holloway, university of London and has written extensively about the future of friendship.&nbsp;</p>
<p><br></p>
<p>People claim that friendships are not what they used to be. People are always staring at their phones, even when in public &nbsp;Social media has turned us into narcissists who are always managing our own PR rather than being present with each other. Anxiety about the negative effects of technology are as old as the written word. Is technology bad for friendships? Can you have friends through screens? Does social media cause polarization? And is that a bad thing? Does it promote quantity over quality? Rebecca thinks that social media and echo chambers are less ominous to friendship on closer inspection.&nbsp;</p>
<p><br></p>
<p>00:00:32 Teaser clip from Rebecca and her new manuscript on friendship</p>
<p>00:02:52 Introduction&nbsp;</p>
<p>00:04:56 Memorisation vs reasoning / is technology enhancing friendships&nbsp;</p>
<p>00:09:29 Word of warcraft / gaming communities / echo chambers / polarisation&nbsp;</p>
<p>00:12:34 Horizontal vs Vertical social attributes&nbsp;</p>
<p>00:17:18 Exclusion of others opinions&nbsp;</p>
<p>00:20:36 The power to silence others / truth verification&nbsp;</p>
<p>00:23:58 Misinformation&nbsp;</p>
<p>00:27:28 Norms / memes / political terms and co-opting / bullying&nbsp;</p>
<p>00:31:57 Redefinition of political terms i.e. racism&nbsp;</p>
<p>00:36:13 Virtue signalling&nbsp;</p>
<p>00:38:57 How many friends can you have / spread thin / Dunbars 150&nbsp;</p>
<p>00:42:54 Is it morally objectionable to believe or contemplate objectionable ideas, punishment&nbsp;</p>
<p>00:50:52 Is speaking the same thing as acting &nbsp;</p>
<p>00:52:24 Punishment - deterrence vs retribution / historical&nbsp;</p>
<p>00:53:59 Yannic: contemplating is a form of speaking&nbsp;</p>
<p>00:57:32 silencing/blocking is intellectual laziness - what ideas are we allowed to talk about&nbsp;</p>
<p>01:04:53 Corporate AI ethics frameworks&nbsp;</p>
<p>01:09:14 Autonomous Vehicles&nbsp;</p>
<p>01:10:51 the eternal Facebook world / online vs offline friendships&nbsp;</p>
<p>01:14:05 How do we get the best out of our online friendships&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Social-Dilemma-Part-3---Dr--Rebecca-Roache-ektnhf</link>
			<guid isPermaLink="false">cc0520a9-6385-4b8b-9d21-0bbc9c8ad992</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sun, 11 Oct 2020 23:16:28 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/20945903/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-9-11%2F91c25bf9-ee4b-0776-941c-fff60304589f.mp3" length="74203020" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week join Dr. Tim Scarfe, Yannic Kilcher, and Keith Duggar have a conversation with Dr. Rebecca Roache in the last of our 3-part series on the social dilemma Netflix film. Rebecca is a senior lecturer in philosophy at Royal Holloway, university of London and has written extensively about the future of friendship.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;People claim that friendships are not what they used to be. People are always staring at their phones, even when in public &amp;nbsp;Social media has turned us into narcissists who are always managing our own PR rather than being present with each other. Anxiety about the negative effects of technology are as old as the written word. Is technology bad for friendships? Can you have friends through screens? Does social media cause polarization? And is that a bad thing? Does it promote quantity over quality? Rebecca thinks that social media and echo chambers are less ominous to friendship on closer inspection.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:32 Teaser clip from Rebecca and her new manuscript on friendship&lt;/p&gt;
&lt;p&gt;00:02:52 Introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:04:56 Memorisation vs reasoning / is technology enhancing friendships&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:09:29 Word of warcraft / gaming communities / echo chambers / polarisation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:12:34 Horizontal vs Vertical social attributes&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:17:18 Exclusion of others opinions&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:20:36 The power to silence others / truth verification&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:23:58 Misinformation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:27:28 Norms / memes / political terms and co-opting / bullying&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:31:57 Redefinition of political terms i.e. racism&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:36:13 Virtue signalling&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:38:57 How many friends can you have / spread thin / Dunbars 150&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:42:54 Is it morally objectionable to believe or contemplate objectionable ideas, punishment&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:50:52 Is speaking the same thing as acting &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:52:24 Punishment - deterrence vs retribution / historical&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:53:59 Yannic: contemplating is a form of speaking&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:57:32 silencing/blocking is intellectual laziness - what ideas are we allowed to talk about&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:04:53 Corporate AI ethics frameworks&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:09:14 Autonomous Vehicles&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:10:51 the eternal Facebook world / online vs offline friendships&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:14:05 How do we get the best out of our online friendships&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:16:18</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1602458196272-a45bb4b872511.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>23</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Social Dilemma - Part 1]]></title>
			<description><![CDATA[<p>In this first part of our three part series on the Social Dilemma Netflix film, Dr. Tim Scarfe, Yannic "Lightspeed" Kilcher and Zak Jost gang up with Cybersecurity expert Andy Smith. We give you our take on the film. We are super excited to get your feedback on this one! Hope you enjoy.&nbsp;</p>
<p>&nbsp;</p>
<p>00:00:00 Introduction</p>
<p>00:06:11 Moral hypocrisy &nbsp;</p>
<p>00:12:38 Road to hell is paved with good intentions, attention economy</p>
<p>00:15:04 They know everything about you</p>
<p>00:18:02 Addiction</p>
<p>00:21:22 Differential realities</p>
<p>00:26:12 Self determination and Monetisation</p>
<p>00:29:08 AI: Overwhelm human strengths undermine human vulnerabilities</p>
<p>00:31:51 Conspiracy theory / fake news</p>
<p>00:34:23 Overton window / polarisation</p>
<p>00:39:12 Short attention span / convergent behaviour</p>
<p>00:41:26 Is social media good for you</p>
<p>00:45:17 Your attention time is linear, the things you can pay attention to are a volume, anonymity&nbsp;</p>
<p>00:51:32 Andy question on security: social engineering</p>
<p>00:56:32 Is it a security risk having your information in social media</p>
<p>00:58:02 Retrospective judgement</p>
<p>01:03:06 Free speech and censorship&nbsp;</p>
<p>01:06:06 Technology accelerator</p>
<p><br></p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Social-Dilemma---Part-1-eki9uf</link>
			<guid isPermaLink="false">400d5d35-426a-4390-813b-e700f6834332</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 03 Oct 2020 21:07:20 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/20571535/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-9-3%2F551fbb82-c4ba-bbc4-9381-41f770f1c46b.mp3" length="66003511" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this first part of our three part series on the Social Dilemma Netflix film, Dr. Tim Scarfe, Yannic &quot;Lightspeed&quot; Kilcher and Zak Jost gang up with Cybersecurity expert Andy Smith. We give you our take on the film. We are super excited to get your feedback on this one! Hope you enjoy.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&lt;/p&gt;
&lt;p&gt;00:06:11 Moral hypocrisy &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:12:38 Road to hell is paved with good intentions, attention economy&lt;/p&gt;
&lt;p&gt;00:15:04 They know everything about you&lt;/p&gt;
&lt;p&gt;00:18:02 Addiction&lt;/p&gt;
&lt;p&gt;00:21:22 Differential realities&lt;/p&gt;
&lt;p&gt;00:26:12 Self determination and Monetisation&lt;/p&gt;
&lt;p&gt;00:29:08 AI: Overwhelm human strengths undermine human vulnerabilities&lt;/p&gt;
&lt;p&gt;00:31:51 Conspiracy theory / fake news&lt;/p&gt;
&lt;p&gt;00:34:23 Overton window / polarisation&lt;/p&gt;
&lt;p&gt;00:39:12 Short attention span / convergent behaviour&lt;/p&gt;
&lt;p&gt;00:41:26 Is social media good for you&lt;/p&gt;
&lt;p&gt;00:45:17 Your attention time is linear, the things you can pay attention to are a volume, anonymity&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:51:32 Andy question on security: social engineering&lt;/p&gt;
&lt;p&gt;00:56:32 Is it a security risk having your information in social media&lt;/p&gt;
&lt;p&gt;00:58:02 Retrospective judgement&lt;/p&gt;
&lt;p&gt;01:03:06 Free speech and censorship&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:06:06 Technology accelerator&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:07:19</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1601759249453-e4c081a96badf.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>21</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Capsule Networks and Education Targets]]></title>
			<description><![CDATA[<p>In today's episode, Dr. Keith Duggar, Alex Stenlake and Dr. Tim Scarfe chat about the education chapter in Kenneth Stanley's "Greatness cannot be planned" book, and we relate it to our Algoshambes conversation a few weeks ago. We debate whether objectives in education are a good thing and whether they cause perverse incentives and stifle creativity and innovation. Next up we dissect capsule networks from the top down! We finish off talking about fast algorithms and quantum computing.</p>
<p><br></p>
<p>00:00:00 Introduction</p>
<p>00:01:13 Greatness cannot be planned / education&nbsp;</p>
<p>00:12:03 Perverse incentives</p>
<p>00:19:25 Treasure hunting&nbsp;</p>
<p>00:30:28 Capsule Networks</p>
<p>00:46:08 Capsules As Compositional Networks</p>
<p>00:52:45 Capsule Routing</p>
<p>00:57:10 Loss and Warps</p>
<p>01:09:55 Fast Algorithms and Quantum Computing</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Capsule-Networks-and-Education-Targets-ekbu7u</link>
			<guid isPermaLink="false">22dd8237-c38f-4cf6-987e-1ba888df50ca</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 29 Sep 2020 19:13:03 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/20362942/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-29%2F6ffe5b0d-f765-6859-b04a-7858fd714bad.mp3" length="81708356" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In today&apos;s episode, Dr. Keith Duggar, Alex Stenlake and Dr. Tim Scarfe chat about the education chapter in Kenneth Stanley&apos;s &quot;Greatness cannot be planned&quot; book, and we relate it to our Algoshambes conversation a few weeks ago. We debate whether objectives in education are a good thing and whether they cause perverse incentives and stifle creativity and innovation. Next up we dissect capsule networks from the top down! We finish off talking about fast algorithms and quantum computing.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&lt;/p&gt;
&lt;p&gt;00:01:13 Greatness cannot be planned / education&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:12:03 Perverse incentives&lt;/p&gt;
&lt;p&gt;00:19:25 Treasure hunting&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:30:28 Capsule Networks&lt;/p&gt;
&lt;p&gt;00:46:08 Capsules As Compositional Networks&lt;/p&gt;
&lt;p&gt;00:52:45 Capsule Routing&lt;/p&gt;
&lt;p&gt;00:57:10 Loss and Warps&lt;/p&gt;
&lt;p&gt;01:09:55 Fast Algorithms and Quantum Computing&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:24:08</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1601406791718-533423581418f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>20</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Programming Languages, Software Engineering and Machine Learning]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Dr. Keith Duggar, Yannic "Lightspeed" Kilcher have a conversation with Microsoft Senior Software Engineer Sachin Kundu. We speak about programming languages including which our favourites are and functional programming vs OOP. Next we speak about software engineering and the intersection of software engineering and machine learning. We also talk about applications of ML and finally what makes an exceptional software engineer and tech lead. Sachin is an expert in this field so we hope you enjoy the conversation!</p>
<p>Spoiler alert, how many of you have read the Mythical Man-Month by Frederick P. Brooks?! &nbsp;</p>
<p>00:00:00 Introduction</p>
<p>00:06:37 Programming Languages</p>
<p>00:53:41 Applications of ML</p>
<p>01:55:59 What makes an exceptional SE and tech lead</p>
<p>01:22:08 Outro&nbsp;</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Programming-Languages--Software-Engineering-and-Machine-Learning-ek5rrj</link>
			<guid isPermaLink="false">f312ccb1-e3cf-4ca6-825c-1bd72dbe9483</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 25 Sep 2020 17:32:17 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/20163891/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-25%2F83dd1e61-8ce0-9087-bba4-4544c59fbd5d.mp3" length="81440810" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Dr. Keith Duggar, Yannic &quot;Lightspeed&quot; Kilcher have a conversation with Microsoft Senior Software Engineer Sachin Kundu. We speak about programming languages including which our favourites are and functional programming vs OOP. Next we speak about software engineering and the intersection of software engineering and machine learning. We also talk about applications of ML and finally what makes an exceptional software engineer and tech lead. Sachin is an expert in this field so we hope you enjoy the conversation!&lt;/p&gt;
&lt;p&gt;Spoiler alert, how many of you have read the Mythical Man-Month by Frederick P. Brooks?! &amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&lt;/p&gt;
&lt;p&gt;00:06:37 Programming Languages&lt;/p&gt;
&lt;p&gt;00:53:41 Applications of ML&lt;/p&gt;
&lt;p&gt;01:55:59 What makes an exceptional SE and tech lead&lt;/p&gt;
&lt;p&gt;01:22:08 Outro&amp;nbsp;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:23:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1601055145623-5d7c3692f13c2.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>20</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Computation, Bayesian Model Selection, Interactive Articles]]></title>
			<description><![CDATA[<p>This week Dr. Keith Duggar, Alex Stenlake and Dr. Tim Scarfe discuss the theory of computation, intelligence, Bayesian model selection, the intelligence explosion and the the phenomenon of "interactive articles".&nbsp;</p>
<p><br></p>
<p>00:00:00 Intro</p>
<p>00:01:27 Kernels and context-free grammars</p>
<p>00:06:04 Theory of computation</p>
<p>00:18:41 Intelligence</p>
<p>00:22:03 Bayesian model selection</p>
<p>00:44:05 AI-IQ Measure / Intelligence explosion</p>
<p>00:52:09 Interactive articles</p>
<p>01:12:32 Outro</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Computation--Bayesian-Model-Selection--Interactive-Articles-ek1bcd</link>
			<guid isPermaLink="false">4c74218d-075e-415f-8504-06f1c9030064</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 22 Sep 2020 22:03:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/20015949/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-22%2F998bfce0-734e-db4a-bcb7-953fd7fb5864.mp3" length="71464874" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Keith Duggar, Alex Stenlake and Dr. Tim Scarfe discuss the theory of computation, intelligence, Bayesian model selection, the intelligence explosion and the the phenomenon of &quot;interactive articles&quot;.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:01:27 Kernels and context-free grammars&lt;/p&gt;
&lt;p&gt;00:06:04 Theory of computation&lt;/p&gt;
&lt;p&gt;00:18:41 Intelligence&lt;/p&gt;
&lt;p&gt;00:22:03 Bayesian model selection&lt;/p&gt;
&lt;p&gt;00:44:05 AI-IQ Measure / Intelligence explosion&lt;/p&gt;
&lt;p&gt;00:52:09 Interactive articles&lt;/p&gt;
&lt;p&gt;01:12:32 Outro&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:13:40</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1600812245359-9b4dceb0d6a82.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>19</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Kernels!]]></title>
			<description><![CDATA[<p>Today Yannic Lightspeed Kilcher and I spoke with Alex Stenlake about Kernel Methods. What is a kernel? Do you remember those weird kernel things which everyone obsessed about before deep learning? What about Representer theorem and reproducible kernel hilbert spaces? SVMs and kernel ridge regression? Remember them?! Hope you enjoy the conversation!</p>
<p><br></p>
<p><br></p>
<p>00:00:00 Tim Intro</p>
<p>00:01:35 Yannic clever insight from this discussion&nbsp;</p>
<p>00:03:25 Street talk and Alex intro&nbsp;</p>
<p>00:05:06 How kernels are taught</p>
<p>00:09:20 Computational tractability</p>
<p>00:10:32 Maths&nbsp;</p>
<p>00:11:50 What is a kernel?&nbsp;</p>
<p>00:19:39 Kernel latent expansion&nbsp;</p>
<p>00:23:57 Overfitting&nbsp;</p>
<p>00:24:50 Hilbert spaces&nbsp;</p>
<p>00:30:20 Compare to DL</p>
<p>00:31:18 Back to hilbert spaces</p>
<p>00:45:19 Computational tractability 2</p>
<p>00:52:23 Curse of dimensionality</p>
<p>00:55:01 RBF: infinite taylor series</p>
<p>00:57:20 Margin/SVM&nbsp;</p>
<p>01:00:07 KRR/dual</p>
<p>01:03:26 Complexity compute kernels vs deep learning</p>
<p>01:05:03 Good for small problems? vs deep learning)</p>
<p>01:07:50 Whats special about the RBF kernel</p>
<p>01:11:06 Another DL comparison</p>
<p>01:14:01 Representer theorem</p>
<p>01:20:05 Relation to back prop</p>
<p>01:25:10 Connection with NLP/transformers</p>
<p>01:27:31 Where else kernels good</p>
<p>01:34:34 Deep learning vs dual kernel methods</p>
<p>01:33:29 Thoughts on AI</p>
<p>01:34:35 Outro</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Kernels-ejr9lc</link>
			<guid isPermaLink="false">40627a95-12b0-4ecd-9b32-2969ed96ae8a</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 18 Sep 2020 17:54:56 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/19817580/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-18%2F4b760ffa-af26-7878-0c78-6eb3cb351b92.mp3" length="94526674" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Today Yannic Lightspeed Kilcher and I spoke with Alex Stenlake about Kernel Methods. What is a kernel? Do you remember those weird kernel things which everyone obsessed about before deep learning? What about Representer theorem and reproducible kernel hilbert spaces? SVMs and kernel ridge regression? Remember them?! Hope you enjoy the conversation!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Tim Intro&lt;/p&gt;
&lt;p&gt;00:01:35 Yannic clever insight from this discussion&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:03:25 Street talk and Alex intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:05:06 How kernels are taught&lt;/p&gt;
&lt;p&gt;00:09:20 Computational tractability&lt;/p&gt;
&lt;p&gt;00:10:32 Maths&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:11:50 What is a kernel?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:19:39 Kernel latent expansion&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:23:57 Overfitting&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:24:50 Hilbert spaces&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:30:20 Compare to DL&lt;/p&gt;
&lt;p&gt;00:31:18 Back to hilbert spaces&lt;/p&gt;
&lt;p&gt;00:45:19 Computational tractability 2&lt;/p&gt;
&lt;p&gt;00:52:23 Curse of dimensionality&lt;/p&gt;
&lt;p&gt;00:55:01 RBF: infinite taylor series&lt;/p&gt;
&lt;p&gt;00:57:20 Margin/SVM&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:00:07 KRR/dual&lt;/p&gt;
&lt;p&gt;01:03:26 Complexity compute kernels vs deep learning&lt;/p&gt;
&lt;p&gt;01:05:03 Good for small problems? vs deep learning)&lt;/p&gt;
&lt;p&gt;01:07:50 Whats special about the RBF kernel&lt;/p&gt;
&lt;p&gt;01:11:06 Another DL comparison&lt;/p&gt;
&lt;p&gt;01:14:01 Representer theorem&lt;/p&gt;
&lt;p&gt;01:20:05 Relation to back prop&lt;/p&gt;
&lt;p&gt;01:25:10 Connection with NLP/transformers&lt;/p&gt;
&lt;p&gt;01:27:31 Where else kernels good&lt;/p&gt;
&lt;p&gt;01:34:34 Deep learning vs dual kernel methods&lt;/p&gt;
&lt;p&gt;01:33:29 Thoughts on AI&lt;/p&gt;
&lt;p&gt;01:34:35 Outro&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:37:29</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1600451704669-7d6db89ce3023.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>18</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Explainability, Reasoning, Priors and GPT-3]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe and Dr. Keith Duggar discuss Explainability, Reasoning, Priors and GPT-3. We check out Christoph Molnar's book on intepretability, talk about priors vs experience in NNs, whether NNs are reasoning and also cover articles by Gary Marcus and Walid Saba critiquing deep learning. We finish with a brief discussion of Chollet's ARC challenge and intelligence paper.&nbsp;</p>
<p><br></p>
<p>00:00:00 Intro</p>
<p>00:01:17 Explainability and Christoph Molnars book on Intepretability</p>
<p>00:26:45 Explainability - Feature visualisation</p>
<p>00:33:28 Architecture / CPPNs</p>
<p>00:36:10 Invariance and data parsimony, priors and experience, manifolds</p>
<p>00:42:04 What NNs learn / logical view of modern AI (Walid Saba article)</p>
<p>00:47:10 Core knowledge</p>
<p>00:55:33 Priors vs experience&nbsp;</p>
<p>00:59:44 Mathematical reasoning&nbsp;</p>
<p>01:01:56 Gary Marcus on GPT-3&nbsp;</p>
<p>01:09:14 Can NNs reason at all?&nbsp;</p>
<p>01:18:05 Chollet intelligence paper/ARC challenge</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Explainability--Reasoning--Priors-and-GPT-3-ejngoj</link>
			<guid isPermaLink="false">857e62c4-1752-47d7-8890-5f08ef02d42d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 16 Sep 2020 13:34:35 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/19693779/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-16%2F695d4ae5-f139-a089-e008-df535242c8b8.mp3" length="83514511" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe and Dr. Keith Duggar discuss Explainability, Reasoning, Priors and GPT-3. We check out Christoph Molnar&apos;s book on intepretability, talk about priors vs experience in NNs, whether NNs are reasoning and also cover articles by Gary Marcus and Walid Saba critiquing deep learning. We finish with a brief discussion of Chollet&apos;s ARC challenge and intelligence paper.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:01:17 Explainability and Christoph Molnars book on Intepretability&lt;/p&gt;
&lt;p&gt;00:26:45 Explainability - Feature visualisation&lt;/p&gt;
&lt;p&gt;00:33:28 Architecture / CPPNs&lt;/p&gt;
&lt;p&gt;00:36:10 Invariance and data parsimony, priors and experience, manifolds&lt;/p&gt;
&lt;p&gt;00:42:04 What NNs learn / logical view of modern AI (Walid Saba article)&lt;/p&gt;
&lt;p&gt;00:47:10 Core knowledge&lt;/p&gt;
&lt;p&gt;00:55:33 Priors vs experience&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:59:44 Mathematical reasoning&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:01:56 Gary Marcus on GPT-3&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:09:14 Can NNs reason at all?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:18:05 Chollet intelligence paper/ARC challenge&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:25:52</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1600263283285-c1795dc0088ee.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>18</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[SWaV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments (Mathilde Caron)]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Yannic Lightspeed Kicher, Sayak Paul and Ayush Takur interview Mathilde Caron from Facebook Research (FAIR).</p>
<p>We discuss Mathilde's paper which she wrote with her collaborators "SWaV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments" @ https://arxiv.org/pdf/2006.09882.pdf&nbsp;</p>
<p>This paper is the latest unsupervised contrastive visual representations algorithm and has a new data augmentation strategy and also a new online clustering strategy.&nbsp;</p>
<p>Note; Other authors; Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin</p>
<p>Sayak Paul - &nbsp;@RisingSayak / https://www.linkedin.com/in/sayak-paul/</p>
<p>Ayush Thakur - @ayushthakur0</p>
<p>&nbsp;/ https://www.linkedin.com/in/ayush-thakur-731914149/</p>
<p>The article they wrote;</p>
<p>https://app.wandb.ai/authors/swav-tf/reports/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg</p>
<p><br></p>
<p>00:00:00 Yannic probability challenge (CAN YOU SOLVE IT?)</p>
<p>00:01:29 Intro topic (Tim)</p>
<p>00:08:18 Yannic take</p>
<p>00:09:33 Intro show and guests</p>
<p>00:11:29 SWaV elevator pitch&nbsp;</p>
<p>00:17:31 Clustering approach in general</p>
<p>00:21:17 Sayak and Ayush's article on SWaV&nbsp;</p>
<p>00:23:49 Optional transport problem / Sinkhorn-Knopp algorithm</p>
<p>00:31:43 Is clustering a natural approach for this?</p>
<p>00:44:19 Image augmentations&nbsp;</p>
<p>00:46:20 Priors vs experience (data)</p>
<p>00:48:32 Life at FAIR&nbsp;</p>
<p>00:52:33 Progress of image augmentation&nbsp;</p>
<p>00:56:10 When things do not go to plan with research</p>
<p>01:01:04 Question on architecture</p>
<p>01:01:43 SWaV Results</p>
<p>01:06:26 Reproducing Matilde's code</p>
<p>01:14:51 Do we need the whole dataset to set clustering loss</p>
<p>01:16:40 Self-supervised learning and transfer learning</p>
<p>01:23:25 Link to attention mechanism)</p>
<p>01:24:41 Sayak final thought why unsupervised better</p>
<p>01:25:56 Outro</p>
<p><br></p>
<p>Abstract;&nbsp;</p>
<p>"Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks."</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/SWaV-Unsupervised-Learning-of-Visual-Features-by-Contrasting-Cluster-Assignments-Mathilde-Caron-ejj7j9</link>
			<guid isPermaLink="false">77436cb9-a42d-45c0-b918-0bae81677a22</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 14 Sep 2020 00:22:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/19553321/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-14%2Fddff4801-dcae-e6ed-867f-b4bee7a3e51f.mp3" length="84242885" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Yannic Lightspeed Kicher, Sayak Paul and Ayush Takur interview Mathilde Caron from Facebook Research (FAIR).&lt;/p&gt;
&lt;p&gt;We discuss Mathilde&apos;s paper which she wrote with her collaborators &quot;SWaV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments&quot; @ https://arxiv.org/pdf/2006.09882.pdf&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This paper is the latest unsupervised contrastive visual representations algorithm and has a new data augmentation strategy and also a new online clustering strategy.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Note; Other authors; Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin&lt;/p&gt;
&lt;p&gt;Sayak Paul - &amp;nbsp;@RisingSayak / https://www.linkedin.com/in/sayak-paul/&lt;/p&gt;
&lt;p&gt;Ayush Thakur - @ayushthakur0&lt;/p&gt;
&lt;p&gt;&amp;nbsp;/ https://www.linkedin.com/in/ayush-thakur-731914149/&lt;/p&gt;
&lt;p&gt;The article they wrote;&lt;/p&gt;
&lt;p&gt;https://app.wandb.ai/authors/swav-tf/reports/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Yannic probability challenge (CAN YOU SOLVE IT?)&lt;/p&gt;
&lt;p&gt;00:01:29 Intro topic (Tim)&lt;/p&gt;
&lt;p&gt;00:08:18 Yannic take&lt;/p&gt;
&lt;p&gt;00:09:33 Intro show and guests&lt;/p&gt;
&lt;p&gt;00:11:29 SWaV elevator pitch&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:17:31 Clustering approach in general&lt;/p&gt;
&lt;p&gt;00:21:17 Sayak and Ayush&apos;s article on SWaV&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:23:49 Optional transport problem / Sinkhorn-Knopp algorithm&lt;/p&gt;
&lt;p&gt;00:31:43 Is clustering a natural approach for this?&lt;/p&gt;
&lt;p&gt;00:44:19 Image augmentations&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:46:20 Priors vs experience (data)&lt;/p&gt;
&lt;p&gt;00:48:32 Life at FAIR&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:52:33 Progress of image augmentation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:56:10 When things do not go to plan with research&lt;/p&gt;
&lt;p&gt;01:01:04 Question on architecture&lt;/p&gt;
&lt;p&gt;01:01:43 SWaV Results&lt;/p&gt;
&lt;p&gt;01:06:26 Reproducing Matilde&apos;s code&lt;/p&gt;
&lt;p&gt;01:14:51 Do we need the whole dataset to set clustering loss&lt;/p&gt;
&lt;p&gt;01:16:40 Self-supervised learning and transfer learning&lt;/p&gt;
&lt;p&gt;01:23:25 Link to attention mechanism)&lt;/p&gt;
&lt;p&gt;01:24:41 Sayak final thought why unsupervised better&lt;/p&gt;
&lt;p&gt;01:25:56 Outro&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Abstract;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&quot;Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.&quot;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:27:36</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1600042966813-5775ba8ba489f.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>17</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[UK Algoshambles, Neuralink, GPT-3 and Intelligence]]></title>
			<description><![CDATA[<p>This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic "Lightspeed" Kilcher respond to the "Algoshambles" exam fiasco in the UK where the government were forced to step in to standardise the grades which were grossly inflated by the schools. &nbsp;The schools and teachers are all paid on metrics related to the grades received by students, what could possibly go wrong?! The result is that we end up with grades which have lost all their value and students are coached for the exams and don't actually learn the subject. &nbsp;&nbsp;We also cover the second Francois Chollet interview on the Lex Fridman podcast. We cover GPT-3, Neuralink, and discussion of intelligence.</p>
<p><br></p>
<p><a href="https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;t=0s">00:00:00</a> Algoshambles&nbsp;</p>
<p><a href="https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;t=2740s">00:45:40</a> Lex Fridman/Chollet: Intro&nbsp;</p>
<p><a href="https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;t=3321s">00:55:21</a> Lex Fridman/Chollet: Neuralink&nbsp;</p>
<p><a href="https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;t=3988s">01:06:28</a> Lex Fridman/Chollet: GPT-3&nbsp;</p>
<p><a href="https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;t=5023s">01:23:43</a> Lex Fridman/Chollet: Intelligence discussion</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/UK-Algoshambles--Neuralink--GPT-3-and-Intelligence-ej8c11</link>
			<guid isPermaLink="false">174d6fbc-d3e0-46c7-a63c-8e8a7b30385b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 07 Sep 2020 09:40:07 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/19197409/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-8-7%2Fbfb2a61f-5517-e7ca-6490-cfa053cb16fd.mp3" length="92239645" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Dr. Tim Scarfe, Dr. Keith Duggar and Yannic &quot;Lightspeed&quot; Kilcher respond to the &quot;Algoshambles&quot; exam fiasco in the UK where the government were forced to step in to standardise the grades which were grossly inflated by the schools. &amp;nbsp;The schools and teachers are all paid on metrics related to the grades received by students, what could possibly go wrong?! The result is that we end up with grades which have lost all their value and students are coached for the exams and don&apos;t actually learn the subject. &amp;nbsp;&amp;nbsp;We also cover the second Francois Chollet interview on the Lex Fridman podcast. We cover GPT-3, Neuralink, and discussion of intelligence.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;amp;t=0s&quot;&gt;00:00:00&lt;/a&gt; Algoshambles&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;amp;t=2740s&quot;&gt;00:45:40&lt;/a&gt; Lex Fridman/Chollet: Intro&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;amp;t=3321s&quot;&gt;00:55:21&lt;/a&gt; Lex Fridman/Chollet: Neuralink&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;amp;t=3988s&quot;&gt;01:06:28&lt;/a&gt; Lex Fridman/Chollet: GPT-3&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=H3LiiGzyOYI&amp;amp;t=5023s&quot;&gt;01:23:43&lt;/a&gt; Lex Fridman/Chollet: Intelligence discussion&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:34:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1599471615676-be6388e38b76a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>16</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Sayak Paul]]></title>
			<description><![CDATA[<p>This week we spoke with <a href="https://www.linkedin.com/feed/#">Sayak Paul</a>, who is extremely active in the machine learning community. We discussed the AI landscape in India, unsupervised representation learning, data augmentation and contrastive learning, explainability, abstract scene representations and finally pruning and the recent super positions paper. I really enjoyed this conversation and I hope you folks do too!</p>
<p><br></p>
<p>00:00:00 Intro to Sayak</p>
<p>00:17:50 AI landscape in India</p>
<p>00:24:20 Unsupervised representation learning</p>
<p>00:26:11 DATA AUGMENTATION/Contrastive learning</p>
<p>00:59:20 EXPLAINABILITY</p>
<p>01:12:10 ABSTRACT SCENE REPRESENTATIONS</p>
<p>01:14:50 PRUNING and super position paper</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Sayak-Paul-egs2c4</link>
			<guid isPermaLink="false">efb98fb7-f4c7-456f-8ad7-44a7cff94515</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 17 Jul 2020 10:04:50 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/16697156/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-6-17%2F3cb6d557-e479-843e-52c8-c2e4148cca73.mp3" length="92425861" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week we spoke with &lt;a href=&quot;https://www.linkedin.com/feed/#&quot;&gt;Sayak Paul&lt;/a&gt;, who is extremely active in the machine learning community. We discussed the AI landscape in India, unsupervised representation learning, data augmentation and contrastive learning, explainability, abstract scene representations and finally pruning and the recent super positions paper. I really enjoyed this conversation and I hope you folks do too!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro to Sayak&lt;/p&gt;
&lt;p&gt;00:17:50 AI landscape in India&lt;/p&gt;
&lt;p&gt;00:24:20 Unsupervised representation learning&lt;/p&gt;
&lt;p&gt;00:26:11 DATA AUGMENTATION/Contrastive learning&lt;/p&gt;
&lt;p&gt;00:59:20 EXPLAINABILITY&lt;/p&gt;
&lt;p&gt;01:12:10 ABSTRACT SCENE REPRESENTATIONS&lt;/p&gt;
&lt;p&gt;01:14:50 PRUNING and super position paper&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:36:16</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1594980298862-c4bd546758b45.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>15</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Robert Lange on NN Pruning and Collective Intelligence]]></title>
			<description><![CDATA[<p>We speak with Robert Lange!</p>
<p>Robert is a PhD student at the Technical University Berlin. His research combines Deep Multi-Agent Reinforcement Learning and Cognitive Science to study the learning dynamics of large collectives. He has a brilliant blog where he distils and explains cutting edge ML research. We spoke about his story, economics, multi-agent RL, intelligence and AGI, and his recent article summarising the state of the art in neural network pruning.&nbsp;</p>
<p>Robert's article on pruning in NNs https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/</p>
<p><br></p>
<p>00:00:00 Intro</p>
<p>00:04:17 Show start and intro to Robert</p>
<p>00:11:39 Economics background&nbsp;</p>
<p>00:27:20 Intrinsic motivation&nbsp;</p>
<p>00:33:22 Intelligence/consciousness</p>
<p>00:48:16 Lottery ticket/pruning article discussion</p>
<p>01:43:21 Robert's advice for younger self and state of deep learning</p>
<p><br></p>
<p>Robert's LinkedIn: https://www.linkedin.com/in/robert-tjarko-lange-19539a12a/</p>
<p>@RobertTLange</p>
<p>#machinelearning #deeplearning</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Robert-Lange-on-NN-Pruning-and-Collective-Intelligence-egfb3v</link>
			<guid isPermaLink="false">0896bd40-d4e5-4304-afdf-b8648bd52917</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 08 Jul 2020 12:27:54 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/16280127/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fstaging%2F2020-6-8%2Fc3d59f03-d389-7237-2c3d-d4422d4040a6.mp3" length="102831361" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We speak with Robert Lange!&lt;/p&gt;
&lt;p&gt;Robert is a PhD student at the Technical University Berlin. His research combines Deep Multi-Agent Reinforcement Learning and Cognitive Science to study the learning dynamics of large collectives. He has a brilliant blog where he distils and explains cutting edge ML research. We spoke about his story, economics, multi-agent RL, intelligence and AGI, and his recent article summarising the state of the art in neural network pruning.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Robert&apos;s article on pruning in NNs https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:04:17 Show start and intro to Robert&lt;/p&gt;
&lt;p&gt;00:11:39 Economics background&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:27:20 Intrinsic motivation&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:33:22 Intelligence/consciousness&lt;/p&gt;
&lt;p&gt;00:48:16 Lottery ticket/pruning article discussion&lt;/p&gt;
&lt;p&gt;01:43:21 Robert&apos;s advice for younger self and state of deep learning&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Robert&apos;s LinkedIn: https://www.linkedin.com/in/robert-tjarko-lange-19539a12a/&lt;/p&gt;
&lt;p&gt;@RobertTLange&lt;/p&gt;
&lt;p&gt;#machinelearning #deeplearning&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:45:41</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1594211282861-1c025ae5df24a.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>14</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[WelcomeAIOverlords (Zak Jost)]]></title>
			<description><![CDATA[<p>We welcome Zak Jost from the WelcomeAIOverlords channel. Zak is an ML research scientist at Amazon. He has a great blog at http://blog.zakjost.com and also a Discord channel at https://discord.gg/xh2chKX</p>
<p>WelcomeAIOverlords: https://www.youtube.com/channel/UCxw9_WYmLqlj5PyXu2AWU_g&nbsp;</p>
<p>00:00:00 INTRO START</p>
<p>00:01:07 MAIN SHOW START</p>
<p>00:01:59 ZAK'S STORY</p>
<p>00:05:06 YOUTUBE DISCUSSION</p>
<p>00:24:12 UNDERSTANDING PAPERS</p>
<p>00:29:53 CONTRASTIVE LEARNING INTRO</p>
<p>00:33:00 BRING YOUR OWN LATENT PAPER</p>
<p>01:03:13 GRAPHS IN ML AND KNOWLEDGE GRAPHS&nbsp;</p>
<p>01:21:36 GRAPH USE CASES - FRAUD</p>
<p>01:30:15 KNOWLEDGE GRAPHS</p>
<p>01:34:22 GRAPHS IN ML</p>
<p>01:38:53 AUTOMATED ML</p>
<p>01:57:32 OUTRO</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/WelcomeAIOverlords-Zak-Jost-eg4fnd</link>
			<guid isPermaLink="false">aa5b43e9-a202-4617-8fbe-d501bc353b6b</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 30 Jun 2020 12:39:58 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/15924397/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-30%2F86511009-48000-2-8f90cc71d5a58.mp3" length="114049663" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We welcome Zak Jost from the WelcomeAIOverlords channel. Zak is an ML research scientist at Amazon. He has a great blog at http://blog.zakjost.com and also a Discord channel at https://discord.gg/xh2chKX&lt;/p&gt;
&lt;p&gt;WelcomeAIOverlords: https://www.youtube.com/channel/UCxw9_WYmLqlj5PyXu2AWU_g&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:00:00 INTRO START&lt;/p&gt;
&lt;p&gt;00:01:07 MAIN SHOW START&lt;/p&gt;
&lt;p&gt;00:01:59 ZAK&apos;S STORY&lt;/p&gt;
&lt;p&gt;00:05:06 YOUTUBE DISCUSSION&lt;/p&gt;
&lt;p&gt;00:24:12 UNDERSTANDING PAPERS&lt;/p&gt;
&lt;p&gt;00:29:53 CONTRASTIVE LEARNING INTRO&lt;/p&gt;
&lt;p&gt;00:33:00 BRING YOUR OWN LATENT PAPER&lt;/p&gt;
&lt;p&gt;01:03:13 GRAPHS IN ML AND KNOWLEDGE GRAPHS&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:21:36 GRAPH USE CASES - FRAUD&lt;/p&gt;
&lt;p&gt;01:30:15 KNOWLEDGE GRAPHS&lt;/p&gt;
&lt;p&gt;01:34:22 GRAPHS IN ML&lt;/p&gt;
&lt;p&gt;01:38:53 AUTOMATED ML&lt;/p&gt;
&lt;p&gt;01:57:32 OUTRO&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:58:02</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1593520840935-91a4af16962d2.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>14</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Facebook Research - Unsupervised Translation of Programming Languages]]></title>
			<description><![CDATA[<p>In this episode of Machine Learning Street Talk Dr. Tim Scarfe, Yannic Kilcher and Connor Shorten spoke with Marie-Anne Lachaux, Baptiste Roziere and Dr. Guillaume Lample from Facebook Research (FAIR) in Paris. They recently released the paper "Unsupervised Translation of Programming Languages" which was an exciting new approach to learned translation of programming languages (learned transcoder) using an unsupervised encoder trained on individual monolingual corpora i.e. no parallel language data needed. The trick they used what that there is significant token overlap when using word-piece embeddings. It was incredible to talk with this talented group of researchers and I hope you enjoy the conversation too.&nbsp;</p>
<p>Yannic's video on this got watched over 120K times! Check it out too https://www.youtube.com/watch?v=xTzFJIknh7E</p>
<p>Paper https://arxiv.org/abs/2006.03511;&nbsp;</p>
<p>Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample</p>
<p>Abstract;</p>
<p>"A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin."</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Facebook-Research---Unsupervised-Translation-of-Programming-Languages-efs7bj</link>
			<guid isPermaLink="false">e649861f-6a0a-4cc8-9831-7906d51ec65c</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 24 Jun 2020 16:50:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/15653683/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-24%2F84969626-48000-2-3694d8a694668.mp3" length="60542749" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this episode of Machine Learning Street Talk Dr. Tim Scarfe, Yannic Kilcher and Connor Shorten spoke with Marie-Anne Lachaux, Baptiste Roziere and Dr. Guillaume Lample from Facebook Research (FAIR) in Paris. They recently released the paper &quot;Unsupervised Translation of Programming Languages&quot; which was an exciting new approach to learned translation of programming languages (learned transcoder) using an unsupervised encoder trained on individual monolingual corpora i.e. no parallel language data needed. The trick they used what that there is significant token overlap when using word-piece embeddings. It was incredible to talk with this talented group of researchers and I hope you enjoy the conversation too.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Yannic&apos;s video on this got watched over 120K times! Check it out too https://www.youtube.com/watch?v=xTzFJIknh7E&lt;/p&gt;
&lt;p&gt;Paper https://arxiv.org/abs/2006.03511;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume Lample&lt;/p&gt;
&lt;p&gt;Abstract;&lt;/p&gt;
&lt;p&gt;&quot;A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.&quot;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:02:33</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1593017449352-c7efd93a26227.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>12</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Francois Chollet - On the Measure of Intelligence]]></title>
			<description><![CDATA[<p>We cover Francois Chollet's recent paper.</p>
<p>Abstract; To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Francois-Chollet---On-the-Measure-of-Intelligence-efkbcp</link>
			<guid isPermaLink="false">61afba5f-e133-4fca-b7a5-ed103ada3327</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 19 Jun 2020 00:35:10 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/15395673/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-19%2F83504865-48000-2-c2d94a36d34b2.mp3" length="147650449" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;We cover Francois Chollet&apos;s recent paper.&lt;/p&gt;
&lt;p&gt;Abstract; To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to &quot;buy&quot; arbitrary levels of skills for a system, in a way that masks the system&apos;s own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:33:31</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1592526919087-07480084bfd65.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>11</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[OpenAI GPT-3: Language Models are Few-Shot Learners]]></title>
			<description><![CDATA[<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten discuss their takeaways from OpenAI’s GPT-3 language model. With the help of Microsoft’s ZeRO-2 / DeepSpeed optimiser, OpenAI trained an 175 BILLION parameter autoregressive language model. The paper demonstrates how self-supervised language modelling at this scale can perform many downstream tasks without fine-tuning.</p>
<p><br></p>
<p>00:00:00 Intro</p>
<p>00:00:54 ZeRO1+2 (model + Data parallelism) (Connor)</p>
<p>00:03:17 Recent history of NLP (Tim)</p>
<p>00:06:04 Yannic "Light-speed" Kilcher's brief overview of GPT-3</p>
<p>00:14:25 Reviewing Yannic's YT comments on his GPT-3 video (Tim)</p>
<p>00:20:26 Main show intro</p>
<p>00:23:03 Is GPT-3 reasoning?&nbsp;</p>
<p>00:28:15 Architecture discussion and autoregressive (GPT*) vs denoising autoencoder (BERT)</p>
<p>00:36:18 Utility of GPT-3 in industry</p>
<p>00:43:03 Can GPT-3 do math? (reasoning/system 1/system 2)</p>
<p>00:51:03 Generalisation</p>
<p>00:56:48 Esoterics of language models</p>
<p>00:58:46 Architectural trade-offs</p>
<p>01:07:37 Memorization machines and intepretability</p>
<p>01:17:16 Nearest neighbour probes / watermarks</p>
<p>01:20:03 YouTube comments on GPT-3 video&nbsp;</p>
<p>01:21:50 GPT-3 news article generation issue</p>
<p>01:27:36 Sampling data for language models / bias / fairness / politics</p>
<p>01:51:12 Outro</p>
<p><br></p>
<p>These paradigms of task adaptation are divided into zero, one, and few shot learning. Zero-shot learning is a very extreme case where we expect a language model to perform a task such as sentiment classification or extractive question answering, without any additional supervision. One and Few-shot learning provide some examples to the model. However, GPT-3s definition of this diverges a bit from the conventional literature. GPT-3 provides one and few-shot examples in the form of “In-Context Learning”. Instead of fine-tuning the model on a few examples, the model has to use the input to infer the downstream task. For example, the GPT-3 transformer has an input sequence of 2048 tokens, so demonstrations of a task such as yelp sentiment reviews, would have to fit in this input sequence as well as the new review.</p>
<p><br></p>
<p>Thanks for watching! Please Subscribe!</p>
<p>Paper Links:</p>
<p>GPT-3: https://arxiv.org/abs/2005.14165</p>
<p>ZeRO: https://arxiv.org/abs/1910.02054</p>
<p>ZeRO (Blog Post): https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</p>
<p>ZeRO-2 (Blog Post): https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/?OCID=msr_blog_deepspeed2_build_tw</p>
<p><br></p>
<p>#machinelearning #naturallanguageprocessing #deeplearning #gpt3</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/OpenAI-GPT-3-Language-Models-are-Few-Shot-Learners-ef39ef</link>
			<guid isPermaLink="false">cb3044a4-657d-435b-9f10-0d85b74e347d</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 06 Jun 2020 23:42:04 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14836623/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-6%2F80107678-44100-2-e8d0859bdb673.mp3" length="161416568" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten discuss their takeaways from OpenAI’s GPT-3 language model. With the help of Microsoft’s ZeRO-2 / DeepSpeed optimiser, OpenAI trained an 175 BILLION parameter autoregressive language model. The paper demonstrates how self-supervised language modelling at this scale can perform many downstream tasks without fine-tuning.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro&lt;/p&gt;
&lt;p&gt;00:00:54 ZeRO1+2 (model + Data parallelism) (Connor)&lt;/p&gt;
&lt;p&gt;00:03:17 Recent history of NLP (Tim)&lt;/p&gt;
&lt;p&gt;00:06:04 Yannic &quot;Light-speed&quot; Kilcher&apos;s brief overview of GPT-3&lt;/p&gt;
&lt;p&gt;00:14:25 Reviewing Yannic&apos;s YT comments on his GPT-3 video (Tim)&lt;/p&gt;
&lt;p&gt;00:20:26 Main show intro&lt;/p&gt;
&lt;p&gt;00:23:03 Is GPT-3 reasoning?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:28:15 Architecture discussion and autoregressive (GPT*) vs denoising autoencoder (BERT)&lt;/p&gt;
&lt;p&gt;00:36:18 Utility of GPT-3 in industry&lt;/p&gt;
&lt;p&gt;00:43:03 Can GPT-3 do math? (reasoning/system 1/system 2)&lt;/p&gt;
&lt;p&gt;00:51:03 Generalisation&lt;/p&gt;
&lt;p&gt;00:56:48 Esoterics of language models&lt;/p&gt;
&lt;p&gt;00:58:46 Architectural trade-offs&lt;/p&gt;
&lt;p&gt;01:07:37 Memorization machines and intepretability&lt;/p&gt;
&lt;p&gt;01:17:16 Nearest neighbour probes / watermarks&lt;/p&gt;
&lt;p&gt;01:20:03 YouTube comments on GPT-3 video&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:21:50 GPT-3 news article generation issue&lt;/p&gt;
&lt;p&gt;01:27:36 Sampling data for language models / bias / fairness / politics&lt;/p&gt;
&lt;p&gt;01:51:12 Outro&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;These paradigms of task adaptation are divided into zero, one, and few shot learning. Zero-shot learning is a very extreme case where we expect a language model to perform a task such as sentiment classification or extractive question answering, without any additional supervision. One and Few-shot learning provide some examples to the model. However, GPT-3s definition of this diverges a bit from the conventional literature. GPT-3 provides one and few-shot examples in the form of “In-Context Learning”. Instead of fine-tuning the model on a few examples, the model has to use the input to infer the downstream task. For example, the GPT-3 transformer has an input sequence of 2048 tokens, so demonstrations of a task such as yelp sentiment reviews, would have to fit in this input sequence as well as the new review.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thanks for watching! Please Subscribe!&lt;/p&gt;
&lt;p&gt;Paper Links:&lt;/p&gt;
&lt;p&gt;GPT-3: https://arxiv.org/abs/2005.14165&lt;/p&gt;
&lt;p&gt;ZeRO: https://arxiv.org/abs/1910.02054&lt;/p&gt;
&lt;p&gt;ZeRO (Blog Post): https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&lt;/p&gt;
&lt;p&gt;ZeRO-2 (Blog Post): https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/?OCID=msr_blog_deepspeed2_build_tw&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#machinelearning #naturallanguageprocessing #deeplearning #gpt3&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:51:37</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1591486932486-9a95863a6e9eb.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>9</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Jordan Edwards: ML Engineering and DevOps on AzureML]]></title>
			<description><![CDATA[<p>This week we had a super insightful conversation with &nbsp;Jordan Edwards, Principal Program Manager for the AzureML team! &nbsp;Jordan is on the coalface of turning machine learning software engineering into a reality for some of Microsoft's largest customers.&nbsp;</p>
<p>ML DevOps is all about increasing the velocity of- and orchastrating the non-interactive phase of- software deployments for ML. We cover ML DevOps and Microsoft Azure ML. We discuss model governance, testing, intepretability, tooling. We cover the age-old discussion of the dichotomy between science and engineering and how you can bridge the gap with ML DevOps. We cover Jordan's maturity model for ML DevOps.&nbsp;</p>
<p>We also cover off some of the exciting ML announcments from the recent Microsoft Build conference i.e. FairLearn, IntepretML, SEAL, WhiteNoise, OpenAI code generation, OpenAI GPT-3.&nbsp;</p>
<p>00:00:04 Introduction to ML DevOps and Microsoft Build ML Announcements</p>
<p>00:10:29 Main show kick-off</p>
<p>00:11:06 Jordan's story</p>
<p>00:14:36 Typical ML DevOps workflow</p>
<p>00:17:38 Tim's articulation of ML DevOps</p>
<p>00:19:31 Intepretability / Fairness</p>
<p>00:24:31 Testing / Robustness</p>
<p>00:28:10 Using GANs to generate testing data</p>
<p>00:30:26 Gratuitous DL?</p>
<p>00:33:46 Challenges of making an ML DevOps framework / IaaS</p>
<p>00:38:48 Cultural battles in ML DevOps</p>
<p>00:43:04 Maturity Model for Ml DevOps</p>
<p>00:49:19 ML: High interest credit card of technical debt paper</p>
<p>00:50:19 ML Engineering at Microsoft</p>
<p>01:01:20 ML Flow</p>
<p>01:03:05 Company-wide governance&nbsp;</p>
<p>01:08:15 What's coming next</p>
<p>01:12:10 Jordan's hillarious piece of advice for his younger self</p>
<p><br></p>
<p>Super happy with how this turned out, this is not one to miss folks!&nbsp;</p>
<p>#deeplearning #machinelearning #devops #mldevops</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Jordan-Edwards-ML-Engineering-and-DevOps-on-AzureML-eeu0rs</link>
			<guid isPermaLink="false">7f482252-675d-41dd-9d5e-c0e552c7e466</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Wed, 03 Jun 2020 00:31:55 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14663996/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-3%2F79031753-44100-2-7454e8a63f806.mp3" length="105321694" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week we had a super insightful conversation with &amp;nbsp;Jordan Edwards, Principal Program Manager for the AzureML team! &amp;nbsp;Jordan is on the coalface of turning machine learning software engineering into a reality for some of Microsoft&apos;s largest customers.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;ML DevOps is all about increasing the velocity of- and orchastrating the non-interactive phase of- software deployments for ML. We cover ML DevOps and Microsoft Azure ML. We discuss model governance, testing, intepretability, tooling. We cover the age-old discussion of the dichotomy between science and engineering and how you can bridge the gap with ML DevOps. We cover Jordan&apos;s maturity model for ML DevOps.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;We also cover off some of the exciting ML announcments from the recent Microsoft Build conference i.e. FairLearn, IntepretML, SEAL, WhiteNoise, OpenAI code generation, OpenAI GPT-3.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:00:04 Introduction to ML DevOps and Microsoft Build ML Announcements&lt;/p&gt;
&lt;p&gt;00:10:29 Main show kick-off&lt;/p&gt;
&lt;p&gt;00:11:06 Jordan&apos;s story&lt;/p&gt;
&lt;p&gt;00:14:36 Typical ML DevOps workflow&lt;/p&gt;
&lt;p&gt;00:17:38 Tim&apos;s articulation of ML DevOps&lt;/p&gt;
&lt;p&gt;00:19:31 Intepretability / Fairness&lt;/p&gt;
&lt;p&gt;00:24:31 Testing / Robustness&lt;/p&gt;
&lt;p&gt;00:28:10 Using GANs to generate testing data&lt;/p&gt;
&lt;p&gt;00:30:26 Gratuitous DL?&lt;/p&gt;
&lt;p&gt;00:33:46 Challenges of making an ML DevOps framework / IaaS&lt;/p&gt;
&lt;p&gt;00:38:48 Cultural battles in ML DevOps&lt;/p&gt;
&lt;p&gt;00:43:04 Maturity Model for Ml DevOps&lt;/p&gt;
&lt;p&gt;00:49:19 ML: High interest credit card of technical debt paper&lt;/p&gt;
&lt;p&gt;00:50:19 ML Engineering at Microsoft&lt;/p&gt;
&lt;p&gt;01:01:20 ML Flow&lt;/p&gt;
&lt;p&gt;01:03:05 Company-wide governance&amp;nbsp;&lt;/p&gt;
&lt;p&gt;01:08:15 What&apos;s coming next&lt;/p&gt;
&lt;p&gt;01:12:10 Jordan&apos;s hillarious piece of advice for his younger self&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Super happy with how this turned out, this is not one to miss folks!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;#deeplearning #machinelearning #devops #mldevops&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:12:45</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1591144323504-b47aeea3e1863.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>9</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[One Shot and Metric Learning - Quadruplet Loss (Machine Learning Dojo)]]></title>
			<description><![CDATA[<p>*Note this is an episode from Tim's Machine Learning Dojo YouTube channel.&nbsp;</p>
<p>Join Eric Craeymeersch on a wonderful discussion all about ML engineering, computer vision, siamese networks, contrastive loss, one shot learning and metric learning.&nbsp;</p>
<p>00:00:00 Introduction&nbsp;</p>
<p>00:11:47 ML Engineering Discussion</p>
<p>00:35:59 Intro to the main topic</p>
<p>00:42:13 Siamese Networks</p>
<p>00:48:36 Mining strategies</p>
<p>00:51:15 Contrastive Loss</p>
<p>00:57:44 Trip loss paper</p>
<p>01:09:35 Quad loss paper</p>
<p>01:25:49 Eric's Quadloss Medium Article&nbsp;</p>
<p>02:17:32 Metric learning reality check</p>
<p>02:21:06 Engineering discussion II</p>
<p>02:26:22 Outro</p>
<p>In our second paper review call, Tess Ferrandez covered off the FaceNet paper from Google which was a one-shot siamese network with the so called triplet loss. It was an interesting change of direction for NN architecture i.e. using a contrastive loss instead of having a fixed number of output classes. Contrastive architectures have been taking over the ML landscape recently i.e. SimCLR, MOCO, BERT.&nbsp;</p>
<p>Eric wrote an article about this at the time: https://medium.com/@crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352&nbsp;</p>
<p>He then discovered there was a new approach to one shot learning in vision using a quadruplet loss and metric learning. Eric wrote a new article and several experiments on this @ https://medium.com/@crimy/beyond-triplet-loss-one-shot-learning-experiments-with-quadruplet-loss-16671ed51290?source=friends_link&amp;sk=bf41673664ad8a52e322380f2a456e8b</p>
<p>Paper details:&nbsp;</p>
<p>Beyond triplet loss: a deep quadruplet network for person re-identification</p>
<p>https://arxiv.org/abs/1704.01719 (Chen at al '17)</p>
<p>"Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method."</p>
<p>Original facenet paper;&nbsp;</p>
<p>https://arxiv.org/abs/1503.03832</p>
<p>#deeplearning #machinelearning</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/One-Shot-and-Metric-Learning---Quadruplet-Loss-Machine-Learning-Dojo-eet4kb</link>
			<guid isPermaLink="false">7b3edf0f-52bf-45f0-b2b8-d8283a8d69d6</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 02 Jun 2020 11:30:49 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14635083/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-5-2%2F78855008-44100-2-15cfdc796862e.mp3" length="142565921" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;*Note this is an episode from Tim&apos;s Machine Learning Dojo YouTube channel.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Join Eric Craeymeersch on a wonderful discussion all about ML engineering, computer vision, siamese networks, contrastive loss, one shot learning and metric learning.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:00:00 Introduction&amp;nbsp;&lt;/p&gt;
&lt;p&gt;00:11:47 ML Engineering Discussion&lt;/p&gt;
&lt;p&gt;00:35:59 Intro to the main topic&lt;/p&gt;
&lt;p&gt;00:42:13 Siamese Networks&lt;/p&gt;
&lt;p&gt;00:48:36 Mining strategies&lt;/p&gt;
&lt;p&gt;00:51:15 Contrastive Loss&lt;/p&gt;
&lt;p&gt;00:57:44 Trip loss paper&lt;/p&gt;
&lt;p&gt;01:09:35 Quad loss paper&lt;/p&gt;
&lt;p&gt;01:25:49 Eric&apos;s Quadloss Medium Article&amp;nbsp;&lt;/p&gt;
&lt;p&gt;02:17:32 Metric learning reality check&lt;/p&gt;
&lt;p&gt;02:21:06 Engineering discussion II&lt;/p&gt;
&lt;p&gt;02:26:22 Outro&lt;/p&gt;
&lt;p&gt;In our second paper review call, Tess Ferrandez covered off the FaceNet paper from Google which was a one-shot siamese network with the so called triplet loss. It was an interesting change of direction for NN architecture i.e. using a contrastive loss instead of having a fixed number of output classes. Contrastive architectures have been taking over the ML landscape recently i.e. SimCLR, MOCO, BERT.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Eric wrote an article about this at the time: https://medium.com/@crimy/one-shot-learning-siamese-networks-and-triplet-loss-with-keras-2885ed022352&amp;nbsp;&lt;/p&gt;
&lt;p&gt;He then discovered there was a new approach to one shot learning in vision using a quadruplet loss and metric learning. Eric wrote a new article and several experiments on this @ https://medium.com/@crimy/beyond-triplet-loss-one-shot-learning-experiments-with-quadruplet-loss-16671ed51290?source=friends_link&amp;amp;sk=bf41673664ad8a52e322380f2a456e8b&lt;/p&gt;
&lt;p&gt;Paper details:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Beyond triplet loss: a deep quadruplet network for person re-identification&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1704.01719 (Chen at al &apos;17)&lt;/p&gt;
&lt;p&gt;&quot;Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.&quot;&lt;/p&gt;
&lt;p&gt;Original facenet paper;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;https://arxiv.org/abs/1503.03832&lt;/p&gt;
&lt;p&gt;#deeplearning #machinelearning&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:28:30</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1591097458356-cf1c5d5333aba.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>8</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Harri Valpola: System 2 AI and Planning in Model-Based Reinforcement Learning]]></title>
			<description><![CDATA[<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten interviewed Harri Valpola, CEO and Founder of Curious AI. We continued our discussion of System 1 and System 2 thinking in Deep Learning, as well as miscellaneous topics around Model-based Reinforcement Learning. Dr. Valpola describes some of the challenges of modelling industrial control processes such as water sewage filters and paper mills with the use of model-based RL. Dr. Valpola and his collaborators recently published “Regularizing Trajectory Optimization with Denoising Autoencoders” that addresses some of the concerns of planning algorithms that exploit inaccuracies in their world models!</p>
<p><br></p>
<p>00:00:00 Intro to Harri and Curious AI System1/System 2</p>
<p>00:04:50 Background on model-based RL challenges from Tim</p>
<p>00:06:26 Other interesting research papers on model-based RL from Connor</p>
<p>00:08:36 Intro to Curious AI recent NeurIPS paper on model-based RL and denoising autoencoders from Yannic</p>
<p>00:21:00 Main show kick off, system 1/2</p>
<p>00:31:50 Where does the simulator come from?</p>
<p>00:33:59 Evolutionary priors</p>
<p>00:37:17 Consciousness</p>
<p>00:40:37 How does one build a company like Curious AI?</p>
<p>00:46:42 Deep Q Networks</p>
<p>00:49:04 Planning and Model based RL</p>
<p>00:53:04 Learning good representations</p>
<p>00:55:55 Typical problem Curious AI might solve in industry</p>
<p>01:00:56 Exploration</p>
<p>01:08:00 Their paper - regularizing trajectory optimization with denoising</p>
<p>01:13:47 What is Epistemic uncertainty</p>
<p>01:16:44 How would Curious develop these models</p>
<p>01:18:00 Explainability and simulations</p>
<p>01:22:33 How system 2 works in humans</p>
<p>01:26:11 Planning</p>
<p>01:27:04 Advice for starting an AI company</p>
<p>01:31:31 Real world implementation of planning models</p>
<p>01:33:49 Publishing research and openness</p>
<p><br></p>
<p>We really hope you enjoy this episode, please subscribe!</p>
<p><br></p>
<p>Regularizing Trajectory Optimization with Denoising Autoencoders: https://papers.nips.cc/paper/8552-regularizing-trajectory-optimization-with-denoising-autoencoders.pdf</p>
<p>Pulp, Paper &amp; Packaging: A Future Transformed through Deep Learning: https://thecuriousaicompany.com/pulp-paper-packaging-a-future-transformed-through-deep-learning/</p>
<p>Curious AI: https://thecuriousaicompany.com/</p>
<p>Harri Valpola Publications: https://scholar.google.com/citations?user=1uT7-84AAAAJ&amp;hl=en&amp;oi=ao</p>
<p>Some interesting papers around Model-Based RL:</p>
<p>GameGAN: https://cdn.arstechnica.net/wp-content/uploads/2020/05/Nvidia_GameGAN_Research.pdf</p>
<p>Plan2Explore: https://ramanans1.github.io/plan2explore/</p>
<p>World Models: https://worldmodels.github.io/</p>
<p>MuZero: https://arxiv.org/pdf/1911.08265.pdf</p>
<p>PlaNet: A Deep Planning Network for RL: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html</p>
<p>Dreamer: Scalable RL using World Models: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html</p>
<p>Model Based RL for Atari: https://arxiv.org/pdf/1903.00374.pdf</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Harri-Valpola-System-2-AI-and-Planning-in-Model-Based-Reinforcement-Learning-eehij7</link>
			<guid isPermaLink="false">533b0bec-d3fd-4e57-a110-30e634d452f1</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Mon, 25 May 2020 11:00:36 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14256167/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-25%2F76442340-44100-2-3294b91fdb2.mp3" length="142193901" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten interviewed Harri Valpola, CEO and Founder of Curious AI. We continued our discussion of System 1 and System 2 thinking in Deep Learning, as well as miscellaneous topics around Model-based Reinforcement Learning. Dr. Valpola describes some of the challenges of modelling industrial control processes such as water sewage filters and paper mills with the use of model-based RL. Dr. Valpola and his collaborators recently published “Regularizing Trajectory Optimization with Denoising Autoencoders” that addresses some of the concerns of planning algorithms that exploit inaccuracies in their world models!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;00:00:00 Intro to Harri and Curious AI System1/System 2&lt;/p&gt;
&lt;p&gt;00:04:50 Background on model-based RL challenges from Tim&lt;/p&gt;
&lt;p&gt;00:06:26 Other interesting research papers on model-based RL from Connor&lt;/p&gt;
&lt;p&gt;00:08:36 Intro to Curious AI recent NeurIPS paper on model-based RL and denoising autoencoders from Yannic&lt;/p&gt;
&lt;p&gt;00:21:00 Main show kick off, system 1/2&lt;/p&gt;
&lt;p&gt;00:31:50 Where does the simulator come from?&lt;/p&gt;
&lt;p&gt;00:33:59 Evolutionary priors&lt;/p&gt;
&lt;p&gt;00:37:17 Consciousness&lt;/p&gt;
&lt;p&gt;00:40:37 How does one build a company like Curious AI?&lt;/p&gt;
&lt;p&gt;00:46:42 Deep Q Networks&lt;/p&gt;
&lt;p&gt;00:49:04 Planning and Model based RL&lt;/p&gt;
&lt;p&gt;00:53:04 Learning good representations&lt;/p&gt;
&lt;p&gt;00:55:55 Typical problem Curious AI might solve in industry&lt;/p&gt;
&lt;p&gt;01:00:56 Exploration&lt;/p&gt;
&lt;p&gt;01:08:00 Their paper - regularizing trajectory optimization with denoising&lt;/p&gt;
&lt;p&gt;01:13:47 What is Epistemic uncertainty&lt;/p&gt;
&lt;p&gt;01:16:44 How would Curious develop these models&lt;/p&gt;
&lt;p&gt;01:18:00 Explainability and simulations&lt;/p&gt;
&lt;p&gt;01:22:33 How system 2 works in humans&lt;/p&gt;
&lt;p&gt;01:26:11 Planning&lt;/p&gt;
&lt;p&gt;01:27:04 Advice for starting an AI company&lt;/p&gt;
&lt;p&gt;01:31:31 Real world implementation of planning models&lt;/p&gt;
&lt;p&gt;01:33:49 Publishing research and openness&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We really hope you enjoy this episode, please subscribe!&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Regularizing Trajectory Optimization with Denoising Autoencoders: https://papers.nips.cc/paper/8552-regularizing-trajectory-optimization-with-denoising-autoencoders.pdf&lt;/p&gt;
&lt;p&gt;Pulp, Paper &amp;amp; Packaging: A Future Transformed through Deep Learning: https://thecuriousaicompany.com/pulp-paper-packaging-a-future-transformed-through-deep-learning/&lt;/p&gt;
&lt;p&gt;Curious AI: https://thecuriousaicompany.com/&lt;/p&gt;
&lt;p&gt;Harri Valpola Publications: https://scholar.google.com/citations?user=1uT7-84AAAAJ&amp;amp;hl=en&amp;amp;oi=ao&lt;/p&gt;
&lt;p&gt;Some interesting papers around Model-Based RL:&lt;/p&gt;
&lt;p&gt;GameGAN: https://cdn.arstechnica.net/wp-content/uploads/2020/05/Nvidia_GameGAN_Research.pdf&lt;/p&gt;
&lt;p&gt;Plan2Explore: https://ramanans1.github.io/plan2explore/&lt;/p&gt;
&lt;p&gt;World Models: https://worldmodels.github.io/&lt;/p&gt;
&lt;p&gt;MuZero: https://arxiv.org/pdf/1911.08265.pdf&lt;/p&gt;
&lt;p&gt;PlaNet: A Deep Planning Network for RL: https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html&lt;/p&gt;
&lt;p&gt;Dreamer: Scalable RL using World Models: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html&lt;/p&gt;
&lt;p&gt;Model Based RL for Atari: https://arxiv.org/pdf/1903.00374.pdf&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:38:16</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1590404445321-12da3eee7c025.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>7</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ICLR 2020: Yoshua Bengio and the Nature of Consciousness]]></title>
			<description><![CDATA[<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Connor Shorten and Yannic Kilcher react to Yoshua Bengio’s ICLR 2020 Keynote “Deep Learning Priors Associated with Conscious Processing”. Bengio takes on many future directions for research in Deep Learning such as the role of attention in consciousness, sparse factor graphs and causality, and the study of systematic generalization. Bengio also presents big ideas in Intelligence that border on the line of philosophy and practical machine learning. This includes ideas such as consciousness in machines and System 1 and System 2 thinking, as described in Daniel Kahneman’s book “Thinking Fast and Slow”. Similar to Yann LeCun’s half of the 2020 ICLR keynote, this talk takes on many challenging ideas and hopefully this video helps you get a better understanding of some of them! Thanks for watching!&nbsp;</p>
<p>Please Subscribe for more videos!</p>
<p>Paper Links:</p>
<p>Link to Talk: https://iclr.cc/virtual_2020/speaker_7.html</p>
<p>The Consciousness Prior: https://arxiv.org/abs/1709.08568</p>
<p>Thinking Fast and Slow: https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555</p>
<p>Systematic Generalization: https://arxiv.org/abs/1811.12889</p>
<p>CLOSURE: Assessing Systematic Generalization of CLEVR Models: https://arxiv.org/abs/1912.05783</p>
<p>Neural Module Networks: https://arxiv.org/abs/1511.02799</p>
<p>Experience Grounds Language: https://arxiv.org/pdf/2004.10151.pdf</p>
<p>Benchmarking Graph Neural Networks: https://arxiv.org/pdf/2003.00982.pdf</p>
<p>On the Measure of Intelligence: https://arxiv.org/abs/1911.01547</p>
<p>Please check out our individual channels as well!</p>
<p>Machine Learning Dojo with Tim Scarfe: https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA</p>
<p>Yannic Kilcher: https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfe</p>
<p>Henry AI Labs: https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw</p>
<p>00:00:00 Tim and Yannics takes</p>
<p>00:01:37 Intro to Bengio</p>
<p>00:03:13 System 2, language and Chomsky</p>
<p>00:05:58 Cristof Koch on conciousness</p>
<p>00:07:25 Francois Chollet on intelligence and consciousness</p>
<p>00:09:29 Meditation and Sam Harris on consciousness</p>
<p>00:11:35 Connor Intro</p>
<p>00:13:20 Show Main Intro</p>
<p>00:17:55 Priors associated with Conscious Processing</p>
<p>00:26:25 System 1 / System 2</p>
<p>00:42:47 Implicit and Verbalized Knowledge [DONT MISS THIS!]</p>
<p>01:08:24 Inductive Priors for DL 2.0</p>
<p>01:27:20 Systematic Generalization</p>
<p>01:37:53 Contrast with the Symbolic AI Program</p>
<p>01:54:55 Attention</p>
<p>02:00:25 From Attention to Consciousness</p>
<p>02:05:31 Thoughts, Consciousness, Language</p>
<p>02:06:55 Sparse Factor graph</p>
<p>02:10:52 Sparse Change in Abstract Latent Space</p>
<p>02:15:10 Discovering Cause and Effect</p>
<p>02:20:00 Factorize the joint distribution</p>
<p>02:22:30 RIMS: Modular Computation</p>
<p>02:24:30 Conclusion</p>
<p>#machinelearning #deeplearning</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/ICLR-2020-Yoshua-Bengio-and-the-Nature-of-Consciousness-eeecct</link>
			<guid isPermaLink="false">399f019b-16f3-4ba9-8cd8-c5b2daa64502</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 22 May 2020 21:49:38 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14151517/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-22%2F75773970-48000-2-e0411642470b9.mp3" length="149430055" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this episode of Machine Learning Street Talk, Tim Scarfe, Connor Shorten and Yannic Kilcher react to Yoshua Bengio’s ICLR 2020 Keynote “Deep Learning Priors Associated with Conscious Processing”. Bengio takes on many future directions for research in Deep Learning such as the role of attention in consciousness, sparse factor graphs and causality, and the study of systematic generalization. Bengio also presents big ideas in Intelligence that border on the line of philosophy and practical machine learning. This includes ideas such as consciousness in machines and System 1 and System 2 thinking, as described in Daniel Kahneman’s book “Thinking Fast and Slow”. Similar to Yann LeCun’s half of the 2020 ICLR keynote, this talk takes on many challenging ideas and hopefully this video helps you get a better understanding of some of them! Thanks for watching!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Please Subscribe for more videos!&lt;/p&gt;
&lt;p&gt;Paper Links:&lt;/p&gt;
&lt;p&gt;Link to Talk: https://iclr.cc/virtual_2020/speaker_7.html&lt;/p&gt;
&lt;p&gt;The Consciousness Prior: https://arxiv.org/abs/1709.08568&lt;/p&gt;
&lt;p&gt;Thinking Fast and Slow: https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555&lt;/p&gt;
&lt;p&gt;Systematic Generalization: https://arxiv.org/abs/1811.12889&lt;/p&gt;
&lt;p&gt;CLOSURE: Assessing Systematic Generalization of CLEVR Models: https://arxiv.org/abs/1912.05783&lt;/p&gt;
&lt;p&gt;Neural Module Networks: https://arxiv.org/abs/1511.02799&lt;/p&gt;
&lt;p&gt;Experience Grounds Language: https://arxiv.org/pdf/2004.10151.pdf&lt;/p&gt;
&lt;p&gt;Benchmarking Graph Neural Networks: https://arxiv.org/pdf/2003.00982.pdf&lt;/p&gt;
&lt;p&gt;On the Measure of Intelligence: https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;Please check out our individual channels as well!&lt;/p&gt;
&lt;p&gt;Machine Learning Dojo with Tim Scarfe: https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA&lt;/p&gt;
&lt;p&gt;Yannic Kilcher: https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfe&lt;/p&gt;
&lt;p&gt;Henry AI Labs: https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw&lt;/p&gt;
&lt;p&gt;00:00:00 Tim and Yannics takes&lt;/p&gt;
&lt;p&gt;00:01:37 Intro to Bengio&lt;/p&gt;
&lt;p&gt;00:03:13 System 2, language and Chomsky&lt;/p&gt;
&lt;p&gt;00:05:58 Cristof Koch on conciousness&lt;/p&gt;
&lt;p&gt;00:07:25 Francois Chollet on intelligence and consciousness&lt;/p&gt;
&lt;p&gt;00:09:29 Meditation and Sam Harris on consciousness&lt;/p&gt;
&lt;p&gt;00:11:35 Connor Intro&lt;/p&gt;
&lt;p&gt;00:13:20 Show Main Intro&lt;/p&gt;
&lt;p&gt;00:17:55 Priors associated with Conscious Processing&lt;/p&gt;
&lt;p&gt;00:26:25 System 1 / System 2&lt;/p&gt;
&lt;p&gt;00:42:47 Implicit and Verbalized Knowledge [DONT MISS THIS!]&lt;/p&gt;
&lt;p&gt;01:08:24 Inductive Priors for DL 2.0&lt;/p&gt;
&lt;p&gt;01:27:20 Systematic Generalization&lt;/p&gt;
&lt;p&gt;01:37:53 Contrast with the Symbolic AI Program&lt;/p&gt;
&lt;p&gt;01:54:55 Attention&lt;/p&gt;
&lt;p&gt;02:00:25 From Attention to Consciousness&lt;/p&gt;
&lt;p&gt;02:05:31 Thoughts, Consciousness, Language&lt;/p&gt;
&lt;p&gt;02:06:55 Sparse Factor graph&lt;/p&gt;
&lt;p&gt;02:10:52 Sparse Change in Abstract Latent Space&lt;/p&gt;
&lt;p&gt;02:15:10 Discovering Cause and Effect&lt;/p&gt;
&lt;p&gt;02:20:00 Factorize the joint distribution&lt;/p&gt;
&lt;p&gt;02:22:30 RIMS: Modular Computation&lt;/p&gt;
&lt;p&gt;02:24:30 Conclusion&lt;/p&gt;
&lt;p&gt;#machinelearning #deeplearning&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:34:17</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1590184186758-b2aa80f8c19a9.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>6</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[ICLR 2020: Yann LeCun and Energy-Based Models]]></title>
			<description><![CDATA[<p>This week Connor Shorten, Yannic Kilcher and Tim Scarfe reacted to Yann LeCun's keynote speech at this year's ICLR conference which just passed. ICLR is the number two ML conference and was completely open this year, with all the sessions publicly accessible via the internet. Yann spent most of his talk speaking about self-supervised learning, Energy-based models (EBMs) and manifold learning. Don't worry if you hadn't heard of EBMs before, neither had we!</p>
<p>Thanks for watching! Please Subscribe!</p>
<p>Paper Links:</p>
<p>ICLR 2020 Keynote Talk: https://iclr.cc/virtual_2020/speaker_7.html</p>
<p>A Tutorial on Energy-Based Learning: http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf</p>
<p>Concept Learning with Energy-Based Models (Yannic's Explanation): https://www.youtube.com/watch?v=Cs_j-oNwGgg</p>
<p>Concept Learning with Energy-Based Models (Paper): https://arxiv.org/pdf/1811.02486.pdf</p>
<p>Concept Learning with Energy-Based Models (OpenAI Blog Post): https://openai.com/blog/learning-concepts-with-energy-functions/</p>
<p>#deeplearning #machinelearning #iclr #iclr2020 #yannlecun</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/ICLR-2020-Yann-LeCun-and-Energy-Based-Models-ee9ton</link>
			<guid isPermaLink="false">66e500c7-d49e-400c-90f6-e40b1b740426</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 19 May 2020 22:35:39 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14005463/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-19%2F74816439-48000-2-d6ad767cfb623.mp3" length="128653805" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;This week Connor Shorten, Yannic Kilcher and Tim Scarfe reacted to Yann LeCun&apos;s keynote speech at this year&apos;s ICLR conference which just passed. ICLR is the number two ML conference and was completely open this year, with all the sessions publicly accessible via the internet. Yann spent most of his talk speaking about self-supervised learning, Energy-based models (EBMs) and manifold learning. Don&apos;t worry if you hadn&apos;t heard of EBMs before, neither had we!&lt;/p&gt;
&lt;p&gt;Thanks for watching! Please Subscribe!&lt;/p&gt;
&lt;p&gt;Paper Links:&lt;/p&gt;
&lt;p&gt;ICLR 2020 Keynote Talk: https://iclr.cc/virtual_2020/speaker_7.html&lt;/p&gt;
&lt;p&gt;A Tutorial on Energy-Based Learning: http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf&lt;/p&gt;
&lt;p&gt;Concept Learning with Energy-Based Models (Yannic&apos;s Explanation): https://www.youtube.com/watch?v=Cs_j-oNwGgg&lt;/p&gt;
&lt;p&gt;Concept Learning with Energy-Based Models (Paper): https://arxiv.org/pdf/1811.02486.pdf&lt;/p&gt;
&lt;p&gt;Concept Learning with Energy-Based Models (OpenAI Blog Post): https://openai.com/blog/learning-concepts-with-energy-functions/&lt;/p&gt;
&lt;p&gt;#deeplearning #machinelearning #iclr #iclr2020 #yannlecun&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>02:12:11</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1589927747519-669d872ba8d4c.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>5</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[The Lottery Ticket Hypothesis with Jonathan Frankle]]></title>
			<description><![CDATA[<p>In this episode of Machine Learning Street Talk, we chat with Jonathan Frankle, author of The Lottery Ticket Hypothesis. Frankle has continued researching Sparse Neural Networks, Pruning, and Lottery Tickets leading to some really exciting follow-on papers! This chat discusses some of these papers such as Linear Mode Connectivity, Comparing and Rewinding and Fine-tuning in Neural Network Pruning, and more (full list of papers linked below). We also chat about how Jonathan got into Deep Learning research, his Information Diet, and work on developing Technology Policy for Artificial Intelligence!&nbsp;</p>
<p>This was a really fun chat, I hope you enjoy listening to it and learn something from it!</p>
<p>Thanks for watching and please subscribe!</p>
<p>Huge thanks to everyone on r/MachineLearning who asked questions!</p>
<p>Paper Links discussed in the chat:</p>
<p>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks: https://arxiv.org/abs/1803.03635</p>
<p>Linear Mode Connectivity and the Lottery Ticket Hypothesis: https://arxiv.org/abs/1912.05671</p>
<p>Dissecting Pruned Neural Networks: https://arxiv.org/abs/1907.00262</p>
<p>Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs: https://arxiv.org/abs/2003.00152</p>
<p>What is the State of Neural Network Pruning? https://arxiv.org/abs/2003.03033</p>
<p>The Early Phase of Neural Network Training: https://arxiv.org/abs/2002.10365</p>
<p>Comparing Rewinding and Fine-tuning in Neural Network Pruning: https://arxiv.org/abs/2003.02389</p>
<p>(Also Mentioned)</p>
<p>Block-Sparse GPU Kernels: https://openai.com/blog/block-sparse-gpu-kernels/</p>
<p>Balanced Sparsity for Efficient DNN Inference on GPU: https://arxiv.org/pdf/1811.00206.pdf</p>
<p>Playing the Lottery with Rewards and Multiple Languages: Lottery Tickets in RL and NLP: https://arxiv.org/pdf/1906.02768.pdf</p>
<p>r/MachineLearning question list: https://www.reddit.com/r/MachineLearning/comments/g9jqe0/d_lottery_ticket_hypothesis_ask_the_author_a/ (edited)&nbsp;</p>
<p>#machinelearning #deeplearning</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/The-Lottery-Ticket-Hypothesis-with-Jonathan-Frankle-ee9sg4</link>
			<guid isPermaLink="false">c7d6a397-cca5-4b8b-be98-b045e6c21cf0</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 19 May 2020 22:01:49 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/14004164/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-19%2F74809790-48000-2-6758698a387e9.mp3" length="166935465" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this episode of Machine Learning Street Talk, we chat with Jonathan Frankle, author of The Lottery Ticket Hypothesis. Frankle has continued researching Sparse Neural Networks, Pruning, and Lottery Tickets leading to some really exciting follow-on papers! This chat discusses some of these papers such as Linear Mode Connectivity, Comparing and Rewinding and Fine-tuning in Neural Network Pruning, and more (full list of papers linked below). We also chat about how Jonathan got into Deep Learning research, his Information Diet, and work on developing Technology Policy for Artificial Intelligence!&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This was a really fun chat, I hope you enjoy listening to it and learn something from it!&lt;/p&gt;
&lt;p&gt;Thanks for watching and please subscribe!&lt;/p&gt;
&lt;p&gt;Huge thanks to everyone on r/MachineLearning who asked questions!&lt;/p&gt;
&lt;p&gt;Paper Links discussed in the chat:&lt;/p&gt;
&lt;p&gt;The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks: https://arxiv.org/abs/1803.03635&lt;/p&gt;
&lt;p&gt;Linear Mode Connectivity and the Lottery Ticket Hypothesis: https://arxiv.org/abs/1912.05671&lt;/p&gt;
&lt;p&gt;Dissecting Pruned Neural Networks: https://arxiv.org/abs/1907.00262&lt;/p&gt;
&lt;p&gt;Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs: https://arxiv.org/abs/2003.00152&lt;/p&gt;
&lt;p&gt;What is the State of Neural Network Pruning? https://arxiv.org/abs/2003.03033&lt;/p&gt;
&lt;p&gt;The Early Phase of Neural Network Training: https://arxiv.org/abs/2002.10365&lt;/p&gt;
&lt;p&gt;Comparing Rewinding and Fine-tuning in Neural Network Pruning: https://arxiv.org/abs/2003.02389&lt;/p&gt;
&lt;p&gt;(Also Mentioned)&lt;/p&gt;
&lt;p&gt;Block-Sparse GPU Kernels: https://openai.com/blog/block-sparse-gpu-kernels/&lt;/p&gt;
&lt;p&gt;Balanced Sparsity for Efficient DNN Inference on GPU: https://arxiv.org/pdf/1811.00206.pdf&lt;/p&gt;
&lt;p&gt;Playing the Lottery with Rewards and Multiple Languages: Lottery Tickets in RL and NLP: https://arxiv.org/pdf/1906.02768.pdf&lt;/p&gt;
&lt;p&gt;r/MachineLearning question list: https://www.reddit.com/r/MachineLearning/comments/g9jqe0/d_lottery_ticket_hypothesis_ask_the_author_a/ (edited)&amp;nbsp;&lt;/p&gt;
&lt;p&gt;#machinelearning #deeplearning&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:26:43</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1589925717438-7e94cf1acddef.jpg"/>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer]]></title>
			<description><![CDATA[<p>In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten chat about Large-scale Transfer Learning in Natural Language Processing. The Text-to-Text Transfer Transformer (T5) model from Google AI does an exhaustive survey of what’s important for Transfer Learning in NLP and what’s not. In this conversation, we go through the key takeaways of the paper, text-to-text input/output format, architecture choice, dataset size and composition, fine-tuning strategy, and how to best use more computation.</p>
<p>Beginning with these topics, we diverge into exciting ideas such as embodied cognition, meta-learning, and the measure of intelligence. We are still beginning our podcast journey and really appreciate any feedback from our listeners. Is the chat too technical? Do you prefer group discussions, interviewing experts, or chats between the three of us? Thanks for watching and if you haven’t already, Please Subscribe!</p>
<p>Paper Links discussed in the chat:</p>
<p>Text-to-Text Transfer Transformer: https://arxiv.org/abs/1910.10683</p>
<p>Experience Grounds Language (relevant to divergent discussion about embodied cognition): https://arxiv.org/pdf/2004.10151.pdf</p>
<p>On the Measure of Intelligence: https://arxiv.org/abs/1911.01547</p>
<p>Train Large, Then Compress: https://arxiv.org/pdf/2002.11794.pdf</p>
<p>Scaling Laws for Neural Language Models: https://arxiv.org/pdf/2001.08361.pdf</p>
<p>The Illustrated Transformer: http://jalammar.github.io/illustrated...</p>
<p>ELECTRA: https://arxiv.org/pdf/2003.10555.pdf</p>
<p>Transformer-XL: https://arxiv.org/pdf/1901.02860.pdf</p>
<p>Reformer: The Efficient Transformer: https://openreview.net/pdf?id=rkgNKkHtvB</p>
<p>The Evolved Transformer: https://arxiv.org/pdf/1901.11117.pdf</p>
<p>DistilBERT: https://arxiv.org/pdf/1910.01108.pdf</p>
<p>How to generate text (HIGHLY RECOMMEND): https://huggingface.co/blog/how-to-ge...</p>
<p>Tokenizers: https://blog.floydhub.com/tokenization-nlp/</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Exploring-the-Limits-of-Transfer-Learning-with-a-Unified-Text-to-Text-Transformer-ee9nch</link>
			<guid isPermaLink="false">18628435-2b55-40e9-9e94-5da7f0d7f1fe</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Tue, 19 May 2020 21:34:40 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/13998929/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-19%2F74746811-44100-2-b702b6c9a9d44.mp3" length="192383042" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;In this episode of Machine Learning Street Talk, Tim Scarfe, Yannic Kilcher and Connor Shorten chat about Large-scale Transfer Learning in Natural Language Processing. The Text-to-Text Transfer Transformer (T5) model from Google AI does an exhaustive survey of what’s important for Transfer Learning in NLP and what’s not. In this conversation, we go through the key takeaways of the paper, text-to-text input/output format, architecture choice, dataset size and composition, fine-tuning strategy, and how to best use more computation.&lt;/p&gt;
&lt;p&gt;Beginning with these topics, we diverge into exciting ideas such as embodied cognition, meta-learning, and the measure of intelligence. We are still beginning our podcast journey and really appreciate any feedback from our listeners. Is the chat too technical? Do you prefer group discussions, interviewing experts, or chats between the three of us? Thanks for watching and if you haven’t already, Please Subscribe!&lt;/p&gt;
&lt;p&gt;Paper Links discussed in the chat:&lt;/p&gt;
&lt;p&gt;Text-to-Text Transfer Transformer: https://arxiv.org/abs/1910.10683&lt;/p&gt;
&lt;p&gt;Experience Grounds Language (relevant to divergent discussion about embodied cognition): https://arxiv.org/pdf/2004.10151.pdf&lt;/p&gt;
&lt;p&gt;On the Measure of Intelligence: https://arxiv.org/abs/1911.01547&lt;/p&gt;
&lt;p&gt;Train Large, Then Compress: https://arxiv.org/pdf/2002.11794.pdf&lt;/p&gt;
&lt;p&gt;Scaling Laws for Neural Language Models: https://arxiv.org/pdf/2001.08361.pdf&lt;/p&gt;
&lt;p&gt;The Illustrated Transformer: http://jalammar.github.io/illustrated...&lt;/p&gt;
&lt;p&gt;ELECTRA: https://arxiv.org/pdf/2003.10555.pdf&lt;/p&gt;
&lt;p&gt;Transformer-XL: https://arxiv.org/pdf/1901.02860.pdf&lt;/p&gt;
&lt;p&gt;Reformer: The Efficient Transformer: https://openreview.net/pdf?id=rkgNKkHtvB&lt;/p&gt;
&lt;p&gt;The Evolved Transformer: https://arxiv.org/pdf/1901.11117.pdf&lt;/p&gt;
&lt;p&gt;DistilBERT: https://arxiv.org/pdf/1910.01108.pdf&lt;/p&gt;
&lt;p&gt;How to generate text (HIGHLY RECOMMEND): https://huggingface.co/blog/how-to-ge...&lt;/p&gt;
&lt;p&gt;Tokenizers: https://blog.floydhub.com/tokenization-nlp/&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:40:02</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1589924089469-eed46485db056.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>3</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[CURL: Contrastive Unsupervised Representations for Reinforcement Learning]]></title>
			<description><![CDATA[<p>According to Yann Le Cun, the next big thing in machine learning is unsupervised learning. Self-supervision has changed the entire game in the last few years in deep learning, first transforming the language world with word2vec and BERT -- but now it's turning computer vision upside down.&nbsp;</p>
<p><br></p>
<p>This week Yannic, Connor and I spoke with one of the authors, Aravind Srinivas who recently co-led the hot-off-the-press CURL: Contrastive Unsupervised Representations for Reinforcement Learning alongside Michael (Misha) Laskin. CURL has had an incredible reception in the ML community in the last month or so. Remember the Deep Mind paper which solved the Atari games using the raw pixels? Aravind's approach uses contrastive unsupervised learning to featurise the pixels before applying RL. CURL is the first image-based algorithm to nearly match the sample-efficiency and performance of methods that use state-based features! This is a huge step forwards in being able to apply RL in the real world.&nbsp;</p>
<p><br></p>
<p>We explore RL and self-supervision for computer vision in detail and find out about how Aravind got into machine learning.&nbsp;</p>
<p><br></p>
<p>Original YouTube Video: https://youtu.be/1MprzvYNpY8</p>
<p><br></p>
<p>Paper:</p>
<p>CURL: Contrastive Unsupervised Representations for Reinforcement Learning</p>
<p>Aravind Srinivas, Michael Laskin, Pieter Abbeel</p>
<p>https://arxiv.org/pdf/2004.04136.pdf</p>
<p><br></p>
<p>Yannic's analysis video: https://www.youtube.com/watch?v=hg2Q_O5b9w4&nbsp;</p>
<p><br></p>
<p>#machinelearning #reinforcementlearning #curl #timscarfe #yannickilcher #connorshorten</p>
<p><br></p>
<p>Music credit; https://soundcloud.com/errxrmusic/in-my-mind</p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/CURL-Contrastive-Unsupervised-Representations-for-Reinforcement-Learning-edh6hh</link>
			<guid isPermaLink="false">9dcaf9f8-fb17-4ebb-b0a2-246e010fc96f</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Sat, 02 May 2020 10:40:54 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/13195249/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-4-2%2F69522116-44100-2-2db64b2e73e33.mp3" length="71712182" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;According to Yann Le Cun, the next big thing in machine learning is unsupervised learning. Self-supervision has changed the entire game in the last few years in deep learning, first transforming the language world with word2vec and BERT -- but now it&apos;s turning computer vision upside down.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;This week Yannic, Connor and I spoke with one of the authors, Aravind Srinivas who recently co-led the hot-off-the-press CURL: Contrastive Unsupervised Representations for Reinforcement Learning alongside Michael (Misha) Laskin. CURL has had an incredible reception in the ML community in the last month or so. Remember the Deep Mind paper which solved the Atari games using the raw pixels? Aravind&apos;s approach uses contrastive unsupervised learning to featurise the pixels before applying RL. CURL is the first image-based algorithm to nearly match the sample-efficiency and performance of methods that use state-based features! This is a huge step forwards in being able to apply RL in the real world.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We explore RL and self-supervision for computer vision in detail and find out about how Aravind got into machine learning.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Original YouTube Video: https://youtu.be/1MprzvYNpY8&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Paper:&lt;/p&gt;
&lt;p&gt;CURL: Contrastive Unsupervised Representations for Reinforcement Learning&lt;/p&gt;
&lt;p&gt;Aravind Srinivas, Michael Laskin, Pieter Abbeel&lt;/p&gt;
&lt;p&gt;https://arxiv.org/pdf/2004.04136.pdf&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Yannic&apos;s analysis video: https://www.youtube.com/watch?v=hg2Q_O5b9w4&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#machinelearning #reinforcementlearning #curl #timscarfe #yannickilcher #connorshorten&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Music credit; https://soundcloud.com/errxrmusic/in-my-mind&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:14:42</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1588416209881-a4a1f54a9e5.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>2</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
		<item>
			<title><![CDATA[Exploring Open-Ended Algorithms: POET]]></title>
			<description><![CDATA[<p>Three YouTubers; Tim Scarfe - Machine Learning Dojo (https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA), Connor Shorten - Henry AI Labs (https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw) and Yannic Kilcher (https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew). We made a new YouTube channel called Machine Learning Street Talk. Every week we will talk about the latest and greatest in AI. Subscribe now!</p>
<p>Special guests this week; Dr. Mathew Salvaris (https://www.linkedin.com/in/drmathewsalvaris/), Eric Craeymeersch (https://www.linkedin.com/in/ericcraeymeersch/), Dr. Keith Duggar (https://www.linkedin.com/in/dr-keith-duggar/), &nbsp;Dmitri Soshnikov (https://www.linkedin.com/in/shwars/)</p>
<p>We discuss the new concept of an open-ended, or "AI-Generating" algorithm. Open-endedness is a class of algorithms which generate problems and solutions to increasingly complex and diverse tasks. These algorithms create their own curriculum of learning. Complex tasks become tractable because they are now the final stepping stone in a lineage of progressions. In many respects, it's better to trust the machine to develop the learning curriculum, because the best curriculum might be counter-intuitive. These algorithms can generate a radiating tree of evolving challenges and solutions just like natural evolution. Evolution has produced an eternity of diversity and complexity and even produced human intelligence as a side-effect! Could AI-generating algorithms be the next big thing in machine learning?</p>
<p>Wang, Rui, et al. "Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions." arXiv preprint arXiv:2003.08536 (2020). https://arxiv.org/abs/2003.08536</p>
<p>Wang, Rui, et al. "Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions." arXiv preprint arXiv:1901.01753 (2019). https://arxiv.org/abs/1901.01753</p>
<p>Watch Yannic’s video on POET: https://www.youtube.com/watch?v=8wkgDnNxiVs<br>
and on the extended POET: https://youtu.be/gbG1X8Xq-T8<br>
Watch Connor’s video https://www.youtube.com/watch?v=jxIkPxkN10U<br>
UberAI labs video: https://www.youtube.com/watch?v=RX0sKDRq400 &nbsp;&nbsp;</p>
<p>#reinforcementlearning #machinelearning #uber #deeplearning #rl #timscarfe #connorshorten #yannickilcher</p>
<p><br></p>
]]></description>
			<link>https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Exploring-Open-Ended-Algorithms-POET-ed6lba</link>
			<guid isPermaLink="false">5ccf7b37-bbf4-4b37-80e3-b42b624eaad9</guid>
			<dc:creator><![CDATA[Machine Learning Street Talk (MLST)]]></dc:creator>
			<pubDate>Fri, 24 Apr 2020 10:51:55 GMT</pubDate>
			<enclosure url="https://anchor.fm/s/1e4a0eac/podcast/play/12849962/https%3A%2F%2Fd3ctxlq1ktw2nl.cloudfront.net%2Fproduction%2F2020-3-24%2F67213919-44100-2-ee2cb2695df05.mp3" length="70026135" type="audio/mpeg"/>
			<itunes:summary>&lt;p&gt;Three YouTubers; Tim Scarfe - Machine Learning Dojo (https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA), Connor Shorten - Henry AI Labs (https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw) and Yannic Kilcher (https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew). We made a new YouTube channel called Machine Learning Street Talk. Every week we will talk about the latest and greatest in AI. Subscribe now!&lt;/p&gt;
&lt;p&gt;Special guests this week; Dr. Mathew Salvaris (https://www.linkedin.com/in/drmathewsalvaris/), Eric Craeymeersch (https://www.linkedin.com/in/ericcraeymeersch/), Dr. Keith Duggar (https://www.linkedin.com/in/dr-keith-duggar/), &amp;nbsp;Dmitri Soshnikov (https://www.linkedin.com/in/shwars/)&lt;/p&gt;
&lt;p&gt;We discuss the new concept of an open-ended, or &quot;AI-Generating&quot; algorithm. Open-endedness is a class of algorithms which generate problems and solutions to increasingly complex and diverse tasks. These algorithms create their own curriculum of learning. Complex tasks become tractable because they are now the final stepping stone in a lineage of progressions. In many respects, it&apos;s better to trust the machine to develop the learning curriculum, because the best curriculum might be counter-intuitive. These algorithms can generate a radiating tree of evolving challenges and solutions just like natural evolution. Evolution has produced an eternity of diversity and complexity and even produced human intelligence as a side-effect! Could AI-generating algorithms be the next big thing in machine learning?&lt;/p&gt;
&lt;p&gt;Wang, Rui, et al. &quot;Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions.&quot; arXiv preprint arXiv:2003.08536 (2020). https://arxiv.org/abs/2003.08536&lt;/p&gt;
&lt;p&gt;Wang, Rui, et al. &quot;Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions.&quot; arXiv preprint arXiv:1901.01753 (2019). https://arxiv.org/abs/1901.01753&lt;/p&gt;
&lt;p&gt;Watch Yannic’s video on POET: https://www.youtube.com/watch?v=8wkgDnNxiVs&lt;br&gt;
and on the extended POET: https://youtu.be/gbG1X8Xq-T8&lt;br&gt;
Watch Connor’s video https://www.youtube.com/watch?v=jxIkPxkN10U&lt;br&gt;
UberAI labs video: https://www.youtube.com/watch?v=RX0sKDRq400 &amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;#reinforcementlearning #machinelearning #uber #deeplearning #rl #timscarfe #connorshorten #yannickilcher&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
</itunes:summary>
			<itunes:explicit>false</itunes:explicit>
			<itunes:duration>01:12:56</itunes:duration>
			<itunes:image href="https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_episode/4981699/4981699-1587725523623-4c1df33b397ec.jpg"/>
			<itunes:season>1</itunes:season>
			<itunes:episode>1</itunes:episode>
			<itunes:episodeType>full</itunes:episodeType>
		</item>
	</channel>
</rss>